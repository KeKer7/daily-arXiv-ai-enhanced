<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 123]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: 本文提出了PolypSeg-GradCAM，一种结合U-Net架构和Grad-CAM的可解释深度学习框架，用于结直肠息肉分割，旨在提高AI辅助结肠镜检查的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是全球主要的癌症死因之一，胃肠息肉是其重要前兆。虽然深度学习在自动息肉分析方面显示出潜力，但其有限的可解释性阻碍了临床应用。

Method: 整合U-Net架构和梯度加权类激活映射(Grad-CAM)，在Kvasir-SEG数据集（1000张标注内镜图像）上进行训练和评估。

Result: 在测试集上达到平均IoU 0.9257，训练和验证集的Dice系数均高于0.96，Grad-CAM可视化证实预测基于临床相关区域。

Conclusion: PolypSeg-GradCAM通过结合高分割精度和可解释性，向可靠、可信的AI辅助结肠镜检查和改善早期结直肠癌预防迈出了重要一步。

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [2] [PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.18160)
*Akwasi Asare,Isaac Baffour Senkyire,Emmanuel Freeman,Simon Hilary Ayinedenaba Aluze-Ele,Kelvin Kwao*

Main category: cs.CV

TL;DR: PerceptronCARE是一个基于深度学习的远程眼科应用，用于通过视网膜图像自动检测糖尿病视网膜病变，准确率达到85.4%，适用于临床和远程医疗环境。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是成人视力丧失的主要原因，特别是在医疗资源匮乏地区，需要开发高效、可扩展的筛查解决方案。

Method: 使用多种卷积神经网络（ResNet-18、EfficientNet-B0、SqueezeNet）开发和评估系统，以在准确性和计算效率之间找到最佳平衡。

Result: 最终模型的疾病严重程度分类准确率为85.4%，具备实时筛查能力，集成了云可扩展性、安全数据管理和多用户框架。

Conclusion: 该研究展示了AI驱动的远程医疗解决方案在扩大糖尿病视网膜病变筛查可及性方面的潜力，特别是在偏远和资源受限环境中。

Abstract: Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.

</details>


### [3] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: 提出了Self Identity Mapping (SIM)正则化框架，通过逆向映射机制重构输入来减少前向传播中的信息损失，并开发了ρSIM版本降低计算复杂度。该方法是模型无关、任务无关的即插即用模块。


<details>
  <summary>Details</summary>
Motivation: 传统正则化技术通常依赖启发式方法，在不同设置下效果不稳定且不可靠，需要一种更有效的数据内在正则化方法。

Method: 使用逆向映射机制从变换后的输出重构输入，减少前向传播信息损失。ρSIM通过补丁级特征采样和基于投影的方法重构潜在特征来降低复杂度。

Result: 在图像分类、少样本提示学习和领域泛化等任务中相比基线方法均有持续改进，证明能有效增强表示学习能力，且与现有正则化方法正交。

Conclusion: SIM是有效的模型无关正则化器，能保持语义信息并提升密集到密集任务（如语义分割）和非视觉领域（音频分类）的性能。

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [4] [MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion](https://arxiv.org/abs/2509.18170)
*Zhanting Zhou,Jinbo Wang,Zeqin Wu,Fengli Zhang*

Main category: cs.CV

TL;DR: MAGIA是一种基于动量的自适应梯度反演攻击方法，能够在单轮平均梯度SAG机制下实现高保真度的多图像重建，特别是在大批量场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究单轮平均梯度SAG机制下的梯度反演问题，解决传统方法在大批量场景中因样本线索纠缠而失效的挑战。

Method: 提出MAGIA框架，包含组合重缩放和动量混合两个核心创新：1）闭式组合重缩放提供更紧的优化边界；2）基于动量的整批和子集损失混合确保重建鲁棒性。

Result: 大量实验表明MAGIA在大批量场景中显著优于先进方法，实现高保真多图像重建，且计算开销与标准求解器相当，无需辅助信息。

Conclusion: MAGIA在单轮平均梯度SAG机制下成功解决了梯度反演问题，为大批量场景下的隐私保护研究提供了重要突破。

Abstract: We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.

</details>


### [5] [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
*Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CV

TL;DR: 提出了Baseer，一个专门针对阿拉伯文档OCR的视觉语言模型，通过大规模数据集训练，在阿拉伯文档OCR领域取得了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯文档OCR由于草书字体、多样化字体、变音符号和从右到左的书写方向而具有挑战性，现有MLLM在阿拉伯语上的性能有限。

Method: 使用结合合成和真实世界文档的大规模数据集，采用仅解码器的微调策略来适配预训练的MLLM，同时保留通用视觉特征。

Result: Baseer显著优于现有的开源和商业解决方案，实现了0.25的WER，在阿拉伯文档OCR领域建立了新的最先进水平。

Conclusion: 特定领域适配通用MLLM具有明显优势，为形态丰富的语言（如阿拉伯语）的高精度OCR建立了强大基准。

Abstract: Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.

</details>


### [6] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 提出了一种新颖的深度学习框架，将稀疏InSAR时间序列数据转换为密集时空张量，首次实现直接应用计算机视觉架构进行地面变形预测。


<details>
  <summary>Details</summary>
Motivation: 监测地面位移对城市基础设施稳定性和地质灾害缓解至关重要，但基于稀疏InSAR时间序列数据预测未来变形仍面临重大挑战。

Method: 设计并实现了混合CNN-LSTM模型，专门用于从生成的数据张量中同时学习空间模式和时间依赖性，并与LightGBM和LASSO回归等机器学习基线进行对比。

Result: 结果表明，所提出的架构提供了显著更准确和空间一致的预测，为该任务建立了新的性能基准。可解释性分析显示基线模型往往依赖简单的持续性模式。

Conclusion: 研究结果证实了时空深度学习在高分辨率变形预测中的有效性和潜力，强调了集成时空方法捕捉地面变形复杂动态的必要性。

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [7] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: Scrapbook框架是一种生成大规模数据集的新方法，用于评估AI模型对基本概念的理解能力，包括物体识别、位置关系和属性识别等。实验发现当前模型在位置理解和约束性问题处理方面存在挑战。


<details>
  <summary>Details</summary>
Motivation: 为了系统性地评估AI模型对基本概念的理解能力，需要生成包含大量问题和语言变体的数据集，以验证模型在应对复杂任务前是否真正掌握了这些基础元素。

Method: 开发Scrapbook框架，生成包含物体识别、绝对和相对位置、属性识别等基本概念的大规模数据集，通过大量问题和语言变体来测试模型的理解能力。

Result: 实验显示当代模型在物体识别和枚举方面表现良好，但在理解位置信息和处理约束性问题时存在困难。MobileVLM-V2模型出现显著答案不一致和错误答案，其他模型则表现出肯定答案偏好，在几何形状和位置信息问题上表现不佳。

Conclusion: Scrapbook框架为生成多样化数据集提供了有效工具，可用于系统评估和提升AI模型的性能，特别是在基础概念理解方面需要进一步改进。

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [8] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 本文通过实证分析量化了视觉-语言-视觉管道中的信息损失，发现99.3%的样本存在显著感知退化，91.5%存在结构信息损失。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI系统在创意工作流中的集成日益增加，理解视觉-语言-视觉管道中的信息损失对于评估系统局限性变得重要。然而，视觉内容通过文本中介时发生的退化尚未得到充分量化。

Method: 生成了150对图像通过描述-生成管道，并应用现有指标（LPIPS、SSIM和颜色距离）来测量感知、结构和色彩维度上的信息保存。

Result: 评估显示99.3%的样本表现出显著的感知退化，91.5%的样本表现出显著的结构信息损失。

Conclusion: 描述-生成瓶颈代表了当代多模态系统中可测量且一致的局限性。

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [9] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: AI驱动的工作流，通过高分辨率卫星影像自动推断屋顶属性，用于小岛屿发展中国家（SIDS）的建筑结构信息收集，以支持灾害风险评估和城市韧性规划。


<details>
  <summary>Details</summary>
Motivation: 许多气候脆弱地区（如加勒比海的小岛屿发展中国家）缺乏详细的建筑结构信息，而这些信息对于评估飓风、洪水和山体滑坡等灾害的潜在损害至关重要。

Method: 比较了地理空间基础模型结合浅层分类器与微调的深度学习模型在屋顶分类中的效果，并评估了引入邻近SIDS额外训练数据对模型性能的影响。

Result: 最佳模型在屋顶坡度和屋顶材料分类上分别达到了0.88和0.83的F1分数。

Conclusion: 结合本地能力建设，该工作旨在为SIDS提供利用AI和地球观测数据的新能力，以实现更高效、基于证据的城市治理。

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [10] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: 本文提出了VLA-LPAF轻量级模块来解决VLA模型在多视角视觉观察中的视角异质性问题，通过在潜在空间融合多视角数据，显著提升了模型的任务成功率。


<details>
  <summary>Details</summary>
Motivation: VLA模型在处理来自不同视角和数量的视觉观察时存在视角异质性问题，这限制了模型的泛化能力。不同环境的全局和局部摄像头捕获的视觉特征差异显著，需要一种方法来有效融合多视角信息。

Method: 提出轻量级模块VLA-LPAF，使用单视角图像进行微调，在潜在空间融合多视角观察数据。基于RoboFlamingo模型实例化构建RoboFlamingo-LPAF框架。

Result: 在CALVIN数据集上平均提升约8%的任务成功率，在LIBERO数据集上提升15%，在定制仿真基准上提升30%。真实世界任务验证了模型具备视角自适应特性。

Conclusion: VLA-LPAF能够有效且高效地弥合视角不一致带来的差距，显著提升VLA模型在多视角环境中的适应性和性能。

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [11] [URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation](https://arxiv.org/abs/2509.18184)
*Yifeng Cheng,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: 本文提出了一种基于事件相机的立体深度估计方法URNet，通过局部-全局精炼模块和KL散度不确定性建模，在DSEC数据集上超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时间分辨率、高动态范围和低延迟的优势，但现有的基于事件相机的立体深度估计方法在细节捕捉和预测可靠性方面仍有改进空间。

Method: 提出URNet网络，包含局部-全局精炼模块来捕捉细粒度局部细节和长距离全局上下文，并引入基于KL散度的不确定性建模方法来增强预测可靠性。

Result: 在DSEC数据集上的大量实验表明，URNet在定性和定量评估中都一致优于现有的最先进方法。

Conclusion: URNet通过有效的精炼模块和不确定性建模，显著提升了基于事件相机的立体深度估计性能。

Abstract: Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.

</details>


### [12] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: Visionerves是一个用于从多梯度DWI和形态学MRI数据中识别周围神经系统的新型混合AI框架，通过模糊空间关系编码解剖知识，无需手动选择ROI，在子宫内膜异位症患者的腰骶丛神经识别中表现优于标准纤维束成像方法。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症常导致慢性盆腔疼痛和可能的神经受累，但周围神经的成像仍然是一个挑战。需要开发能够自动识别周围神经系统的非侵入性诊断方法。

Method: 提出两阶段混合AI框架：(A)使用深度学习模型自动分割解剖结构，(B)通过符号空间推理进行纤维束成像和神经识别。该方法利用模糊空间关系编码解剖知识，无需手动选择感兴趣区域。

Result: 在10名子宫内膜异位症（确诊或疑似）女性的腰骶丛神经应用中，Visionerves相比标准纤维束成像方法有显著改进，Dice评分提高达25%，空间误差减少到小于5毫米。

Conclusion: 这种自动且可重复的方法能够进行详细的神经分析，为子宫内膜异位症相关神经病变以及其他涉及神经的疾病的非侵入性诊断开辟了新途径。

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [13] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: V-SenseDrive是首个在巴基斯坦驾驶环境中收集的隐私保护多模态驾驶员行为数据集，结合智能手机传感器数据和同步的路面视频，填补了发展中国家驾驶行为数据的空白。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要来自发达国家，缺乏新兴经济体（如巴基斯坦）的驾驶行为多样性，且驾驶员面部记录侵犯隐私。需要可靠的不安全驾驶行为检测来改善道路安全。

Method: 使用定制Android应用收集高频率加速度计、陀螺仪和GPS数据，结合同步的路面视频，在多种道路类型上记录三种目标驾驶行为（正常、激进、危险）。

Result: 创建了包含原始、处理和语义层的结构化数据集，支持多模态分析，为驾驶员行为分类、交通安全分析和ADAS开发提供基础。

Conclusion: V-SenseDrive填补了全球驾驶员行为数据集的关键空白，为情境感知智能交通解决方案奠定了基础。

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [14] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: Qianfan-VL是一个参数规模从3B到70B的多模态大语言模型系列，通过创新的领域增强技术实现了最先进的性能表现。


<details>
  <summary>Details</summary>
Motivation: 开发能够在保持强大通用性能的同时，通过领域增强技术提升特定领域能力的多模态模型，以满足多样化企业部署需求。

Method: 采用多阶段渐进式训练和高精度数据合成流水线，在百度昆仑P800芯片上进行完全训练，实现了超过90%的扩展效率。

Result: 在通用基准测试中与领先开源模型相当，在CCBench、SEEDBench IMG、ScienceQA和MMStar等基准上达到最先进性能，在OCR和文档理解领域表现突出（OCRBench 873分，DocVQA 94.75%），数学推理（MathVista 78.6%）和逻辑推理任务表现优异。

Conclusion: 该工作为开发适合多样化企业部署场景的领域增强多模态模型建立了有效的方法论，验证了大规模AI基础设施训练SOTA级多模态模型的能力。

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [15] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: HazeFlow是一个基于ODE的去雾框架，通过将大气散射模型重新表述为常微分方程，学习从有雾图像到清晰图像的最优ODE轨迹，只需单次推理即可实现高效去雾。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法缺乏配对的真实世界训练数据，导致域差距问题，而传统基于大气散射模型的物理方法难以处理真实世界的复杂雾霾模式。

Method: 提出HazeFlow框架，将大气散射模型重新表述为ODE，借鉴Rectified Flow思想学习最优ODE轨迹；同时引入基于马尔可夫链布朗运动的非均匀雾霾生成方法来解决训练数据稀缺问题。

Result: 在多个真实世界去雾基准数据集上，HazeFlow实现了最先进的性能表现。

Conclusion: HazeFlow通过ODE框架和创新的雾霾生成方法，有效解决了真实世界去雾的泛化问题，为物理基础学习提供了新的思路。

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [16] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: 本文提出了一种压缩版EcoWeedNet模型，通过结构化通道剪枝、量化感知训练和TensorRT加速，在Jetson Orin Nano上实现了高效的杂草检测。


<details>
  <summary>Details</summary>
Motivation: 农业边缘设备资源有限，需要轻量化的深度学习模型进行杂草检测，但现有模型在复杂架构上部署困难。

Method: 采用结构化通道剪枝处理残差连接、注意力机制、拼接和CSP块等复杂架构，结合量化感知训练和TensorRT加速。

Result: 模型大小减少68.5%，计算量减少3.2 GFLOPs，推理速度达到184 FPS（FP16），比基线快28.7%。在CottonWeedDet12数据集上，39.5%剪枝率的EcoWeedNet优于YOLO11n和YOLO12n，达到83.7%精确率、77.5%召回率和85.9% mAP50。

Conclusion: 该方法证明了在保持高精度的同时实现模型高效压缩的可行性，适用于精准农业应用。

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [17] [Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction](https://arxiv.org/abs/2509.18284)
*Yi Gu,Kuniaki Saito,Jiaxin Ma*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态学习框架，通过增强模态dropout和对比学习来解决医学诊断中的模态不平衡和缺失问题，在临床数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 医学诊断越来越多地使用多模态数据，但现有模型难以有效融合异构信息并在模态缺失时保持鲁棒性。需要解决模态不平衡和缺失等现实限制。

Method: 引入可学习的模态token来改进缺失感知的模态融合，并在传统单模态对比目标基础上增加融合多模态表示。结合增强模态dropout和对比学习。

Result: 在大规模临床数据集上的实验表明，该方法在疾病检测和预测任务中达到最先进性能，特别是在只有单一模态可用的实际场景中表现优异。

Conclusion: 该方法具有高效性、可扩展性和通用性，为多模态学习提供了可扩展、低成本的解决方案，在真实临床应用中具有重要潜力。

Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.

</details>


### [18] [Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model](https://arxiv.org/abs/2509.18308)
*Yixin Zhang,Ryan Chamberlain,Lawrance Ngo,Kevin Kramer,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 本研究系统评估了9种分割架构在肺栓塞分割任务中的表现，发现3D U-Net with ResNet encoder是最有效的架构，CNN模型优于ViT模型，分类预训练反而会降低分割性能。


<details>
  <summary>Details</summary>
Motivation: 肺栓塞分割在临床诊断中具有重要意义，但目前缺乏对不同分割架构在该任务上的系统性评估。本研究旨在通过统一的测试框架，比较不同架构在肺栓塞分割中的表现。

Method: 使用490个CTPA扫描构建内部数据集，系统评估9种分割架构（包括CNN和ViT家族），采用预训练或随机权重初始化，在统一测试框架下进行性能审计。

Result: 最佳模型达到平均Dice分数0.7131，在60个测试扫描中检测到181个栓子，有49个假阳性和28个假阴性。3D模型更适合肺栓塞分割任务，CNN模型性能优于ViT模型。

Conclusion: 3D U-Net with ResNet encoder是肺栓塞分割的有效架构，远端栓子分割仍具挑战性，不同模型架构在相同数据上表现出高度一致的分割性能模式。

Abstract: In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.

</details>


### [19] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: 提出一种新颖的图神经网络，将时间动态与静态手形配置分离，用于手语手形识别，在37个手形类别上达到46%的准确率。


<details>
  <summary>Details</summary>
Motivation: 手形在手语中具有基本音系学作用，但计算方法很少显式建模手形，限制了识别准确性和语言分析。

Method: 结合解剖学启发的图结构和对比学习，解决手形识别中的关键挑战，包括细微的类间差异和时间变化。

Result: 在签名序列中建立了首个结构化手形识别基准，准确率达到46%（基线方法为25%）。

Conclusion: 该方法显著提升了手形识别性能，为手语计算分析提供了新的技术途径。

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [20] [Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound](https://arxiv.org/abs/2509.18326)
*Chun Kit Wong,Anders N. Christensen,Cosmin I. Bercea,Julia A. Schnabel,Martin G. Tolsgaard,Aasa Feragen*

Main category: cs.CV

TL;DR: 本文研究了分类任务本身对胎儿超声图像中OOD检测性能的影响，发现OOD检测性能随任务显著变化，且最佳任务取决于ID-OOD标准（图像特征偏移或解剖特征偏移）。


<details>
  <summary>Details</summary>
Motivation: 可靠的外分布检测对于在胎儿超声图像中安全部署深度学习模型至关重要，现有研究主要关注不确定性量化方法，而本文研究分类任务本身的影响。

Method: 通过8种不确定性量化方法在4个分类任务上的实验，分析不同任务对OOD检测性能的影响。

Result: OOD检测性能随分类任务显著变化，最佳任务取决于ID-OOD标准类型；同时发现优越的OOD检测并不保证最优的弃权预测性能。

Conclusion: 在医学图像分析中，需要根据具体下游应用来对齐任务选择和不确定性策略。

Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.

</details>


### [21] [OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata](https://arxiv.org/abs/2509.18350)
*Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 本文提出了OrthoLoC数据集，这是第一个大规模无人机图像与正射地理数据配对的数据集，用于解决航空视觉定位问题，并提出了AdHoP改进技术提升特征匹配性能。


<details>
  <summary>Details</summary>
Motivation: 在无互联网连接或GNSS/GPS支持的限制资源环境下，现有视觉定位系统依赖大型图像数据库或重型3D模型不实用。正射地理数据轻量且易获取，但此前未被充分探索作为替代方案。

Method: 创建包含16,425张德国和美国无人机图像的多模态数据集，解耦图像检索和特征匹配以进行公平评估。提出AdHoP改进技术，可与任何特征匹配器集成。

Result: 通过全面评估分析了域偏移、数据分辨率和共视性对定位精度的影响。AdHoP技术将匹配性能提升高达95%，平移误差降低高达63%。

Conclusion: OrthoLoC数据集为航空视觉定位提供了新的基准，证明了正射地理数据的实用性，AdHoP技术显著提升了定位性能，为资源受限环境下的高精度定位提供了有效解决方案。

Abstract: Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.

</details>


### [22] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: 提出SSDnet方法，无需训练数据即可实现单张图像的异常检测，利用卷积神经网络的归纳偏置和自重构机制来定位图像中的异常区域。


<details>
  <summary>Details</summary>
Motivation: 解决现实场景中缺乏训练数据的零样本异常检测问题，仅使用测试图像本身进行异常定位。

Method: 基于Deep Image Prior思想，设计patch-based训练框架，通过掩码、patch打乱和噪声注入避免恒等映射，使用感知损失捕捉结构信息。

Result: 在MVTec-AD数据集上达到0.99 AUROC和0.60 AUPRC，在fabric数据集上达到0.98 AUROC和0.67 AUPRC，优于现有方法。

Conclusion: SSDnet无需外部训练数据、标签或参考样本，在噪声和像素缺失情况下仍保持鲁棒性，为零样本异常检测提供了有效解决方案。

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [23] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: 该论文提出了一种针对低资源孟加拉语的计算感知图像描述生成管道，通过三重损失目标（PAL+InfoNCE+OT）解决视觉-语言模型在低资源语言中的语义对齐问题。


<details>
  <summary>Details</summary>
Motivation: 解决视觉-语言模型在低资源语言（如孟加拉语）中存在的挑战：配对数据稀缺、翻译枢轴破坏对齐、以及英语中心预训练忽略目标语言语义，导致模型生成关于错误对象的流利文本。

Method: 使用LaBSE验证的英-孟配对数据和11万双语提示合成图像训练。采用冻结的MaxViT提取视觉特征，孟加拉语原生mBART-50解码，轻量级桥接模块连接模态。核心创新是三重损失目标：PAL损失对齐真实和合成补丁描述符，InfoNCE强制全局真实-合成分离，基于Sinkhorn的最优传输确保平衡的细粒度补丁对应。

Result: 在Flickr30k-1k（BLEU-4 12.29, METEOR 27.98, BERTScore-F1 71.20）和MSCOCO-1k（BLEU-4 12.00, METEOR 28.14, BERTScore-F1 75.40）上表现优异，超越强CE基线，并将真实-合成质心差距缩小41%。

Conclusion: PAL+InfoNCE+OT的协同作用显著改善了低资源语言的语义对齐，减少了虚假匹配，为低资源语言的视觉-语言建模提供了有效解决方案。

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [24] [TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning](https://arxiv.org/abs/2509.18372)
*Reeshad Khan,John Gauch*

Main category: cs.CV

TL;DR: TinyBEV是一个轻量化的纯摄像头BEV框架，通过知识蒸馏将大型规划导向教师模型(UniAD)的能力迁移到紧凑的实时学生模型中，参数减少78%，支持完整的自动驾驶堆栈功能。


<details>
  <summary>Details</summary>
Motivation: 解决现有高效摄像头基线模型功能不完整的问题，将大规模多模态感知规划模型的能力迁移到资源受限的实时部署环境中。

Method: 采用模型无关的多阶段蒸馏策略，结合特征级、输出级和自适应区域感知监督，将高容量多模态知识迁移到轻量级BEV表示中。

Result: 在nuScenes数据集上，检测mAP达到39.0，运动预测minADE为1.08，碰撞率0.32，运行速度提升5倍(11 FPS)，仅需摄像头输入。

Conclusion: TinyBEV证明了在资源受限环境下仍可保留完整的驾驶智能，弥合了大规模多模态模型与部署就绪实时系统之间的差距。

Abstract: We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.

</details>


### [25] [BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking](https://arxiv.org/abs/2509.18387)
*Thomas Gossard,Filip Radovic,Andreas Ziegler,Andrea Zell*

Main category: cs.CV

TL;DR: 本文提出了一种新的运动模糊球体标注策略，将球标注在模糊条纹的中心而非前缘，并显式标注模糊属性，从而提升检测性能并实现轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 现有标注方法将球标记在模糊条纹的前缘，存在不对称性且忽略了与速度相关的运动线索，这限制了检测系统的性能。

Method: 引入新的标注策略，将球置于模糊条纹中心并标注模糊属性；提出BlurBall模型，通过多帧输入的注意力机制（如Squeeze-and-Excitation）联合估计球位置和运动模糊属性。

Result: 新标注方法在各种模型上一致提升检测性能；BlurBall模型在球检测任务中达到最先进水平，同时实现更可靠的轨迹预测。

Conclusion: 利用运动模糊信息不仅能提高检测精度，还能实现更可靠的轨迹预测，有益于实时体育分析。

Abstract: Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.

</details>


### [26] [MVP: Motion Vector Propagation for Zero-Shot Video Object Detection](https://arxiv.org/abs/2509.18388)
*Binhua Huang,Ni Wang,Wendong Yao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 提出了一种无需训练的视频目标检测方法MVP，通过关键帧检测和压缩域运动向量传播来减少计算开销，同时保持开放词汇检测能力。


<details>
  <summary>Details</summary>
Motivation: 解决在大规模开放词汇检测器中逐帧运行的高计算成本问题，旨在通过关键帧检测和运动传播来降低检测器调用频率。

Method: 使用固定间隔的关键帧调用OWLv2检测器，通过压缩域运动向量（MV）将检测结果传播到中间帧，采用3x3网格聚合运动向量进行平移和均匀尺度更新。

Result: 在ILSVRC2015-VID数据集上达到mAP@0.5=0.609，在宽松IoU阈值下接近逐帧检测性能，优于基于跟踪器的传播方法。

Conclusion: 压缩域传播是一种实用的方法，可以在保持强大零样本覆盖的同时显著减少检测器调用次数。

Abstract: Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.

</details>


### [27] [Improving the color accuracy of lighting estimation models](https://arxiv.org/abs/2509.18390)
*Zitian Zhang,Joshua Urban Davis,Jeanne Phuong Anh Vu,Jiangtao Kuang,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 本文研究了高动态范围（HDR）光照估计方法在颜色鲁棒性方面的表现，提出使用预训练的白平衡网络预处理输入图像来提升现有模型的颜色准确性，无需重新训练光照估计模型。


<details>
  <summary>Details</summary>
Motivation: 现有的单图像HDR光照估计方法在颜色准确性方面存在不足，而颜色鲁棒性对于增强现实应用中虚拟物体的真实渲染至关重要。大多数评估方法将颜色与其他光照属性混为一谈，需要专门研究颜色这一关键因素。

Method: 通过使用包含多样化光照颜色的新型HDR数据集，系统评估了多种适应策略。主要方法是使用预训练的白平衡网络对输入图像进行预处理，然后将其输入到现有的光照估计模型中。

Result: 实验结果表明，使用预训练白平衡网络预处理输入图像的方法在所有测试场景中都优于其他策略，显著提升了颜色鲁棒性。该方法在三种最先进的光照估计方法上都验证了有效性。

Conclusion: 简单的预处理技术（如白平衡）可以有效提升现有光照估计模型的颜色准确性，且无需重新训练模型，为增强现实应用提供了实用的改进方案。

Abstract: Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.

</details>


### [28] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: 提出一种基于视觉语言模型和多模态大语言模型的免训练支票字段检测框架，实现零样本检测支票关键字段，解决传统方法依赖大量标注数据的问题。


<details>
  <summary>Details</summary>
Motivation: 支票作为金融系统基础工具面临欺诈风险，传统检测方法依赖大量标注数据，但这类数据因专有性和隐私问题难以获取。

Method: 结合视觉语言模型(VLM)和多模态大语言模型(MLLM)，开发免训练框架进行支票字段的零样本检测，无需预先训练即可识别签名、MICR线、金额等关键字段。

Result: 在包含110张不同格式支票的手工标注数据集上验证，模型表现出强大的性能和泛化能力。

Conclusion: 该框架不仅降低实际金融场景部署门槛，还可作为生成高质量标注数据的引导机制，为开发专门化的实时目标检测模型奠定基础。

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [29] [Losing the Plot: How VLM responses degrade on imperfect charts](https://arxiv.org/abs/2509.18425)
*Philip Wootaek Shin,Jack Sampson,Vijaykrishnan Narayanan,Andres Marquez,Mahantesh Halappanavar*

Main category: cs.CV

TL;DR: 该论文评估了当前主流视觉语言模型在图表理解任务中的表现，发现它们在面对图像失真或遮挡时性能显著下降，并出现幻觉问题。作者提出了CHART NOISe数据集，结合了图像损坏、遮挡和逆向不一致性测试，为提升图表理解的鲁棒性提供了基准。


<details>
  <summary>Details</summary>
Motivation: 现有的图表理解基准假设图表图像干净且问题基于事实，而现实世界中的图表常包含失真和遮挡，需要更复杂的推理能力。当前模型在这些挑战性场景下表现不佳，存在过度自信和幻觉问题。

Method: 作者评估了ChatGPT 4o、Claude Sonnet 4和Gemini 2.5 Pro等主流VLMs，引入了CHART NOISe数据集，包含图表损坏、遮挡和基于韩国CSAT英语部分的多选题。关键创新是逆向不一致性测试，即让模型确认和否认同一陈述时出现矛盾。

Result: 研究发现模型在图像损坏或遮挡情况下性能急剧下降，幻觉问题（如数值捏造、趋势误读、实体混淆）更加频繁。模型在退化设置下仍过度自信，生成看似合理但无依据的解释。

Conclusion: 该研究揭示了图表推理中的系统性漏洞，建立了首个结合损坏、遮挡和逆向不一致性的测试基准，并提出了质量过滤和遮挡检测等基线缓解策略，为提升图表理解的鲁棒性和可靠性奠定了基础。

Abstract: Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.

</details>


### [30] [CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction](https://arxiv.org/abs/2509.18427)
*Xinyang Wu,Muheng Li,Xia Li,Orso Pusterla,Sairos Safai,Philippe C. Cattin,Antony J. Lomax,Ye Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于神经表示的4D-MRI重建方法，通过两个协同网络（空间解剖网络和时序运动网络）替代传统的离散排序方法，实现了对呼吸运动的连续建模，显著提高了处理效率和重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统4D-MRI重建方法（如相位分箱或独立模板扫描）难以捕捉时间变异性、工作流程复杂且计算负担重，需要一种更高效准确的方法来重建呼吸运动。

Method: 采用神经表示框架，将呼吸运动建模为由1D替代信号引导的平滑连续变形。使用空间解剖网络（SAN）编码连续3D解剖表示，时序运动网络（TMN）基于Transformer生成的呼吸信号产生时序一致的变形场。

Result: 在19名志愿者的自由呼吸数据集上评估，该方法能准确捕捉规则和不规则呼吸模式，保持血管和支气管连续性，处理时间从传统方法的约5小时减少到15分钟训练时间，每帧3D体积推断时间小于1秒。

Conclusion: 该方法在4D放射治疗规划和实时自适应治疗中具有强大应用潜力，实现了任意呼吸状态下3D图像的准确重建，性能优于传统方法。

Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.

</details>


### [31] [An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects](https://arxiv.org/abs/2509.18451)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony Maida*

Main category: cs.CV

TL;DR: 本文评估了五种基于卡尔曼滤波的跟踪方法在快速移动微小物体（如壁球）跟踪中的性能，发现DeepOCSORT在跟踪精度上表现最佳，但所有方法都存在显著的跟踪漂移问题。


<details>
  <summary>Details</summary>
Motivation: 快速移动微小物体的精确跟踪是计算机视觉中的挑战性问题，特别是在体育机器人应用中。现有卡尔曼滤波跟踪方法在处理不规则弹跳行为的快速移动物体时性能显著下降。

Method: 使用包含10,000个标注壁球帧的自定义数据集，评估OCSORT、DeepOCSORT、ByteTrack、BoTSORT和StrongSORT五种跟踪方法的性能，重点关注推理速度和每图像更新频率对跟踪精度的影响。

Result: DeepOCSORT获得最低跟踪误差（平均ADE 31.15像素），ByteTrack处理速度最快（平均推理时间26.6ms）。所有跟踪器都表现出显著的跟踪漂移，空间误差范围3-11cm。

Conclusion: 当前跟踪方法存在根本性局限，误差率比标准物体跟踪基准高3-4倍，需要开发专门针对快速移动微小物体跟踪的专业方法。

Abstract: Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.

</details>


### [32] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCrop是一种基于运动感知的自适应裁剪模块，用于压缩域视频动作识别，利用H.264视频中的运动向量定位运动密集区域，无需训练即可提升识别精度或降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统视频动作识别方法计算成本高，特别是在压缩域中。MoCrop旨在通过利用视频压缩中已有的运动向量信息，实现高效的动作识别，减少计算开销。

Method: MoCrop采用轻量级流程，包括去噪与合并(DM)、蒙特卡洛采样(MCS)和自适应裁剪(AC)，通过运动密度子矩阵搜索生成鲁棒的裁剪区域。该方法无需训练，不增加参数，可灵活集成到不同骨干网络中。

Result: 在UCF101数据集上，MoCrop在相同FLOPs下提升Top-1准确率3.5%，或在减少26.5% FLOPs的同时提升2.4%准确率。应用于CoViAR时，在原始计算成本下达到89.2%准确率，或在减少计算量（从11.6降至8.5 GFLOPs）时保持88.5%准确率。

Conclusion: MoCrop在多种骨干网络（ResNet-50、MobileNet-V3、EfficientNet-B1、Swin-B）上均表现一致提升，展示了强泛化能力和实际部署价值，特别适合压缩域实时应用。

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.

</details>


### [33] [Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems](https://arxiv.org/abs/2509.18481)
*Xinyu Wang,Zikun Zhou,Yingjian Li,Xin An,Hongpeng Wang*

Main category: cs.CV

TL;DR: 提出了一种基于码本的自适应特征压缩框架CAFC-SE，通过向量量化将视觉特征映射到离散索引，在低比特率条件下保持更多信息性视觉模式，提升边缘-云系统的分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低比特率条件下性能较差，要么保留过多冗余细节，要么学习过度集中的符号分布。需要一种能在低比特率下保持分析性能的有效特征压缩方法。

Method: 使用向量量化(VQ)将连续视觉特征映射到码本中的离散索引，选择性传输到云端。通过将特征向量投影到最近的视觉基元来保留信息性视觉模式。

Result: 大量实验证明该方法在码率和准确率方面具有优越性，特别是在低比特率条件下表现更好。

Conclusion: CAFC-SE框架通过码本自适应特征压缩和语义增强，有效解决了低比特率条件下的特征压缩问题，提升了边缘-云系统的分析性能。

Abstract: Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.

</details>


### [34] [MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation](https://arxiv.org/abs/2509.18493)
*Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: MK-UNet是一种超轻量级多核U形CNN，专为医学图像分割设计，在极低计算成本下实现优于现有方法的性能


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中现有方法计算复杂度高、参数量大，难以在资源受限环境（如即时诊断设备）中部署的问题

Method: 设计了多核深度卷积块（MKDC）处理多分辨率空间关系，结合通道、空间和分组门控注意力机制突出图像显著特征

Result: 仅需0.316M参数和0.314G FLOPs，在六个医学图像基准测试中DICE分数优于TransUNet、UNeXt等SOTA方法，参数减少333倍，性能提升最高6.7%

Conclusion: MK-UNet在极低计算成本下实现了卓越的分割性能，为资源受限环境中的实时高保真医学诊断提供了无与伦比的解决方案

Abstract: In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.

</details>


### [35] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: BridgeSplat是一种新颖的可变形手术导航方法，通过将术中3D重建与术前CT数据耦合，在手术视频和体积患者数据之间建立桥梁。该方法将3D高斯分布与CT网格绑定，通过光度监督联合优化高斯参数和网格变形。


<details>
  <summary>Details</summary>
Motivation: 解决手术视频与体积患者数据之间的差距问题，实现术中实时导航和术前CT数据的变形更新，提高手术导航的准确性和实用性。

Method: 将3D高斯分布绑定到CT网格上，使每个高斯分布相对于其父网格三角形进行参数化，通过光度监督联合优化高斯参数和网格变形，确保高斯分布与网格对齐，并将变形传播回CT数据。

Result: 在猪内脏手术和人类肝脏合成数据上验证了方法的有效性，能够在单目RGB数据上实现对术前CT的合理变形。

Conclusion: BridgeSplat成功地将术中3D重建与术前CT数据相结合，为可变形手术导航提供了一种有效的解决方案，代码和数据已公开。

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [36] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: 本文提出了一种名为DGLE的扩散引导标签增强框架，用于解决源域数据不可访问情况下的无源域自适应语义分割问题。该方法从少量高质量伪标签开始，通过扩散模型传播生成完整的高质量伪标签集。


<details>
  <summary>Details</summary>
Motivation: 当前无源域自适应研究在语义分割中面临伪标签质量差的问题，传统方法难以同时优化包含大量噪声的完整伪标签集，限制了自训练方法的性能。

Method: 首先通过置信度过滤和超分辨率增强获得少量高质量伪标签作为种子，然后利用扩散模型对不规则分布的种子伪标签进行传播，利用其强大的去噪能力和复杂分布建模能力生成完整的高质量伪标签。

Result: 该方法有效避免了直接优化完整伪标签集的困难，显著提高了伪标签质量，从而提升了模型在目标域的性能。

Conclusion: DGLE框架通过扩散模型引导的标签增强策略，为无源域自适应语义分割提供了一种有效的伪标签优化解决方案。

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.

</details>


### [37] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: 该论文提出在双曲空间中实现从粗到精的少样本类增量学习（C2FSCIL），通过双曲对比学习和最大熵分布增强来提升分层数据的表示能力。


<details>
  <summary>Details</summary>
Motivation: 双曲空间相比欧几里得空间对分层数据具有更好的表示能力，特别是在C2FSCIL任务中，可以更好地解释"从粗到精"的学习范式。

Method: 使用庞加莱球模型将特征提取器嵌入双曲空间，引入双曲对比损失和双曲全连接层，并利用双曲空间中的最大熵分布进行特征增强以缓解少样本过拟合。

Result: 在C2FSCIL基准测试中，该方法有效提高了粗类和细类分类的准确率。

Conclusion: 双曲空间嵌入为C2FSCIL任务提供了有效的解决方案，通过双曲对比学习和特征增强技术显著提升了模型性能。

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [38] [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)
*Zixin Zhu,Haoxiang Li,Xuelu Feng,He Wu,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出了一种几何感知的两阶段框架，将物体移除分解为几何移除和外观渲染，以消除目标物体及其因果视觉伪影（如阴影和反射）。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像外观的方法要么严格遵循掩码对齐训练而无法移除未明确掩码的因果效应，要么采用松散掩码对齐策略缺乏可控性并可能过度擦除其他物体。这些限制源于忽略了物体几何存在与其视觉效应之间的因果关系。

Method: 两阶段框架：第一阶段从几何（如深度）中直接移除物体，使用严格掩码对齐监督实现强几何约束的结构感知编辑；第二阶段基于更新后的几何渲染逼真RGB图像，因果视觉效应作为修改3D几何的结果被隐式考虑。引入基于正负样本对的偏好驱动目标来指导几何移除阶段的学习。

Result: 在两个流行基准测试上实现了最先进的性能，能够有效移除物体及其相关伪影。

Conclusion: 该方法通过几何感知的分解方法成功解决了物体移除中的因果视觉伪影问题，提供了更好的可控性和编辑质量。

Abstract: Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.

</details>


### [39] [SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models](https://arxiv.org/abs/2509.18546)
*Yujia Liu,Dingquan Li,Tiejun Huang*

Main category: cs.CV

TL;DR: 本文提出了SEGA方法，通过高斯平滑和梯度集成来提高无参考图像质量评估模型的对抗攻击迁移性


<details>
  <summary>Details</summary>
Motivation: 现有的NR-IQA模型在白盒攻击下表现良好，但在更现实的黑盒场景中迁移性较差，需要解决低迁移性问题

Method: SEGA方法通过应用高斯平滑到源模型并集成其平滑梯度来近似目标模型的梯度，并使用专门设计的扰动过滤掩码确保对抗扰动的不可感知性

Result: 在CLIVE数据集上的实验结果表明SEGA具有优越的迁移性，验证了其在基于迁移的黑盒攻击中的有效性

Conclusion: SEGA是首个解决NR-IQA模型攻击低迁移性挑战的方法，成功实现了对未知目标模型的有效黑盒攻击

Abstract: No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.

</details>


### [40] [HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles](https://arxiv.org/abs/2509.18550)
*Mohammad Junayed Hasan,Nabeel Mohammed,Shafin Rahman,Philipp Koehn*

Main category: cs.CV

TL;DR: HadaSmileNet是一个新颖的特征融合框架，通过参数自由的Hadamard乘法交互直接整合基于transformer的表征和生理学基础的D-Marker特征，在微笑表情识别任务中实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有多任务学习框架在微笑表情识别中的计算效率低下问题，这些方法由于辅助任务监督和复杂的损失平衡需求而效率不高。

Method: 提出HadaSmileNet框架，系统评估了15种融合策略，发现Hadamard乘法融合能够实现直接特征交互同时保持计算效率。

Result: 在四个基准数据集上建立了深度学习方法的新的最先进结果：UvA-NEMO (88.7%, +0.8)、MMI (99.7%)、SPOS (98.5%, +0.7)和BBC (100%, +5.0)，参数减少26%且训练简化。

Conclusion: 该框架的效率和有效性使其特别适合需要实时情感计算能力的多媒体数据挖掘应用的实际部署。

Abstract: The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.

</details>


### [41] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机和3D高斯泼溅的联合动态人体与静态场景重建框架，通过事件流指导解决快速运动下的运动模糊问题


<details>
  <summary>Details</summary>
Motivation: 从单目视频重建动态人体和静态场景存在困难，特别是在快速运动时RGB帧会出现运动模糊。事件相机具有微秒级时间分辨率的优势，更适合动态人体重建

Method: 使用统一的3D高斯集合，其中包含可学习的语义属性；只有被分类为人体的高斯会进行变形动画，场景高斯保持静态。提出事件引导损失函数，匹配连续渲染之间的模拟亮度变化与事件流

Result: 在两个基准数据集ZJU-MoCap-Blur和MMHPSD-Blur上实现了最先进的人体-场景重建效果，在PSNR/SSIM指标上显著优于强基线，LPIPS降低，特别是在高速运动对象上表现突出

Conclusion: 该方法无需外部人体掩码，简化了单独高斯集合的管理，有效解决了快速运动下的重建模糊问题

Abstract: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [42] [Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought](https://arxiv.org/abs/2509.18571)
*Yuhan Wang,Cheng Liu,Zihan Zhao,Weichao Wu*

Main category: cs.CV

TL;DR: Live-E2T是一个实时威胁监控框架，通过结构化语义元组、在线事件去重和基于LLM的推理机制，同时实现高性能威胁检测和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时满足实时性和决策可解释性的要求，需要一种统一框架来解决这一矛盾。

Method: 1. 将视频帧解构为结构化的人-物-交互-地点语义元组；2. 提出高效的在线事件去重和更新机制；3. 使用思维链策略微调大语言模型进行透明推理。

Result: 在XD-Violence和UCF-Crime基准数据集上的实验表明，Live-E2T在威胁检测精度、实时效率和可解释性方面显著优于最先进方法。

Conclusion: Live-E2T成功统一了实时性能和决策可解释性两个目标，为实时威胁监控提供了有效的解决方案。

Abstract: Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.

</details>


### [43] [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)
*Daiqing Qi,Handong Zhao,Jing Shi,Simon Jenni,Yifei Fan,Franck Dernoncourt,Scott Cohen,Sheng Li*

Main category: cs.CV

TL;DR: 本文提出了PhotoCritique数据集、PhotoEye模型和PhotoBench基准，旨在提升多模态大语言模型在美学视觉理解方面的能力，特别是专业摄影领域的美学分析。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在美学视觉理解方面存在显著差距，无法像专业摄影师那样从美学角度（如色彩、光线、构图等）分析图像，而仅限于一般性的物体识别。

Method: 1）构建大规模专业摄影数据集PhotoCritique；2）提出PhotoEye模型，采用语言引导的多视角视觉融合机制；3）建立专业美学评估基准PhotoBench。

Result: 在现有基准和新提出的PhotoBench上，PhotoEye模型相比现有模型展现出明显优势。

Conclusion: 通过专业数据集、创新模型和评估基准的结合，显著提升了多模态大语言模型在美学视觉理解方面的能力。

Abstract: While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.

</details>


### [44] [Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network](https://arxiv.org/abs/2509.18591)
*Pengchao Deng,Shengqi Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于XMem模型的实时MRI引导放疗肿瘤分割框架，用于TrackRAD2025挑战赛。该方法利用记忆增强架构在长序列cine-MRI中分割肿瘤，满足临床实时需求。


<details>
  <summary>Details</summary>
Motivation: 提高MRI引导放疗中肿瘤跟踪的精度，这对于提升癌症治疗的准确性和安全性至关重要。

Method: 采用XMem模型（记忆增强架构），通过集成记忆机制来实时跟踪肿瘤运动，即使在标注数据有限的情况下也能实现高分割精度。

Result: 由于详细的实验记录丢失，无法报告精确的定量结果。但从初步开发印象看，基于XMem的框架表现出合理的分割性能并满足临床实时要求。

Conclusion: 该工作有助于改善MRI引导放疗中的肿瘤跟踪精度，为癌症治疗的安全性和准确性做出贡献。

Abstract: This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.

</details>


### [45] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出SSCM模型解决多对比度MRI超分辨率问题，通过动态空间扭曲模块、语义感知令牌聚合块和空间频率融合块实现空间语义一致性，在减少参数的同时达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在建模空间语义一致性和利用频域信息方面不足，导致细粒度对齐差和高频细节恢复不充分。

Method: SSCM模型包含三个核心模块：动态空间扭曲模块用于跨对比度空间对齐，语义感知令牌聚合块确保长程语义一致性，空间频率融合块用于精细结构恢复。

Result: 在公开和私有数据集上的实验表明，SSCM以更少的参数实现了最先进的性能，确保了空间和语义一致的重建结果。

Conclusion: SSCM模型有效解决了多对比度MRI超分辨率中的空间语义一致性问题，在保持解剖细节的同时提高了成像效率。

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.

</details>


### [46] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: 提出OraPO框架和FactS奖励机制，在有限计算资源下实现高效的放射学报告生成，显著减少训练数据需求并达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 解决现有放射学报告生成方法对大规模数据和计算资源的依赖问题，在预算受限条件下实现高效训练

Method: 使用Oracle-educated GRPO（OraPO）框架进行单阶段强化学习训练，通过轻量级oracle步骤将失败的探索转化为偏好监督；采用FactScore-based奖励机制提取临床事实并与真实标签进行蕴含检查

Result: 在CheXpert Plus数据集上达到0.341的F1分数，使用训练数据减少2-3个数量级，在普通硬件上实现SOTA性能

Conclusion: OraPO和FactS共同构成了紧凑而强大的框架，显著提高了临床挑战性病例的学习效率

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


### [47] [Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation](https://arxiv.org/abs/2509.18602)
*Xu Liu,Yibo Lu,Xinxian Wang,Xinyu Wu*

Main category: cs.CV

TL;DR: AMSF是一个基于参考图像的无需训练框架，能够在扩散模型中实现可控的多风格融合，解决了现有方法只能处理单风格图像和缺乏平衡多种风格影响机制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于参考的方法存在两个主要限制：(a)只能接受一个风格图像，无法实现混合美学和扩展到更多风格；(b)缺乏平衡多种风格影响的原理性机制。AMSF旨在解决这些挑战。

Method: 通过语义标记分解模块编码所有风格图像和文本提示，自适应注入到冻结扩散模型的每个交叉注意力层中。相似性感知重加权模块在每个去噪步骤重新校准分配给每个风格组件的注意力权重。

Result: 定性和定量评估显示，AMSF产生的多风格融合结果始终优于最先进方法，且其融合设计可无缝扩展到两种或更多风格。

Conclusion: AMSF代表了扩散模型中表达性多风格生成的实际进展，无需微调或外部适配器即可实现平衡且用户可控的风格融合。

Abstract: We propose Adaptive Multi-Style Fusion (AMSF), a reference-based
training-free framework that enables controllable fusion of multiple reference
styles in diffusion models. Most of the existing reference-based methods are
limited by (a) acceptance of only one style image, thus prohibiting hybrid
aesthetics and scalability to more styles, and (b) lack of a principled
mechanism to balance several stylistic influences. AMSF mitigates these
challenges by encoding all style images and textual hints with a semantic token
decomposition module that is adaptively injected into every cross-attention
layer of an frozen diffusion model. A similarity-aware re-weighting module then
recalibrates, at each denoising step, the attention allocated to every style
component, yielding balanced and user-controllable blends without any
fine-tuning or external adapters. Both qualitative and quantitative evaluations
show that AMSF produces multi-style fusion results that consistently outperform
the state-of-the-art approaches, while its fusion design scales seamlessly to
two or more styles. These capabilities position AMSF as a practical step toward
expressive multi-style generation in diffusion models.

</details>


### [48] [MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2509.18613)
*Yuzhi Wu,Li Xiao,Jun Liu,Guangfeng Jiang,XiangGen Xia*

Main category: cs.CV

TL;DR: 本文提出MLF-4DRCNet，一种新颖的两阶段框架，通过4D雷达和相机图像的多级融合进行3D目标检测，解决了现有方法忽视雷达点云稀疏性和不完整几何形状的问题。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达在自动驾驶中具有成本效益和鲁棒性，但其点云存在显著稀疏性和噪声，限制了在3D目标检测中的独立应用。现有雷达-相机融合方法大多采用为LiDAR-相机融合设计的显式鸟瞰图融合范式，忽视了雷达的固有缺陷。

Method: 提出MLF-4DRCNet框架，包含三个关键组件：增强雷达点编码器（ERPE）模块、分层场景融合池化（HSFP）模块和提议级融合增强（PLFE）模块。ERPE通过2D图像实例增强雷达点云密度，HSFP使用可变形注意力动态集成多尺度体素特征与2D图像特征，PLFE通过融合图像特征细化区域提议。

Result: 在View-of-Delft（VoD）和TJ4DRadSet数据集上的实验结果表明，MLF-4DRCNet实现了最先进的性能，在VoD数据集上达到了与基于LiDAR的模型相当的性能。

Conclusion: MLF-4DRCNet通过多级融合策略有效解决了4D雷达点云的稀疏性和噪声问题，为雷达-相机融合在3D目标检测中的应用提供了有效的解决方案。

Abstract: The emerging 4D millimeter-wave radar, measuring the range, azimuth,
elevation, and Doppler velocity of objects, is recognized for its
cost-effectiveness and robustness in autonomous driving. Nevertheless, its
point clouds exhibit significant sparsity and noise, restricting its standalone
application in 3D object detection. Recent 4D radar-camera fusion methods have
provided effective perception. Most existing approaches, however, adopt
explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera
fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the
sparse and incomplete geometry of radar point clouds and restrict fusion to
coarse scene-level integration. To address these problems, we propose
MLF-4DRCNet, a novel two-stage framework for 3D object detection via
multi-level fusion of 4D radar and camera images. Our model incorporates the
point-, scene-, and proposal-level multi-modal information, enabling
comprehensive feature representation. It comprises three crucial components:
the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion
Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.
Operating at the point-level, ERPE densities radar point clouds with 2D image
instances and encodes them into voxels via the proposed Triple-Attention Voxel
Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D
image features using deformable attention to capture scene context and adopts
pooling to the fused features. PLFE refines region proposals by fusing image
features, and further integrates with the pooled features from HSFP.
Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets
demonstrate that MLF-4DRCNet achieves the state-of-the-art performance.
Notably, it attains performance comparable to LiDAR-based models on the VoD
dataset.

</details>


### [49] [Prompt-Guided Dual Latent Steering for Inversion Problems](https://arxiv.org/abs/2509.18619)
*Yichen Wu,Xu Liu,Chenxuan Zhao,Xinyu Wu*

Main category: cs.CV

TL;DR: PDLS是一种基于Rectified Flow模型的无训练框架，通过双潜在流（结构路径和语义路径）解决扩散模型在图像反演中的语义漂移问题，使用LQR控制器动态指导生成轨迹，在多种图像修复任务中表现优于单潜在基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法将图像编码为单个潜在向量时难以平衡结构保真度和语义准确性，导致重建图像出现语义漂移（如细节模糊或属性错误）。

Method: 提出Prompt-Guided Dual Latent Steering (PDLS)框架，将反演过程分解为结构路径（保持源图像完整性）和语义路径（由提示词引导），通过LQR控制器形成闭式解来动态指导生成轨迹。

Result: 在FFHQ-1K和ImageNet-1K数据集上的多种反演任务（高斯去模糊、运动去模糊、超分辨率和自由形式修复）实验表明，PDLS比单潜在基线方法产生更忠实于原始图像且语义信息更一致的重建结果。

Conclusion: PDLS通过双潜在流和最优控制方法有效解决了扩散模型图像反演中的语义漂移问题，无需昂贵的逐图像优化即可保持细节完整性。

Abstract: Inverting corrupted images into the latent space of diffusion models is
challenging. Current methods, which encode an image into a single latent
vector, struggle to balance structural fidelity with semantic accuracy, leading
to reconstructions with semantic drift, such as blurred details or incorrect
attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering
(PDLS), a novel, training-free framework built upon Rectified Flow models for
their stable inversion paths. PDLS decomposes the inversion process into two
complementary streams: a structural path to preserve source integrity and a
semantic path guided by a prompt. We formulate this dual guidance as an optimal
control problem and derive a closed-form solution via a Linear Quadratic
Regulator (LQR). This controller dynamically steers the generative trajectory
at each step, preventing semantic drift while ensuring the preservation of fine
detail without costly, per-image optimization. Extensive experiments on FFHQ-1K
and ImageNet-1K under various inversion tasks, including Gaussian deblurring,
motion deblurring, super-resolution and freeform inpainting, demonstrate that
PDLS produces reconstructions that are both more faithful to the original image
and better aligned with the semantic information than single-latent baselines.

</details>


### [50] [Learning neuroimaging models from health system-scale data](https://arxiv.org/abs/2509.18638)
*Yiwei Lyu,Samir Harake,Asadur Chowdury,Soumyanil Banerjee,Rachel Gologorsky,Shixuan Liu,Anna-Katharina Meissner,Akshay Rao,Chenhui Zhao,Akhil Kondepudi,Cheng Jiang,Xinhai Hou,Rushikesh S. Joshi,Volker Neuschmelting,Ashok Srinivasan,Dawn Kleindorfer,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: 开发了Prima，首个用于神经影像的视觉语言模型，在52种神经疾病诊断中平均AUC达到92.0，优于现有AI模型，可提供可解释诊断、工作优先级排序和转诊建议。


<details>
  <summary>Details</summary>
Motivation: 全球MRI需求增长给医疗系统带来压力，特别是在资源匮乏地区。需要AI解决方案来缓解医生负担、缩短周转时间并减少医疗偏见。

Method: 利用大型学术医疗系统作为数据引擎，训练基于22万+MRI研究的视觉语言模型Prima，采用分层视觉架构提供通用可转移的MRI特征。

Result: 在包含3万MRI研究的1年系统测试中，Prima在主要神经疾病（肿瘤、炎症、感染、发育异常）诊断中表现优异，平均AUC达92.0。

Conclusion: Prima展示了健康系统规模视觉语言模型的变革潜力，能够推进AI驱动的医疗保健，特别是在减少医疗偏见和改善资源分配方面。

Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

</details>


### [51] [Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation](https://arxiv.org/abs/2509.18639)
*Yuanhuiyi Lyu,Chi Kit Wong,Chenfei Liao,Lutao Jiang,Xu Zheng,Zexin Lu,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 本文提出了一种新的统一模型推理框架Understanding-in-Generation (UiG)，通过在推理过程中整合理解能力来增强文本到图像生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的推理方法将理解和生成过程分离，限制了它们指导统一模型解决生成能力不足的能力。

Method: 引入"图像编辑"作为桥梁，在推理过程中通过验证生成图像并整合模型的理解能力来逐步增强图像质量。

Result: UiG框架在文本到图像生成方面表现出显著性能提升，在TIIF基准测试的长提示设置上获得了3.92%的增益。

Conclusion: UiG框架通过将理解能力融入生成过程，有效缓解了统一模型在生成能力方面的局限性。

Abstract: Recent works have made notable advancements in enhancing unified models for
text-to-image generation through the Chain-of-Thought (CoT). However, these
reasoning methods separate the processes of understanding and generation, which
limits their ability to guide the reasoning of unified models in addressing the
deficiencies of their generative capabilities. To this end, we propose a novel
reasoning framework for unified models, Understanding-in-Generation (UiG),
which harnesses the robust understanding capabilities of unified models to
reinforce their performance in image generation. The core insight of our UiG is
to integrate generative guidance by the strong understanding capabilities
during the reasoning process, thereby mitigating the limitations of generative
abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse
understanding into the generation process. Initially, we verify the generated
image and incorporate the understanding of unified models into the editing
instructions. Subsequently, we enhance the generated image step by step,
gradually infusing the understanding into the generation process. Our UiG
framework demonstrates a significant performance improvement in text-to-image
generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on
the long prompt setting of the TIIF benchmark. The project code:
https://github.com/QC-LY/UiG

</details>


### [52] [Zero-shot Monocular Metric Depth for Endoscopic Images](https://arxiv.org/abs/2509.18642)
*Nicolas Toussaint,Emanuele Colleoni,Ricardo Sanchez-Matilla,Joshua Sutcliffe,Vanessa Thompson,Muhammad Asad,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 本文提出了一个用于内窥镜图像的深度估计综合基准测试，并发布了新的合成数据集EndoSynth，通过微调深度基础模型显著提升了在真实内窥镜图像上的性能。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像的深度估计缺乏稳健的基准测试和高质量数据集，限制了该领域的发展和应用。

Method: 创建了包含真实内窥镜图像的基准测试，并开发了带有真实深度和分割掩码的合成数据集EndoSynth，用于微调深度基础模型。

Result: 使用合成数据集微调深度基础模型后，在大多数未见过的真实数据上精度显著提升。

Conclusion: 这项工作通过提供基准测试和合成数据集，推动了内窥镜图像深度估计领域的发展，为未来研究提供了重要资源。

Abstract: Monocular relative and metric depth estimation has seen a tremendous boost in
the last few years due to the sharp advancements in foundation models and in
particular transformer based networks. As we start to see applications to the
domain of endoscopic images, there is still a lack of robust benchmarks and
high-quality datasets in that area. This paper addresses these limitations by
presenting a comprehensive benchmark of state-of-the-art (metric and relative)
depth estimation models evaluated on real, unseen endoscopic images, providing
critical insights into their generalisation and performance in clinical
scenarios. Additionally, we introduce and publish a novel synthetic dataset
(EndoSynth) of endoscopic surgical instruments paired with ground truth metric
depth and segmentation masks, designed to bridge the gap between synthetic and
real-world data. We demonstrate that fine-tuning depth foundation models using
our synthetic dataset boosts accuracy on most unseen real data by a significant
margin. By providing both a benchmark and a synthetic dataset, this work
advances the field of depth estimation for endoscopic images and serves as an
important resource for future research. Project page, EndoSynth dataset and
trained weights are available at https://github.com/TouchSurgery/EndoSynth.

</details>


### [53] [LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection](https://arxiv.org/abs/2509.18683)
*Lanhu Wu,Zilin Gao,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 提出了LEAF-Mamba模型，通过局部强调状态空间模块和自适应融合模块，在RGB-D显著目标检测任务中实现了性能与计算效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D显著目标检测方法存在局部感受野限制或二次复杂度问题，需要平衡性能与计算效率。状态空间模型具有线性复杂度优势，但直接应用会导致局部语义不足和跨模态融合不充分。

Method: LEAF-Mamba模型包含两个核心组件：1）局部强调状态空间模块（LE-SSM）捕获多尺度局部依赖；2）基于SSM的自适应融合模块（AFM）实现互补的跨模态交互和可靠融合。

Result: 在16个最先进RGB-D SOD方法中，LEAF-Mamba在效果和效率上均表现最优，同时在RGB-T SOD任务上也展现出优秀的泛化能力。

Conclusion: LEAF-Mamba成功解决了RGB-D SOD中局部语义不足和跨模态融合问题，在保持线性复杂度的同时实现了优异的性能，具有良好的通用性。

Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.

</details>


### [54] [Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification](https://arxiv.org/abs/2509.18692)
*Xinle Gao,Linghui Ye,Zhiyong Xiao*

Main category: cs.CV

TL;DR: 提出一种轻量级食品图像分类算法，结合窗口多头注意力机制和空间注意力机制，在降低计算复杂度的同时保持高分类准确率


<details>
  <summary>Details</summary>
Motivation: 随着社会发展和科技进步，食品行业对生产质量和效率要求不断提高，但Vision Transformer模型参数多、计算复杂度高，难以在资源受限环境中部署

Method: 集成窗口多头注意力机制（WMHAM）和空间注意力机制（SAM），WMHAM通过窗口划分降低计算成本，SAM自适应强调关键空间区域以提升特征表示能力

Result: 在Food-101和Vireo Food-172数据集上分别达到95.24%和94.33%的准确率，同时显著减少参数和FLOPs

Conclusion: 该方法在计算效率和分类性能之间实现了有效平衡，适合在资源受限环境中部署

Abstract: With the rapid development of society and continuous advances in science and
technology, the food industry increasingly demands higher production quality
and efficiency. Food image classification plays a vital role in enabling
automated quality control on production lines, supporting food safety
supervision, and promoting intelligent agricultural production. However, this
task faces challenges due to the large number of parameters and high
computational complexity of Vision Transformer models. To address these issues,
we propose a lightweight food image classification algorithm that integrates a
Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism
(SAM). The WMHAM reduces computational cost by capturing local and global
contextual features through efficient window partitioning, while the SAM
adaptively emphasizes key spatial regions to improve discriminative feature
representation. Experiments conducted on the Food-101 and Vireo Food-172
datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,
respectively, while significantly reducing parameters and FLOPs compared with
baseline methods. These results confirm that the proposed approach achieves an
effective balance between computational efficiency and classification
performance, making it well-suited for deployment in resource-constrained
environments.

</details>


### [55] [OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery](https://arxiv.org/abs/2509.18693)
*Siyi Chen,Kai Wang,Weicong Pang,Ruiming Yang,Ziru Chen,Renjun Gao,Alexis Kai Hon Lau,Dasa Gu,Chenchen Zhang,Cheng Li*

Main category: cs.CV

TL;DR: OSDA是一个用于遥感图像开放集土地覆盖分析的三阶段框架，无需人工标注即可实现发现、分割和描述功能


<details>
  <summary>Details</summary>
Motivation: 遥感中的开放集土地覆盖分析需要同时实现精细空间定位和语义开放分类，既要检测和分割无类别监督的新物体，又要通过多模态推理为其分配可解释的语义标签

Method: 三阶段框架：1）使用可提示的微调分割模型（SAM）进行精确发现和掩码提取；2）通过两阶段微调的多模态大语言模型（MLLM）进行语义归因和上下文描述；3）使用LLM作为评判者并结合人工评分进行MLLM评估

Result: 该框架结合像素级精度与高级语义理解，解决了开放世界遥感解释中的关键挑战，支持在不同卫星图像上进行鲁棒评估

Conclusion: OSDA为动态土地覆盖监测提供了可扩展和可解释的解决方案，在自动化制图更新和大规模地球观测分析方面显示出强大潜力

Abstract: Open-set land-cover analysis in remote sensing requires the ability to
achieve fine-grained spatial localization and semantically open categorization.
This involves not only detecting and segmenting novel objects without
categorical supervision but also assigning them interpretable semantic labels
through multimodal reasoning. In this study, we introduce OSDA, an integrated
three-stage framework for annotation-free open-set land-cover discovery,
segmentation, and description. The pipeline consists of: (1) precise discovery
and mask extraction with a promptable fine-tuned segmentation model (SAM), (2)
semantic attribution and contextual description via a two-phase fine-tuned
multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring
of the MLLMs evaluation. By combining pixel-level accuracy with high-level
semantic understanding, OSDA addresses key challenges in open-world remote
sensing interpretation. Designed to be architecture-agnostic and label-free,
the framework supports robust evaluation across diverse satellite imagery
without requiring manual annotation. Our work provides a scalable and
interpretable solution for dynamic land-cover monitoring, showing strong
potential for automated cartographic updating and large-scale earth observation
analysis.

</details>


### [56] [Overview of PlantCLEF 2021: cross-domain plant identification](https://arxiv.org/abs/2509.18697)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: LifeCLEF 2021植物识别挑战赛旨在评估如何利用植物标本馆收藏改进生物多样性丰富但数据贫乏地区的植物自动识别，通过跨域分类任务结合标本馆标本和野外照片进行训练。


<details>
  <summary>Details</summary>
Motivation: 当前自动植物识别主要依赖北美和西欧的野外照片数据，而生物多样性最丰富的热带地区数据稀缺。但植物标本馆收藏了大量热带地区的标本，可作为补充数据源。

Method: 采用跨域分类方法，训练集包含数十万份标本馆标本和数千张野外照片，测试集仅包含野外照片。数据集聚焦圭亚那地盾地区的约1000种植物，并包含5种形态和功能性状数据。

Result: 挑战赛评估了不同研究团队的方法，分析了跨域识别的主要结果和性能。

Conclusion: 利用植物标本馆收藏可以有效改进数据贫乏热带地区的植物自动识别，跨域学习方法为解决生物多样性热点地区的物种识别提供了可行方案。

Abstract: Automated plant identification has improved considerably thanks to recent
advances in deep learning and the availability of training data with more and
more field photos. However, this profusion of data concerns only a few tens of
thousands of species, mainly located in North America and Western Europe, much
less in the richest regions in terms of biodiversity such as tropical
countries. On the other hand, for several centuries, botanists have
systematically collected, catalogued and stored plant specimens in herbaria,
especially in tropical regions, and recent efforts by the biodiversity
informatics community have made it possible to put millions of digitised
records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF
2021") was designed to assess the extent to which automated identification of
flora in data-poor regions can be improved by using herbarium collections. It
is based on a dataset of about 1,000 species mainly focused on the Guiana
Shield of South America, a region known to have one of the highest plant
diversities in the world. The challenge was evaluated as a cross-domain
classification task where the training set consisted of several hundred
thousand herbarium sheets and a few thousand photos to allow learning a
correspondence between the two domains. In addition to the usual metadata
(location, date, author, taxonomy), the training data also includes the values
of 5 morphological and functional traits for each species. The test set
consisted exclusively of photos taken in the field. This article presents the
resources and evaluations of the assessment carried out, summarises the
approaches and systems used by the participating research groups and provides
an analysis of the main results.

</details>


### [57] [AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping](https://arxiv.org/abs/2509.18699)
*Zedong Zhang,Ying Tai,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为AGSwap的新方法，用于解决文本到图像生成中跨类别对象融合的问题，同时引入了COF数据集作为基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨类别对象融合时经常产生有偏差、视觉混乱或语义不一致的结果，且缺乏全面的基准数据集限制了该领域的发展。

Method: AGSwap方法包含两个关键组件：组级嵌入交换（通过特征操作融合不同概念的语义属性）和自适应组更新（基于平衡评估分数的动态优化机制）。

Result: 大量实验表明，AGSwap在简单和复杂提示下都优于现有的最先进组合T2I方法，包括GPT-Image-1。

Conclusion: AGSwap是一种简单而高效的跨类别对象融合方法，配合新构建的COF数据集，为解决该领域的挑战提供了有效解决方案。

Abstract: Fusing cross-category objects to a single coherent object has gained
increasing attention in text-to-image (T2I) generation due to its broad
applications in virtual reality, digital media, film, and gaming. However,
existing methods often produce biased, visually chaotic, or semantically
inconsistent results due to overlapping artifacts and poor integration.
Moreover, progress in this field has been limited by the absence of a
comprehensive benchmark dataset. To address these problems, we propose
\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective
approach comprising two key components: (1) Group-wise Embedding Swapping,
which fuses semantic attributes from different concepts through feature
manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism
guided by a balance evaluation score to ensure coherent synthesis.
Additionally, we introduce \textbf{Cross-category Object Fusion (COF)}, a
large-scale, hierarchically structured dataset built upon ImageNet-1K and
WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling
451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap
outperforms state-of-the-art compositional T2I methods, including GPT-Image-1
using simple and complex prompts.

</details>


### [58] [Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries](https://arxiv.org/abs/2509.18705)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 该论文介绍了LifeCLEF 2019植物识别挑战赛，旨在评估在数据稀缺地区（如圭亚那地盾和亚马逊雨林）的植物自动识别系统性能，并与热带植物专家的表现进行比较。


<details>
  <summary>Details</summary>
Motivation: 当前植物自动识别技术主要针对少数几万种物种，而地球上有近36.9万种植物。该挑战赛旨在解决数据稀缺地区的植物识别问题。

Method: 基于包含1万种物种的数据集，主要关注圭亚那地盾和北亚马逊雨林地区，比较了参与研究组的各种自动化系统与热带植物专家的识别性能。

Result: 论文提供了挑战赛的资源、评估结果，总结了参与研究组采用的方法和系统，并分析了主要成果。

Conclusion: 该挑战赛为数据稀缺地区的植物自动识别技术发展提供了重要基准，推动了该领域的研究进展。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data.
However, this profusion of data only concerns a few tens of thousands of
species, while the planet has nearly 369K. The LifeCLEF 2019 Plant
Identification challenge (or "PlantCLEF 2019") was designed to evaluate
automated identification on the flora of data deficient regions. It is based on
a dataset of 10K species mainly focused on the Guiana shield and the Northern
Amazon rainforest, an area known to have one of the greatest diversity of
plants and animals in the world. As in the previous edition, a comparison of
the performance of the systems evaluated with the best tropical flora experts
was carried out. This paper presents the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [59] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: RSVG-ZeroOV是一个无需训练的零样本开放词汇遥感视觉定位框架，利用冻结的基础模型实现开放世界场景下的目标定位


<details>
  <summary>Details</summary>
Motivation: 解决现有遥感视觉定位方法受限于封闭词汇集、依赖高质量数据集和耗时微调的问题

Method: 采用三阶段框架：1)利用VLM获取跨注意力图；2)利用扩散模型补充结构信息；3)通过注意力进化模块净化分割掩码

Result: 在广泛实验中，该框架持续优于现有的弱监督和零样本方法

Conclusion: RSVG-ZeroOV提供了一个无需特定任务训练的高效可扩展解决方案，展示了冻结通用基础模型在零样本开放词汇遥感视觉定位中的潜力

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.

</details>


### [60] [What Makes You Unique? Attribute Prompt Composition for Object Re-Identification](https://arxiv.org/abs/2509.18715)
*Yingquan Wang,Pingping Zhang,Chong Sun,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出了一种属性提示组合框架，利用文本语义联合增强行人重识别任务的判别性和泛化性，通过属性提示生成器和快慢训练策略在传统和领域泛化数据集上实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别模型要么局限于单域场景容易过拟合，要么依赖多样化归一化策略可能抑制身份特异性判别线索，需要一种能同时增强判别性和泛化性的方法。

Method: 提出属性提示组合框架，包含属性提示生成器（语义属性字典和提示组合模块）和快慢训练策略，通过文本语义和视觉语言模型的泛化能力来平衡重识别特异性判别和通用表示学习。

Result: 在传统和领域泛化行人重识别数据集上的大量实验表明，该框架超越了最先进方法，在判别性和泛化性方面都表现出优越性能。

Conclusion: 该工作通过利用文本语义和视觉语言模型的优势，成功解决了行人重识别中判别性与泛化性的平衡问题，为实际应用提供了更实用的解决方案。

Abstract: Object Re-IDentification (ReID) aims to recognize individuals across
non-overlapping camera views. While recent advances have achieved remarkable
progress, most existing models are constrained to either single-domain or
cross-domain scenarios, limiting their real-world applicability. Single-domain
models tend to overfit to domain-specific features, whereas cross-domain models
often rely on diverse normalization strategies that may inadvertently suppress
identity-specific discriminative cues. To address these limitations, we propose
an Attribute Prompt Composition (APC) framework, which exploits textual
semantics to jointly enhance discrimination and generalization. Specifically,
we design an Attribute Prompt Generator (APG) consisting of a Semantic
Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an
over-complete attribute dictionary to provide rich semantic descriptions, while
PCM adaptively composes relevant attributes from SAD to generate discriminative
attribute-aware features. In addition, motivated by the strong generalization
ability of Vision-Language Models (VLM), we propose a Fast-Slow Training
Strategy (FSTS) to balance ReID-specific discrimination and generalizable
representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)
to rapidly acquire ReID-specific discriminative knowledge and a Slow Update
Stream (SUS) to retain the generalizable knowledge inherited from the
pre-trained VLM. Through a mutual interaction, the framework effectively
focuses on ReID-relevant features while mitigating overfitting. Extensive
experiments on both conventional and Domain Generalized (DG) ReID datasets
demonstrate that our framework surpasses state-of-the-art methods, exhibiting
superior performances in terms of both discrimination and generalization. The
source code is available at https://github.com/AWangYQ/APC.

</details>


### [61] [Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment](https://arxiv.org/abs/2509.18717)
*Tong Zhang,Kuofeng Gao,Jiawang Bai,Leo Yu Zhang,Xin Yin,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: OTCCLIP是一个基于最优传输的框架，用于重建图像-标题对，通过细粒度视觉和文本特征对齐来防御CLIP模型的投毒攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的防御方法仅依赖图像和标题的全局表示来匹配新标题，忽略了细粒度特征，可能导致错误的图像-标题对并损害CLIP预训练。

Method: 提出基于最优传输的细粒度视觉和文本特征集距离度量，重新分配新标题，并通过最优传输目标函数促进模态间和模态内的细粒度对齐。

Result: OTCCLIP成功降低了投毒攻击的成功率，相比之前方法显著提高了CLIP在投毒数据集上的零样本和线性探测性能。

Conclusion: OTCCLIP通过细粒度特征对齐有效防御CLIP模型的投毒攻击，提升了模型的安全性和性能。

Abstract: Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)
models are threatened by targeted data poisoning and backdoor attacks due to
massive training image-caption pairs crawled from the Internet. Previous
defense methods correct poisoned image-caption pairs by matching a new caption
for each image. However, the matching process relies solely on the global
representations of images and captions, overlooking fine-grained features of
visual and textual features. It may introduce incorrect image-caption pairs and
harm the CLIP pre-training. To address their limitations, we propose an Optimal
Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We
propose a new optimal transport-based distance measure between fine-grained
visual and textual feature sets and re-assign new captions based on the
proposed optimal transport distance. Additionally, to further reduce the
negative impact of mismatched pairs, we encourage the inter- and intra-modality
fine-grained alignment by employing optimal transport-based objective
functions. Our experiments demonstrate that OTCCLIP can successfully decrease
the attack success rates of poisoning attacks. Also, compared to previous
methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing
performance trained on poisoned datasets.

</details>


### [62] [Knowledge Transfer from Interaction Learning](https://arxiv.org/abs/2509.18733)
*Yilin Gao,Kangyi Chen,Zhongxing Peng,Hengjie Lu,Shugong Xu*

Main category: cs.CV

TL;DR: 提出LFI框架，通过显式建模视觉理解作为交互过程，解决视觉基础模型从视觉语言模型知识迁移的局限性，实现更忠实高效的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 当前视觉基础模型在从视觉语言模型迁移知识时存在根本限制，主要采用结果导向范式而忽略了底层的交互过程，这种表示差异阻碍了有效的知识迁移并限制了跨视觉任务的泛化能力。

Method: LFI框架包含两个技术创新：交互查询（维护跨网络层的持久关系结构）和基于交互的监督（源自视觉语言模型的跨模态注意力机制）。

Result: 在多个基准测试中取得一致改进，在TinyImageNet分类上提升3.3mAP，COCO检测/分割提升1.6mAP/2.4AP，在跨域设置中表现优异，PACS和VLCS零样本学习分别提升2.4和9.3。

Conclusion: LFI框架通过建模交互过程实现了更有效的知识迁移，在参数开销最小和收敛更快的情况下，显著提升了视觉任务的性能，特别是在跨域场景中表现出色。

Abstract: Current visual foundation models (VFMs) face a fundamental limitation in
transferring knowledge from vision language models (VLMs), while VLMs excel at
modeling cross-modal interactions through unified representation spaces,
existing VFMs predominantly adopt result-oriented paradigms that neglect the
underlying interaction processes. This representational discrepancy hinders
effective knowledge transfer and limits generalization across diverse vision
tasks. We propose Learning from Interactions (LFI), a cognitive-inspired
framework that addresses this gap by explicitly modeling visual understanding
as an interactive process. Our key insight is that capturing the dynamic
interaction patterns encoded in pre-trained VLMs enables more faithful and
efficient knowledge transfer to VFMs. The approach centers on two technical
innovations, Interaction Queries, which maintain persistent relational
structures across network layers, and interaction-based supervision, derived
from the cross-modal attention mechanisms of VLMs. Comprehensive experiments
demonstrate consistent improvements across multiple benchmarks, achieving 3.3
and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO
detection/segmentation respectively, with minimal parameter overhead and faster
convergence. The framework particularly excels in cross-domain settings,
delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human
evaluations further confirm its cognitive alignment, outperforming
result-oriented methods by 2.7 times in semantic consistency metrics.

</details>


### [63] [HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection](https://arxiv.org/abs/2509.18738)
*Ruichao Hou,Xingyuan Li,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的混合提示驱动的Segment Anything模型（HyPSAM），用于RGB-热成像显著目标检测，通过动态融合网络和即插即用优化网络，利用SAM的零样本泛化能力来解决多模态特征融合和数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: RGB-热成像显著目标检测面临内在特征融合不足和外在数据稀缺的双重挑战，现有方法难以学习精确边界和完整目标。

Method: 提出HyPSAM框架：1）动态融合网络（DFNet）生成高质量初始显著图作为视觉提示；2）即插即用优化网络（P2RNet）使用混合提示（文本、掩码、框）指导SAM优化显著图。

Result: 在三个公开数据集上的实验表明，该方法实现了最先进的性能，并具有显著的多功能性，可与不同RGB-T SOD方法无缝集成实现性能提升。

Conclusion: HyPSAM展示了提示工程在RGB-T SOD领域的潜力，为多模态显著目标检测提供了有效的解决方案。

Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent
objects by integrating complementary information from RGB and thermal
modalities. However, learning the precise boundaries and complete objects
remains challenging due to the intrinsic insufficient feature fusion and the
extrinsic limitations of data scarcity. In this paper, we propose a novel
hybrid prompt-driven segment anything model (HyPSAM), which leverages the
zero-shot generalization capabilities of the segment anything model (SAM) for
RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that
generates high-quality initial saliency maps as visual prompts. DFNet employs
dynamic convolution and multi-branch decoding to facilitate adaptive
cross-modality interaction, overcoming the limitations of fixed-parameter
kernels and enhancing multi-modal feature representation. Moreover, we propose
a plug-and-play refinement network (P2RNet), which serves as a general
optimization strategy to guide SAM in refining saliency maps by using hybrid
prompts. The text prompt ensures reliable modality input, while the mask and
box prompts enable precise salient object localization. Extensive experiments
on three public datasets demonstrate that our method achieves state-of-the-art
performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating
with different RGB-T SOD methods to achieve significant performance gains,
thereby highlighting the potential of prompt engineering in this field. The
code and results of our method are available at:
https://github.com/milotic233/HyPSAM.

</details>


### [64] [TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing](https://arxiv.org/abs/2509.18743)
*Susmit Neogi*

Main category: cs.CV

TL;DR: TriFusion-AE是一种多模态跨注意力自编码器，通过融合文本先验、多视图图像的单目深度图和LiDAR点云，提高点云去噪和重建的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云在自动驾驶和机器人感知中至关重要，但原始点云容易受到噪声、遮挡和对抗性干扰的影响。传统自编码器在真实世界挑战性条件下性能下降。

Method: 提出TriFusion-AE模型，通过跨注意力机制对齐文本的语义线索、图像的几何特征和LiDAR的空间结构，学习对随机噪声和对抗性扰动具有弹性的表示。

Result: 在nuScenes-mini数据集上评估，模型在强对抗性攻击和重噪声条件下显著优于CNN自编码器，在轻度扰动下提升有限。

Conclusion: TriFusion-AE是一个模型无关的多模态融合框架，可与任何基于CNN的点云自编码器无缝集成进行联合表示学习。

Abstract: LiDAR-based perception is central to autonomous driving and robotics, yet raw
point clouds remain highly vulnerable to noise, occlusion, and adversarial
corruptions. Autoencoders offer a natural framework for denoising and
reconstruction, but their performance degrades under challenging real-world
conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention
autoencoder that integrates textual priors, monocular depth maps from
multi-view images, and LiDAR point clouds to improve robustness. By aligning
semantic cues from text, geometric (depth) features from images, and spatial
structure from LiDAR, TriFusion-AE learns representations that are resilient to
stochastic noise and adversarial perturbations. Interestingly, while showing
limited gains under mild perturbations, our model achieves significantly more
robust reconstruction under strong adversarial attacks and heavy noise, where
CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to
reflect realistic low-data deployment scenarios. Our multimodal fusion
framework is designed to be model-agnostic, enabling seamless integration with
any CNN-based point cloud autoencoder for joint representation learning.

</details>


### [65] [COLT: Enhancing Video Large Language Models with Continual Tool Usage](https://arxiv.org/abs/2509.18754)
*Yuyang Liu,Xinyuan Shi,Bang Yang,Peilin Zhou,Jiahua Dong,Long Chen,Ian Reid,Xiaondan Liang*

Main category: cs.CV

TL;DR: 本文提出COLT方法，增强开源视频大语言模型的持续工具使用能力，解决现有方法在工具数据持续演化时的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频LLM工具使用方法假设工具库固定，难以适应现实环境中工具数据持续演化和流式输入的情况。

Method: COLT方法包含可学习的工具代码本作为工具特定记忆系统，根据用户指令与工具特征的相似度动态选择相关工具，并使用VideoToolBench数据集进行指令调优。

Result: 在视频LLM基准测试和工具使用专用数据集VideoToolBench上的广泛实验表明，COLT达到了最先进的性能。

Conclusion: COLT方法成功实现了视频LLM在连续工具流中的自动工具使用能力，避免了灾难性遗忘问题。

Abstract: The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.

</details>


### [66] [FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation](https://arxiv.org/abs/2509.18759)
*Zhaorui Wang,Yi Gu,Deming Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: FixingGS是一种无需训练的方法，利用现有扩散模型增强稀疏视角3D高斯泼溅重建，通过改进的蒸馏方法提供更准确和跨视图一致的扩散先验，实现有效的伪影去除和修复。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角下的3D场景重建由于视觉信息不足导致明显的伪影，现有基于生成先验的方法难以保证多视图一致性，导致结构模糊和细节不真实。

Method: 提出FixingGS方法，包括：1）蒸馏方法提供准确且跨视图一致的扩散先验；2）自适应渐进增强方案进一步优化欠约束区域的细节。

Result: 大量实验表明FixingGS在视觉质量和重建性能上优于现有最先进方法。

Conclusion: FixingGS通过充分利用扩散模型能力，成功解决了稀疏视角3D高斯泼溅重建中的伪影和多视图一致性问题。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.

</details>


### [67] [Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models](https://arxiv.org/abs/2509.18763)
*Xijun Wang,Junyun Huang,Rayyan Abdalla,Chengyuan Zhang,Ruiqi Xian,Dinesh Manocha*

Main category: cs.CV

TL;DR: Bi-VLM提出了一种基于高斯分位数的非均匀权重分离方法，通过显著性感知混合量化算法对视觉语言模型进行超低比特量化（≤2位），显著提升硬件受限环境下的效率。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的计算需求和内存要求过高，限制了其在硬件受限环境中的应用，需要开发超低比特量化技术来提高效率。

Method: 基于高斯分位数非均匀分离模型权重为异常值（显著）和多个正常值（不显著）子集，提出显著性感知混合量化算法，根据显著性度量和压缩目标对缩放器和二进制矩阵施加不同约束。

Result: 在视觉问答任务上，Bi-VLM在语言模型部分比SOTA提升3%-47%，在整个VLM上提升4%-45%。量化模型存在90%-99%的图像令牌冗余，可进一步剪枝提升效率。

Conclusion: Bi-VLM通过创新的量化方法成功实现了视觉语言模型的超低比特高效压缩，为硬件受限环境下的应用提供了可行解决方案。

Abstract: We address the critical gap between the computational demands of
vision-language models and the possible ultra-low-bit weight precision
(bitwidth $\leq2$ bits) we can use for higher efficiency. Our work is motivated
by the substantial computational cost and memory requirements of VLMs, which
restrict their applicability in hardware-constrained environments. We propose
Bi-VLM, which separates model weights non-uniformly based on the Gaussian
quantiles. Our formulation groups the model weights into outlier (salient) and
multiple inlier (unsalient) subsets, ensuring that each subset contains a
proportion of weights corresponding to its quantile in the distribution. We
propose a saliency-aware hybrid quantization algorithm and use it to quantize
weights by imposing different constraints on the scaler and binary matrices
based on the saliency metric and compression objective. We have evaluated our
approach on different VLMs. For the language model part of the VLM, our Bi-VLM
outperforms the SOTA by 3%-47% on the visual question answering task in terms
of four different benchmarks and three different models. For the overall VLM,
our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the
quantized models and observe that there is redundancy of image tokens 90% - 99%
in the quantized models. This helps us to further prune the visual tokens to
improve efficiency.

</details>


### [68] [DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision](https://arxiv.org/abs/2509.18765)
*Azad Singh,Deepak Mishra*

Main category: cs.CV

TL;DR: DiSSECT是一个自监督学习框架，通过多尺度向量量化在医学图像中创建离散表示瓶颈，抑制捷径学习，提高特征的可迁移性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法依赖复杂架构、解剖学先验或精心调优的数据增强，容易产生捷径学习，特别是在解剖相似度高、病理特征细微的胸部X光等模态中。

Method: 将多尺度向量量化整合到自监督学习流程中，施加离散表示瓶颈，约束模型学习可重复、结构感知的特征，同时抑制视图特定或低效用模式。

Result: DiSSECT在分类和分割任务上表现优异，需要极少或无需微调，在低标签场景下具有高标签效率，在多个公共医学影像数据集上验证了其鲁棒性和泛化性。

Conclusion: DiSSECT框架通过离散表示瓶颈有效解决了医学图像自监督学习中的捷径学习问题，显著提升了特征表示的可迁移性和临床实用性。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for medical
image representation learning, particularly in settings with limited labeled
data. However, existing SSL methods often rely on complex architectures,
anatomy-specific priors, or heavily tuned augmentations, which limit their
scalability and generalizability. More critically, these models are prone to
shortcut learning, especially in modalities like chest X-rays, where anatomical
similarity is high and pathology is subtle. In this work, we introduce DiSSECT
-- Discrete Self-Supervision for Efficient Clinical Transferable
Representations, a framework that integrates multi-scale vector quantization
into the SSL pipeline to impose a discrete representational bottleneck. This
constrains the model to learn repeatable, structure-aware features while
suppressing view-specific or low-utility patterns, improving representation
transfer across tasks and domains. DiSSECT achieves strong performance on both
classification and segmentation tasks, requiring minimal or no fine-tuning, and
shows particularly high label efficiency in low-label regimes. We validate
DiSSECT across multiple public medical imaging datasets, demonstrating its
robustness and generalizability compared to existing state-of-the-art
approaches.

</details>


### [69] [Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning](https://arxiv.org/abs/2509.18779)
*Hemanth Puppala,Wayne Sarasua,Srinivas Biyaguda,Farhad Farzinpour,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: 本文提出了一种结合热成像、深度学习和车联网通信的实时检测系统，用于减少鹿车碰撞事故，系统在热成像数据集上表现出色，检测精度达98.84%，并在实地测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 美国每年发生约210万起鹿车碰撞事故，造成440人死亡、5.9万人受伤和100亿美元经济损失，同时导致鹿群数量下降，急需有效的预防技术。

Method: 使用热成像技术和深度学习模型，在收集的1.2万张热成像鹿图像数据集上进行训练和验证，结合车联网通信技术实现实时检测和驾驶员预警。

Result: 系统达到98.84%的平均精度、95.44%的精确率和95.96%的召回率，在恶劣天气条件下检测精度保持在88-92%，而传统可见光相机效果低于60%。端到端延迟低于100毫秒。

Conclusion: 该系统通过热成像和车联网技术为减少鹿车碰撞提供了可行的技术路径，具有实际应用价值。

Abstract: Deer-vehicle collisions represent a critical safety challenge in the United
States, causing nearly 2.1 million incidents annually and resulting in
approximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic
damages. These collisions also contribute significantly to declining deer
populations. This paper presents a real-time detection and driver warning
system that integrates thermal imaging, deep learning, and
vehicle-to-everything communication to help mitigate deer-vehicle collisions.
Our system was trained and validated on a custom dataset of over 12,000 thermal
deer images collected in Mars Hill, North Carolina. Experimental evaluation
demonstrates exceptional performance with 98.84 percent mean average precision,
95.44 percent precision, and 95.96 percent recall. The system was field tested
during a follow-up visit to Mars Hill and readily sensed deer providing the
driver with advanced warning. Field testing validates robust operation across
diverse weather conditions, with thermal imaging maintaining between 88 and 92
percent detection accuracy in challenging scenarios where conventional visible
light based cameras achieve less than 60 percent effectiveness. When a high
probability threshold is reached sensor data sharing messages are broadcast to
surrounding vehicles and roadside units via cellular vehicle to everything
(CV2X) communication devices. Overall, our system achieves end to end latency
consistently under 100 milliseconds from detection to driver alert. This
research establishes a viable technological pathway for reducing deer-vehicle
collisions through thermal imaging and connected vehicles.

</details>


### [70] [Towards Application Aligned Synthetic Surgical Image Synthesis](https://arxiv.org/abs/2509.18796)
*Danush Kumar Venkatesh,Stefanie Speidel*

Main category: cs.CV

TL;DR: SAADi框架通过将扩散模型与下游模型偏好的样本对齐，解决手术数据稀缺问题，提升分类和分割任务性能


<details>
  <summary>Details</summary>
Motivation: 手术标注数据稀缺限制了深度学习系统在计算机辅助干预中的发展，现有扩散模型存在数据记忆问题，导致样本不一致或缺乏多样性，可能损害下游性能

Method: 构建偏好和非偏好合成图像对，通过轻量级微调扩散模型，使图像生成过程与下游目标明确对齐

Result: 在三个手术数据集上，分类任务提升7-9%，分割任务提升2-10%，对代表性不足类别改进显著，迭代细化进一步提升4-10%性能

Conclusion: SAADi方法克服了样本退化问题，确立了任务感知对齐作为缓解数据稀缺和推进手术视觉应用的关键原则

Abstract: The scarcity of annotated surgical data poses a significant challenge for
developing deep learning systems in computer-assisted interventions. While
diffusion models can synthesize realistic images, they often suffer from data
memorization, resulting in inconsistent or non-diverse samples that may fail to
improve, or even harm, downstream performance. We introduce \emph{Surgical
Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion
models with samples preferred by downstream models. Our method constructs pairs
of \emph{preferred} and \emph{non-preferred} synthetic images and employs
lightweight fine-tuning of diffusion models to align the image generation
process with downstream objectives explicitly. Experiments on three surgical
datasets demonstrate consistent gains of $7$--$9\%$ in classification and
$2$--$10\%$ in segmentation tasks, with the considerable improvements observed
for underrepresented classes. Iterative refinement of synthetic samples further
boosts performance by $4$--$10\%$. Unlike baseline approaches, our method
overcomes sample degradation and establishes task-aware alignment as a key
principle for mitigating data scarcity and advancing surgical vision
applications.

</details>


### [71] [A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising](https://arxiv.org/abs/2509.18801)
*Kuang Xiaodong,Li Bingxuan,Li Yuan,Rao Fan,Ma Gege,Xie Qingguo,Mok Greta S P,Liu Huafeng,Zhu Wentao*

Main category: cs.CV

TL;DR: 提出基于核空间多维稀疏模型的神经网络方法KMDS-Net，用于动态PET图像去噪，通过端到端学习实现自适应参数优化，在模拟和真实数据上表现出优于基线方法的去噪性能。


<details>
  <summary>Details</summary>
Motivation: 动态PET中短时间帧的图像质量因统计量有限而难以保证，深度学习在医学图像去噪任务中已显示出广泛应用价值。

Method: 建立基于核空间的多维稀疏模型，利用动态PET的帧间空间相关性和帧内结构一致性，用神经网络替代参数估计过程，构建端到端的KMDS-Net。

Result: 在模拟和真实数据上的广泛实验表明，KMDS-Net在动态PET去噪方面表现出强大的性能，优于之前的基线方法。

Conclusion: 该方法可有效实现动态PET的高时间和空间分辨率，代码已开源。

Abstract: Achieving high image quality for temporal frames in dynamic positron emission
tomography (PET) is challenging due to the limited statistic especially for the
short frames. Recent studies have shown that deep learning (DL) is useful in a
wide range of medical image denoising tasks. In this paper, we propose a
model-based neural network for dynamic PET image denoising. The inter-frame
spatial correlation and intra-frame structural consistency in dynamic PET are
used to establish the kernel space-based multidimensional sparse (KMDS) model.
We then substitute the inherent forms of the parameter estimation with neural
networks to enable adaptive parameters optimization, forming the end-to-end
neural KMDS-Net. Extensive experimental results from simulated and real data
demonstrate that the neural KMDS-Net exhibits strong denoising performance for
dynamic PET, outperforming previous baseline methods. The proposed method may
be used to effectively achieve high temporal and spatial resolution for dynamic
PET. Our source code is available at
https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.

</details>


### [72] [Surgical Video Understanding with Label Interpolation](https://arxiv.org/abs/2509.18802)
*Garam Kim,Tae Kyeong Jeong,Juyoun Park*

Main category: cs.CV

TL;DR: 提出了一种结合光流分割标签插值和多任务学习的新框架，用于解决机器人辅助手术中视觉数据理解面临的时空不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助手术需要精确理解手术过程中的视觉数据，但现有方法多为单任务学习，难以处理复杂的手术场景动态和器械交互。此外，像素级分割数据标注成本高，导致长期标注（如手术阶段）和短期标注（如器械分割）之间存在时空不平衡。

Method: 使用基于光流的标签插值方法，从标注的关键帧中估计光流，并将标签传播到相邻未标注帧，从而丰富稀疏的空间监督信息。结合多任务学习框架，平衡时空信息用于训练。

Result: 该框架提高了手术场景理解的准确性和效率，增强了机器人辅助手术的实用性。

Conclusion: 通过光流标签插值和多任务学习的结合，有效解决了手术视觉数据理解中的时空不平衡问题，为机器人辅助手术提供了更全面的场景理解能力。

Abstract: Robot-assisted surgery (RAS) has become a critical paradigm in modern
surgery, promoting patient recovery and reducing the burden on surgeons through
minimally invasive approaches. To fully realize its potential, however, a
precise understanding of the visual data generated during surgical procedures
is essential. Previous studies have predominantly focused on single-task
approaches, but real surgical scenes involve complex temporal dynamics and
diverse instrument interactions that limit comprehensive understanding.
Moreover, the effective application of multi-task learning (MTL) requires
sufficient pixel-level segmentation data, which are difficult to obtain due to
the high cost and expertise required for annotation. In particular, long-term
annotations such as phases and steps are available for every frame, whereas
short-term annotations such as surgical instrument segmentation and action
detection are provided only for key frames, resulting in a significant
temporal-spatial imbalance. To address these challenges, we propose a novel
framework that combines optical flow-based segmentation label interpolation
with multi-task learning. optical flow estimated from annotated key frames is
used to propagate labels to adjacent unlabeled frames, thereby enriching sparse
spatial supervision and balancing temporal and spatial information for
training. This integration improves both the accuracy and efficiency of
surgical scene understanding and, in turn, enhances the utility of RAS.

</details>


### [73] [Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2509.18824)
*Yanzuo Lu,Xin Xia,Manlin Zhang,Huafeng Kuang,Jianbin Zheng,Yuxi Ren,Xuefeng Xiao*

Main category: cs.CV

TL;DR: Hyper-Bagel是一个统一的多模态加速框架，通过推测解码和多阶段蒸馏技术，同时加速多模态理解和生成任务，实现2倍以上的理解速度提升和16-22倍的生成速度提升。


<details>
  <summary>Details</summary>
Motivation: 随着多模态上下文包含越来越多的交错多模态标记，扩散去噪和自回归解码的迭代过程带来了显著的计算开销，需要高效的加速方案。

Method: 采用分治策略，使用推测解码进行下一个标记预测，并通过多阶段蒸馏过程加速扩散去噪。结合对抗蒸馏和人类反馈学习开发高效的1-NFE模型。

Result: 在多模态理解任务上实现超过2倍加速；在生成任务中，无损6-NFE模型实现文本到图像生成16.67倍加速和图像编辑22倍加速；1-NFE模型支持近乎实时的交互式编辑和生成。

Conclusion: Hyper-Bagel框架通过先进的加速技术，在保持原始模型高质量输出的同时，显著提升了多模态任务的效率，使复杂的多模态交互变得无缝且即时。

Abstract: Unified multimodal models have recently attracted considerable attention for
their remarkable abilities in jointly understanding and generating diverse
content. However, as contexts integrate increasingly numerous interleaved
multimodal tokens, the iterative processes of diffusion denoising and
autoregressive decoding impose significant computational overhead. To address
this, we propose Hyper-Bagel, a unified acceleration framework designed to
simultaneously speed up both multimodal understanding and generation tasks. Our
approach uses a divide-and-conquer strategy, employing speculative decoding for
next-token prediction and a multi-stage distillation process for diffusion
denoising. The framework delivers substantial performance gains, achieving over
a 2x speedup in multimodal understanding. For generative tasks, our resulting
lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a
22x speedup in image editing, all while preserving the high-quality output of
the original model. We further develop a highly efficient 1-NFE model that
enables near real-time interactive editing and generation. By combining
advanced adversarial distillation with human feedback learning, this model
achieves ultimate cost-effectiveness and responsiveness, making complex
multimodal interactions seamless and instantaneous.

</details>


### [74] [Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography](https://arxiv.org/abs/2509.18839)
*Gianmarco Spinaci,Lukas Klic,Giovanni Colavizza*

Main category: cs.CV

TL;DR: 评估多模态大语言模型和视觉语言模型在基督教圣像学单标签分类任务中的能力，发现GPT-4o和Gemini 2.5 Pro优于ResNet50基线，支持将LLMs作为数字人文工作流中的元数据管理工具。


<details>
  <summary>Details</summary>
Motivation: 评估通用多模态模型能否解释通常由监督分类器处理的圣像学图像，并评估其性能，为数字人文领域的元数据管理提供新工具。

Method: 使用ArtDL、ICONCLASS和Wikidata三个数据集，在三种条件下测试模型：仅类别标签、带Iconclass描述、少量样本学习。与ResNet50基线进行比较。

Result: Gemini-2.5 Pro和GPT-4o优于ResNet50基线；在Wikidata数据集上准确率显著下降；添加类别描述通常能提高零样本性能；少量学习效果较差。

Conclusion: 通用多模态LLMs能够在视觉复杂的文化遗产领域进行分类，支持将LLMs作为数字人文工作流中的元数据管理工具，未来需研究提示优化和其他分类策略。

Abstract: This study evaluates the capabilities of Multimodal Large Language Models
(LLMs) and Vision Language Models (VLMs) in the task of single-label
classification of Christian Iconography. The goal was to assess whether
general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,
can interpret the Iconography, typically addressed by supervised classifiers,
and evaluate their performance. Two research questions guided the analysis:
(RQ1) How do multimodal LLMs perform on image classification of Christian
saints? And (RQ2), how does performance vary when enriching input with
contextual information or few-shot exemplars? We conducted a benchmarking study
using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and
Wikidata, filtered to include the top 10 most frequent classes. Models were
tested under three conditions: (1) classification using class labels, (2)
classification with Iconclass descriptions, and (3) few-shot learning with five
exemplars. Results were compared against ResNet50 baselines fine-tuned on the
same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed
the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,
where Siglip reached the highest accuracy score, suggesting model sensitivity
to image size and metadata alignment. Enriching prompts with class descriptions
generally improved zero-shot performance, while few-shot learning produced
lower results, with only occasional and minimal increments in accuracy. We
conclude that general-purpose multimodal LLMs are capable of classification in
visually complex cultural heritage domains. These results support the
application of LLMs as metadata curation tools in digital humanities workflows,
suggesting future research on prompt optimization and the expansion of the
study to other classification strategies and models.

</details>


### [75] [ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction](https://arxiv.org/abs/2509.18840)
*Ismael Elsharkawi,Hossam Sharara,Ahmed Rafea*

Main category: cs.CV

TL;DR: 本文提出了一种可学习的重参数化图构建方法（LRGC），用于视觉图神经网络，通过可学习的注意力机制和软阈值重参数化来构建图像图表示，无需超参数搜索。


<details>
  <summary>Details</summary>
Motivation: 传统的ViG模型使用非参数化的统计方法构建图结构，无法为每个节点选择最佳邻域，且依赖超参数搜索。需要一种可学习的、无需超参数的图构建方法。

Method: LRGC方法在每对节点间应用键值注意力机制，然后使用软阈值重参数化进行边选择，通过可学习参数自动调整阈值，实现端到端的图构建训练。

Result: 在ImageNet-1k基准数据集上，ViG-LRGC方法在相似模型规模下优于最先进的ViG模型。

Conclusion: LRGC提供了一种可学习的、无需超参数搜索的图构建方法，能够更好地捕捉节点间关系，提升视觉图神经网络的性能。

Abstract: Image Representation Learning is an important problem in Computer Vision.
Traditionally, images were processed as grids, using Convolutional Neural
Networks or as a sequence of visual tokens, using Vision Transformers.
Recently, Vision Graph Neural Networks (ViG) have proposed the treatment of
images as a graph of nodes; which provides a more intuitive image
representation. The challenge is to construct a graph of nodes in each layer
that best represents the relations between nodes and does not need a
hyper-parameter search. ViG models in the literature depend on
non-parameterized and non-learnable statistical methods that operate on the
latent features of nodes to create a graph. This might not select the best
neighborhood for each node. Starting from k-NN graph construction to HyperGraph
Construction and Similarity-Thresholded graph construction, these methods lack
the ability to provide a learnable hyper-parameter-free graph construction
method. To overcome those challenges, we present the Learnable Reparameterized
Graph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies
key-query attention between every pair of nodes; then uses soft-threshold
reparameterization for edge selection, which allows the use of a differentiable
mathematical model for training. Using learnable parameters to select the
neighborhood removes the bias that is induced by any clustering or thresholding
methods previously introduced in the literature. In addition, LRGC allows
tuning the threshold in each layer to the training data since the thresholds
are learnable through training and are not provided as hyper-parameters to the
model. We demonstrate that the proposed ViG-LRGC approach outperforms
state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark
dataset.

</details>


### [76] [Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](https://arxiv.org/abs/2509.18847)
*Junhao Su,Yuanliang Wan,Junwei Yang,Hengyu Shi,Tianyang Han,Junfeng Luo,Yurui Qiu*

Main category: cs.CV

TL;DR: 提出结构化反思方法，将错误修复路径转化为明确、可控、可训练的动作，通过Reflect-Call-Final策略优化工具调用成功率


<details>
  <summary>Details</summary>
Motivation: 当前工具增强LLM的训练方法依赖监督模仿或粗粒度强化学习，自我反思实践基于启发式提示或单向推理，在多轮交互中脆弱且容易重复错误

Method: 结合DAPO和GSPO目标与针对工具使用的奖励方案，优化Reflect-Call-Final策略；引入Tool-Reflection-Bench基准进行程序化评估

Result: 在BFCL v3和Tool-Reflection-Bench上的实验显示多轮工具调用成功率和错误恢复能力显著提升，冗余调用减少

Conclusion: 使反思过程显式化并直接优化，可提高工具交互的可靠性，为智能体从失败中学习提供可复现路径

Abstract: Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.

</details>


### [77] [Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model](https://arxiv.org/abs/2509.18891)
*Xueyu Liu,Xiaoyi Zhang,Guangze Shi,Meilin Liu,Yexin Lai,Yongfei Wu,Mingqiang Wei*

Main category: cs.CV

TL;DR: 提出了Point Prompt Defender框架，采用攻击-防御的对抗强化学习范式来自动优化点提示，提升SAM的分割性能


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式或手动设计的提示，限制了可扩展性和泛化能力，需要自动优化提示质量的方法

Method: 构建任务无关的点提示环境，将图像块表示为双空间图中的节点，使用攻击者和防御者两个智能体进行对抗训练，基于分割质量变化进行DQN训练

Result: 实验表明该方法有效提升了SAM的鲁棒性和泛化能力，建立了灵活、可解释的即插即用框架

Conclusion: Point Prompt Defender为基于提示的分割提供了一个有效的自动优化框架，无需重新训练即可提升SAM性能

Abstract: Prompt quality plays a critical role in the performance of the Segment
Anything Model (SAM), yet existing approaches often rely on heuristic or
manually crafted prompts, limiting scalability and generalization. In this
paper, we propose Point Prompt Defender, an adversarial reinforcement learning
framework that adopts an attack-for-defense paradigm to automatically optimize
point prompts. We construct a task-agnostic point prompt environment by
representing image patches as nodes in a dual-space graph, where edges encode
both physical and semantic distances. Within this environment, an attacker
agent learns to activate a subset of prompts that maximally degrade SAM's
segmentation performance, while a defender agent learns to suppress these
disruptive prompts and restore accuracy. Both agents are trained using Deep
Q-Networks with a reward signal based on segmentation quality variation. During
inference, only the defender is deployed to refine arbitrary coarse prompt
sets, enabling enhanced SAM segmentation performance across diverse tasks
without retraining. Extensive experiments show that Point Prompt Defender
effectively improves SAM's robustness and generalization, establishing a
flexible, interpretable, and plug-and-play framework for prompt-based
segmentation.

</details>


### [78] [SmartWilds: Multimodal Wildlife Monitoring Dataset](https://arxiv.org/abs/2509.18894)
*Jenna Kline,Anirudh Potlapally,Bharath Pillai,Tanishka Wani,Rugved Katole,Vedant Patil,Penelope Covey,Hari Subramoni,Tanya Berger-Wolf,Christopher Stewart*

Main category: cs.CV

TL;DR: SmartWilds是一个多模态野生动物监测数据集，包含无人机图像、相机陷阱照片和视频以及生物声学记录，支持多模态AI研究用于全面环境监测。


<details>
  <summary>Details</summary>
Motivation: 解决濒危物种研究、保护生态学和栖息地管理中的关键需求，为保护计算机视觉研究提供开放数据集。

Method: 在俄亥俄州野生动物园进行为期四天的同步监测，覆盖220英亩牧场，收集三种模态的数据，并进行传感器模态性能比较分析。

Result: 展示了不同传感器模态在土地利用模式、物种检测、行为分析和栖息地监测方面的互补优势。

Conclusion: 建立了可重复的多模态野生动物监测协议，未来版本将包括同步GPS跟踪数据、公民科学数据和跨多个季节的扩展时间覆盖。

Abstract: We present the first release of SmartWilds, a multimodal wildlife monitoring
dataset. SmartWilds is a synchronized collection of drone imagery, camera trap
photographs and videos, and bioacoustic recordings collected during summer 2025
at The Wilds safari park in Ohio. This dataset supports multimodal AI research
for comprehensive environmental monitoring, addressing critical needs in
endangered species research, conservation ecology, and habitat management. Our
pilot deployment captured four days of synchronized monitoring across three
modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,
Przewalski's horses, as well as species native to Ohio, including bald eagles,
white-tailed deer, and coyotes. We provide a comparative analysis of sensor
modality performance, demonstrating complementary strengths for landuse
patterns, species detection, behavioral analysis, and habitat monitoring. This
work establishes reproducible protocols for multimodal wildlife monitoring
while contributing open datasets to advance conservation computer vision
research. Future releases will include synchronized GPS tracking data from
tagged individuals, citizen science data, and expanded temporal coverage across
multiple seasons.

</details>


### [79] [RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing](https://arxiv.org/abs/2509.18897)
*Jiayu Wang,Ruizhi Wang,Jie Song,Haofei Zhang,Mingli Song,Zunlei Feng,Li Sun*

Main category: cs.CV

TL;DR: 本文提出了RS3DBench基准数据集，包含54,951对遥感图像和像素级对齐的深度图，用于推动遥感图像3D视觉模型的发展，并基于稳定扩散技术开发了先进的深度估计模型。


<details>
  <summary>Details</summary>
Motivation: 现有遥感数据集缺乏全面的深度信息或深度数据与遥感图像之间的精确对齐，这限制了3D视觉模型在遥感领域的发展。

Method: 构建了包含54,951对遥感图像和像素级对齐深度图的RS3DBench数据集，并基于稳定扩散技术开发了遥感深度估计模型，利用其多模态融合能力。

Result: 提出的深度估计模型在RS3DBench数据集上实现了最先进的性能，为遥感3D视觉感知提供了有效的训练和评估工具。

Conclusion: RS3DBench数据集和相关模型将显著促进遥感领域3D视觉感知模型的发展和地理人工智能的进步，所有资源将在https://rs3dbench.github.io上公开。

Abstract: In this paper, we introduce a novel benchmark designed to propel the
advancement of general-purpose, large-scale 3D vision models for remote sensing
imagery. While several datasets have been proposed within the realm of remote
sensing, many existing collections either lack comprehensive depth information
or fail to establish precise alignment between depth data and remote sensing
images. To address this deficiency, we present a visual Benchmark for 3D
understanding of Remotely Sensed images, dubbed RS3DBench. This dataset
encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth
maps, accompanied by corresponding textual descriptions, spanning a broad array
of geographical contexts. It serves as a tool for training and assessing 3D
visual perception models within remote sensing image spatial understanding
tasks. Furthermore, we introduce a remotely sensed depth estimation model
derived from stable diffusion, harnessing its multimodal fusion capabilities,
thereby delivering state-of-the-art performance on our dataset. Our endeavor
seeks to make a profound contribution to the evolution of 3D visual perception
models and the advancement of geographic artificial intelligence within the
remote sensing domain. The dataset, models and code will be accessed on the
https://rs3dbench.github.io.

</details>


### [80] [DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring](https://arxiv.org/abs/2509.18898)
*Pengteng Li,Yunfan Lu,Pinhao Song,Weiyu Guo,Huizai Yao,F. Richard Yu,Hui Xiong*

Main category: cs.CV

TL;DR: DeblurSplat是首个无需SfM的基于事件相机的去模糊3D高斯泼溅方法，通过利用预训练的密集立体模块直接获取初始点云，并引入事件流进行精细监督，实现高效高质量的去模糊3D重建。


<details>
  <summary>Details</summary>
Motivation: 解决传统SfM方法在运动模糊场景中因相机位姿估计不准确导致的累积误差问题，以及利用事件相机对动态变化的高敏感性来提升去模糊效果。

Method: 1) 使用DUSt3R密集立体模块直接从模糊图像获取准确初始点云，避免相机位姿估计的中间步骤；2) 将事件流引入去模糊流程，通过解码潜在清晰图像为场景重建优化提供精细监督信号。

Result: 在多种场景下的实验表明，DeblurSplat不仅能生成高保真度的新视角图像，而且在渲染效率上显著优于当前最先进的去模糊3D-GS方法。

Conclusion: 该方法成功实现了无需SfM的高效去模糊3D重建，为运动模糊场景的3D视觉任务提供了新的解决方案。

Abstract: In this paper, we propose the first Structure-from-Motion (SfM)-free
deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
We address the motion-deblurring problem in two ways. First, we leverage the
pretrained capability of the dense stereo module (DUSt3R) to directly obtain
accurate initial point clouds from blurred images. Without calculating camera
poses as an intermediate result, we avoid the cumulative errors transfer from
inaccurate camera poses to the initial point clouds' positions. Second, we
introduce the event stream into the deblur pipeline for its high sensitivity to
dynamic change. By decoding the latent sharp images from the event stream and
blurred images, we can provide a fine-grained supervision signal for scene
reconstruction optimization. Extensive experiments across a range of scenes
demonstrate that DeblurSplat not only excels in generating high-fidelity novel
views but also achieves significant rendering efficiency compared to the SOTAs
in deblur 3D-GS.

</details>


### [81] [MoiréNet: A Compact Dual-Domain Network for Image Demoiréing](https://arxiv.org/abs/2509.18910)
*Shuwei Guo,Simin Luan,Yan Ke,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: MoiréNet是一种基于卷积神经U-Net的框架，通过协同整合频域和空间域特征来有效去除图像中的摩尔纹伪影。该方法引入了方向频率空间编码器和频率空间自适应选择器，在参数效率方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 摩尔纹是由显示像素晶格和相机传感器网格之间的频谱混叠产生的各向异性、多尺度伪影，对数字图像去摩尔纹提出了重大挑战。

Method: MoiréNet采用U-Net架构，包含两个关键组件：方向频率空间编码器（通过方向差分卷积识别摩尔纹方向）和频率空间自适应选择器（实现精确的特征自适应抑制）。

Result: 实验表明MoiréNet在公共数据集上达到最先进性能，仅需5.513M参数（比ESDNet-L减少48%），在恢复质量和参数效率方面表现优异。

Conclusion: MoiréNet结合了卓越的恢复质量和参数效率，特别适合资源受限的应用场景，如智能手机摄影、工业成像和增强现实。

Abstract: Moir\'e patterns arise from spectral aliasing between display pixel lattices
and camera sensor grids, manifesting as anisotropic, multi-scale artifacts that
pose significant challenges for digital image demoir\'eing. We propose
Moir\'eNet, a convolutional neural U-Net-based framework that synergistically
integrates frequency and spatial domain features for effective artifact
removal. Moir\'eNet introduces two key components: a Directional
Frequency-Spatial Encoder (DFSE) that discerns moir\'e orientation via
directional difference convolution, and a Frequency-Spatial Adaptive Selector
(FSAS) that enables precise, feature-adaptive suppression. Extensive
experiments demonstrate that Moir\'eNet achieves state-of-the-art performance
on public and actively used datasets while being highly parameter-efficient.
With only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,
Moir\'eNet combines superior restoration quality with parameter efficiency,
making it well-suited for resource-constrained applications including
smartphone photography, industrial imaging, and augmented reality.

</details>


### [82] [Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation](https://arxiv.org/abs/2509.18912)
*Yunzhe Shen,Kai Peng,Leiye Liu,Wei Ji,Jingjing Li,Miao Zhang,Yongri Piao,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了一个频率感知的音频-视觉分割框架FAVS，通过频率域分解和重组来解决音频和视觉模态在高频信号中的固有矛盾，实现了更精确的多模态分割。


<details>
  <summary>Details</summary>
Motivation: 现有的音频-视觉分割方法忽视了音频和视觉模态在频率域上的固有矛盾：音频高频信号普遍存在干扰噪声，而视觉高频信号包含丰富的结构细节。忽略这些差异会导致性能不佳。

Method: 提出FAVS框架，包含两个关键模块：频率域增强分解器(FDED)模块使用基于残差的迭代频率分解来区分模态特定的语义和结构特征；协同跨模态一致性(SCMC)模块采用专家混合架构，通过动态专家路由增强语义一致性和模态特定特征保留。

Result: 在三个基准数据集上的大量实验表明，FAVS框架实现了最先进的性能，丰富的定性可视化进一步验证了FDED和SCMC模块的有效性。

Conclusion: 将AVS任务重新定义为频率域分解和重组问题，提出的FAVS框架能够有效处理音频-视觉模态的频率域矛盾，显著提升分割性能。

Abstract: Audio-visual segmentation (AVS) plays a critical role in multimodal machine
learning by effectively integrating audio and visual cues to precisely segment
objects or regions within visual scenes. Recent AVS methods have demonstrated
significant improvements. However, they overlook the inherent frequency-domain
contradictions between audio and visual modalities--the pervasively interfering
noise in audio high-frequency signals vs. the structurally rich details in
visual high-frequency signals. Ignoring these differences can result in
suboptimal performance. In this paper, we rethink the AVS task from a deeper
perspective by reformulating AVS task as a frequency-domain decomposition and
recomposition problem. To this end, we introduce a novel Frequency-Aware
Audio-Visual Segmentation (FAVS) framework consisting of two key modules:
Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal
Consistency (SCMC) module. FDED module employs a residual-based iterative
frequency decomposition to discriminate modality-specific semantics and
structural features, and SCMC module leverages a mixture-of-experts
architecture to reinforce semantic consistency and modality-specific feature
preservation through dynamic expert routing. Extensive experiments demonstrate
that our FAVS framework achieves state-of-the-art performance on three
benchmark datasets, and abundant qualitative visualizations further verify the
effectiveness of the proposed FDED and SCMC modules. The code will be released
as open source upon acceptance of the paper.

</details>


### [83] [xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision](https://arxiv.org/abs/2509.18913)
*Nguyen Van Tu,Pham Nguyen Hai Long,Vo Hoai Viet*

Main category: cs.CV

TL;DR: 这篇论文综述了可解释人工智能（xAI）在视觉感知任务中的四种代表性方法：显著性图、概念瓶颈模型、基于原型的方法和混合方法，分析了它们的机制、优缺点及评估指标。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在图像分析中表现出色但成为"黑箱"，决策过程难以解释，在关键应用中存在可靠性问题。xAI领域旨在提供理解AI模型决策过程的方法。

Method: 采用文献综述方法，系统分析四种xAI方法：显著性图、概念瓶颈模型、原型方法和混合方法，比较它们的机制和评估指标。

Result: 提供了对四种xAI方法的全面分析，揭示了各自的优势和局限性，为未来研究和应用提供了指导框架。

Conclusion: xAI对于提高AI模型在视觉任务中的透明度和可信度至关重要，需要继续研究以改进现有方法并开发新的解释技术。

Abstract: Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance. However, this approach
often results in "black-box" models, whose decision-making processes are
difficult to interpret, raising concerns about reliability in critical
applications. To address this challenge and provide human a method to
understand how AI model process and make decision, the field of xAI has
emerged. This paper surveys four representative approaches in xAI for visual
perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),
(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their
underlying mechanisms, strengths and limitations, as well as evaluation
metrics, thereby providing a comprehensive overview to guide future research
and applications.

</details>


### [84] [LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2509.18917)
*Amirhesam Aghanouri,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 该论文提出了一种基于去噪扩散概率模型（DDPM）的方法，通过改进噪声调度和时间步嵌入技术来生成高质量合成LiDAR数据，用于增强自动驾驶车辆的3D视觉系统性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖LiDAR数据进行环境感知，但真实LiDAR数据采集耗时且易受噪声和稀疏性影响。需要生成高质量合成数据来提升计算机视觉任务性能。

Method: 使用改进的DDPM模型，引入新颖的噪声调度和时间步嵌入技术，优化去噪过程和时间感知能力，生成更真实的点云数据。

Result: 在IAMCV和KITTI-360数据集上的评估显示，该方法在四项性能指标上优于大多数现有基线方法，能有效缓解噪声和稀疏LiDAR数据的影响。

Conclusion: 提出的方法能够生成具有丰富空间关系和结构细节的多样化点云，显著提升了自动驾驶感知系统的性能。

Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by
improving efficiency and safety. Their success relies on 3D vision systems that
effectively sense the environment and detect traffic agents. Among sensors AVs
use to create a comprehensive view of surroundings, LiDAR provides
high-resolution depth data enabling accurate object detection, safe navigation,
and collision avoidance. However, collecting real-world LiDAR data is
time-consuming and often affected by noise and sparsity due to adverse weather
or sensor limitations. This work applies a denoising diffusion probabilistic
model (DDPM), enhanced with novel noise scheduling and time-step embedding
techniques to generate high-quality synthetic data for augmentation, thereby
improving performance across a range of computer vision tasks, particularly in
AV perception. These modifications impact the denoising process and the model's
temporal awareness, allowing it to produce more realistic point clouds based on
the projection. The proposed method was extensively evaluated under various
configurations using the IAMCV and KITTI-360 datasets, with four performance
metrics compared against state-of-the-art (SOTA) methods. The results
demonstrate the model's superior performance over most existing baselines and
its effectiveness in mitigating the effects of noisy and sparse LiDAR data,
producing diverse point clouds with rich spatial relationships and structural
detail.

</details>


### [85] [Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset](https://arxiv.org/abs/2509.18919)
*Chuni Liu,Hongjie Li,Jiaqi Du,Yangyang Hou,Qian Sun,Lei Jin,Ke Xu*

Main category: cs.CV

TL;DR: AGSSP是一种新颖的异常引导自监督预训练范式，通过异常先验指导表示学习，解决金属表面缺陷检测中预训练-微调范式的领域差距问题。


<details>
  <summary>Details</summary>
Motivation: 解决金属表面缺陷检测中预训练的两难困境：自然图像预训练存在领域差距，而传统自监督预训练难以区分细微缺陷与复杂背景噪声。

Method: 采用两阶段框架：1) 通过异常图知识蒸馏预训练骨干网络，捕获缺陷显著特征；2) 使用异常图生成的伪缺陷框预训练检测器，对齐定位任务。

Result: 在多种设置下AGSSP均能提升性能，相比基于ImageNet的模型，mAP@0.5提升高达10%，mAP@0.5:0.95提升11.4%。

Conclusion: AGSSP通过异常引导的自监督预训练有效解决了工业缺陷检测中的领域适应问题，显著提升了检测性能。

Abstract: The pretraining-finetuning paradigm is a crucial strategy in metallic surface
defect detection for mitigating the challenges posed by data scarcity. However,
its implementation presents a critical dilemma. Pretraining on natural image
datasets such as ImageNet, faces a significant domain gap. Meanwhile, naive
self-supervised pretraining on in-domain industrial data is often ineffective
due to the inability of existing learning objectives to distinguish subtle
defect patterns from complex background noise and textures. To resolve this, we
introduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm
that explicitly guides representation learning through anomaly priors. AGSSP
employs a two-stage framework: (1) it first pretrains the model's backbone by
distilling knowledge from anomaly maps, encouraging the network to capture
defect-salient features; (2) it then pretrains the detector using pseudo-defect
boxes derived from these maps, aligning it with localization tasks. To enable
this, we develop a knowledge-enhanced method to generate high-quality anomaly
maps and collect a large-scale industrial dataset of 120,000 images.
Additionally, we present two small-scale, pixel-level labeled metallic surface
defect datasets for validation. Extensive experiments demonstrate that AGSSP
consistently enhances performance across various settings, achieving up to a
10\% improvement in mAP@0.5 and 11.4\% in mAP@0.5:0.95 compared to
ImageNet-based models. All code, pretrained models, and datasets are publicly
available at https://clovermini.github.io/AGSSP-Dev/.

</details>


### [86] [Audio-Driven Universal Gaussian Head Avatars](https://arxiv.org/abs/2509.18924)
*Kartik Teotia,Helge Rhodin,Mohit Mendiratta,Hyeongwoo Kim,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 提出了首个音频驱动的通用逼真头像合成方法，结合了与人物无关的语音模型和新的通用头部头像先验（UHAP），能够同时处理几何变形和外观变化，实现高保真度的头像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要将音频特征映射到几何变形，而忽略了音频相关的外观变化。本文旨在开发一个能够同时处理几何和外观变化的通用音频驱动头像模型。

Method: 使用跨身份多视角视频训练UHAP，通过中性扫描数据进行监督学习。构建通用语音模型将原始音频输入直接映射到UHAP潜在表达空间，该空间编码了几何和外观变化。采用单目编码器进行个性化微调。

Result: 方法能够生成高度逼真的头像，具有精确的唇部同步和细腻的表情细节（如眉毛运动、视线转移、真实的口腔内部外观和运动）。在唇同步准确性、图像质量和感知真实感方面优于现有几何方法。

Conclusion: 这是首个能够进行详细外观建模和渲染的通用音频驱动头像模型，在多个评估指标上均优于竞争对手的几何方法。

Abstract: We introduce the first method for audio-driven universal photorealistic
avatar synthesis, combining a person-agnostic speech model with our novel
Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity
multi-view videos. In particular, our UHAP is supervised with neutral scan
data, enabling it to capture the identity-specific details at high fidelity. In
contrast to previous approaches, which predominantly map audio features to
geometric deformations only while ignoring audio-dependent appearance
variations, our universal speech model directly maps raw audio inputs into the
UHAP latent expression space. This expression space inherently encodes, both,
geometric and appearance variations. For efficient personalization to new
subjects, we employ a monocular encoder, which enables lightweight regression
of dynamic expression variations across video frames. By accounting for these
expression-dependent changes, it enables the subsequent model fine-tuning stage
to focus exclusively on capturing the subject's global appearance and geometry.
Decoding these audio-driven expression codes via UHAP generates highly
realistic avatars with precise lip synchronization and nuanced expressive
details, such as eyebrow movement, gaze shifts, and realistic mouth interior
appearance as well as motion. Extensive evaluations demonstrate that our method
is not only the first generalizable audio-driven avatar model that can account
for detailed appearance modeling and rendering, but it also outperforms
competing (geometry-only) methods across metrics measuring lip-sync accuracy,
quantitative image quality, and perceptual realism.

</details>


### [87] [SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines](https://arxiv.org/abs/2509.18926)
*Pamela Osuna-Vargas,Altug Kamacioglu,Dominik F. Aschauer,Petros E. Vlachos,Sercan Alipek,Jochen Triesch,Simon Rumpel,Matthias Kaschube*

Main category: cs.CV

TL;DR: 本文提出了一个基于机器学习的模块化管道，用于自动检测、跟踪和提取树突棘在3D+时间显微镜数据中的特征，解决了大规模分析树突棘结构动力学的挑战。


<details>
  <summary>Details</summary>
Motivation: 树突棘是大脑中兴奋性突触的关键结构组成部分，其大小可作为突触效能的指标。然而，在3D+时间显微镜数据中对树突棘结构动力学进行大规模分析仍然具有挑战性且劳动密集。

Method: 采用模块化机器学习管道，结合基于transformer的检测模块、集成空间特征的深度跟踪组件、利用空间一致性关联3D树突棘的时间跟踪模块，以及量化生物学相关脊柱特征的特征提取单元。

Result: 在开源标记的脊柱数据和两个新发布的注释数据集上验证了该方法，这些数据集包括检测和深度跟踪数据以及时间跟踪数据（据作者所知是此类数据的首次发布）。

Conclusion: 该方法为可扩展的端到端树突棘动力学分析建立了基准，作者发布了数据、代码和预训练权重以促进未来研究。

Abstract: Dendritic spines are key structural components of excitatory synapses in the
brain. Given the size of dendritic spines provides a proxy for synaptic
efficacy, their detection and tracking across time is important for studies of
the neural basis of learning and memory. Despite their relevance, large-scale
analyses of the structural dynamics of dendritic spines in 3D+time microscopy
data remain challenging and labor-intense. Here, we present a modular machine
learning-based pipeline designed to automate the detection, time-tracking, and
feature extraction of dendritic spines in volumes chronically recorded with
two-photon microscopy. Our approach tackles the challenges posed by biological
data by combining a transformer-based detection module, a depth-tracking
component that integrates spatial features, a time-tracking module to associate
3D spines across time by leveraging spatial consistency, and a feature
extraction unit that quantifies biologically relevant spine properties. We
validate our method on open-source labeled spine data, and on two complementary
annotated datasets that we publish alongside this work: one for detection and
depth-tracking, and one for time-tracking, which, to the best of our knowledge,
is the first data of this kind. To encourage future research, we release our
data, code, and pre-trained weights at
https://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,
end-to-end analysis of dendritic spine dynamics.

</details>


### [88] [No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning](https://arxiv.org/abs/2509.18938)
*Matheus Vinícius Todescato,Joel Luís Carbonera*

Main category: cs.CV

TL;DR: 提出一种结合视觉语言模型和预训练视觉模型的自学习零样本图像分类框架，无需标注数据，仅使用类别名称即可训练轻量级分类器


<details>
  <summary>Details</summary>
Motivation: 深度学习通常依赖大量标注数据，但在实际应用中标注数据稀缺。视觉语言模型和预训练视觉模型为解决这一问题提供了可能

Method: 使用置信度伪标签策略，在测试数据上直接训练轻量级分类器。VLM识别高置信度样本，预训练视觉模型增强其特征表示，通过迭代训练捕获互补的语义和视觉线索

Result: 在十个多样化数据集上的实验表明，该方法优于基线零样本方法

Conclusion: 该方法无需VLM微调或大型语言模型，仅依赖视觉模型减少对语义表示的依赖，实现了动态适应的零样本分类

Abstract: While deep learning, including Convolutional Neural Networks (CNNs) and
Vision Transformers (ViTs), has significantly advanced classification
performance, its typical reliance on extensive annotated datasets presents a
major obstacle in many practical scenarios where such data is scarce.
Vision-language models (VLMs) and transfer learning with pre-trained visual
models appear as promising techniques to deal with this problem. This paper
proposes a novel zero-shot image classification framework that combines a VLM
and a pre-trained visual model within a self-learning cycle. Requiring only the
set of class names and no labeled training data, our method utilizes a
confidence-based pseudo-labeling strategy to train a lightweight classifier
directly on the test data, enabling dynamic adaptation. The VLM identifies
high-confidence samples, and the pre-trained visual model enhances their visual
representations. These enhanced features then iteratively train the classifier,
allowing the system to capture complementary semantic and visual cues without
supervision. Notably, our approach avoids VLM fine-tuning and the use of large
language models, relying on the visual-only model to reduce the dependence on
semantic representation. Experimental evaluations on ten diverse datasets
demonstrate that our approach outperforms the baseline zero-shot method.

</details>


### [89] [Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting](https://arxiv.org/abs/2509.18956)
*Zijing Guo,Yunyang Zhao,Lin Wang*

Main category: cs.CV

TL;DR: 本文提出了MirrorScene3D数据集和ReflectiveGS方法，用于解决含镜面环境中的3D重建和新视角合成问题，通过利用镜面反射作为补充视角来提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法如NeRF和3DGS在镜面环境中性能下降，主要关注镜面表面的对称映射，但忽略了镜面反射携带的丰富信息，这些反射可以提供补充视角来填补缺失细节。

Method: 提出了ReflectiveGS方法，基于3D高斯泼溅技术，将镜面反射作为补充视角而非简单的对称伪影，从而增强场景几何和恢复缺失细节。

Result: 在MirrorScene3D数据集上的实验表明，ReflectiveGS在SSIM、PSNR、LPIPS和训练速度方面均优于现有方法。

Conclusion: ReflectiveGS为镜面丰富环境中的3D重建设定了新的基准，展示了利用镜面反射作为补充视角的有效性。

Abstract: Mirror-containing environments pose unique challenges for 3D reconstruction
and novel view synthesis (NVS), as reflective surfaces introduce view-dependent
distortions and inconsistencies. While cutting-edge methods such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical
scenes, their performance deteriorates in the presence of mirrors. Existing
solutions mainly focus on handling mirror surfaces through symmetry mapping but
often overlook the rich information carried by mirror reflections. These
reflections offer complementary perspectives that can fill in absent details
and significantly enhance reconstruction quality. To advance 3D reconstruction
in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset
featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror
masks, providing a benchmark for evaluating reconstruction methods in
reflective settings. Building on this, we propose ReflectiveGS, an extension of
3D Gaussian Splatting that utilizes mirror reflections as complementary
viewpoints rather than simple symmetry artifacts, enhancing scene geometry and
recovering absent details. Experiments on MirrorScene3D show that
ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and
training speed, setting a new benchmark for 3D reconstruction in mirror-rich
environments.

</details>


### [90] [Generative data augmentation for biliary tract detection on intraoperative images](https://arxiv.org/abs/2509.18958)
*Cristina Iacono,Mariarosaria Meola,Federica Conte,Laura Mecozzi,Umberto Bracale,Pietro Falco,Fanny Ficuciello*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于深度学习的胆道定位方法，通过Yolo检测算法和GAN生成合成训练数据，旨在提高腹腔镜胆囊切除术中胆管的可视化，从而降低胆管损伤风险。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜胆囊切除术虽然恢复快、美容效果好，但胆管损伤风险较高，严重影响患者生活质量和生存率。为了降低这一风险，需要改进术中胆管的可视化。

Method: 构建并标注图像数据库训练Yolo检测算法，采用经典数据增强技术，并提出使用生成对抗网络（GAN）生成部分合成训练数据集。

Result: 实验结果表明，该方法能够有效定位胆道结构，但具体性能指标未在摘要中详细说明。

Conclusion: 该研究为腹腔镜手术中的胆管识别提供了可行的深度学习解决方案，同时讨论了相关的伦理考量。

Abstract: Cholecystectomy is one of the most frequently performed procedures in
gastrointestinal surgery, and the laparoscopic approach is the gold standard
for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the
advantages of a significantly faster recovery and better cosmetic results, the
laparoscopic approach bears a higher risk of bile duct injury, which has a
significant impact on quality of life and survival. To avoid bile duct injury,
it is essential to improve the intraoperative visualization of the bile duct.
This work aims to address this problem by leveraging a deep-learning approach
for the localization of the biliary tract from white-light images acquired
during the surgical procedures. To this end, the construction and annotation of
an image database to train the Yolo detection algorithm has been employed.
Besides classical data augmentation techniques, the paper proposes Generative
Adversarial Network (GAN) for the generation of a synthetic portion of the
training dataset. Experimental results have been discussed along with ethical
considerations.

</details>


### [91] [Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images](https://arxiv.org/abs/2509.18973)
*Jiabao Chen,Shan Xiong,Jialin Peng*

Main category: cs.CV

TL;DR: 提出Prompt-DAS框架，通过多任务学习和提示机制实现电子显微镜图像中细胞器的领域自适应分割，支持无监督、弱监督和交互式分割


<details>
  <summary>Details</summary>
Motivation: 解决大规模电子显微镜图像中细胞器实例分割需要大量标注数据的问题，实现标注高效的学习

Method: 基于SAM的提示式多任务框架，结合辅助中心点检测任务和提示引导对比学习，支持不同提示配置（全点、稀疏点、无点）

Result: 在多个挑战性基准测试中优于现有的UDA、WDA和基于SAM的方法

Conclusion: Prompt-DAS是一个灵活有效的领域自适应分割框架，能够显著减少标注需求并提高分割性能

Abstract: Domain adaptive segmentation (DAS) of numerous organelle instances from
large-scale electron microscopy (EM) is a promising way to enable
annotation-efficient learning. Inspired by SAM, we propose a promptable
multitask framework, namely Prompt-DAS, which is flexible enough to utilize any
number of point prompts during the adaptation training stage and testing stage.
Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised
domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well
as interactive segmentation during testing. Unlike the foundation model SAM,
which necessitates a prompt for each individual object instance, Prompt-DAS is
only trained on a small dataset and can utilize full points on all instances,
sparse points on partial instances, or even no points at all, facilitated by
the incorporation of an auxiliary center-point detection task. Moreover, a
novel prompt-guided contrastive learning is proposed to enhance discriminative
feature learning. Comprehensive experiments conducted on challenging benchmarks
demonstrate the effectiveness of the proposed approach over existing UDA, WDA,
and SAM-based approaches.

</details>


### [92] [VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](https://arxiv.org/abs/2509.19002)
*Hao Wang,Eiki Murata,Lingfang Zhang,Ayako Sato,So Fukuda,Ziqi Yin,Wentao Hu,Keisuke Nakao,Yusuke Nakamura,Sebastian Zwirner,Yi-Chia Chen,Hiroyuki Otomo,Hiroki Ouchi,Daisuke Kawahara*

Main category: cs.CV

TL;DR: 提出了VIR-Bench基准测试，包含200个旅行视频，用于评估多模态大语言模型在长距离地理时空轨迹理解方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前视频基准测试主要关注室内场景或短距离户外活动，缺乏对长距离旅行挑战的探索，而掌握扩展的地理时空轨迹对于下一代MLLMs至关重要。

Method: 构建包含200个旅行视频的VIR-Bench基准，将行程重建作为评估任务，并开发原型旅行规划代理进行案例研究。

Result: 实验表明最先进的MLLMs（包括专有模型）在该基准上表现不佳，验证了处理扩展时空尺度视频的难度。

Conclusion: VIR-Bench不仅能有效评估模型性能，其评估协议还能转化为用户应用中的具体性能提升，为MLLMs的地理时空智能发展提供重要基准。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.

</details>


### [93] [Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards](https://arxiv.org/abs/2509.19003)
*Honghao Chen,Xingzhou Lou,Xiaokun Feng,Kaiqi Huang,Xinlong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种用于视觉语言模型的链式逐步推理框架，通过细粒度奖励评估和强化学习提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言推理方法通常在粗粒度层面使用推理链，难以进行细粒度结构化推理，且难以评估中间推理步骤的质量和奖励。

Method: 提出了一个简单有效的透明框架，包括步骤级推理数据、过程奖励模型（PRM）和强化学习训练，实现细粒度奖励评估和推理时间缩放。

Result: 模型在具有挑战性的视觉语言基准测试中取得了显著的改进，建立了强大的基线。

Conclusion: 本文为视觉语言模型提供了基准，并为更复杂的多模态推理提供了见解，相关数据集、PRM和代码将开源。

Abstract: Chain of thought reasoning has demonstrated remarkable success in large
language models, yet its adaptation to vision-language reasoning remains an
open challenge with unclear best practices. Existing attempts typically employ
reasoning chains at a coarse-grained level, which struggles to perform
fine-grained structured reasoning and, more importantly, are difficult to
evaluate the reward and quality of intermediate reasoning. In this work, we
delve into chain of step reasoning for vision-language models, enabling
assessing reasoning step quality accurately and leading to effective
reinforcement learning and inference-time scaling with fine-grained rewards. We
present a simple, effective, and fully transparent framework, including the
step-level reasoning data, process reward model (PRM), and reinforcement
learning training. With the proposed approaches, our models set strong
baselines with consistent improvements on challenging vision-language
benchmarks. More importantly, we conduct a thorough empirical analysis and
ablation study, unveiling the impact of each component and several intriguing
properties of inference-time scaling. We believe this paper serves as a
baseline for vision-language models and offers insights into more complex
multimodal reasoning. Our dataset, PRM, and code will be available at
https://github.com/baaivision/CoS.

</details>


### [94] [Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model](https://arxiv.org/abs/2509.19028)
*Ioannis Sarafis,Alexandros Papadopoulos,Anastasios Delopoulos*

Main category: cs.CV

TL;DR: 提出一种基于SAM和ViT的弱监督食物图像语义分割方法，仅需图像级标注即可生成高质量分割掩码


<details>
  <summary>Details</summary>
Motivation: 利用SAM的零样本能力和ViT的注意力机制，避免对食物图像进行像素级标注的繁琐工作

Method: 使用Swin Transformer生成类激活图作为SAM的提示，结合图像预处理技术和单/多掩码生成策略

Result: 在FoodSeg103数据集上达到mIoU 0.54，每张图像平均生成2.4个掩码（不含背景）

Conclusion: 该方法可作为加速食物图像标注的工具，或集成到食物营养追踪应用中

Abstract: In this paper, we propose a weakly supervised semantic segmentation approach
for food images which takes advantage of the zero-shot capabilities and
promptability of the Segment Anything Model (SAM) along with the attention
mechanisms of Vision Transformers (ViTs). Specifically, we use class activation
maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable
for food image segmentation. The ViT model, a Swin Transformer, is trained
exclusively using image-level annotations, eliminating the need for pixel-level
annotations during training. Additionally, to enhance the quality of the
SAM-generated masks, we examine the use of image preprocessing techniques in
combination with single-mask and multi-mask SAM generation strategies. The
methodology is evaluated on the FoodSeg103 dataset, generating an average of
2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for
the multi-mask scenario. We envision the proposed approach as a tool to
accelerate food image annotation tasks or as an integrated component in food
and nutrition tracking applications.

</details>


### [95] [A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation](https://arxiv.org/abs/2509.19052)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: DyL-UNet是一种基于动态学习的时间一致性U-Net分割架构，旨在实现超声心动图分割的时间稳定性和精确性，通过构建回声动态图和引入心脏相位动态注意力机制来解决帧间分割抖动问题。


<details>
  <summary>Details</summary>
Motivation: 超声心动图容易发生变形和散斑噪声，导致帧间分割抖动。即使单帧分割精度高，时间不稳定性也会削弱功能估计并影响临床解释性。

Method: 提出DyL-UNet框架，通过动态学习构建回声动态图提取视频动态信息，采用多个基于Swin-Transformer的编码器-解码器分支处理单帧图像，在跳跃连接处引入心脏相位动态注意力机制，利用动态特征和心脏相位线索强制时间一致性。

Result: 在CAMUS和EchoNet-Dynamic数据集上的广泛实验表明，DyL-UNet在保持与现有方法相当的分割精度的同时，实现了更优的时间一致性。

Conclusion: DyL-UNet为自动化临床超声心动图提供了可靠的解决方案，能够实现时间稳定且精确的分割。

Abstract: Accurate segmentation of cardiac anatomy in echocardiography is essential for
cardiovascular diagnosis and treatment. Yet echocardiography is prone to
deformation and speckle noise, causing frame-to-frame segmentation jitter. Even
with high accuracy in single-frame segmentation, temporal instability can
weaken functional estimates and impair clinical interpretability. To address
these issues, we propose DyL-UNet, a dynamic learning-based temporal
consistency U-Net segmentation architecture designed to achieve temporally
stable and precise echocardiographic segmentation. The framework constructs an
Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic
information from videos. DyL-UNet incorporates multiple Swin-Transformer-based
encoder-decoder branches for processing single-frame images. It further
introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,
which uses EDG-encoded dynamic features and cardiac-phase cues to enforce
temporal consistency during segmentation. Extensive experiments on the CAMUS
and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation
accuracy comparable to existing methods while achieving superior temporal
consistency, providing a reliable solution for automated clinical
echocardiography.

</details>


### [96] [ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?](https://arxiv.org/abs/2509.19070)
*Zijian Ling,Han Zhang,Yazhuo Zhou,Jiahao Cui*

Main category: cs.CV

TL;DR: ColorBlindnessEval是一个新颖的基准测试，用于评估视觉语言模型在受色盲测试启发的视觉对抗场景中的鲁棒性。该数据集包含500张类似石原色盲测试的图像，挑战模型识别复杂视觉模式中嵌入的数字信息。


<details>
  <summary>Details</summary>
Motivation: 评估VLMs在视觉对抗场景中的鲁棒性，特别是在受色盲测试启发的复杂视觉环境中准确识别数字信息的能力。

Method: 创建包含500张石原色盲测试风格图像的数据集，数字范围0-99，使用不同颜色组合。评估9个VLMs模型，采用是/否和开放式提示，并与人类参与者表现进行对比。

Result: 实验显示模型在对抗性环境中解释数字的能力存在局限性，存在普遍的幻觉问题。模型表现不如人类参与者。

Conclusion: 研究结果强调了在复杂视觉环境中提高VLMs鲁棒性的必要性。ColorBlindnessEval可作为基准工具，帮助提升VLMs在需要高准确性的实际应用中的可靠性。

Abstract: This paper presents ColorBlindnessEval, a novel benchmark designed to
evaluate the robustness of Vision-Language Models (VLMs) in visually
adversarial scenarios inspired by the Ishihara color blindness test. Our
dataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with
varying color combinations, challenging VLMs to accurately recognize numerical
information embedded in complex visual patterns. We assess 9 VLMs using Yes/No
and open-ended prompts and compare their performance with human participants.
Our experiments reveal limitations in the models' ability to interpret numbers
in adversarial contexts, highlighting prevalent hallucination issues. These
findings underscore the need to improve the robustness of VLMs in complex
visual environments. ColorBlindnessEval serves as a valuable tool for
benchmarking and improving the reliability of VLMs in real-world applications
where accuracy is critical.

</details>


### [97] [WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction](https://arxiv.org/abs/2509.19073)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: WaveletGaussian是一个高效稀疏视图3D高斯对象重建框架，通过将扩散过程转移到小波域，仅在低分辨率子带应用扩散，高频子带使用轻量网络优化，显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射在稀疏视图设置下性能急剧下降，现有方法使用扩散模型修复损坏的渲染但计算成本高昂。

Method: 将扩散过程转移到小波域，仅在LL子带应用扩散，高频子带使用轻量网络优化；提出在线随机掩码策略替代低效的留一法。

Result: 在Mip-NeRF 360和OmniObject3D数据集上的实验表明，WaveletGaussian在保持竞争性渲染质量的同时大幅减少训练时间。

Conclusion: WaveletGaussian通过小波域扩散和高效训练策略，实现了稀疏视图3D高斯重建的高效优化。

Abstract: 3D Gaussian Splatting (3DGS) has become a powerful representation for
image-based object reconstruction, yet its performance drops sharply in
sparse-view settings. Prior works address this limitation by employing
diffusion models to repair corrupted renders, subsequently using them as pseudo
ground truths for later optimization. While effective, such approaches incur
heavy computation from the diffusion fine-tuning and repair steps. We present
WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object
reconstruction. Our key idea is to shift diffusion into the wavelet domain:
diffusion is applied only to the low-resolution LL subband, while
high-frequency subbands are refined with a lightweight network. We further
propose an efficient online random masking strategy to curate training pairs
for diffusion fine-tuning, replacing the commonly used, but inefficient,
leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360
and OmniObject3D, show WaveletGaussian achieves competitive rendering quality
while substantially reducing training time.

</details>


### [98] [3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference](https://arxiv.org/abs/2509.19082)
*Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: Sa2VA-i是对Sa2VA模型的改进版本，通过修正训练和推理过程中的不一致性，显著提升了在视频对象分割任务上的性能，在多个基准测试中创造了新的最先进结果。


<details>
  <summary>Details</summary>
Motivation: 发现Sa2VA模型在视频对象分割任务中未能充分发挥潜力，主要原因是训练和推理过程存在不一致性，限制了模型性能。

Method: 提出Sa2VA-i模型，通过修正训练和推理过程中的不一致性问题来改进原Sa2VA模型。

Result: Sa2VA-i在多个视频基准测试中创造了新的最先进结果：MeViS提升+11.6 J&F，Ref-YT-VOS提升+1.4，Ref-DAVIS提升+3.3，ReVOS提升+4.1。Sa2VA-i-1B模型在MeViS基准上与原始Sa2VA-26B模型性能相当。

Conclusion: 这项工作强调了看似微不足道的实现细节的重要性，为视频分割领域提供了有价值的见解。代码和更新模型已开源。

Abstract: Sa2VA is a recent model for language-guided dense grounding in images and
video that achieves state-of-the-art results on multiple segmentation
benchmarks and that has become widely popular. However, we found that Sa2VA
does not perform according to its full potential for referring video object
segmentation tasks. We identify inconsistencies between training and inference
procedures as the key factor holding it back. To mitigate this issue, we
propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and
improves the results. In fact, Sa2VA-i sets a new state of the art for multiple
video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on
Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA
checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the
original Sa2VA-26B model on the MeViS benchmark. We hope that this work will
show the importance of seemingly trivial implementation details and that it
will provide valuable insights for the referring video segmentation field. We
provide the code and updated models at https://github.com/kumuji/sa2va-i

</details>


### [99] [Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications](https://arxiv.org/abs/2509.19087)
*Ganesh Mallya,Yotam Gigi,Dahun Kim,Maxim Neumann,Genady Beryozkin,Tomer Shekel,Anelia Angelova*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，将多光谱数据作为零样本输入引入到仅训练于RGB数据的通用多模态模型中，使这些模型能够理解专业的多光谱信号。


<details>
  <summary>Details</summary>
Motivation: 多光谱图像在遥感应用中很重要，但现有的机器学习模型需要专门训练且成本高。通用多模态模型虽然强大，但无法处理多光谱输入。

Method: 利用多模态模型对视觉空间的理解，将输入适配到该空间，并将领域特定信息作为指令注入模型，以零样本方式处理多光谱数据。

Result: 在Gemini2.5模型上验证，在土地覆盖和土地利用分类的遥感基准测试中获得了显著的零样本性能提升。

Conclusion: 该方法展示了地理空间专业人员可以轻松利用强大多模态模型来处理非标准专业输入，加速工作并受益于其丰富的推理能力。

Abstract: Multi-spectral imagery plays a crucial role in diverse Remote Sensing
applications including land-use classification, environmental monitoring and
urban planning. These images are widely adopted because their additional
spectral bands correlate strongly with physical materials on the ground, such
as ice, water, and vegetation. This allows for more accurate identification,
and their public availability from missions, such as Sentinel-2 and Landsat,
only adds to their value. Currently, the automatic analysis of such data is
predominantly managed through machine learning models specifically trained for
multi-spectral input, which are costly to train and support. Furthermore,
although providing a lot of utility for Remote Sensing, such additional inputs
cannot be used with powerful generalist large multimodal models, which are
capable of solving many visual problems, but are not able to understand
specialized multi-spectral signals.
  To address this, we propose a training-free approach which introduces new
multi-spectral data in a Zero-Shot-only mode, as inputs to generalist
multimodal models, trained on RGB-only inputs. Our approach leverages the
multimodal models' understanding of the visual space, and proposes to adapt to
inputs to that space, and to inject domain-specific information as instructions
into the model. We exemplify this idea with the Gemini2.5 model and observe
strong Zero-Shot performance gains of the approach on popular Remote Sensing
benchmarks for land cover and land use classification and demonstrate the easy
adaptability of Gemini2.5 to new inputs. These results highlight the potential
for geospatial professionals, working with non-standard specialized inputs, to
easily leverage powerful multimodal models, such as Gemini2.5, to accelerate
their work, benefiting from their rich reasoning and contextual capabilities,
grounded in the specialized sensor data.

</details>


### [100] [Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning](https://arxiv.org/abs/2509.19090)
*Guoxin Wang,Jun Zhao,Xinyi Liu,Yanbo Liu,Xuyang Cao,Chao Li,Zhuoyun Liu,Qintian Sun,Fangru Zhou,Haoqiang Xing,Zhenhong Yang*

Main category: cs.CV

TL;DR: Citrus-V是一个多模态医学基础模型，结合图像分析和文本推理，在单一框架中实现病变定位、分割和诊断推理，超越现有开源医学模型。


<details>
  <summary>Details</summary>
Motivation: 现有医学成像模型过于专业化且泛化能力有限，而临床应用需要精确的视觉定位、多模态整合和推理能力。

Method: 提出新颖的多模态训练方法，整合检测、分割和多模态链式推理，并发布开源数据集。

Result: 在多个基准测试中超越现有开源医学模型和专家级成像系统，支持精确病变量化、自动报告生成和可靠第二意见。

Conclusion: Citrus-V提供了从视觉定位到临床推理的统一流程，为临床诊断和治疗决策提供有力支持。

Abstract: Medical imaging provides critical evidence for clinical diagnosis, treatment
planning, and surgical decisions, yet most existing imaging models are narrowly
focused and require multiple specialized networks, limiting their
generalization. Although large-scale language and multimodal models exhibit
strong reasoning and multi-task capabilities, real-world clinical applications
demand precise visual grounding, multimodal integration, and chain-of-thought
reasoning. We introduce Citrus-V, a multimodal medical foundation model that
combines image analysis with textual reasoning. The model integrates detection,
segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level
lesion localization, structured report generation, and physician-like
diagnostic inference in a single framework. We propose a novel multimodal
training approach and release a curated open-source data suite covering
reasoning, detection, segmentation, and document understanding tasks.
Evaluations demonstrate that Citrus-V outperforms existing open-source medical
models and expert-level imaging systems across multiple benchmarks, delivering
a unified pipeline from visual grounding to clinical reasoning and supporting
precise lesion quantification, automated reporting, and reliable second
opinions.

</details>


### [101] [Investigating Traffic Accident Detection Using Multimodal Large Language Models](https://arxiv.org/abs/2509.19096)
*Ilhan Skender,Kailin Tong,Selim Solmaz,Daniel Watzenig*

Main category: cs.CV

TL;DR: 该研究评估了多模态大语言模型在零样本条件下检测和描述交通事故的能力，通过整合YOLO、Deep SORT和SAM等视觉分析技术提升模型性能，在模拟数据集上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 交通事故检测对公共安全至关重要，但缺乏多样化的基础设施摄像头事故数据。研究旨在利用MLLMs的零样本能力，减少对大量标注数据的依赖，实现实时自动化监控。

Method: 使用CARLA模拟的DeepAccident数据集，评估Gemini 1.5/2.0、Gemma 3和Pixtral等MLLMs的零样本性能，并整合YOLO、Deep SORT和SAM进行视觉分析增强提示。

Result: Pixtral表现最佳（F1-score 0.71，召回率83%），Gemini模型通过增强提示精度提升至90%但F1和召回率下降，Gemma 3性能最平衡。

Conclusion: MLLMs与先进视觉分析技术结合在自动化交通监控系统中具有巨大应用潜力，能够有效提升事故检测的准确性和可解释性。

Abstract: Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of acci-
dents directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.

</details>


### [102] [Track-On2: Enhancing Online Point Tracking with Memory](https://arxiv.org/abs/2509.19115)
*Görkay Aydemir,Weidi Xie,Fatma Güney*

Main category: cs.CV

TL;DR: Track-On2是一个基于Transformer的在线长期点跟踪模型，通过架构优化、内存机制改进和合成训练策略提升性能，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决长期点跟踪问题，需要在视频帧间保持点的一致性识别，应对显著的外观变化、运动和遮挡，特别针对在线实时应用场景。

Method: 扩展Track-On模型为Track-On2，采用因果处理框架，通过内存机制维持时间一致性，使用粗粒度补丁级分类后细化的推理策略，并系统研究合成训练对内存行为的影响。

Result: 在五个合成和真实世界基准测试中取得最先进结果，超越现有在线跟踪器甚至利用双向上下文的离线方法。

Conclusion: 基于因果处理和内存机制的架构，结合纯合成数据训练，为实际点跟踪问题提供了可扩展的有效解决方案。

Abstract: In this paper, we consider the problem of long-term point tracking, which
requires consistent identification of points across video frames under
significant appearance changes, motion, and occlusion. We target the online
setting, i.e. tracking points frame-by-frame, making it suitable for real-time
and streaming applications. We extend our prior model Track-On into Track-On2,
a simple and efficient transformer-based model for online long-term tracking.
Track-On2 improves both performance and efficiency through architectural
refinements, more effective use of memory, and improved synthetic training
strategies. Unlike prior approaches that rely on full-sequence access or
iterative updates, our model processes frames causally and maintains temporal
coherence via a memory mechanism, which is key to handling drift and occlusions
without requiring future frames. At inference, we perform coarse patch-level
classification followed by refinement. Beyond architecture, we systematically
study synthetic training setups and their impact on memory behavior, showing
how they shape temporal robustness over long sequences. Through comprehensive
experiments, Track-On2 achieves state-of-the-art results across five synthetic
and real-world benchmarks, surpassing prior online trackers and even strong
offline methods that exploit bidirectional context. These results highlight the
effectiveness of causal, memory-based architectures trained purely on synthetic
data as scalable solutions for real-world point tracking. Project page:
https://kuis-ai.github.io/track_on2

</details>


### [103] [KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments](https://arxiv.org/abs/2509.19129)
*Adam Romlein,Benjamin X. Hou,Yuval Boss,Cynthia L. Christman,Stacie Koslovsky,Erin E. Moreland,Jason Parham,Anthony Hoogs*

Main category: cs.CV

TL;DR: KAMERA是一个多摄像头、多光谱同步系统，用于实时检测海豹和北极熊，可将数据处理时间减少80%，所有软件和模型都开源。


<details>
  <summary>Details</summary>
Motivation: 开发一个综合系统来改进冰相关海豹的空中调查，提高数据处理效率和检测准确性。

Method: 使用多摄像头和多光谱技术进行硬件同步和校准，实现实时物体检测，并将所有数据映射到世界平面上。

Result: 在阿拉斯加周边海域的调查中，KAMERA将数据集处理时间减少了80%，并提供了准确的调查区域估计。

Conclusion: KAMERA系统成功提高了海豹和北极熊检测的效率和准确性，希望其开源软件能激励科学界的其他测绘和检测工作。

Abstract: We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears. Utilized in
aerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort
seas around Alaska, KAMERA provides up to an 80% reduction in dataset
processing time over previous methods. Our rigorous calibration and hardware
synchronization enable using multiple spectra for object detection. All
collected data are annotated with metadata so they can be easily referenced
later. All imagery and animal detections from a survey are mapped onto a world
plane for accurate surveyed area estimates and quick assessment of survey
results. We hope KAMERA will inspire other mapping and detection efforts in the
scientific community, with all software, models, and schematics fully
open-sourced.

</details>


### [104] [NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit](https://arxiv.org/abs/2509.19156)
*Maurf Hassan,Steven Davy,Muhammad Zawish,Owais Bin Zuber,Nouman Ashraf*

Main category: cs.CV

TL;DR: NeuCODEX是一种神经形态协同推理架构，通过联合优化空间和时间冗余，显著减少数据传输和边缘能耗，同时降低端到端延迟，实现资源受限环境下的高效SNN部署。


<details>
  <summary>Details</summary>
Motivation: 解决在边缘设备上进行完整SNN推理时因固定高时间步开销导致的延迟和能耗问题，以及边缘-云协同推理系统中高延迟和特征传输成本的问题。

Method: 引入学习型脉冲驱动压缩模块减少数据传输，采用动态提前退出机制基于输出置信度自适应终止推理，在真实边缘到云测试平台上基于ResNet-18和VGG-16骨干网络进行原型验证。

Result: 数据传输减少高达2048倍，边缘能耗降低超过90%，端到端延迟相比仅边缘推理降低3倍，准确率下降小于2%。

Conclusion: NeuCODEX能够在资源受限环境中实现实用、高性能的SNN部署，为边缘智能提供有效的解决方案。

Abstract: Spiking Neural Networks (SNNs) offer significant potential for enabling
energy-efficient intelligence at the edge. However, performing full SNN
inference at the edge can be challenging due to the latency and energy
constraints arising from fixed and high timestep overheads. Edge-cloud
co-inference systems present a promising solution, but their deployment is
often hindered by high latency and feature transmission costs. To address these
issues, we introduce NeuCODEX, a neuromorphic co-inference architecture that
jointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a
learned spike-driven compression module to reduce data transmission and employs
a dynamic early-exit mechanism to adaptively terminate inference based on
output confidence. We evaluated NeuCODEX on both static images (CIFAR10 and
Caltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To
demonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16
backbones in a real edge-to-cloud testbed. Our proposed system reduces data
transfer by up to 2048x and edge energy consumption by over 90%, while reducing
end-to-end latency by up to 3x compared to edge-only inference, all with a
negligible accuracy drop of less than 2%. In doing so, NeuCODEX enables
practical, high-performance SNN deployment in resource-constrained
environments.

</details>


### [105] [RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions](https://arxiv.org/abs/2509.19165)
*Yun Wang,Junjie Hu,Junhui Hou,Chenghao Zhang,Renwei Yang,Dapeng Oliver Wu*

Main category: cs.CV

TL;DR: 提出RoSe方法，通过视觉基础模型的鲁棒先验和场景对应学习，解决自监督立体匹配在恶劣天气条件下性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自监督立体匹配方法在恶劣天气（夜晚、雨、雾）下性能显著下降，主要原因是CNN特征提取器在退化区域表现不佳，以及光度一致性假设在恶劣天气下失效。

Method: 1) 从视觉基础模型注入鲁棒先验到CNN特征提取器；2) 构建场景对应先验替代光度一致性假设；3) 创建合成恶劣天气立体数据集；4) 提出鲁棒自监督训练范式，包括场景对应学习和恶劣天气蒸馏。

Result: 大量实验证明该方法有效且通用，在恶劣天气条件下优于现有最先进的自监督方法。

Conclusion: RoSe方法通过结合视觉基础模型的鲁棒先验和场景对应学习，显著提升了自监督立体匹配在恶劣天气条件下的性能。

Abstract: Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.

</details>


### [106] [YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives](https://arxiv.org/abs/2509.19166)
*Siddharth Gupta,Jitin Singla*

Main category: cs.CV

TL;DR: 本文提出YOLO-LAN，一种基于YOLO的息肉检测管道，通过M2IoU损失、数据增强和负样本训练，在结肠镜检测中实现高精度实时息肉检测。


<details>
  <summary>Details</summary>
Motivation: 结肠癌筛查中，结肠镜手动检测息肉存在不一致性和漏检问题，需要更准确、实时的AI辅助诊断方案。

Method: 使用YOLO-based检测管道，结合M2IoU损失函数、多样化数据增强技术和负样本训练，模拟真实临床场景。

Result: 在Kvasir-seg数据集上，YOLOv12达到mAP50 0.9619、mAP50:95 0.8599；YOLOv8达到mAP50 0.9540、mAP50:95 0.8487，显著优于现有方法。

Conclusion: YOLO-LAN在息肉大小和精确定位检测方面表现出鲁棒性，具有临床应用的潜力，可用于AI辅助结直肠筛查。

Abstract: Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal
mucosal cell proliferation called polyps in the inner wall of the colon. When
left undetected, polyps can become malignant tumors. Colonoscopy is the
standard procedure for detecting polyps, as it enables direct visualization and
removal of suspicious lesions. Manual detection by colonoscopy can be
inconsistent and is subject to oversight. Therefore, object detection based on
deep learning offers a better solution for a more accurate and real-time
diagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based
polyp detection pipeline, trained using M2IoU loss, versatile data
augmentations and negative data to replicate real clinical situations. Our
pipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp
datasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12
and mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg
dataset. The significant increase is achieved in mAP$_{50:95}$ score, showing
the precision of polyp detection. We show robustness based on polyp size and
precise location detection, making it clinically relevant in AI-assisted
colorectal screening.

</details>


### [107] [The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC](https://arxiv.org/abs/2509.19183)
*Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han*

Main category: cs.CV

TL;DR: 本文分析了MOSEv2挑战赛中的SeC框架，研究了其长期记忆和概念感知记忆机制，在LSVOS挑战赛中获得第一名。


<details>
  <summary>Details</summary>
Motivation: 探索复杂半监督视频对象分割任务，分析SeC框架在MOSEv2挑战赛中的表现，研究长期记忆和概念感知记忆对时序连续性和语义先验的贡献。

Method: 分析和改进SeC框架（增强版SAM-2），详细研究其长期记忆和概念感知记忆机制，评估这些机制在遮挡和重现情况下的表现。

Result: 在MOSEv2测试集上获得39.89%的JF分数，在LSVOS挑战赛MOSEv2赛道中排名第一。

Conclusion: 长期记忆在遮挡和重现情况下保持时序连续性，概念感知记忆提供语义先验以抑制干扰物，这些特性直接有益于MOSEv2的核心挑战。

Abstract: This technical report explores the MOSEv2 track of the LSVOS Challenge, which
targets complex semi-supervised video object segmentation. By analysing and
adapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its
long-term memory and concept-aware memory, showing that long-term memory
preserves temporal continuity under occlusion and reappearance, while
concept-aware memory supplies semantic priors that suppress distractors;
together, these traits directly benefit several MOSEv2's core challenges. Our
solution achieves a JF score of 39.89% on the test set, ranking 1st in the
MOSEv2 track of the LSVOS Challenge.

</details>


### [108] [Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models](https://arxiv.org/abs/2509.19191)
*Yueyan Li,Chenggong Zhao,Zeyuan Zang,Caixia Yuan,Xiaojie Wang*

Main category: cs.CV

TL;DR: 该论文分析了视觉语言模型(VLMs)的视觉处理机制，基于人类视觉的双流假说将视觉处理解构为物体识别和空间感知两个独立过程，并提出了相应的优化方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs通过序列化图像处理视觉信息，这与人类视觉的并行处理方式存在差异，且模型内部机制不透明，阻碍了深入理解和架构创新。

Method: 将视觉处理解构为物体识别和空间感知：1）将图像转换为文本标记图分析物体识别过程；2）理论推导和实证验证VLMs中位置表示的几何结构；3）提出指令无关的标记压缩算法和RoPE缩放技术。

Result: 发现物体识别是从浅层到深层的两阶段过程（属性识别到语义消歧），验证了位置表示的几何结构，提出的优化方法有效提升了解码效率和空间推理能力。

Conclusion: 该研究深化了对VLM内部机制的理解，为设计更强大的未来架构提供了清晰的设计原则。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable performance across
a variety of real-world tasks. However, existing VLMs typically process visual
information by serializing images, a method that diverges significantly from
the parallel nature of human vision. Moreover, their opaque internal mechanisms
hinder both deeper understanding and architectural innovation. Inspired by the
dual-stream hypothesis of human vision, which distinguishes the "what" and
"where" pathways, we deconstruct the visual processing in VLMs into object
recognition and spatial perception for separate study. For object recognition,
we convert images into text token maps and find that the model's perception of
image content unfolds as a two-stage process from shallow to deep layers,
beginning with attribute recognition and culminating in semantic
disambiguation. For spatial perception, we theoretically derive and empirically
verify the geometric structure underlying the positional representation in
VLMs. Based on these findings, we introduce an instruction-agnostic token
compression algorithm based on a plug-and-play visual decoder to improve
decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.
Through rigorous experiments, our work validates these analyses, offering a
deeper understanding of VLM internals and providing clear principles for
designing more capable future architectures.

</details>


### [109] [Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](https://arxiv.org/abs/2509.19203)
*Ioanna Ntinou,Alexandros Xenos,Yassine Ouali,Adrian Bulat,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: 本文提出了一种无需视觉编码器的文本到文本检索方法，通过VLLM生成结构化图像描述来替代传统文本到图像检索，解决了模态差距问题并提升了组合性能力。


<details>
  <summary>Details</summary>
Motivation: 传统对比学习的视觉语言模型存在模态差距、浅层语言理解、计算成本高和隐私问题等局限性，需要更高效和隐私友好的替代方案。

Method: 采用文本到文本检索范式，使用VLLM生成结构化图像描述，仅需少量GPU时间进行校准，无需视觉编码器。

Result: 该方法在多个检索和组合性基准测试中达到最先进的零样本性能，模型参数仅需0.3B，性能优于传统多模态模型。

Conclusion: 文本到文本检索范式能有效减少模态差距，提升组合性能力，同时提供更隐私友好的解决方案，展示了小模型在大规模任务上的潜力。

Abstract: Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have
become the standard approach for learning discriminative vision-language
representations. However, these models often exhibit shallow language
understanding, manifesting bag-of-words behaviour. These limitations are
reinforced by their dual-encoder design, which induces a modality gap.
Additionally, the reliance on vast web-collected data corpora for training
makes the process computationally expensive and introduces significant privacy
concerns. To address these limitations, in this work, we challenge the
necessity of vision encoders for retrieval tasks by introducing a vision-free,
single-encoder retrieval pipeline. Departing from the traditional text-to-image
retrieval paradigm, we migrate to a text-to-text paradigm with the assistance
of VLLM-generated structured image descriptions. We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval. To further assess
generalisation and address some of the shortcomings of prior compositionality
benchmarks, we release two benchmarks derived from Flickr30k and COCO,
containing diverse compositional queries made of short captions, which we coin
subFlickr and subCOCO. Our vision-free retriever matches and often surpasses
traditional multimodal models. Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters. Code is
available at: https://github.com/IoannaNti/LexiCLIP

</details>


### [110] [Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs](https://arxiv.org/abs/2509.19207)
*Israfel Salazar,Desmond Elliott,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 本文研究了对比视觉语言模型（VLMs）中组合性与长标题理解之间的双向关系，发现两者可以相互促进，但效果受数据质量和模型设计的影响。


<details>
  <summary>Details</summary>
Motivation: 当前对比视觉语言模型在绑定视觉和文本信息方面取得了显著进展，但理解长而密集的标题仍然是一个开放挑战。作者假设组合性（推理对象属性绑定和对象间关系的能力）是理解长标题的关键。

Method: 训练和评估了一系列针对组合性和长标题理解能力的模型，研究了不同训练策略（如冻结位置嵌入）和数据质量对模型性能的影响。

Result: 发现组合性训练能提高长标题检索性能，长标题训练也能促进组合性，但这些增益对数据质量和模型设计敏感。高质量的长标题数据训练可以在两个任务上都取得强劲表现。

Conclusion: 组合性理解和长标题理解是相互交织的能力，可以通过在密集、有基础描述的文本上进行训练来共同学习，为改进VLM泛化提供了实用指导。

Abstract: Contrastive vision-language models (VLMs) have made significant progress in
binding visual and textual information, but understanding long, dense captions
remains an open challenge. We hypothesize that compositionality, the capacity
to reason about object-attribute bindings and inter-object relationships, is
key to understanding longer captions. In this paper, we investigate the
interaction between compositionality and long-caption understanding, asking
whether training for one property enhances the other. We train and evaluate a
range of models that target each of these capabilities. Our results reveal a
bidirectional relationship: compositional training improves performance on
long-caption retrieval, and training on long captions promotes
compositionality. However, these gains are sensitive to data quality and model
design. We find that training on poorly structured captions, or with limited
parameter updates, fails to support generalization. Likewise, strategies that
aim at retaining general alignment, such as freezing positional embeddings, do
not improve compositional understanding. Overall, we find that compositional
understanding and long-caption understanding are intertwined capabilities that
can be jointly learned through training on dense, grounded descriptions.
Despite these challenges, we show that models trained on high-quality,
long-caption data can achieve strong performance in both tasks, offering
practical guidance for improving VLM generalization.

</details>


### [111] [Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data](https://arxiv.org/abs/2509.19208)
*Earl Ranario,Ismael Mayanja,Heesup Yun,Brian N. Bailey,J. Mason Earles*

Main category: cs.CV

TL;DR: 提出一个利用合成RGB图像、少量真实标注和GAN跨模态对齐来增强热图像语义分割的框架，在复杂田间环境中显著提升植物分割性能


<details>
  <summary>Details</summary>
Motivation: 解决热图像中植物分割的挑战，特别是在户外环境中植物与杂草对比度低、遮挡频繁导致性能不佳的问题

Method: 使用1,128张合成图像训练模型生成作物和杂草分割掩码，结合少量真实标注图像，通过CycleGAN-turbo实现RGB到热图像的跨模态对齐

Result: 结合所有合成图像和少量真实标注图像，杂草类别相对改进达22%，植物类别达17%，相比全真实数据基线有显著提升

Conclusion: 合成数据与有限手动标注结合，通过生成模型的跨域转换可以显著提升复杂田间环境中多模态图像的语义分割性能

Abstract: Accurate plant segmentation in thermal imagery remains a significant
challenge for high throughput field phenotyping, particularly in outdoor
environments where low contrast between plants and weeds and frequent
occlusions hinder performance. To address this, we present a framework that
leverages synthetic RGB imagery, a limited set of real annotations, and
GAN-based cross-modality alignment to enhance semantic segmentation in thermal
images. We trained models on 1,128 synthetic images containing complex mixtures
of crop and weed plants in order to generate image segmentation masks for crop
and weed plants. We additionally evaluated the benefit of integrating as few as
five real, manually segmented field images within the training process using
various sampling strategies. When combining all the synthetic images with a few
labeled real images, we observed a maximum relative improvement of 22% for the
weed class and 17% for the plant class compared to the full real-data baseline.
Cross-modal alignment was enabled by translating RGB to thermal using
CycleGAN-turbo, allowing robust template matching without calibration. Results
demonstrated that combining synthetic data with limited manual annotations and
cross-domain translation via generative models can significantly boost
segmentation performance in complex field environments for multi-model imagery.

</details>


### [112] [HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus](https://arxiv.org/abs/2509.19218)
*Yunzhi Xu,Yushuang Ding,Hu Sun,Hongxi Zhang,Li Zhao*

Main category: cs.CV

TL;DR: 提出了HyKid数据集，这是一个包含48名儿童脑积水患者的开源数据集，包含3D MRI图像、手动校正的脑组织分割以及临床放射学报告的结构化数据，揭示了脉络丛体积与总脑脊液体积之间的强相关性可作为脑积水评估的生物标志物


<details>
  <summary>Details</summary>
Motivation: 解决儿童脑积水评估的挑战，填补公开可用的专家标注数据集（特别是脉络丛分割）的空白

Method: 使用切片到体积算法从常规低分辨率图像重建1mm各向同性分辨率的3D MRI，由经验丰富的神经学家手动校正脑组织分割，使用检索增强生成框架从临床放射学报告中提取结构化数据

Result: 脉络丛体积与总脑脊液体积之间存在强相关性，预测模型达到AUC=0.87的优秀性能

Conclusion: HyKid数据集为神经影像算法开发提供了高质量基准，揭示了脉络丛相关特征在脑积水评估中的重要性

Abstract: Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.

</details>


### [113] [MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation](https://arxiv.org/abs/2509.19227)
*Tongshuai Wu,Chao Lu,Ze Song,Yunlong Lin,Sizhe Fan,Xuemei Chen*

Main category: cs.CV

TL;DR: 提出了一种多尺度特征交互网络（MsFIN），用于从行车记录仪视频中进行早期事故预测，通过多尺度特征聚合、时间特征处理和后融合来解决交通参与者特征交互建模和多时间尺度行为线索捕获的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着行车记录仪的广泛部署和计算机视觉的进步，从行车记录仪视角开发事故预测模型对于主动安全干预变得至关重要。但存在两个关键挑战：建模交通参与者之间的特征级交互（在行车记录仪视图中经常被遮挡）和捕捉事故前复杂、异步的多时间行为线索。

Method: MsFIN包含三层：多尺度特征聚合层使用多尺度模块提取短期、中期和长期时间尺度的场景表示，并利用Transformer架构促进全面特征交互；时间特征处理层在因果约束下捕捉场景和对象特征的序列演化；多尺度特征后融合层将多个时间尺度的场景和对象特征融合生成全面的风险表示。

Result: 在DAD和DADA数据集上的实验表明，MsFIN在预测准确性和及时性方面显著优于采用单尺度特征提取的最先进模型。消融研究验证了MsFIN中每个模块的有效性。

Conclusion: MsFIN通过多尺度特征融合和上下文交互建模实现了优越的性能，为行车记录仪视频的早期事故预测提供了有效的解决方案。

Abstract: With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.

</details>


### [114] [DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces](https://arxiv.org/abs/2509.19230)
*Tianshuo Zhang,Li Gao,Siran Peng,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出了一种基于持续学习的面部伪造检测方法，采用发展性专家混合架构，通过Real-LoRA和Fake-LoRAs分别学习真实面部和不同伪造类型，有效应对不断演变的伪造技术。


<details>
  <summary>Details</summary>
Motivation: 面对数字面部生成和操纵技术的快速发展，现有检测模型难以跟上技术迭代速度。需要让模型能够快速适应新领域，同时避免遗忘已学习的伪造类型。

Method: 采用发展性专家混合架构，使用LoRA模型作为专家。分为Real-LoRA学习真实面部知识，多个Fake-LoRAs捕获不同伪造类型的增量信息。通过正交梯度和正交损失防止灾难性遗忘。

Result: 在数据集和操纵类型增量协议下的实验结果表明该方法有效，能够持续学习新伪造类型而不遗忘旧知识。

Conclusion: 该方法为应对不断演变的伪造技术提供了一种有效的持续学习解决方案，通过正交约束确保学习过程的稳定性。

Abstract: The rise of realistic digital face generation and manipulation poses
significant social risks. The primary challenge lies in the rapid and diverse
evolution of generation techniques, which often outstrip the detection
capabilities of existing models. To defend against the ever-evolving new types
of forgery, we need to enable our model to quickly adapt to new domains with
limited computation and data while avoiding forgetting previously learned
forgery types. In this work, we posit that genuine facial samples are abundant
and relatively stable in acquisition methods, while forgery faces continuously
evolve with the iteration of manipulation techniques. Given the practical
infeasibility of exhaustively collecting all forgery variants, we frame face
forgery detection as a continual learning problem and allow the model to
develop as new forgery types emerge. Specifically, we employ a Developmental
Mixture of Experts (MoE) architecture that uses LoRA models as its individual
experts. These experts are organized into two groups: a Real-LoRA to learn and
refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental
information from different forgery types. To prevent catastrophic forgetting,
we ensure that the learning direction of Fake-LoRAs is orthogonal to the
established subspace. Moreover, we integrate orthogonal gradients into the
orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the
training process of each task. Experimental results under both the datasets and
manipulation types incremental protocols demonstrate the effectiveness of our
method.

</details>


### [115] [Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2509.19244)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Lavida-O是一个统一的多模态掩码扩散模型，支持图像理解和生成任务，具备物体定位、图像编辑和高分辨率图像合成等新能力，并在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态扩散语言模型（如MMaDa和Muddit）仅支持简单的图像级理解任务和低分辨率图像生成，无法满足复杂任务需求。Lavida-O旨在克服这些限制，提供更强大的多模态能力。

Method: 采用弹性混合Transformer架构、通用文本条件化和分层采样等新技术，通过规划和迭代自反思利用理解能力来改进图像生成和编辑结果。

Result: 在RefCOCO物体定位、GenEval文本到图像生成和ImgEdit图像编辑等多个基准测试中表现优异，超越了Qwen2.5-VL和FluxKontext-dev等现有自回归和连续扩散模型，并在推理时提供显著加速。

Conclusion: Lavida-O是首个统一的掩码扩散模型，成功实现了图像理解与生成的深度融合，为多模态AI系统提供了新的技术路径。

Abstract: We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)
capable of image understanding and generation tasks. Unlike existing multimodal
diffsion language models such as MMaDa and Muddit which only support simple
image-level understanding tasks and low-resolution image generation, Lavida-O
exhibits many new capabilities such as object grounding, image-editing, and
high-resolution (1024px) image synthesis. It is also the first unified MDM that
uses its understanding capabilities to improve image generation and editing
results through planning and iterative self-reflection. To allow effective and
efficient training and sampling, Lavida-O ntroduces many novel techniques such
as Elastic Mixture-of-Transformer architecture, universal text conditioning,
and stratified sampling. \ours~achieves state-of-the-art performance on a wide
range of benchmarks such as RefCOCO object grounding, GenEval text-to-image
generation, and ImgEdit image editing, outperforming existing autoregressive
and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while
offering considerable speedup at inference.

</details>


### [116] [ConViS-Bench: Estimating Video Similarity Through Semantic Concepts](https://arxiv.org/abs/2509.19245)
*Benedetta Liberatori,Alessandro Conti,Lorenzo Vaquero,Yiming Wang,Elisa Ricci,Paolo Rota*

Main category: cs.CV

TL;DR: 论文提出了概念化视频相似度估计任务，通过预定义的关键语义概念对视频对进行可解释的相似度评分，并构建了包含多领域视频对的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有视频相似度评估方法依赖全局相似度分数，无法像人类那样从不同角度比较视频。大型多模态模型为利用自然语言进行视频比较任务提供了新机会。

Method: 提出ConViS任务框架，通过预定义语义概念计算视频对的相似度分数；构建ConViS-Bench基准数据集，包含精心标注的视频对和概念级相似度分数；对多个先进模型进行基准测试。

Result: 不同模型在ConViS任务上表现差异显著，某些概念对视频相似度估计更具挑战性。

Conclusion: ConViS-Bench将成为推动语言驱动视频理解研究的重要资源，能够支持人类化的视频相似度推理和新应用开发。

Abstract: What does it mean for two videos to be similar? Videos may appear similar
when judged by the actions they depict, yet entirely different if evaluated
based on the locations where they were filmed. While humans naturally compare
videos by taking different aspects into account, this ability has not been
thoroughly studied and presents a challenge for models that often depend on
broad global similarity scores. Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks. We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts. ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval. To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.
Each pair comes with concept-level similarity scores and textual descriptions
of both differences and similarities. Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments. Our results reveal significant performance differences on
ConViS, indicating that some concepts present greater challenges for estimating
video similarity. We believe that ConViS-Bench will serve as a valuable
resource for advancing research in language-driven video understanding.

</details>


### [117] [Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](https://arxiv.org/abs/2509.19252)
*Gabriel Maldonado,Narges Rashvand,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 提出了一种基于对抗性精炼的VQ-GAN框架，通过密集运动标记化技术压缩时空热图，有效解决人体运动重建中的运动模糊和时间不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 连续人体运动理解在计算机视觉中具有高维度和内在冗余性的挑战，需要高效的压缩和表示方法来分析复杂运动动态。

Method: 结合密集运动标记化和对抗性精炼的VQ-GAN框架，通过密集标记化压缩时空热图，利用对抗性训练消除重建伪影。

Result: 在CMU Panoptic数据集上，方法比dVAE基线SSIM提升9.31%，时间不稳定性降低37.1%。发现2D运动最优表示需要128个标记，3D运动需要1024个标记。

Conclusion: 该方法为运动分析应用提供了实用的部署可行性，证明了密集标记化策略在运动复杂度分析中的有效性。

Abstract: Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.

</details>


### [118] [Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies](https://arxiv.org/abs/2509.19258)
*Dheerendranath Battalapalli,Apoorva Safai,Maria Jaramillo,Hyemin Um,Gustavo Adalfo Pineda Ortiz,Ulas Bagci,Manmeet Singh Ahluwalia,Marwa Ismail,Pallavi Tiwari*

Main category: cs.CV

TL;DR: 本文提出了一种新的图放射组学学习（GrRAiL）描述符，用于在临床MRI扫描中表征病灶内异质性，通过图论方法量化病灶内空间关系，在区分肿瘤复发与放射性效应方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决实体肿瘤影像学中可靠区分恶性病变与混淆病理的挑战，传统放射组学方法会丢失复杂的空间关系信息。

Method: GrRAiL方法：1）使用逐体素放射组学测量识别亚区域簇；2）计算图论指标量化簇间空间关联；生成加权图编码病灶内高阶空间关系。

Result: 在947名患者的多中心评估中，GrRAiL在三个应用场景中均显著优于现有方法：胶质母细胞瘤（测试准确率78%，提升>10%）、脑转移瘤（74%，提升>13%）、胰腺IPMN风险分层（75%，提升>10%）。

Conclusion: GrRAiL能够可靠捕获病灶内异质性，有效区分恶性病变与混淆病理，在临床应用中表现出优越性能。

Abstract: A significant challenge in solid tumors is reliably distinguishing
confounding pathologies from malignant neoplasms on routine imaging. While
radiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,
many aggregate features across the region of interest (ROI) and miss complex
spatial relationships among varying intensity compositions. We present a new
Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional
heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of
sub-regions using per-voxel radiomic measurements, then (2) computes
graph-theoretic metrics to quantify spatial associations among clusters. The
resulting weighted graphs encode higher-order spatial relationships within the
ROI, aiming to reliably capture ILH and disambiguate confounding pathologies
from malignancy. To assess efficacy and clinical feasibility, GrRAiL was
evaluated in n=947 subjects spanning three use cases: differentiating tumor
recurrence from radiation effects in glioblastoma (GBM; n=106) and brain
metastasis (n=233), and stratifying pancreatic intraductal papillary mucinous
neoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional
setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph
Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In
GBM, cross-validation (CV) and test accuracies for recurrence vs
pseudo-progression were 89% and 78% with >10% test-accuracy gains over
comparators. In brain metastasis, CV and test accuracies for recurrence vs
radiation necrosis were 84% and 74% (>13% improvement). For IPMN risk
stratification, CV and test accuracies were 84% and 75%, showing >10%
improvement.

</details>


### [119] [Moving by Looking: Towards Vision-Driven Avatar Motion Generation](https://arxiv.org/abs/2509.19259)
*Markos Diomataris,Berat Mert Albaba,Giorgio Becherini,Partha Ghosh,Omid Taheri,Michael J. Black*

Main category: cs.CV

TL;DR: CLOPS是第一个仅使用自我中心视觉感知环境并导航的人类化身，通过将低级运动技能学习与高级视觉控制解耦，实现了人类化的人类运动生成。


<details>
  <summary>Details</summary>
Motivation: 当前的人类运动生成方法忽视了感知与运动之间的相互依赖性，使用与人类感知差异巨大的任务特定"感知"。作者认为生成人类化化身行为需要人类化的感知。

Method: 首先在大型运动捕捉数据集上训练运动先验模型，然后使用Q学习训练策略，将自我中心视觉输入映射到运动先验的高级控制命令。

Result: 实验证明自我中心视觉能够使化身产生人类化的运动特征，例如化身会避开视觉场中的障碍物行走。

Conclusion: 为化身配备人类化传感器（特别是自我中心视觉）有望训练出行为像人类的化身。

Abstract: The way we perceive the world fundamentally shapes how we move, whether it is
how we navigate in a room or how we interact with other humans. Current human
motion generation methods, neglect this interdependency and use task-specific
``perception'' that differs radically from that of humans. We argue that the
generation of human-like avatar behavior requires human-like perception.
Consequently, in this work we present CLOPS, the first human avatar that solely
uses egocentric vision to perceive its surroundings and navigate. Using vision
as the primary driver of motion however, gives rise to a significant challenge
for training avatars: existing datasets have either isolated human motion,
without the context of a scene, or lack scale. We overcome this challenge by
decoupling the learning of low-level motion skills from learning of high-level
control that maps visual input to motion. First, we train a motion prior model
on a large motion capture dataset. Then, a policy is trained using Q-learning
to map egocentric visual inputs to high-level control commands for the motion
prior. Our experiments empirically demonstrate that egocentric vision can give
rise to human-like motion characteristics in our avatars. For example, the
avatars walk such that they avoid obstacles present in their visual field.
These findings suggest that equipping avatars with human-like sensors,
particularly egocentric vision, holds promise for training avatars that behave
like humans.

</details>


### [120] [OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps](https://arxiv.org/abs/2509.19282)
*Bingnan Li,Chen-Yu Wang,Haiyang Xu,Xiang Zhang,Ethan Armand,Divyansh Srivastava,Xiaojun Shan,Zeyuan Chen,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 该论文针对布局到图像生成中边界框重叠问题，提出了OverLayScore评估指标和OverLayBench基准，并开发了CreatiLayout-AM模型来改善重叠区域的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前布局到图像生成方法在处理包含显著重叠边界框的布局时表现不佳，特别是在大重叠区域和语义区分度小的重叠实例上存在困难。现有基准偏向于简单案例，无法有效评估模型在挑战性条件下的性能。

Method: 1. 提出OverLayScore指标量化重叠边界框的复杂性；2. 构建OverLayBench基准，包含高质量标注和平衡的OverLayScore分布；3. 开发CreatiLayout-AM模型，在精心策划的amodal掩码数据集上进行微调。

Result: 分析表明现有基准存在偏向简单案例的偏差。新提出的OverLayBench提供了更全面的评估框架，CreatiLayout-AM模型在复杂重叠场景下表现出改进潜力。

Conclusion: 该研究为在现实和挑战性场景下实现更鲁棒的布局到图像生成奠定了基础，为解决重叠边界框问题提供了系统性的评估方法和初步解决方案。

Abstract: Despite steady progress in layout-to-image generation, current methods still
struggle with layouts containing significant overlap between bounding boxes. We
identify two primary challenges: (1) large overlapping regions and (2)
overlapping instances with minimal semantic distinction. Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality. To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes. Our analysis reveals that existing benchmarks are
biased toward simpler cases with low OverLayScore values, limiting their
effectiveness in evaluating model performance under more challenging
conditions. To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore. As an initial step toward improving performance on
complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a
curated amodal mask dataset. Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.

</details>


### [121] [Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](https://arxiv.org/abs/2509.19296)
*Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi Ren*

Main category: cs.CV

TL;DR: 提出一种自蒸馏框架，将视频扩散模型中的隐式3D知识蒸馏到显式的3D高斯泼溅表示中，无需多视角训练数据即可生成3D场景


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的3D重建方法依赖真实世界多视角数据，但这些数据不易获取。视频扩散模型具有强大的想象力，但其2D特性限制了在机器人导航等需要3D交互的应用

Method: 在RGB解码器基础上增加3DGS解码器，通过RGB解码器的输出进行监督训练。3DGS解码器完全使用视频扩散模型生成的合成数据进行训练

Result: 实验结果表明，该框架在静态和动态3D场景生成方面达到了最先进的性能

Conclusion: 该框架能够从文本提示或单张图像实时合成3D场景，并可扩展到从单目视频生成动态3D场景

Abstract: The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.

</details>


### [122] [VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction](https://arxiv.org/abs/2509.19297)
*Weijie Wang,Yeqing Chen,Zeyu Zhang,Hengyu Liu,Haoxiao Wang,Zhiyuan Feng,Wenkang Qin,Zheng Zhu,Donny Y. Chen,Bohan Zhuang*

Main category: cs.CV

TL;DR: VolSplat提出了一种新的多视图前馈3D高斯泼溅方法，用体素对齐的高斯预测取代传统的像素对齐方法，解决了像素对齐在视图依赖、密度分布偏差和对齐误差方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于像素对齐的3D高斯泼溅方法存在三个主要问题：重建的3D模型严重依赖输入视图数量、产生视图偏差的密度分布、在源视图存在遮挡或低纹理时引入对齐误差。

Method: VolSplat采用体素对齐的高斯预测范式，直接从预测的3D体素网格预测高斯，避免了像素对齐对容易出错的2D特征匹配的依赖，确保多视图一致性，并能基于3D场景复杂度自适应控制高斯密度。

Result: 在RealEstate10K和ScanNet等基准测试中，VolSplat实现了最先进的性能，产生了更合理和视图一致的高斯重建，提高了几何一致性和新视角渲染质量。

Conclusion: VolSplat不仅取得了优越的结果，还为前馈3D重建建立了一个更可扩展的框架，提供了更密集和更鲁棒的表示，为更广泛社区的研究铺平了道路。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.

</details>


### [123] [CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching](https://arxiv.org/abs/2509.19300)
*Chen Chen,Pengsheng Guo,Liangchen Song,Jiasen Lu,Rui Qian,Xinze Wang,Tsu-Jui Fu,Wei Liu,Yinfei Yang,Alex Schwing*

Main category: cs.CV

TL;DR: CAR-Flow是一种轻量级的条件感知重参数化方法，通过调整源分布和目标分布来缩短概率路径，从而加速流匹配模型的训练并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的条件生成建模方法需要模型同时学习质量传输和条件注入，这增加了模型的学习负担。为了减轻模型负担，作者提出了条件感知重参数化方法。

Method: 提出CAR-Flow方法，通过学习一个轻量级的偏移量来调整源分布、目标分布或两者，从而缩短概率路径。该方法可以集成到现有的流匹配框架中。

Result: 在低维合成数据上可视化和量化了CAR的效果。在ImageNet-256数据集上，将CAR-Flow集成到SiT-XL/2模型中，FID从2.07降低到1.68，仅增加不到0.6%的参数。

Conclusion: CAR-Flow通过简单的重参数化策略有效提升了条件生成模型的性能，证明了调整概率路径长度对模型学习效率的重要性。

Abstract: Conditional generative modeling aims to learn a conditional data distribution
from samples containing data-condition pairs. For this, diffusion and
flow-based methods have attained compelling results. These methods use a
learned (flow) model to transport an initial standard Gaussian noise that
ignores the condition to the conditional data distribution. The model is hence
required to learn both mass transport and conditional injection. To ease the
demand on the model, we propose Condition-Aware Reparameterization for Flow
Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source,
the target, or both distributions. By relocating these distributions, CAR-Flow
shortens the probability path the model must learn, leading to faster training
in practice. On low-dimensional synthetic data, we visualize and quantify the
effects of CAR. On higher-dimensional natural image data (ImageNet-256),
equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while
introducing less than 0.6% additional parameters.

</details>
