<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 72]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices](https://arxiv.org/abs/2510.23775)
*Aryan Mathur,Asaduddin Ahmed,Pushti Amit Vasoya,Simeon Kandan Sonar,Yasir Z,Madesh Kuppusamy*

Main category: cs.CV

TL;DR: 提出可解释的图像真实性检测系统，结合轻量级卷积分类器和视觉语言模型，在32x32低分辨率图像上实现96.5%的准确率，并能定位和解释视觉伪影。


<details>
  <summary>Details</summary>
Motivation: AI生成图像越来越逼真，给视觉真实性验证带来挑战，需要开发可解释的检测系统。

Method: 结合轻量级卷积分类器(Faster-Than-Lies)和视觉语言模型(Qwen2-VL-7B)，使用自编码器重建误差图生成伪影定位热图。

Result: 在增强的CiFAKE数据集上达到96.5%准确率，推理时间175ms，支持在本地或边缘设备部署。

Conclusion: 证明了视觉和语言推理结合在低分辨率图像真实性检测中的可行性，在取证、工业检测和社交媒体审核等领域有应用潜力。

Abstract: The increasing realism of AI-generated imagery poses challenges for verifying
visual authenticity. We present an explainable image authenticity detection
system that combines a lightweight convolutional classifier
("Faster-Than-Lies") with a Vision-Language Model (Qwen2-VL-7B) to classify,
localize, and explain artifacts in 32x32 images. Our model achieves 96.5%
accuracy on the extended CiFAKE dataset augmented with adversarial
perturbations and maintains an inference time of 175ms on 8-core CPUs, enabling
deployment on local or edge devices. Using autoencoder-based reconstruction
error maps, we generate artifact localization heatmaps, which enhance
interpretability for both humans and the VLM. We further categorize 70 visual
artifact types into eight semantic groups and demonstrate explainable text
generation for each detected anomaly. This work highlights the feasibility of
combining visual and linguistic reasoning for interpretable authenticity
detection in low-resolution imagery and outlines potential cross-domain
applications in forensics, industrial inspection, and social media moderation.

</details>


### [2] [CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting](https://arxiv.org/abs/2510.23785)
*Md Tanvir Hossain,Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CV

TL;DR: CountFormer是一个基于transformer的类无关物体计数框架，通过识别视觉重复和结构关系来实现类似人类的计数能力，在复杂形状和密集场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有计数模型在处理复杂形状、内部对称或重叠物体时经常出错，无法像人类那样通过感知视觉重复和结构关系来计数。

Method: 基于CounTR架构，用自监督基础模型DINOv2替换视觉编码器以获得更丰富和空间一致的特征表示，并加入位置嵌入融合来保持几何关系，最后通过轻量卷积解码器生成密度图。

Result: 在FSC-147数据集上达到与当前最先进方法相当的性能，在结构复杂或密集场景中表现出更高的准确性。

Conclusion: 集成DINOv2等基础模型使计数系统能够接近人类的结构感知能力，朝着真正通用且无需示例的计数范式迈进。

Abstract: Humans can effortlessly count diverse objects by perceiving visual repetition
and structural relationships rather than relying on class identity. However,
most existing counting models fail to replicate this ability; they often
miscount when objects exhibit complex shapes, internal symmetry, or overlapping
components. In this work, we introduce CountFormer, a transformer-based
framework that learns to recognize repetition and structural coherence for
class-agnostic object counting. Built upon the CounTR architecture, our model
replaces its visual encoder with the self-supervised foundation model DINOv2,
which produces richer and spatially consistent feature representations. We
further incorporate positional embedding fusion to preserve geometric
relationships before decoding these features into density maps through a
lightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model
achieves performance comparable to current state-of-the-art methods while
demonstrating superior accuracy on structurally intricate or densely packed
scenes. Our findings indicate that integrating foundation models such as DINOv2
enables counting systems to approach human-like structural perception,
advancing toward a truly general and exemplar-free counting paradigm.

</details>


### [3] [A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras](https://arxiv.org/abs/2510.23798)
*Gauthier Grimmer,Romain Wenger,Clément Flint,Germain Forestier,Gilles Rixhon,Valentin Chardon*

Main category: cs.CV

TL;DR: 提出基于固定原位摄像头和深度学习的新方法，用于河流漂浮垃圾的连续监测和量化，通过几何模型估算物体实际尺寸，验证了低成本自动化监测系统的可行性。


<details>
  <summary>Details</summary>
Motivation: 河流中漂浮的人为垃圾日益增多，对生物多样性、水质以及航运、娱乐等人类活动造成负面影响，需要有效的监测方法。

Method: 使用固定原位摄像头采集数据，应用深度学习模型进行垃圾检测和量化，通过几何模型结合相机内外参数从2D图像估算物体实际尺寸，并测试不同环境条件和学习配置。

Result: 验证了数据集构建协议的重要性，特别是负样本整合和时间泄漏考虑，证明了结合投影几何和回归校正的度量物体估算方法的可行性。

Conclusion: 该方法为开发鲁棒、低成本的自动化城市水环境监测系统奠定了基础。

Abstract: The proliferation of floating anthropogenic debris in rivers has emerged as a
pressing environmental concern, exerting a detrimental influence on
biodiversity, water quality, and human activities such as navigation and
recreation. The present study proposes a novel methodological framework for the
monitoring the aforementioned waste, utilising fixed, in-situ cameras. This
study provides two key contributions: (i) the continuous quantification and
monitoring of floating debris using deep learning and (ii) the identification
of the most suitable deep learning model in terms of accuracy and inference
speed under complex environmental conditions. These models are tested in a
range of environmental conditions and learning configurations, including
experiments on biases related to data leakage. Furthermore, a geometric model
is implemented to estimate the actual size of detected objects from a 2D image.
This model takes advantage of both intrinsic and extrinsic characteristics of
the camera. The findings of this study underscore the significance of the
dataset constitution protocol, particularly with respect to the integration of
negative images and the consideration of temporal leakage. In conclusion, the
feasibility of metric object estimation using projective geometry coupled with
regression corrections is demonstrated. This approach paves the way for the
development of robust, low-cost, automated monitoring systems for urban aquatic
environments.

</details>


### [4] [RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features](https://arxiv.org/abs/2510.23816)
*Forouzan Fallah,Wenwen Li,Chia-Yu Hsu,Hyunho Lee,Yezhou Yang*

Main category: cs.CV

TL;DR: RareFlow是一个针对遥感图像超分辨率的物理感知框架，通过双条件架构和不确定性量化来解决分布外条件下的鲁棒性问题，在盲评估中被地球物理专家评为接近真实图像质量。


<details>
  <summary>Details</summary>
Motivation: 遥感图像超分辨率在分布外条件下（如不同传感器捕获的稀有地貌特征）往往会产生视觉合理但物理不准确的结果，需要开发具有物理感知能力的鲁棒框架。

Method: 采用双条件架构：门控ControlNet保持低分辨率输入的几何保真度，文本提示为复杂特征合成提供语义指导；引入多面损失函数确保光谱和辐射一致性；通过随机前向传播量化预测不确定性。

Result: 在精心策划的多传感器卫星图像基准测试中，地球物理专家盲评估显示模型输出接近真实图像质量，显著优于最先进基线，FID指标降低近40%。

Conclusion: RareFlow为数据稀缺科学领域的高保真合成提供了鲁棒框架，并为严重域偏移下的受控生成提供了新范式。

Abstract: Super-resolution (SR) for remote sensing imagery often fails under
out-of-distribution (OOD) conditions, such as rare geomorphic features captured
by diverse sensors, producing visually plausible but physically inaccurate
results. We present RareFlow, a physics-aware SR framework designed for OOD
robustness. RareFlow's core is a dual-conditioning architecture. A Gated
ControlNet preserves fine-grained geometric fidelity from the low-resolution
input, while textual prompts provide semantic guidance for synthesizing complex
features. To ensure physically sound outputs, we introduce a multifaceted loss
function that enforces both spectral and radiometric consistency with sensor
properties. Furthermore, the framework quantifies its own predictive
uncertainty by employing a stochastic forward pass approach; the resulting
output variance directly identifies unfamiliar inputs, mitigating feature
hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor
satellite imagery. In blind evaluations, geophysical experts rated our model's
outputs as approaching the fidelity of ground truth imagery, significantly
outperforming state-of-the-art baselines. This qualitative superiority is
corroborated by quantitative gains in perceptual metrics, including a nearly
40\% reduction in FID. RareFlow provides a robust framework for high-fidelity
synthesis in data-scarce scientific domains and offers a new paradigm for
controlled generation under severe domain shift.

</details>


### [5] [TRELLISWorld: Training-Free World Generation from Object Generators](https://arxiv.org/abs/2510.23880)
*Hanke Chen,Yuan Liu,Minchen Li*

Main category: cs.CV

TL;DR: 提出了一种无需训练的3D场景生成方法，通过将通用文本到3D对象扩散模型重新用作模块化瓦片生成器，实现大规模、连贯场景的可扩展合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常局限于单对象生成、需要领域特定训练或缺乏完整的360度视角支持，需要一种更通用的3D场景生成解决方案。

Method: 将场景生成重新表述为多瓦片去噪问题，独立生成重叠的3D区域并通过加权平均无缝融合，保留局部语义控制。

Result: 该方法支持多样化的场景布局、高效生成和灵活编辑，无需场景级数据集或重新训练，继承了对象级先验的泛化能力。

Conclusion: 建立了一个简单而强大的基础，用于通用语言驱动的3D场景构建，消除了对场景级数据集或重新训练的依赖。

Abstract: Text-driven 3D scene generation holds promise for a wide range of
applications, from virtual prototyping to AR/VR and simulation. However,
existing methods are often constrained to single-object generation, require
domain-specific training, or lack support for full 360-degree viewability. In
this work, we present a training-free approach to 3D scene synthesis by
repurposing general-purpose text-to-3D object diffusion models as modular tile
generators. We reformulate scene generation as a multi-tile denoising problem,
where overlapping 3D regions are independently generated and seamlessly blended
via weighted averaging. This enables scalable synthesis of large, coherent
scenes while preserving local semantic control. Our method eliminates the need
for scene-level datasets or retraining, relies on minimal heuristics, and
inherits the generalization capabilities of object-level priors. We demonstrate
that our approach supports diverse scene layouts, efficient generation, and
flexible editing, establishing a simple yet powerful foundation for
general-purpose, language-driven 3D scene construction.

</details>


### [6] [Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2510.23894)
*Jinxin Zhou,Jiachen Jiang,Zhihui Zhu*

Main category: cs.CV

TL;DR: LHT-CLIP是一个无需训练的分割框架，通过分析CLIP模型在层、头和token层面的视觉判别性，提出了三种互补技术来提升分割性能，在8个基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: CLIP模型的图像级预训练目标与像素级视觉理解存在不对齐问题，先前方法继承了全局对齐偏差导致分割性能不佳。

Method: 通过层、头、token三个层面的分析，提出语义空间重加权、选择性头增强和异常token替换三种无需训练的技术。

Result: 在8个常见语义分割基准测试中实现了最先进的性能，证明了方法的有效性和实际部署可行性。

Conclusion: LHT-CLIP通过系统利用CLIP模型的视觉判别性，无需额外训练即可显著提升分割性能，具有很好的实用性。

Abstract: Extending CLIP models to semantic segmentation remains challenging due to the
misalignment between their image-level pre-training objectives and the
pixel-level visual understanding required for dense prediction. While prior
efforts have achieved encouraging results by reorganizing the final layer and
features, they often inherit the global alignment bias of preceding layers,
leading to suboptimal segmentation performance. In this work, we propose
LHT-CLIP, a novel training-free framework that systematically exploits the
visual discriminability of CLIP across layer, head, and token levels. Through
comprehensive analysis, we reveal three key insights: (i) the final layers
primarily strengthen image-text alignment with sacrifice of visual
discriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),
partly due to the emergence of anomalous tokens; (ii) a subset of attention
heads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual
discriminability across datasets; (iii) abnormal tokens display sparse and
consistent activation pattern compared to normal tokens. Based on these
findings, we propose three complementary techniques: semantic-spatial
reweighting, selective head enhancement, and abnormal token replacement to
effectively restore visual discriminability and improve segmentation
performance without any additional training, auxiliary pre-trained networks, or
extensive hyperparameter tuning. Extensive experiments on 8 common semantic
segmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art
performance across diverse scenarios, highlighting its effectiveness and
practicality for real-world deployment.

</details>


### [7] [DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning](https://arxiv.org/abs/2510.23907)
*Eddison Pham,Prisha Priyadarshini,Adrian Maliackel,Kanishk Bandi,Cristian Meo,Kevin Zhu*

Main category: cs.CV

TL;DR: DynaStride是一个无需手动场景分割的管道，通过自适应帧采样和多模态窗口技术生成连贯的场景级视频字幕，在YouCookII数据集上优于VLLaMA3和GPT-4o等基线模型。


<details>
  <summary>Details</summary>
Motivation: 教学视频中的场景级字幕需要理解视觉线索和时间结构，但现有字幕缺乏连贯性和质量，可能造成混淆并削弱视频的教育意图。

Method: 使用自适应帧采样和多模态窗口捕捉场景内关键转换，采用多模态思维链生成多个动作-对象对，通过动态步长窗口选择算法平衡时间上下文和冗余，最终生成融合视觉语义和时间推理的教学字幕。

Result: 在N-gram指标（BLEU、METEOR）和语义相似度指标（BERTScore、CLIPScore）上均优于强基线模型，定性分析显示生成的字幕具有更好的时间连贯性和信息量。

Conclusion: DynaStride为改进AI驱动的教学内容生成提供了一个有前景的方向，能够产生更连贯和信息丰富的场景级字幕。

Abstract: Scene-level captioning in instructional videos can enhance learning by
requiring an understanding of both visual cues and temporal structure. By
aligning visual cues with textual guidance, this understanding supports
procedural learning and multimodal reasoning, providing a richer context for
skill acquisition. However, captions that fail to capture this structure may
lack coherence and quality, which can create confusion and undermine the
video's educational intent. To address this gap, we introduce DynaStride, a
pipeline to generate coherent, scene-level captions without requiring manual
scene segmentation. Using the YouCookII dataset's scene annotations, DynaStride
performs adaptive frame sampling and multimodal windowing to capture key
transitions within each scene. It then employs a multimodal chain-of-thought
process to produce multiple action-object pairs, which are refined and fused
using a dynamic stride window selection algorithm that adaptively balances
temporal context and redundancy. The final scene-level caption integrates
visual semantics and temporal reasoning in a single instructional caption.
Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,
demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and
semantic similarity measures (BERTScore, CLIPScore). Qualitative analyses
further show that DynaStride produces captions that are more temporally
coherent and informative, suggesting a promising direction for improving
AI-powered instructional content generation.

</details>


### [8] [TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis](https://arxiv.org/abs/2510.23929)
*Emily Kim,Julieta Martinez,Timur Bagautdinov,Jessica Hodgins*

Main category: cs.CV

TL;DR: TurboPortrait3D是一种低延迟的人像新视角合成方法，通过结合图像到3D模型和扩散模型，在保持3D感知的同时显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的人像生成图像到3D模型存在视觉伪影、细节缺失和身份保持不足的问题，而图像扩散模型虽然质量高但缺乏3D一致性且计算成本高。

Method: 使用前馈图像到虚拟形象生成管道获得初始3D表示和噪声渲染，然后通过单步扩散模型进行多视角一致的精炼，采用预训练加微调的训练策略。

Result: 该方法在质量和数量上都优于当前最先进的人像新视角合成方法，同时保持时间效率。

Conclusion: TurboPortrait3D成功将图像扩散模型用于增强图像到虚拟形象方法，实现了高质量、低延迟且3D一致的人像新视角合成。

Abstract: We introduce TurboPortrait3D: a method for low-latency novel-view synthesis
of human portraits. Our approach builds on the observation that existing
image-to-3D models for portrait generation, while capable of producing
renderable 3D representations, are prone to visual artifacts, often lack of
detail, and tend to fail at fully preserving the identity of the subject. On
the other hand, image diffusion models excel at generating high-quality images,
but besides being computationally expensive, are not grounded in 3D and thus
are not directly capable of producing multi-view consistent outputs. In this
work, we demonstrate that image-space diffusion models can be used to
significantly enhance the quality of existing image-to-avatar methods, while
maintaining 3D-awareness and running with low-latency. Our method takes a
single frontal image of a subject as input, and applies a feedforward
image-to-avatar generation pipeline to obtain an initial 3D representation and
corresponding noisy renders. These noisy renders are then fed to a single-step
diffusion model which is conditioned on input image(s), and is specifically
trained to refine the renders in a multi-view consistent way. Moreover, we
introduce a novel effective training strategy that includes pre-training on a
large corpus of synthetic multi-view data, followed by fine-tuning on
high-quality real images. We demonstrate that our approach both qualitatively
and quantitatively outperforms current state-of-the-art for portrait novel-view
synthesis, while being efficient in time.

</details>


### [9] [PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors](https://arxiv.org/abs/2510.23930)
*Xirui Jin,Renbiao Jin,Boying Li,Danping Zou,Wenxian Yu*

Main category: cs.CV

TL;DR: PlanarGS是一个基于3D高斯泼溅的室内场景重建框架，通过引入语言提示平面先验和几何先验监督，解决了大范围低纹理区域中几何重建模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯泼溅在室内场景中，由于大面积低纹理区域的存在，仅使用光度损失会导致几何重建模糊，无法恢复高保真3D表面。

Method: 设计了语言提示平面先验(LP3)流程，使用预训练视觉语言分割模型，并通过跨视图融合和几何先验检查优化区域建议；在3D高斯优化中加入平面先验监督项和几何先验监督项。

Result: 在标准室内基准测试中，PlanarGS重建出准确且详细的3D表面，大幅优于现有最先进方法。

Conclusion: PlanarGS通过结合平面和几何先验，有效解决了室内场景重建中的几何模糊问题，显著提升了重建质量。

Abstract: Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an
efficient representation for novel-view synthesis, achieving impressive visual
quality. However, in scenes dominated by large and low-texture regions, common
in indoor environments, the photometric loss used to optimize 3DGS yields
ambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome
this limitation, we introduce PlanarGS, a 3DGS-based framework tailored for
indoor scene reconstruction. Specifically, we design a pipeline for
Language-Prompted Planar Priors (LP3) that employs a pretrained vision-language
segmentation model and refines its region proposals via cross-view fusion and
inspection with geometric priors. 3D Gaussians in our framework are optimized
with two additional terms: a planar prior supervision term that enforces planar
consistency, and a geometric prior supervision term that steers the Gaussians
toward the depth and normal cues. We have conducted extensive experiments on
standard indoor benchmarks. The results show that PlanarGS reconstructs
accurate and detailed 3D surfaces, consistently outperforming state-of-the-art
methods by a large margin. Project page: https://planargs.github.io

</details>


### [10] [Adaptive Training of INRs via Pruning and Densification](https://arxiv.org/abs/2510.23943)
*Diana Aldana,João Paulo Lima,Daniel Csillag,Daniel Perazzo,Haoan Feng,Luiz Velho,Tiago Novello*

Main category: cs.CV

TL;DR: AIRe是一种自适应训练方案，通过神经元剪枝和输入频率密集化来优化隐式神经表示，在减少模型大小的同时保持或提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的隐式神经表示方法在选择输入频率和架构时存在挑战，通常需要启发式方法和繁重的超参数优化，参数冗余问题也未得到有效解决。

Method: 采用神经元剪枝机制避免冗余，通过目标权重衰减将不重要神经元的信息转移到剩余神经元，然后进行结构化剪枝；通过输入频率密集化在信号欠拟合的频谱区域添加频率，扩展表示基础。

Result: 在图像和SDF上的实验表明，AIRe能够减少模型大小，同时保持甚至改善重建质量。

Conclusion: AIRe提供了一种改进的网络大小与重建质量之间的权衡方案，代码和预训练模型将公开发布。

Abstract: Encoding input coordinates with sinusoidal functions into multilayer
perceptrons (MLPs) has proven effective for implicit neural representations
(INRs) of low-dimensional signals, enabling the modeling of high-frequency
details. However, selecting appropriate input frequencies and architectures
while managing parameter redundancy remains an open challenge, often addressed
through heuristics and heavy hyperparameter optimization schemes. In this
paper, we introduce AIRe ($\textbf{A}$daptive $\textbf{I}$mplicit neural
$\textbf{Re}$presentation), an adaptive training scheme that refines the INR
architecture over the course of optimization. Our method uses a neuron pruning
mechanism to avoid redundancy and input frequency densification to improve
representation capacity, leading to an improved trade-off between network size
and reconstruction quality. For pruning, we first identify less-contributory
neurons and apply a targeted weight decay to transfer their information to the
remaining neurons, followed by structured pruning. Next, the densification
stage adds input frequencies to spectrum regions where the signal underfits,
expanding the representational basis. Through experiments on images and SDFs,
we show that AIRe reduces model size while preserving, or even improving,
reconstruction quality. Code and pretrained models will be released for public
use.

</details>


### [11] [Neural USD: An object-centric framework for iterative editing and control](https://arxiv.org/abs/2510.23956)
*Alejandro Escontrela,Shrinu Kushagra,Sjoerd van Steenkiste,Yulia Rubanova,Aleksander Holynski,Kelsey Allen,Kevin Murphy,Thomas Kipf*

Main category: cs.CV

TL;DR: 提出了Neural USD框架，通过结构化、层次化的场景表示实现精确的对象级编辑，解决了现有生成模型在迭代编辑时产生意外全局变化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前可控生成模型在精确迭代对象编辑方面存在挑战，改变条件信号往往导致场景的意外全局变化，需要解决对象级精确控制的问题。

Method: 借鉴计算机图形学中的USD标准，引入Neural USD框架，采用结构化层次化场景表示，结合微调方法确保控制信号的解耦。

Result: 评估了框架的设计考虑，展示了Neural USD如何支持迭代和增量工作流程，实现了对对象外观、几何和姿态的独立控制。

Conclusion: Neural USD框架为生成模型提供了更精确的对象级控制能力，支持迭代编辑工作流，是解决可控生成中对象编辑挑战的重要进展。

Abstract: Amazing progress has been made in controllable generative modeling,
especially over the last few years. However, some challenges remain. One of
them is precise and iterative object editing. In many of the current methods,
trying to edit the generated image (for example, changing the color of a
particular object in the scene or changing the background while keeping other
elements unchanged) by changing the conditioning signals often leads to
unintended global changes in the scene. In this work, we take the first steps
to address the above challenges. Taking inspiration from the Universal Scene
Descriptor (USD) standard developed in the computer graphics community, we
introduce the "Neural Universal Scene Descriptor" or Neural USD. In this
framework, we represent scenes and objects in a structured, hierarchical
manner. This accommodates diverse signals, minimizes model-specific
constraints, and enables per-object control over appearance, geometry, and
pose. We further apply a fine-tuning approach which ensures that the above
control signals are disentangled from one another. We evaluate several design
considerations for our framework, demonstrating how Neural USD enables
iterative and incremental workflows. More information at:
https://escontrela.me/neural_usd .

</details>


### [12] [SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability](https://arxiv.org/abs/2510.23960)
*Peiyang Xu,Minzhou Pan,Zhaorun Chen,Shuang Yang,Chaowei Xiao,Bo Li*

Main category: cs.CV

TL;DR: SafeVision是一个创新的图像护栏系统，通过整合类人推理来提升适应性和透明度，动态适应新兴安全威胁而无需重新训练，在多个基准测试中表现优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 传统图像护栏模型受限于预定义类别，缺乏语义推理导致误分类，且难以适应新兴威胁，需要昂贵的重新训练。

Method: 提出数据收集和生成框架、策略遵循训练流程、定制损失函数，以及多样化的问答生成和训练策略，实现动态策略对齐。

Result: SafeVision在VisionHarm-T上比GPT-4o提升8.6%，在VisionHarm-C上提升15.5%，速度提升16倍以上，达到最先进性能。

Conclusion: SafeVision建立了全面、策略遵循、可解释的图像护栏系统，能够动态适应新兴威胁，无需重新训练。

Abstract: With the rapid proliferation of digital media, the need for efficient and
transparent safeguards against unsafe content is more critical than ever.
Traditional image guardrail models, constrained by predefined categories, often
misclassify content due to their pure feature-based learning without semantic
reasoning. Moreover, these models struggle to adapt to emerging threats,
requiring costly retraining for new threats. To address these limitations, we
introduce SafeVision, a novel image guardrail that integrates human-like
reasoning to enhance adaptability and transparency. Our approach incorporates
an effective data collection and generation framework, a policy-following
training pipeline, and a customized loss function. We also propose a diverse QA
generation and training strategy to enhance learning effectiveness. SafeVision
dynamically aligns with evolving safety policies at inference time, eliminating
the need for retraining while ensuring precise risk assessments and
explanations. Recognizing the limitations of existing unsafe image benchmarks,
which either lack granularity or cover limited risks, we introduce VisionHarm,
a high-quality dataset comprising two subsets: VisionHarm Third-party
(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse
harmful categories. Through extensive experiments, we show that SafeVision
achieves state-of-the-art performance on different benchmarks. SafeVision
outperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while
being over 16x faster. SafeVision sets a comprehensive, policy-following, and
explainable image guardrail with dynamic adaptation to emerging threats.

</details>


### [13] [Reasoning Visual Language Model for Chest X-Ray Analysis](https://arxiv.org/abs/2510.23968)
*Andriy Myronenko,Dong Yang,Baris Turkbey,Mariam Aboian,Sena Azamat,Esra Akcicek,Hongxu Yin,Pavlo Molchanov,Marc Edgar,Yufan He,Pengfei Guo,Yucheng Tang,Daguang Xu*

Main category: cs.CV

TL;DR: 提出了一个将思维链推理引入胸部X光解读的框架，通过两阶段训练方法生成可验证的推理轨迹，提高临床可审计性和人机协作安全性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在医学图像分析中缺乏透明推理过程，无法提供临床医生依赖的逐步推理，需要开发能展示专家推理方式的解释性AI。

Method: 结合高保真视觉编码和两阶段训练：先进行推理风格监督微调，然后使用基于X光异常可验证奖励的强化学习，输出模拟放射科医生思维过程的推理。

Result: 在分布外评估中实现竞争性多标签分类，同时提高可解释性；专家放射科医生研究表明完整推理轨迹增强了信心、支持错误审计并减少报告完成时间。

Conclusion: 该框架推进了胸部放射学及其他医学成像任务中可信赖、可解释AI的发展，其中推理质量与预测质量同等重要。

Abstract: Vision-language models (VLMs) have shown strong promise for medical image
analysis, but most remain opaque, offering predictions without the transparent,
stepwise reasoning clinicians rely on. We present a framework that brings
chain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by
reasoning-first training paradigms, our approach is designed to learn how
experts reason, not just what they conclude, by aligning intermediate steps
with observable image evidence and radiology workflow. Beyond accuracy, the
explicit reasoning traces support clinical auditability: they reveal why a
conclusion was reached, which alternatives were considered, and where
uncertainty remains, enabling quality assurance, error analysis, and safer
human-AI collaboration.
  Our model couples high-fidelity visual encoding with a two-stage training
recipe: a reasoning-style supervised fine-tuning (SFT) followed by
reinforcement learning (RL) that uses verifiable rewards over a list of X-ray
abnormalities. The model outputs reasoning that mirrors radiologists systematic
thought process, uncertainty, and differential diagnosis. In
out-of-distribution evaluation, the approach achieves competitive multi-label
classification while improving interpretability. In a reader study with expert
radiologists, full reasoning traces increased confidence, supported error
auditing, and reduced time to finalize reports. We release code and the model
NV-Reason-CXR-3B to support community progress toward trustworthy, explainable
AI in chest radiography and other medical imaging tasks where reasoning quality
is as critical as prediction quality.

</details>


### [14] [Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints](https://arxiv.org/abs/2510.23978)
*Kazutoshi Akita,Norimichi Ukita*

Main category: cs.CV

TL;DR: 提出联合预测多个傅里叶分量的方法，以改进任意尺度超分辨率的成本与质量可控性


<details>
  <summary>Details</summary>
Motivation: 现有方法使用循环神经网络逐个预测傅里叶分量，导致性能下降和效率低下

Method: 通过联合预测多个傅里叶分量来改进质量和效率

Result: 该方法提高了超分辨率的性能表现和计算效率

Conclusion: 联合预测方法比逐个预测更有效，能同时提升质量与效率

Abstract: Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is
crucial. Existing methods predict Fourier components one by one using a
recurrent neural network. However, this approach leads to performance
degradation and inefficiency due to independent prediction. This paper proposes
predicting multiple components jointly to improve both quality and efficiency.

</details>


### [15] [TeleEgo: Benchmarking Egocentric AI Assistants in the Wild](https://arxiv.org/abs/2510.23981)
*Jiaqi Yan,Ruilong Ren,Jingren Liu,Shuning Xu,Ling Wang,Yiheng Wang,Yun Wang,Long Zhang,Xiangyu Chen,Changzhi Sun,Jixiang Luo,Dell Zhang,Hao Sun,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleEgo是一个用于评估以自我为中心AI助手的长时、流式、全模态基准测试，包含14+小时同步的多模态数据，涵盖12个子任务和3,291个QA项目，旨在测试记忆、理解和跨记忆推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常孤立评估AI助手的能力，缺乏真实的流式场景或仅支持短期任务，无法满足现实世界中对多模态输入、实时响应和长期记忆的需求。

Method: 构建包含工作学习、生活方式、社交活动和外出文化四个领域的同步自我中心视频、音频和文本数据集，所有数据在统一时间线上对齐，并通过人工精炼获得高质量视觉叙述和语音转录。

Result: TeleEgo定义了12个诊断子任务，包含3,291个人工验证的QA项目，涵盖多种问题格式，并提出了实时准确率和记忆持久时间两个关键指标来评估正确性、时间响应性和长期保持能力。

Conclusion: TeleEgo提供了一个现实且全面的评估框架，能够推动实用AI助手的发展，特别是在长时记忆和流式处理能力方面。

Abstract: Egocentric AI assistants in real-world settings must process multi-modal
inputs (video, audio, text), respond in real time, and retain evolving
long-term memory. However, existing benchmarks typically evaluate these
abilities in isolation, lack realistic streaming scenarios, or support only
short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming,
omni-modal benchmark for evaluating egocentric AI assistants in realistic daily
contexts. The dataset features over 14 hours per participant of synchronized
egocentric video, audio, and text across four domains: work \& study, lifestyle
\& routines, social activities, and outings \& culture. All data is aligned on
a unified global timeline and includes high-quality visual narrations and
speech transcripts, curated through human refinement.TeleEgo defines 12
diagnostic subtasks across three core capabilities: Memory (recalling past
events), Understanding (interpreting the current moment), and Cross-Memory
Reasoning (linking distant events). It contains 3,291 human-verified QA items
spanning multiple question formats (single-choice, binary, multi-choice, and
open-ended), evaluated strictly in a streaming setting. We propose two key
metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess
correctness, temporal responsiveness, and long-term retention. TeleEgo provides
a realistic and comprehensive evaluation to advance the development of
practical AI assistants.

</details>


### [16] [AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization](https://arxiv.org/abs/2510.24000)
*Heethanjan Kanagalingam,Thenukan Pathmanathan,Mokeeshan Vathanakumar,Tharmakulasingam Mukunthan*

Main category: cs.CV

TL;DR: 提出AdvBlur方法，通过引入对抗性模糊图像和双损失函数框架来提升糖尿病视网膜病变分类的领域泛化能力，解决因设备、人群和成像条件差异导致的分布变化问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在糖尿病视网膜病变检测中面临分布变化导致的鲁棒性问题，需要提高模型在未见数据上的泛化性能。

Method: AdvBlur方法：1）在数据集中集成对抗性模糊图像；2）采用双损失函数框架处理领域泛化问题。

Result: 在多个数据集上的综合评估显示，该方法能有效缓解未见分布变化的影响，在未见外部数据集上达到与最先进领域泛化模型相当的竞争性能。

Conclusion: AdvBlur方法通过对抗性模糊图像和双损失函数有效提升了糖尿病视网膜病变分类模型的领域泛化能力，为处理分布变化提供了有效解决方案。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet
early and accurate detection can significantly improve treatment outcomes.
While numerous Deep learning (DL) models have been developed to predict DR from
fundus images, many face challenges in maintaining robustness due to
distributional variations caused by differences in acquisition devices,
demographic disparities, and imaging conditions. This paper addresses this
critical limitation by proposing a novel DR classification approach, a method
called AdvBlur. Our method integrates adversarial blurred images into the
dataset and employs a dual-loss function framework to address domain
generalization. This approach effectively mitigates the impact of unseen
distributional variations, as evidenced by comprehensive evaluations across
multiple datasets. Additionally, we conduct extensive experiments to explore
the effects of factors such as camera type, low-quality images, and dataset
size. Furthermore, we perform ablation studies on blurred images and the loss
function to ensure the validity of our choices. The experimental results
demonstrate the effectiveness of our proposed method, achieving competitive
performance compared to state-of-the-art domain generalization DR models on
unseen external datasets.

</details>


### [17] [Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge](https://arxiv.org/abs/2510.24009)
*Yuan Jin,Antonio Pepe,Gian Marco Melito,Yuxuan Chen,Yunsu Byeon,Hyeseong Kim,Kyungwon Kim,Doohyun Park,Euijoon Choi,Dosik Hwang,Andriy Myronenko,Dong Yang,Yufan He,Daguang Xu,Ayman El-Ghotni,Mohamed Nabil,Hossam El-Kady,Ahmed Ayyad,Amr Nasr,Marek Wodzinski,Henning Müller,Hyeongyu Kim,Yejee Shin,Abbas Khan,Muhammad Asad,Alexander Zolotarev,Caroline Roney,Anthony Mathur,Martin Benning,Gregory Slabaugh,Theodoros Panagiotis Vagenas,Konstantinos Georgas,George K. Matsopoulos,Jihan Zhang,Zhen Zhang,Liqin Huang,Christian Mayer,Heinrich Mächler,Jan Egger*

Main category: cs.CV

TL;DR: SEG.A挑战赛引入大型公开多机构主动脉血管树分割数据集，推动了该领域的算法发展。研究发现3D U-Net架构主导顶级提交，模型集成显著优于单个模型，性能与算法设计和训练数据特征密切相关。


<details>
  <summary>Details</summary>
Motivation: 主动脉血管树自动分析具有巨大临床潜力，但缺乏共享高质量数据阻碍了发展。SEG.A挑战赛旨在通过引入大型公开多机构数据集来推动该领域进展。

Method: 挑战赛在隐藏测试集上对自动算法进行基准测试，包括可选的计算仿真表面网格生成任务。主要采用3D U-Net架构的深度学习方法和定制化后处理步骤。

Result: 顶级算法集成显著优于单个模型。性能与算法设计（特别是定制化后处理）和训练数据特征密切相关。3D U-Net架构在顶级提交中占主导地位。

Conclusion: 该计划不仅建立了新的性能基准，还提供了持久资源来推动未来创新，朝着稳健、临床可转化工具的方向发展。

Abstract: The automated analysis of the aortic vessel tree (AVT) from computed
tomography angiography (CTA) holds immense clinical potential, but its
development has been impeded by a lack of shared, high-quality data. We
launched the SEG.A. challenge to catalyze progress in this field by introducing
a large, publicly available, multi-institutional dataset for AVT segmentation.
The challenge benchmarked automated algorithms on a hidden test set, with
subsequent optional tasks in surface meshing for computational simulations. Our
findings reveal a clear convergence on deep learning methodologies, with 3D
U-Net architectures dominating the top submissions. A key result was that an
ensemble of the highest-ranking algorithms significantly outperformed
individual models, highlighting the benefits of model fusion. Performance was
strongly linked to algorithmic design, particularly the use of customized
post-processing steps, and the characteristics of the training data. This
initiative not only establishes a new performance benchmark but also provides a
lasting resource to drive future innovation toward robust, clinically
translatable tools.

</details>


### [18] [Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks](https://arxiv.org/abs/2510.24010)
*Mirali Purohit,Bimal Gajera,Vatsal Malaviya,Irish Mehta,Kunal Kasodekar,Jacob Adler,Steven Lu,Umaa Rebbapragada,Hannah Kerner*

Main category: cs.CV

TL;DR: Mars-Bench是首个用于系统评估火星相关任务的基准测试，包含20个数据集，涵盖分类、分割和检测任务，旨在为火星科学建立标准化的机器学习模型开发框架。


<details>
  <summary>Details</summary>
Motivation: 火星科学领域缺乏标准化的基准测试和评估框架，这限制了火星基础模型的发展。虽然基础模型在其他领域（如地球观测）取得了显著进展，但在火星科学中的应用仍然有限。

Method: 构建了Mars-Bench基准测试，包含20个数据集，涵盖分类、分割和检测任务，专注于关键地质特征（如陨石坑、锥体、巨石和霜）。提供了标准化的数据集，并使用在自然图像、地球卫星数据和最先进的视觉语言模型上预训练的模型进行基线评估。

Result: 所有分析结果表明，火星特定的基础模型可能比通用领域模型具有优势，这促使进一步探索领域自适应预训练。

Conclusion: Mars-Bench旨在为火星科学的机器学习模型开发和比较建立标准化基础，数据、模型和代码已公开可用。

Abstract: Foundation models have enabled rapid progress across many specialized domains
by leveraging large-scale pre-training on unlabeled data, demonstrating strong
generalization to a variety of downstream tasks. While such models have gained
significant attention in fields like Earth Observation, their application to
Mars science remains limited. A key enabler of progress in other domains has
been the availability of standardized benchmarks that support systematic
evaluation. In contrast, Mars science lacks such benchmarks and standardized
evaluation frameworks, which have limited progress toward developing foundation
models for Martian tasks. To address this gap, we introduce Mars-Bench, the
first benchmark designed to systematically evaluate models across a broad range
of Mars-related tasks using both orbital and surface imagery. Mars-Bench
comprises 20 datasets spanning classification, segmentation, and object
detection, focused on key geologic features such as craters, cones, boulders,
and frost. We provide standardized, ready-to-use datasets and baseline
evaluations using models pre-trained on natural images, Earth satellite data,
and state-of-the-art vision-language models. Results from all analyses suggest
that Mars-specific foundation models may offer advantages over general-domain
counterparts, motivating further exploration of domain-adapted pre-training.
Mars-Bench aims to establish a standardized foundation for developing and
comparing machine learning models for Mars science. Our data, models, and code
are available at: https://mars-bench.github.io/.

</details>


### [19] [AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts](https://arxiv.org/abs/2510.24034)
*Yufan Liu,Wanqian Zhang,Huashan Chen,Lin Wang,Xiaojun Jia,Zheng Lin,Weiping Wang*

Main category: cs.CV

TL;DR: APT是一个黑盒框架，利用大语言模型自动生成人类可读的对抗性后缀来绕过文本到图像模型的安全机制。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型的安全机制容易受到对抗性提示的攻击，现有红队测试方法需要白盒访问、效率低下且生成的提示语义无意义容易被过滤器拦截。

Method: 采用交替优化-微调流程，在对抗性后缀优化和LLM微调之间交替进行，并集成双重规避策略来绕过基于困惑度的过滤器和黑名单词过滤器。

Result: 实验证明该方法在红队测试中表现优异，生成的人类可读对抗性提示能有效抵抗过滤器，并具有出色的零样本迁移能力，能即时适应未见过的提示并暴露商业API的关键漏洞。

Conclusion: APT框架成功解决了现有方法的局限性，为评估文本到图像模型安全性提供了有效的黑盒测试工具。

Abstract: Despite rapid advancements in text-to-image (T2I) models, their safety
mechanisms are vulnerable to adversarial prompts, which maliciously generate
unsafe images. Current red-teaming methods for proactively assessing such
vulnerabilities usually require white-box access to T2I models, and rely on
inefficient per-prompt optimization, as well as inevitably generate
semantically meaningless prompts easily blocked by filters. In this paper, we
propose APT (AutoPrompT), a black-box framework that leverages large language
models (LLMs) to automatically generate human-readable adversarial suffixes for
benign prompts. We first introduce an alternating optimization-finetuning
pipeline between adversarial suffix optimization and fine-tuning the LLM
utilizing the optimized suffix. Furthermore, we integrates a dual-evasion
strategy in optimization phase, enabling the bypass of both perplexity-based
filter and blacklist word filter: (1) we constrain the LLM generating
human-readable prompts through an auxiliary LLM perplexity scoring, which
starkly contrasts with prior token-level gibberish, and (2) we also introduce
banned-token penalties to suppress the explicit generation of banned-tokens in
blacklist. Extensive experiments demonstrate the excellent red-teaming
performance of our human-readable, filter-resistant adversarial prompts, as
well as superior zero-shot transferability which enables instant adaptation to
unseen prompts and exposes critical vulnerabilities even in commercial APIs
(e.g., Leonardo.Ai.).

</details>


### [20] [ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning](https://arxiv.org/abs/2510.24036)
*Xingyu Liu,Kun Ming Goh*

Main category: cs.CV

TL;DR: ResNet通过跳跃连接解决深度网络训练中的梯度消失问题，在CIFAR-10数据集上ResNet-18达到89.9%准确率，优于传统深度CNN的84.1%，且收敛更快、训练更稳定。


<details>
  <summary>Details</summary>
Motivation: 解决深度卷积神经网络训练中的梯度消失问题，使训练数百层的深层网络成为可能。

Method: 使用跳跃连接（skip connections）构建残差网络，允许梯度通过捷径连接直接传播，绕过中间层。

Result: 在CIFAR-10数据集上，ResNet-18达到89.9%准确率，相比传统深度CNN的84.1%有显著提升，且收敛速度更快、训练更稳定。

Conclusion: 残差网络通过跳跃连接有效解决了深度网络训练的梯度消失问题，实现了更深层网络的稳定训练和更高性能。

Abstract: Convolutional Neural Networks (CNNs) has revolutionized computer vision, but
training very deep networks has been challenging due to the vanishing gradient
problem. This paper explores Residual Networks (ResNet), introduced by He et
al. (2015), which overcomes this limitation by using skip connections. ResNet
enables the training of networks with hundreds of layers by allowing gradients
to flow directly through shortcut connections that bypass intermediate layers.
In our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%
accuracy compared to 84.1% for a traditional deep CNN of similar depth, while
also converging faster and training more stably.

</details>


### [21] [Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models](https://arxiv.org/abs/2510.24037)
*Shufan Shen,Junshu Sun,Shuhui Wang,Qingming Huang*

Main category: cs.CV

TL;DR: SNELLA是一种单阶段的参数高效微调方法，通过非线性核函数增强低秩分解和自适应双层稀疏分配机制，在减少内存使用的同时提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏调优方法存在两个问题：1）两阶段方法忽略了微调过程中的参数调整，限制了性能；2）梯度稀疏掩码导致高内存使用。

Method: 提出SNELLA方法：1）使用两个低秩可学习矩阵合并成稀疏矩阵来选择性更新权重；2）引入非线性核函数增加合并矩阵的秩；3）自适应双层稀疏分配机制在端到端方式下根据重要性分数分配稀疏度。

Result: 在分类、分割和生成任务上，SNELLA达到SOTA性能，内存使用减少31.1%-39.9%，在FGVC基准上比SPT-LoRA提高1.8%的Top-1准确率。

Conclusion: SNELLA通过单阶段方法有效解决了现有稀疏调优方法的内存和性能限制，在多种视觉任务上表现出色。

Abstract: Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision
models to downstream tasks. Among PEFT paradigms, sparse tuning achieves
remarkable performance by adjusting only the weights most relevant to
downstream tasks, rather than densely tuning the entire weight matrix. Current
methods follow a two-stage paradigm. First, it locates task-relevant weights by
gradient information, which overlooks the parameter adjustments during
fine-tuning and limits the performance. Second, it updates only the located
weights by applying a sparse mask to the gradient of the weight matrix, which
results in high memory usage due to the storage of all weight matrices in the
optimizer. In this paper, we propose a one-stage method named SNELLA to
overcome the above limitations. For memory usage, SNELLA selectively updates
the weight matrix by adding it to another sparse matrix that is merged by two
low-rank learnable matrices. We extend the low-rank decomposition by
introducing nonlinear kernel functions, thereby increasing the rank of the
resulting merged matrix to prevent the interdependency among weight updates,
enabling better adaptation to downstream tasks. For locating task-relevant
weights, we propose an adaptive bi-level sparsity allocation mechanism that
encourages weights to compete across and inside layers based on their
importance scores in an end-to-end manner. Extensive experiments are conducted
on classification, segmentation, and generation tasks using different
pre-trained vision models. The results show that SNELLA achieves SOTA
performance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.
90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.
Compared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%
across models with parameter scales from 86M to 632M. Our source codes are
available at https://github.com/ssfgunner/SNELL.

</details>


### [22] [Enhancing CLIP Robustness via Cross-Modality Alignment](https://arxiv.org/abs/2510.24038)
*Xingyu Zhu,Beier Zhu,Shuo Wang,Kesen Zhao,Hanwang Zhang*

Main category: cs.CV

TL;DR: COLA是一种基于最优传输的跨模态对齐框架，通过恢复特征空间的全局图像-文本对齐和局部结构一致性，有效解决CLIP模型在对抗性扰动下的特征不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性防御方法主要关注对抗性微调或提示优化，但忽视了CLIP编码特征中的差距问题。文本和图像特征在对抗性扰动下会出现严重不对齐，导致分类性能显著下降。

Method: COLA框架包含两个步骤：(1)将对抗性图像嵌入投影到类别文本特征张成的子空间，过滤非语义失真；(2)通过最优传输建模图像和文本作为离散分布，在成本计算中集成子空间投影，确保稳定的跨模态对齐。

Result: 在14个零样本分类基准测试中，COLA在ImageNet及其变体上对PGD对抗攻击的平均改进达到6.7%，同时在干净样本上保持高准确率。

Conclusion: COLA是一种无需训练且与现有微调模型兼容的有效方法，能够显著提升CLIP模型在对抗性条件下的鲁棒性。

Abstract: Vision-language models (VLMs) such as CLIP demonstrate strong generalization
in zero-shot classification but remain highly vulnerable to adversarial
perturbations. Existing methods primarily focus on adversarial fine-tuning or
prompt optimization; they often overlook the gaps in CLIP's encoded features,
which is shown as the text and image features lie far apart from each other.
This misalignment is significantly amplified under adversarial perturbations,
leading to severe degradation in classification performance. To address this
problem, we propose Cross-modality Alignment, dubbed COLA, an optimal
transport-based framework that explicitly addresses adversarial misalignment by
restoring both global image-text alignment and local structural consistency in
the feature space. (1) COLA first projects adversarial image embeddings onto a
subspace spanned by class text features, effectively filtering out non-semantic
distortions while preserving discriminative information. (2) It then models
images and texts as discrete distributions over multiple augmented views and
refines their alignment via OT, with the subspace projection seamlessly
integrated into the cost computation. This design ensures stable cross-modal
alignment even under adversarial conditions. COLA is training-free and
compatible with existing fine-tuned models. Extensive evaluations across 14
zero-shot classification benchmarks demonstrate the effectiveness of COLA,
especially with an average improvement of 6.7% on ImageNet and its variants
under PGD adversarial attacks, while maintaining high accuracy on clean
samples.

</details>


### [23] [Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification](https://arxiv.org/abs/2510.24078)
*William Yang,Xindi Wu,Zhiwei Deng,Esin Tureci,Olga Russakovsky*

Main category: cs.CV

TL;DR: BOB方法通过提取类无关属性并在微调过程中显式条件化，缓解了T2I模型在少样本细粒度分类中的过拟合问题，在多个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决T2I模型在生成合成训练数据时容易过拟合和降低多样性的问题，特别是在少样本细粒度分类任务中。

Method: 首先提取类无关属性（如场景背景和物体姿态），在微调T2I模型时显式条件化这些属性，并在生成过程中将其边缘化。

Result: 在Aircraft数据集上比DataDream提升7.4%，在24个实验设置中有18个优于现有方法，其中14个设置准确率提升超过2%。

Conclusion: BOB方法有效缓解了过拟合，保持了T2I模型的生成先验，在少样本细粒度分类中显著提升了合成数据的有效性。

Abstract: Text-to-image (T2I) models are increasingly used for synthetic dataset
generation, but generating effective synthetic training data for classification
remains challenging. Fine-tuning a T2I model with a few real examples can help
improve the quality of synthetic training data; however, it may also cause
overfitting and reduce diversity in the generated samples. We propose a
fine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for
fine-grained classification. Given a small set of real examples, we first
extract class-agnostic attributes such as scene background and object pose. We
then explicitly condition on these attributes during fine-tuning of the T2I
model and marginalize them out during generation. This design mitigates
overfitting, preserves the T2I model's generative prior, reduces estimation
errors, and further minimizes unintended inter-class associations. Extensive
experiments across multiple T2I models, backbones, and datasets show that our
method achieves state-of-the-art performance in low-shot fine-grained
classification when augmented with synthetic data. Concretely, BOB outperforms
DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning
a CLIP classifier with five real images augmented with 100 synthetic images).
In three of the four benchmarks, fine-tuning downstream models with 5 real
images augmented with BOB achieves better performance than fine-tuning with 10
real images. Collectively, BOB outperforms prior art in 18 of 24 experimental
settings, with 2+% accuracy improvements in 14 of these settings.

</details>


### [24] [OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation](https://arxiv.org/abs/2510.24093)
*Agus Gunawan,Samuel Teodoro,Yun Chen,Soo Ye Kim,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TL;DR: OmniText是一个无需训练的通才方法，能够执行广泛的文本图像操作任务，包括文本移除、重缩放、重定位以及具有风格控制的插入和编辑。它通过自注意力反转和交叉注意力重分布来解决现有方法的局限性，并在多个任务和指标上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的文本修复方法存在三个关键限制：无法移除文本、缺乏对渲染文本风格的控制以及倾向于生成重复字母。这些限制阻碍了它们在更广泛的文本图像操作任务中的应用。

Method: 研究了交叉注意力和自注意力机制的两个关键特性来实现文本移除和控制文本风格与内容。使用自注意力反转来减少文本幻觉，通过交叉注意力重分布来降低文本幻觉。在潜在优化框架中引入了新的损失函数：交叉注意力内容损失以提高文本渲染准确性，自注意力风格损失以促进风格定制。

Result: OmniText是第一个能够执行多样化文本图像操作任务的通才方法。在多个任务和指标上与其他文本修复方法相比达到了最先进的性能，并且与专门方法相当。

Conclusion: OmniText框架通过利用注意力机制的特性，成功解决了现有文本修复方法的局限性，实现了多样化的文本图像操作任务，并在性能上表现出色。

Abstract: Recent advancements in diffusion-based text synthesis have demonstrated
significant performance in inserting and editing text within images via
inpainting. However, despite the potential of text inpainting methods, three
key limitations hinder their applicability to broader Text Image Manipulation
(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over
the style of rendered text, and (iii) a tendency to generate duplicated
letters. To address these challenges, we propose OmniText, a training-free
generalist capable of performing a wide range of TIM tasks. Specifically, we
investigate two key properties of cross- and self-attention mechanisms to
enable text removal and to provide control over both text styles and content.
Our findings reveal that text removal can be achieved by applying
self-attention inversion, which mitigates the model's tendency to focus on
surrounding text, thus reducing text hallucinations. Additionally, we
redistribute cross-attention, as increasing the probability of certain text
tokens reduces text hallucination. For controllable inpainting, we introduce
novel loss functions in a latent optimization framework: a cross-attention
content loss to improve text rendering accuracy and a self-attention style loss
to facilitate style customization. Furthermore, we present OmniText-Bench, a
benchmark dataset for evaluating diverse TIM tasks. It includes input images,
target text with masks, and style references, covering diverse applications
such as text removal, rescaling, repositioning, and insertion and editing with
various styles. Our OmniText framework is the first generalist method capable
of performing diverse TIM tasks. It achieves state-of-the-art performance
across multiple tasks and metrics compared to other text inpainting methods and
is comparable with specialist methods.

</details>


### [25] [Enhancing Pre-trained Representation Classifiability can Boost its Interpretability](https://arxiv.org/abs/2510.24105)
*Shufan Shen,Zhaobo Qi,Junshu Sun,Qingming Huang,Qi Tian,Shuhui Wang*

Main category: cs.CV

TL;DR: 该论文提出了固有可解释性评分(IIS)来量化预训练视觉模型表示的可解释性，发现可解释性与分类能力呈正相关，并提出通过可解释性最大化微调来进一步提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉模型在广泛应用中对表示可解释性提出了新要求，但目前不清楚预训练表示能否同时实现高可解释性和高分类能力。

Method: 提出IIS评分，通过评估解释过程中的信息损失来量化表示可解释性，测量可解释语义在表示中的比例。

Result: 发现可解释性与分类能力呈正相关，分类能力越高的表示包含更多可被解释捕获的可解释语义。

Conclusion: 实践者可以统一改进预训练视觉模型的可解释性和分类能力，通过可解释性最大化微调可进一步提升分类性能，并获得基于解释的预测结果，准确性下降更小。

Abstract: The visual representation of a pre-trained model prioritizes the
classifiability on downstream tasks, while the widespread applications for
pre-trained visual models have posed new requirements for representation
interpretability. However, it remains unclear whether the pre-trained
representations can achieve high interpretability and classifiability
simultaneously. To answer this question, we quantify the representation
interpretability by leveraging its correlation with the ratio of interpretable
semantics within the representations. Given the pre-trained representations,
only the interpretable semantics can be captured by interpretations, whereas
the uninterpretable part leads to information loss. Based on this fact, we
propose the Inherent Interpretability Score (IIS) that evaluates the
information loss, measures the ratio of interpretable semantics, and quantifies
the representation interpretability. In the evaluation of the representation
interpretability with different classifiability, we surprisingly discover that
the interpretability and classifiability are positively correlated, i.e.,
representations with higher classifiability provide more interpretable
semantics that can be captured in the interpretations. This observation further
supports two benefits to the pre-trained representations. First, the
classifiability of representations can be further improved by fine-tuning with
interpretability maximization. Second, with the classifiability improvement for
the representations, we obtain predictions based on their interpretations with
less accuracy degradation. The discovered positive correlation and
corresponding applications show that practitioners can unify the improvements
in interpretability and classifiability for pre-trained vision models. Codes
are available at https://github.com/ssfgunner/IIS.

</details>


### [26] [UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations](https://arxiv.org/abs/2510.24116)
*Fengming Yu,Haiwei Pan,Kejia Zhang,Jian Guan,Haiying Jiang*

Main category: cs.CV

TL;DR: 提出了统一异构知识蒸馏（UHKD）框架，通过在频域利用中间特征来解决异构模型间的语义差异问题，实现跨架构的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法主要针对同构模型设计，在异构场景下性能下降，特别是涉及中间特征时。架构多样性导致语义差异，限制了中间表示的有效利用。

Method: 使用傅里叶变换在频域捕获全局特征信息，包含特征转换模块（FTM）生成紧凑的频域表示，以及可学习的特征对齐模块（FAM）进行多级匹配对齐。

Result: 在CIFAR-100和ImageNet-1K数据集上分别取得了5.59%和0.83%的性能提升，优于现有最佳方法。

Conclusion: UHKD通过频域特征对齐有效统一了异构表示，实现了视觉知识的高效利用，为异构模型间的知识蒸馏提供了有效解决方案。

Abstract: Knowledge distillation (KD) is an effective model compression technique that
transfers knowledge from a high-performance teacher to a lightweight student,
reducing cost while maintaining accuracy. In visual applications, where
large-scale image models are widely used, KD enables efficient deployment.
However, architectural diversity introduces semantic discrepancies that hinder
the use of intermediate representations. Most existing KD methods are designed
for homogeneous models and degrade in heterogeneous scenarios, especially when
intermediate features are involved. Prior studies mainly focus on the logits
space, making limited use of the semantic information in intermediate layers.
To address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)
is proposed as a framework that leverages intermediate features in the
frequency domain for cross-architecture transfer. Fourier transform is applied
to capture global feature information, alleviating representational
discrepancies between heterogeneous teacher-student pairs. A Feature
Transformation Module (FTM) produces compact frequency-domain representations
of teacher features, while a learnable Feature Alignment Module (FAM) projects
student features and aligns them via multi-level matching. Training is guided
by a joint objective combining mean squared error on intermediate features with
Kullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K
demonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD
as an effective approach for unifying heterogeneous representations and
enabling efficient utilization of visual knowledge

</details>


### [27] [DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery](https://arxiv.org/abs/2510.24117)
*Zan Wang,Siyu Chen,Luya Mo,Xinfeng Gao,Yuxin Shen,Lebin Ding,Wei Liang*

Main category: cs.CV

TL;DR: DogMo是一个大规模多视角RGB-D狗运动视频数据集，包含10只不同品种狗的1200个运动序列，用于从图像恢复狗的运动。该数据集解决了现有数据集缺乏多视角、真实3D数据以及规模和多样性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有狗运动数据集存在多视角和真实3D数据缺乏、规模和多样性有限等关键限制，阻碍了狗运动恢复研究的进展。

Method: 提出了一个三阶段、实例特定的优化流程，通过粗对齐、密集对应监督和时间正则化逐步优化SMAL模型的体型和姿态。

Result: 建立了四个运动恢复基准设置，支持单目和多视角、RGB和RGB-D输入的系统评估。

Conclusion: DogMo数据集和方法为推进狗运动恢复研究提供了理论基础，并为计算机视觉、计算机图形学和动物行为建模的交叉研究开辟了新方向。

Abstract: We present DogMo, a large-scale multi-view RGB-D video dataset capturing
diverse canine movements for the task of motion recovery from images. DogMo
comprises 1.2k motion sequences collected from 10 unique dogs, offering rich
variation in both motion and breed. It addresses key limitations of existing
dog motion datasets, including the lack of multi-view and real 3D data, as well
as limited scale and diversity. Leveraging DogMo, we establish four motion
recovery benchmark settings that support systematic evaluation across monocular
and multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,
we further introduce a three-stage, instance-specific optimization pipeline
that fits the SMAL model to the motion sequences. Our method progressively
refines body shape and pose through coarse alignment, dense correspondence
supervision, and temporal regularization. Our dataset and method provide a
principled foundation for advancing research in dog motion recovery and open up
new directions at the intersection of computer vision, computer graphics, and
animal behavior modeling.

</details>


### [28] [ETC: training-free diffusion models acceleration with Error-aware Trend Consistency](https://arxiv.org/abs/2510.24129)
*Jiajian Xie,Hubery Yin,Chen Li,Zhou Zhao,Shengyu Zhang*

Main category: cs.CV

TL;DR: 提出了Error-aware Trend Consistency (ETC)框架，通过趋势一致性预测和模型特定误差容忍度搜索，在保持生成质量的同时实现2.65倍加速。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量优秀，但受到昂贵迭代采样的限制。现有训练免费方法通过重用模型输出来加速，但忽略了去噪趋势且缺乏误差控制，导致轨迹偏差和结果不一致。

Method: ETC框架包含：(1) 趋势一致性预测器，利用扩散轨迹的平滑连续性，将历史去噪模式投影到稳定未来方向；(2) 模型特定误差容忍度搜索机制，通过识别从语义规划到质量精炼的过渡点来推导校正阈值。

Result: 实验显示ETC在FLUX基础上实现了2.65倍加速，一致性退化极小（-0.074 SSIM分数）。

Conclusion: ETC框架有效解决了扩散模型加速中的轨迹偏差问题，在保持生成质量的同时显著提升了采样效率。

Abstract: Diffusion models have achieved remarkable generative quality but remain
bottlenecked by costly iterative sampling. Recent training-free methods
accelerate diffusion process by reusing model outputs. However, these methods
ignore denoising trends and lack error control for model-specific tolerance,
leading to trajectory deviations under multi-step reuse and exacerbating
inconsistencies in the generated results. To address these issues, we introduce
Error-aware Trend Consistency (ETC), a framework that (1) introduces a
consistent trend predictor that leverages the smooth continuity of diffusion
trajectories, projecting historical denoising patterns into stable future
directions and progressively distributing them across multiple approximation
steps to achieve acceleration without deviating; (2) proposes a model-specific
error tolerance search mechanism that derives corrective thresholds by
identifying transition points from volatile semantic planning to stable quality
refinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX
with negligible (-0.074 SSIM score) degradation of consistency.

</details>


### [29] [Compositional Image Synthesis with Inference-Time Scaling](https://arxiv.org/abs/2510.24133)
*Minsuk Ji,Sanghyeok Lee,Namhyuk Ahn*

Main category: cs.CV

TL;DR: 提出了一种无需训练的方法，通过结合目标中心方法和自优化来提升文本到图像生成的布局忠实度，同时保持美学质量。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像模型在组合性方面仍有困难，经常无法准确渲染对象数量、属性和空间关系。

Method: 利用大语言模型从输入提示中合成显式布局，并将这些布局注入图像生成过程，通过目标中心视觉语言模型对多个候选结果进行重排序，迭代选择最符合提示的结果。

Result: 通过将显式布局基础与基于自优化的推理时间扩展相结合，该框架在场景与提示对齐方面比最近的文本到图像模型表现更好。

Conclusion: 该训练免费框架成功提升了文本到图像生成的组合准确性，同时保持了生成图像的美学质量。

Abstract: Despite their impressive realism, modern text-to-image models still struggle
with compositionality, often failing to render accurate object counts,
attributes, and spatial relations. To address this challenge, we present a
training-free framework that combines an object-centric approach with
self-refinement to improve layout faithfulness while preserving aesthetic
quality. Specifically, we leverage large language models (LLMs) to synthesize
explicit layouts from input prompts, and we inject these layouts into the image
generation process, where a object-centric vision-language model (VLM) judge
reranks multiple candidates to select the most prompt-aligned outcome
iteratively. By unifying explicit layout-grounding with self-refine-based
inference-time scaling, our framework achieves stronger scene alignment with
prompts compared to recent text-to-image models. The code are available at
https://github.com/gcl-inha/ReFocus.

</details>


### [30] [VC4VG: Optimizing Video Captions for Text-to-Video Generation](https://arxiv.org/abs/2510.24134)
*Yang Du,Zhuoran Lin,Kaiqiang Song,Biao Wang,Zhicheng Zheng,Tiezheng Ge,Bo Zheng,Qin Jin*

Main category: cs.CV

TL;DR: VC4VG是一个专门为文本到视频生成优化的视频字幕框架，通过分析T2V模型需求设计字幕内容，并构建了VC4VG-Bench评估基准，实验证明优化字幕质量能显著提升视频生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成领域缺乏专门针对T2V训练优化的视频字幕策略，高质量的视频-文本对对于生成连贯且符合指令的视频至关重要。

Method: 提出了VC4VG框架，从T2V角度分析字幕内容，将视频重建所需要素分解为多个维度，并设计原则性的字幕设计方法；构建了VC4VG-Bench基准，包含细粒度、多维度且按必要性分级的评估指标。

Result: 广泛的T2V微调实验表明，改进的字幕质量与视频生成性能之间存在强相关性，验证了该方法的有效性。

Conclusion: VC4VG框架成功提升了文本到视频生成的性能，为T2V训练提供了专门的字幕优化解决方案，并发布了基准工具和代码以支持进一步研究。

Abstract: Recent advances in text-to-video (T2V) generation highlight the critical role
of high-quality video-text pairs in training models capable of producing
coherent and instruction-aligned videos. However, strategies for optimizing
video captions specifically for T2V training remain underexplored. In this
paper, we introduce VC4VG (Video Captioning for Video Generation), a
comprehensive caption optimization framework tailored to the needs of T2V
models.We begin by analyzing caption content from a T2V perspective,
decomposing the essential elements required for video reconstruction into
multiple dimensions, and proposing a principled caption design methodology. To
support evaluation, we construct VC4VG-Bench, a new benchmark featuring
fine-grained, multi-dimensional, and necessity-graded metrics aligned with
T2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a
strong correlation between improved caption quality and video generation
performance, validating the effectiveness of our approach. We release all
benchmark tools and code at https://github.com/qyr0403/VC4VG to support further
research.

</details>


### [31] [MSRANetV2: An Explainable Deep Learning Architecture for Multi-class Classification of Colorectal Histopathological Images](https://arxiv.org/abs/2510.24136)
*Ovi Sarkar,Md Shafiuzzaman,Md. Faysal Ahamed,Golam Mahmud,Muhammad E. H. Chowdhury*

Main category: cs.CV

TL;DR: 提出了MSRANetV2模型用于结直肠癌组织图像分类，在公开数据集上取得了超过99%的准确率，并通过Grad-CAM增强了模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是全球癌症相关死亡的主要原因之一，传统诊断方法存在主观性强、耗时且易变的问题，需要开发更精确高效的诊断工具。

Method: 基于ResNet50V2架构，结合残差注意力机制和SE模块，通过通道对齐和上采样操作融合多尺度特征表示。

Result: 在两个公开数据集上表现优异：CRC-VAL-HE-7K数据集上准确率99.05%，AUC 0.9999；NCT-CRC-HE-100K数据集上准确率99.02%，AUC 0.9997。

Conclusion: MSRANetV2是一个可靠、可解释且高性能的结直肠癌组织分类模型，能够有效提升诊断精度和效率。

Abstract: Colorectal cancer (CRC) is a leading worldwide cause of cancer-related
mortality, and the role of prompt precise detection is of paramount interest in
improving patient outcomes. Conventional diagnostic methods such as colonoscopy
and histological examination routinely exhibit subjectivity, are extremely
time-consuming, and are susceptible to variation. Through the development of
digital pathology, deep learning algorithms have become a powerful approach in
enhancing diagnostic precision and efficiency. In our work, we proposed a
convolutional neural network architecture named MSRANetV2, specially optimized
for the classification of colorectal tissue images. The model employs a
ResNet50V2 backbone, extended with residual attention mechanisms and
squeeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained
spatial features. With channel alignment and upsampling operations, MSRANetV2
effectively fuses multi-scale representations, thereby enhancing the robustness
of the classification. We evaluated our model on a five-fold stratified
cross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and
NCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,
recall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900
plus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and
0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were
0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,
0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM
visualizations were incorporated to enhance model interpretability by
highlighting tissue areas that are medically relevant. These findings validate
that MSRANetV2 is a reliable, interpretable, and high-performing architectural
model for classifying CRC tissues.

</details>


### [32] [Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning](https://arxiv.org/abs/2510.24152)
*Aodi Wu,Xubo Luo*

Main category: cs.CV

TL;DR: 提出了一个基于混合提示路由器的系统框架，通过任务特定提示、视觉组装模块和优化推理参数，在自动驾驶场景理解任务中显著提升了视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在自动驾驶场景理解中面临的多任务干扰问题，提升在感知、预测、规划等安全关键任务上的表现。

Method: 使用混合提示路由器分类问题并分发到任务特定专家提示；任务特定提示包含坐标系统、空间推理规则、思维链/树推理等；视觉组装模块组合多视角图像和对象裁剪；针对任务配置推理参数。

Result: 在Qwen2.5-VL-72B上实现，Phase-1（干净数据）平均准确率70.87%，Phase-2（损坏数据）平均准确率72.85%。

Conclusion: 结构化提示和空间基础显著增强了视觉语言模型在安全关键自动驾驶任务上的性能。

Abstract: This technical report presents our solution for the RoboSense Challenge at
IROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving
scene understanding across perception, prediction, planning, and corruption
detection tasks. We propose a systematic framework built on four core
components. First, a Mixture-of-Prompts router classifies questions and
dispatches them to task-specific expert prompts, eliminating interference
across diverse question types. Second, task-specific prompts embed explicit
coordinate systems, spatial reasoning rules, role-playing,
Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to
each task. Third, a visual assembly module composes multi-view images with
object crops, magenta markers, and adaptive historical frames based on question
requirements. Fourth, we configure model inference parameters (temperature,
top-p, message roles) per task to optimize output quality. Implemented on
Qwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean
data) and 72.85% on Phase-2 (corrupted data), demonstrating that structured
prompting and spatial grounding substantially enhance VLM performance on
safety-critical autonomous driving tasks. Code and prompt are available at
https://github.com/wuaodi/UCAS-CSU-phase2.

</details>


### [33] [Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2](https://arxiv.org/abs/2510.24195)
*Ziqi Zhou,Yifan Hu,Yufei Song,Zijing Li,Shengshan Hu,Leo Yu Zhang,Dezhong Yao,Long Zheng,Hai Jin*

Main category: cs.CV

TL;DR: 提出了UAP-SAM2，这是首个针对SAM2视频分割模型的跨提示通用对抗攻击方法，通过双语义偏差框架显著提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: SAM2作为SAM的继任者，在视频分割方面表现出强大的泛化能力，但其鲁棒性尚未被探索。现有针对SAM的攻击方法是否能直接迁移到SAM2存在疑问，且SAM2的架构差异带来了新的挑战。

Method: 设计了目标扫描策略将每帧划分为k个区域并随机分配提示，以减少优化过程中的提示依赖性。提出双语义偏差框架，通过扭曲当前帧内的语义和破坏连续帧间的语义一致性来优化通用对抗扰动。

Result: 在六个数据集上的两个分割任务实验表明，UAP-SAM2显著优于现有最先进的攻击方法，证明了其有效性。

Conclusion: UAP-SAM2成功解决了SAM2架构带来的挑战，实现了跨提示的通用对抗攻击，为视频分割模型的鲁棒性评估提供了重要工具。

Abstract: Recent studies reveal the vulnerability of the image segmentation foundation
model SAM to adversarial examples. Its successor, SAM2, has attracted
significant attention due to its strong generalization capability in video
segmentation. However, its robustness remains unexplored, and it is unclear
whether existing attacks on SAM can be directly transferred to SAM2. In this
paper, we first analyze the performance gap of existing attacks between SAM and
SAM2 and highlight two key challenges arising from their architectural
differences: directional guidance from the prompt and semantic entanglement
across consecutive frames. To address these issues, we propose UAP-SAM2, the
first cross-prompt universal adversarial attack against SAM2 driven by dual
semantic deviation. For cross-prompt transferability, we begin by designing a
target-scanning strategy that divides each frame into k regions, each randomly
assigned a prompt, to reduce prompt dependency during optimization. For
effectiveness, we design a dual semantic deviation framework that optimizes a
UAP by distorting the semantics within the current frame and disrupting the
semantic consistency across consecutive frames. Extensive experiments on six
datasets across two segmentation tasks demonstrate the effectiveness of the
proposed method for SAM2. The comparative results show that UAP-SAM2
significantly outperforms state-of-the-art (SOTA) attacks by a large margin.

</details>


### [34] [CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and Uncertainty Reduction in Medical Image Segmentation](https://arxiv.org/abs/2510.24202)
*Anshul Kaushal,Kunal Jangid,Vinod K. Kurmi*

Main category: cs.CV

TL;DR: CLFSeg是一种基于编码器-解码器的医学图像分割框架，结合模糊卷积模块和BCE+Dice损失函数，用于息肉和心脏分割，在四个公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统CNN模型在医学图像分割中存在泛化性差、鲁棒性不足、无法处理不确定性问题，影响分割性能。

Method: 提出CLFSeg框架，集成模糊卷积模块结合卷积层和模糊逻辑，使用BCE+Dice损失处理类别不平衡问题，关注微小和边界区域。

Result: 在CVC-ColonDB、CVC-ClinicDB、EtisLaribPolypDB和ACDC四个数据集上表现优异，超越现有SOTA方法。

Conclusion: CLFSeg在保证计算效率的同时提升分割性能，是现实医疗诊断场景的潜在解决方案。

Abstract: Accurate polyp and cardiac segmentation for early detection and treatment is
essential for the diagnosis and treatment planning of cancer-like diseases.
Traditional convolutional neural network (CNN) based models have represented
limited generalizability, robustness, and inability to handle uncertainty,
which affects the segmentation performance. To solve these problems, this paper
introduces CLFSeg, an encoder-decoder based framework that aggregates the
Fuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy
logic. This module enhances the segmentation performance by identifying local
and global features while minimizing the uncertainty, noise, and ambiguity in
boundary regions, ensuring computing efficiency. In order to handle class
imbalance problem while focusing on the areas of interest with tiny and
boundary regions, binary cross-entropy (BCE) with dice loss is incorporated.
Our proposed model exhibits exceptional performance on four publicly available
datasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.
Extensive experiments and visual studies show CLFSeg surpasses the existing
SOTA performance and focuses on relevant regions of interest in anatomical
structures. The proposed CLFSeg improves performance while ensuring computing
efficiency, which makes it a potential solution for real-world medical
diagnostic scenarios. Project page is available at
https://visdomlab.github.io/CLFSeg/

</details>


### [35] [MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration](https://arxiv.org/abs/2510.24211)
*Junhyuk So,Hyunho Kook,Chaeyeon Jang,Eunhyeok Park*

Main category: cs.CV

TL;DR: MC-SJD是一种无需训练、无损的并行解码框架，通过扩展Speculative Jacobi Decoding来加速自回归视觉生成，解决了token跨迭代不稳定性问题，实现了图像生成4.2倍加速和视频生成13.3倍加速。


<details>
  <summary>Details</summary>
Motivation: 自回归建模在视觉生成中面临推理速度慢的问题，需要数千步才能生成单个样本，限制了实际应用。

Method: 提出MC-SJD，基于信息论耦合方法，最大化连续迭代中采样相同draft token的概率，仅需对现有算法进行单行修改。

Result: 在保持输出质量无损的情况下，实现了图像生成约4.2倍加速和视频生成约13.3倍加速。

Conclusion: MC-SJD通过简单的算法修改有效解决了自回归视觉生成的加速问题，具有重要的实用价值。

Abstract: While autoregressive (AR) modeling has recently emerged as a new paradigm in
visual generation, its practical adoption is severely constrained by the slow
inference speed of per-token generation, which often requires thousands of
steps to produce a single sample. To address this challenge, we propose MC-SJD,
a training-free, lossless parallel decoding framework designed to accelerate AR
visual generation by extending the recently introduced Speculative Jacobi
Decoding (SJD). Although SJD shows strong potential for accelerating AR
generation, we demonstrate that token instability across iterations
significantly reduces the acceptance rate, a limitation that primarily arises
from the independent sampling process used during draft token generation. To
overcome this, we introduce MC-SJD, an information-theoretic approach based on
coupling, which substantially accelerates standard SJD by maximizing the
probability of sampling identical draft tokens across consecutive iterations,
all while preserving its lossless property. Remarkably, this method requires
only a single-line modification to the existing algorithm, yet achieves
substantial performance gains, delivering up to a ~4.2x acceleration in image
generation and ~13.3x acceleration in video generation compared to standard AR
decoding, without any degradation in output quality.

</details>


### [36] [Beyond Inference Intervention: Identity-Decoupled Diffusion for Face Anonymization](https://arxiv.org/abs/2510.24213)
*Haoxin Yang,Yihong Lin,Jingdan Kang,Xuemiao Xu,Yue Li,Cheng Xu,Shengfeng He*

Main category: cs.CV

TL;DR: ID²Face是一个训练中心的人脸匿名化框架，通过解耦身份和非身份信息的结构化潜在空间，实现直接可控的匿名化，无需推理时优化。


<details>
  <summary>Details</summary>
Motivation: 解决主流扩散模型依赖推理时干预（如负引导或基于能量的优化）导致分布偏移和身份与非身份属性纠缠的问题，这些方法会降低视觉保真度和数据效用。

Method: 设计条件扩散模型，采用身份掩码学习方案。使用身份变分自编码器建模身份特征，通过双向潜在对齐提取和对齐非身份属性，然后通过软门控融合这些表示。训练时使用基于重构的重组损失来强制解耦。

Result: 实验表明ID²Face在视觉质量、身份抑制和效用保持方面优于现有方法。

Conclusion: ID²Face通过训练中心的方法有效解决了人脸匿名化中的身份泄漏问题，实现了更好的身份抑制和视觉质量保持。

Abstract: Face anonymization aims to conceal identity information while preserving
non-identity attributes. Mainstream diffusion models rely on inference-time
interventions such as negative guidance or energy-based optimization, which are
applied post-training to suppress identity features. These interventions often
introduce distribution shifts and entangle identity with non-identity
attributes, degrading visual fidelity and data utility. To address this, we
propose \textbf{ID\textsuperscript{2}Face}, a training-centric anonymization
framework that removes the need for inference-time optimization. The rationale
of our method is to learn a structured latent space where identity and
non-identity information are explicitly disentangled, enabling direct and
controllable anonymization at inference. To this end, we design a conditional
diffusion model with an identity-masked learning scheme. An Identity-Decoupled
Latent Recomposer uses an Identity Variational Autoencoder to model identity
features, while non-identity attributes are extracted from same-identity pairs
and aligned through bidirectional latent alignment. An Identity-Guided Latent
Harmonizer then fuses these representations via soft-gating conditioned on
noisy feature prediction. The model is trained with a recomposition-based
reconstruction loss to enforce disentanglement. At inference, anonymization is
achieved by sampling a random identity vector from the learned identity space.
To further suppress identity leakage, we introduce an Orthogonal Identity
Mapping strategy that enforces orthogonality between sampled and source
identity vectors. Experiments demonstrate that ID\textsuperscript{2}Face
outperforms existing methods in visual quality, identity suppression, and
utility preservation.

</details>


### [37] [SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs](https://arxiv.org/abs/2510.24214)
*Jinhong Deng,Wen Li,Joey Tianyi Zhou,Yang He*

Main category: cs.CV

TL;DR: 提出SCOPE方法，通过联合建模显著性和覆盖度来修剪多模态大语言模型中的冗余视觉token，提高计算效率同时保持语义完整性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉token剪枝方法主要基于注意力分数选择最显著的token，导致所选token语义不完整。需要同时考虑显著性和覆盖度来更好地保留语义完整性。

Method: 提出SCOPE方法，定义token集合覆盖度，计算未选token的覆盖增益，结合显著性得分得到SCOPE分数，迭代选择分数最高的token。

Result: 在多个视觉语言理解基准测试中使用LLaVA-1.5和LLaVA-Next模型进行实验，结果表明该方法持续优于先前方法。

Conclusion: SCOPE方法通过联合建模显著性和覆盖度，有效解决了视觉token剪枝中的语义不完整问题，显著提升了多模态大语言模型的效率。

Abstract: Multimodal Large Language Models (MLLMs) typically process a large number of
visual tokens, leading to considerable computational overhead, even though many
of these tokens are redundant. Existing visual token pruning methods primarily
focus on selecting the most salient tokens based on attention scores, resulting
in the semantic incompleteness of the selected tokens. In this paper, we
propose a novel visual token pruning strategy, called
\textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
coverage of the selected visual tokens to better preserve semantic
completeness. Specifically, we introduce a set-coverage for a given set of
selected tokens, computed based on the token relationships. We then define a
token-coverage gain for each unselected token, quantifying how much additional
coverage would be obtained by including it. By integrating the saliency score
into the token-coverage gain, we propose our SCOPE score and iteratively select
the token with the highest SCOPE score. We conduct extensive experiments on
multiple vision-language understanding benchmarks using the LLaVA-1.5 and
LLaVA-Next models. Experimental results demonstrate that our method
consistently outperforms prior approaches. Our code is available at
\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.

</details>


### [38] [Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation](https://arxiv.org/abs/2510.24231)
*Waseem Shariff,Timothy Hanley,Maciej Stec,Hossein Javidnia,Peter Corcoran*

Main category: cs.CV

TL;DR: 提出了首个基于事件相机的微眼跳数据集，使用SNN模型实现了约90%的分类准确率，为事件视觉研究建立了基准。


<details>
  <summary>Details</summary>
Motivation: 传统微眼跳研究方法成本高、可扩展性差且时间分辨率有限，而事件感知提供了高速、低延迟的替代方案。

Method: 使用Blender渲染高保真眼动场景，模拟0.5-2.0度角位移的微眼跳，通过v2e转换为事件流，并开发了Spiking-VGG16Flow等SNN模型进行分类。

Result: 模型实现了约90%的平均准确率，能够独立于事件数量或持续时间对微眼跳进行角位移分类。

Conclusion: 证明了SNN在精细运动识别方面的潜力，为基于事件的视觉研究建立了基准数据集和方法框架。

Abstract: Microsaccades are small, involuntary eye movements vital for visual
perception and neural processing. Traditional microsaccade studies typically
use eye trackers or frame-based analysis, which, while precise, are costly and
limited in scalability and temporal resolution. Event-based sensing offers a
high-speed, low-latency alternative by capturing fine-grained spatiotemporal
changes efficiently. This work introduces a pioneering event-based microsaccade
dataset to support research on small eye movement dynamics in cognitive
computing. Using Blender, we render high-fidelity eye movement scenarios and
simulate microsaccades with angular displacements from 0.5 to 2.0 degrees,
divided into seven distinct classes. These are converted to event streams using
v2e, preserving the natural temporal dynamics of microsaccades, with durations
ranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,
Spiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an
optical-flow-enhanced variant implemented in SpikingJelly. The models achieve
around 90 percent average accuracy, successfully classifying microsaccades by
angular displacement, independent of event count or duration. These results
demonstrate the potential of spiking neural networks for fine motion
recognition and establish a benchmark for event-based vision research. The
dataset, code, and trained models will be publicly available at
https://waseemshariff126.github.io/microsaccades/ .

</details>


### [39] [Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy](https://arxiv.org/abs/2510.24232)
*Qing Zhao,Weijian Deng,Pengxu Wei,ZiYi Dong,Hannan Lu,Xiangyang Ji,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出Lipschitz正则化目标检测(LROD)框架，通过将图像恢复直接集成到检测器特征学习中，解决恢复网络与检测网络之间的功能不匹配问题，提高在恶劣条件下的检测稳定性。


<details>
  <summary>Details</summary>
Motivation: 在恶劣条件下（如雾霾和低光照），图像恢复通常作为预处理步骤来提升检测器性能。但恢复网络与检测网络之间的功能不匹配会引入不稳定性，这个问题尚未得到充分研究。

Method: 通过Lipschitz连续性分析恢复网络和检测网络的功能差异，提出Lipschitz正则化目标检测框架，将图像恢复直接集成到检测器特征学习中，在训练过程中协调两个任务的Lipschitz连续性。

Result: 在雾霾和低光照基准测试上的广泛实验表明，LR-YOLO持续提高了检测稳定性、优化平滑度和整体准确率。

Conclusion: LROD框架有效解决了恢复与检测网络之间的功能不匹配问题，通过Lipschitz正则化实现了更稳定和准确的目标检测。

Abstract: To improve detection robustness in adverse conditions (e.g., haze and low
light), image restoration is commonly applied as a pre-processing step to
enhance image quality for the detector. However, the functional mismatch
between restoration and detection networks can introduce instability and hinder
effective integration -- an issue that remains underexplored. We revisit this
limitation through the lens of Lipschitz continuity, analyzing the functional
differences between restoration and detection networks in both the input space
and the parameter space. Our analysis shows that restoration networks perform
smooth, continuous transformations, while object detectors operate with
discontinuous decision boundaries, making them highly sensitive to minor
perturbations. This mismatch introduces instability in traditional cascade
frameworks, where even imperceptible noise from restoration is amplified during
detection, disrupting gradient flow and hindering optimization. To address
this, we propose Lipschitz-regularized object detection (LROD), a simple yet
effective framework that integrates image restoration directly into the
detector's feature learning, harmonizing the Lipschitz continuity of both tasks
during training. We implement this framework as Lipschitz-regularized YOLO
(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive
experiments on haze and low-light benchmarks demonstrate that LR-YOLO
consistently improves detection stability, optimization smoothness, and overall
accuracy.

</details>


### [40] [DeshadowMamba: Deshadowing as 1D Sequential Similarity](https://arxiv.org/abs/2510.24260)
*Zhaotong Yang,Yi Chen,Yanying Li,Shengfeng He,Yangyang Xu,Junyu Dong,Jian Yang,Yong Du*

Main category: cs.CV

TL;DR: 提出DeshadowMamba方法，通过CrossGate方向调制机制和ColorShift正则化，将Mamba选择性状态空间模型应用于图像阴影去除任务，实现更好的结构保持和色彩一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的深度模型在图像阴影去除中，固定的注意力模式容易混合不相关区域的照明线索，导致结构扭曲和色彩不一致。需要探索更有效的方法来捕获长程依赖关系。

Method: 1. 从序列建模角度重新审视阴影去除，使用Mamba选择性状态空间模型；2. 提出CrossGate方向调制机制，在Mamba输入门中注入阴影感知相似性；3. 引入ColorShift正则化，通过对比学习目标抑制色彩污染。

Result: 在公开基准测试上的广泛实验表明，DeshadowMamba在视觉质量和定量性能方面都达到了最先进水平。

Conclusion: 通过将序列建模与阴影去除所需的结构完整性和色彩一致性要求相结合，提出的方法能够有效解决现有注意力模型的局限性，实现高质量的阴影去除效果。

Abstract: Recent deep models for image shadow removal often rely on attention-based
architectures to capture long-range dependencies. However, their fixed
attention patterns tend to mix illumination cues from irrelevant regions,
leading to distorted structures and inconsistent colors. In this work, we
revisit shadow removal from a sequence modeling perspective and explore the use
of Mamba, a selective state space model that propagates global context through
directional state transitions. These transitions yield an efficient global
receptive field while preserving positional continuity. Despite its potential,
directly applying Mamba to image data is suboptimal, since it lacks awareness
of shadow-non-shadow semantics and remains susceptible to color interference
from nearby regions. To address these limitations, we propose CrossGate, a
directional modulation mechanism that injects shadow-aware similarity into
Mamba's input gate, allowing selective integration of relevant context along
transition axes. To further ensure appearance fidelity, we introduce ColorShift
regularization, a contrastive learning objective driven by global color
statistics. By synthesizing structured informative negatives, it guides the
model to suppress color contamination and achieve robust color restoration.
Together, these components adapt sequence modeling to the structural integrity
and chromatic consistency required for shadow removal. Extensive experiments on
public benchmarks demonstrate that DeshadowMamba achieves state-of-the-art
visual quality and strong quantitative performance.

</details>


### [41] [UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation](https://arxiv.org/abs/2510.24262)
*Jiyu Guo,Shuo Yang,Yiming Huang,Yancheng Long,Xiaobo Xia,Xiu Su,Bo Zhao,Zeke Xie,Liqiang Nie*

Main category: cs.CV

TL;DR: 提出了UtilGen，一种基于下游任务反馈的效用中心数据增强框架，通过双重优化策略生成任务特定的高效用训练数据，在多个基准数据集上平均准确率提升3.87%。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法主要关注数据的内在属性（如保真度和多样性），而忽略了不同下游任务的具体需求。训练数据需求因任务和网络架构而异，因此需要任务特定的数据生成方法。

Method: 引入权重分配网络评估合成样本的任务特定效用；采用双重优化策略：模型级优化调整生成模型以适应下游任务，实例级优化在每轮生成中调整生成策略（如提示嵌入和初始噪声）。

Result: 在8个不同复杂度和粒度的基准数据集上进行广泛实验，UtilGen始终优于现有方法，平均准确率比先前SOTA提高3.87%。数据影响和分布分析表明UtilGen生成更具影响力和任务相关性的合成数据。

Conclusion: UtilGen验证了从视觉特征中心到任务效用中心的数据增强范式转变的有效性，能够生成更高质量的任务特定训练数据。

Abstract: Data augmentation using generative models has emerged as a powerful paradigm
for enhancing performance in computer vision tasks. However, most existing
augmentation approaches primarily focus on optimizing intrinsic data attributes
-- such as fidelity and diversity -- to generate visually high-quality
synthetic data, while often neglecting task-specific requirements. Yet, it is
essential for data generators to account for the needs of downstream tasks, as
training data requirements can vary significantly across different tasks and
network architectures. To address these limitations, we propose UtilGen, a
novel utility-centric data augmentation framework that adaptively optimizes the
data generation process to produce task-specific, high-utility training data
via downstream task feedback. Specifically, we first introduce a weight
allocation network to evaluate the task-specific utility of each synthetic
sample. Guided by these evaluations, UtilGen iteratively refines the data
generation process using a dual-level optimization strategy to maximize the
synthetic data utility: (1) model-level optimization tailors the generative
model to the downstream task, and (2) instance-level optimization adjusts
generation policies -- such as prompt embeddings and initial noise -- at each
generation round. Extensive experiments on eight benchmark datasets of varying
complexity and granularity demonstrate that UtilGen consistently achieves
superior performance, with an average accuracy improvement of 3.87% over
previous SOTA. Further analysis of data influence and distribution reveals that
UtilGen produces more impactful and task-relevant synthetic data, validating
the effectiveness of the paradigm shift from visual characteristics-centric to
task utility-centric data augmentation.

</details>


### [42] [Training-free Source Attribution of AI-generated Images via Resynthesis](https://arxiv.org/abs/2510.24278)
*Pietro Bongini,Valentina Molinari,Andrea Costanzo,Benedetta Tondi,Mauro Barni*

Main category: cs.CV

TL;DR: 提出一种基于图像重合成的免训练单样本归因方法，通过生成描述图像的提示词，用候选源模型重合成图像，在特征空间中选择最接近原始图像的重合成结果进行归因。


<details>
  <summary>Details</summary>
Motivation: 解决合成图像来源归因的挑战，特别是在数据稀缺条件下需要少样本或零样本分类能力的情况。

Method: 基于图像重合成的免训练单样本归因方法：生成图像描述提示词，用所有候选源模型重合成图像，在特征空间中选择最接近原始图像的重合成结果。

Result: 提出的重合成方法在只有少量训练样本可用时优于现有技术，新数据集为开发评估少样本和零样本方法提供了有价值的基准。

Conclusion: 基于重合成的方法在数据稀缺条件下表现优异，新数据集为合成图像归因研究提供了具有挑战性的测试平台。

Abstract: Synthetic image source attribution is a challenging task, especially in data
scarcity conditions requiring few-shot or zero-shot classification
capabilities. We present a new training-free one-shot attribution method based
on image resynthesis. A prompt describing the image under analysis is
generated, then it is used to resynthesize the image with all the candidate
sources. The image is attributed to the model which produced the resynthesis
closest to the original image in a proper feature space. We also introduce a
new dataset for synthetic image attribution consisting of face images from
commercial and open-source text-to-image generators. The dataset provides a
challenging attribution framework, useful for developing new attribution models
and testing their capabilities on different generative architectures. The
dataset structure allows to test approaches based on resynthesis and to compare
them to few-shot methods. Results from state-of-the-art few-shot approaches and
other baselines show that the proposed resynthesis method outperforms existing
techniques when only a few samples are available for training or fine-tuning.
The experiments also demonstrate that the new dataset is a challenging one and
represents a valuable benchmark for developing and evaluating future few-shot
and zero-shot methods.

</details>


### [43] [ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model](https://arxiv.org/abs/2510.24285)
*Juntian Zhang,Song Jin,Chuanqi Cheng,Yuhan Liu,Yankai Lin,Xun Zhang,Yufei Zhang,Fei Jiang,Guojun Yin,Wei Lin,Rui Yan*

Main category: cs.CV

TL;DR: 提出了ViPER框架，通过自举式训练解决视觉语言模型在细粒度视觉感知上的瓶颈，实现了从粗到细的渐进式学习，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在细粒度视觉感知能力上的局限性，现有方法如监督微调会损害通用能力，而强化微调则偏重文本推理而非视觉感知。

Method: 提出两阶段任务，将视觉感知学习构建为从粗到细的渐进过程。开发ViPER自举框架，通过自批判和自预测实现迭代进化，整合图像级和实例级重建与两阶段强化学习策略。

Result: 在Qwen2.5-VL系列上应用ViPER得到Qwen-Viper系列，在7个综合基准测试中平均提升1.7%，在细粒度感知任务上提升高达6.0%，同时保持泛化能力。

Conclusion: ViPER不仅实现了感知能力的自我提升，还证明了生成与理解之间的互惠关系，为开发更自主和强大的视觉语言模型提供了突破。

Abstract: The limited capacity for fine-grained visual perception presents a critical
bottleneck for Vision-Language Models (VLMs) in real-world applications.
Addressing this is challenging due to the scarcity of high-quality data and the
limitations of existing methods: supervised fine-tuning (SFT) often compromises
general capabilities, while reinforcement fine-tuning (RFT) prioritizes textual
reasoning over visual perception. To bridge this gap, we propose a novel
two-stage task that structures visual perception learning as a coarse-to-fine
progressive process. Based on this task formulation, we develop ViPER, a
self-bootstrapping framework specifically designed to enable iterative
evolution through self-critiquing and self-prediction. By synergistically
integrating image-level and instance-level reconstruction with a two-stage
reinforcement learning strategy, ViPER establishes a closed-loop training
paradigm, where internally synthesized data directly fuel the enhancement of
perceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the
Qwen-Viper series. With an average gain of 1.7% on seven comprehensive
benchmarks spanning various tasks and up to 6.0% on fine-grained perception,
Qwen-Viper consistently demonstrates superior performance across different
vision-language scenarios while maintaining generalizability. Beyond enabling
self-improvement in perceptual capabilities, ViPER provides concrete evidence
for the reciprocal relationship between generation and understanding, a
breakthrough to developing more autonomous and capable VLMs.

</details>


### [44] [Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning](https://arxiv.org/abs/2510.24321)
*Ivica Dimitrovski,Vlatko Spasev,Ivan Kitanovski*

Main category: cs.CV

TL;DR: 本文系统探索了提示学习作为遥感图像场景分类的轻量级适应策略，在少样本场景下显著优于传统方法，特别是在跨域泛化方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 遥感应用中深度学习性能受限于标注数据稀缺和跨域标注成本高，而现有视觉语言模型存在领域差距和任务特定语义适应需求。

Method: 评估了四种代表性提示学习方法：上下文优化、条件上下文优化、多模态提示学习和自约束提示学习，并与零样本CLIP和线性探测基准进行比较。

Result: 在多个遥感基准数据集上的实验表明，提示学习方法在少样本场景下始终优于两个基准方法，其中自约束提示学习获得最稳健的跨域性能。

Conclusion: 提示学习为卫星和航空影像领域差距提供了可扩展且高效的解决方案，为未来研究奠定了坚实基础。

Abstract: Remote sensing applications increasingly rely on deep learning for scene
classification. However, their performance is often constrained by the scarcity
of labeled data and the high cost of annotation across diverse geographic and
sensor domains. While recent vision-language models like CLIP have shown
promise by learning transferable representations at scale by aligning visual
and textual modalities, their direct application to remote sensing remains
suboptimal due to significant domain gaps and the need for task-specific
semantic adaptation. To address this critical challenge, we systematically
explore prompt learning as a lightweight and efficient adaptation strategy for
few-shot remote sensing image scene classification. We evaluate several
representative methods, including Context Optimization, Conditional Context
Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating
Constraints. These approaches reflect complementary design philosophies: from
static context optimization to conditional prompts for enhanced generalization,
multi-modal prompts for joint vision-language adaptation, and semantically
regularized prompts for stable learning without forgetting. We benchmark these
prompt-learning methods against two standard baselines: zero-shot CLIP with
hand-crafted prompts and a linear probe trained on frozen CLIP features.
Through extensive experiments on multiple benchmark remote sensing datasets,
including cross-dataset generalization tests, we demonstrate that prompt
learning consistently outperforms both baselines in few-shot scenarios.
Notably, Prompting with Self-Regulating Constraints achieves the most robust
cross-domain performance. Our findings underscore prompt learning as a scalable
and efficient solution for bridging the domain gap in satellite and aerial
imagery, providing a strong foundation for future research in this field.

</details>


### [45] [Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2510.24366)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Ba-Thinh Lam,Vi Vu,Bach X. Nguyen,Jianhua Xing,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 提出了一种切换式双学生架构，通过选择最可靠的学生来增强协作并防止错误强化，同时引入损失感知指数移动平均策略来提升伪标签质量，在3D医学图像分割任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的师生框架在医学图像分割中表现良好，但受到师生网络强相关性和不可靠知识传递过程的限制，影响了学习效果。

Method: 采用切换式双学生架构，迭代选择最可靠的学生；引入损失感知指数移动平均策略，动态确保教师从学生处吸收有意义信息。

Result: 在3D医学图像分割数据集上广泛评估，优于最先进的半监督方法，在有限监督下提高了分割精度。

Conclusion: 该即插即用框架通过增强双学生协作和改善伪标签质量，有效提升了半监督医学图像分割的性能。

Abstract: Teacher-student frameworks have emerged as a leading approach in
semi-supervised medical image segmentation, demonstrating strong performance
across various tasks. However, the learning effects are still limited by the
strong correlation and unreliable knowledge transfer process between teacher
and student networks. To overcome this limitation, we introduce a novel
switching Dual-Student architecture that strategically selects the most
reliable student at each iteration to enhance dual-student collaboration and
prevent error reinforcement. We also introduce a strategy of Loss-Aware
Exponential Moving Average to dynamically ensure that the teacher absorbs
meaningful information from students, improving the quality of pseudo-labels.
Our plug-and-play framework is extensively evaluated on 3D medical image
segmentation datasets, where it outperforms state-of-the-art semi-supervised
methods, demonstrating its effectiveness in improving segmentation accuracy
under limited supervision.

</details>


### [46] [Decoupling What to Count and Where to See for Referring Expression Counting](https://arxiv.org/abs/2510.24374)
*Yuda Zou,Zijian Zhang,Yongchao Xu*

Main category: cs.CV

TL;DR: W2-Net是一个解决Referring Expression Counting (REC)问题的新框架，通过双查询机制将问题解耦为"计数什么"和"在哪里看"，显著提升了细粒度子类计数的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的REC方法存在一个根本性问题：标注点通常放在类代表性位置（如头部），导致模型只关注类级特征而忽略其他视觉区域（如腿部）的属性信息，无法有效区分子类。

Method: 提出W2-Net框架，包含双查询机制：what-to-count (w2c)查询用于定位对象，where-to-see (w2s)查询专门从属性特定的视觉区域提取特征。同时引入Subclass Separable Matching (SSM)匹配策略，通过排斥力增强子类间可分离性。

Result: 在REC-8K数据集上显著优于现有最优方法，计数误差分别降低22.5%（验证集）和18.0%（测试集），定位F1分别提升7%和8%。

Conclusion: W2-Net通过明确解耦"计数什么"和"在哪里看"，有效解决了REC中属性信息被忽视的问题，为细粒度子类计数提供了新的解决方案。

Abstract: Referring Expression Counting (REC) extends class-level object counting to
the fine-grained subclass-level, aiming to enumerate objects matching a textual
expression that specifies both the class and distinguishing attribute. A
fundamental challenge, however, has been overlooked: annotation points are
typically placed on class-representative locations (e.g., heads), forcing
models to focus on class-level features while neglecting attribute information
from other visual regions (e.g., legs for "walking"). To address this, we
propose W2-Net, a novel framework that explicitly decouples the problem into
"what to count" and "where to see" via a dual-query mechanism. Specifically,
alongside the standard what-to-count (w2c) queries that localize the object, we
introduce dedicated where-to-see (w2s) queries. The w2s queries are guided to
seek and extract features from attribute-specific visual regions, enabling
precise subclass discrimination. Furthermore, we introduce Subclass Separable
Matching (SSM), a novel matching strategy that incorporates a repulsive force
to enhance inter-subclass separability during label assignment. W2-Net
significantly outperforms the state-of-the-art on the REC-8K dataset, reducing
counting error by 22.5% (validation) and 18.0% (test), and improving
localization F1 by 7% and 8%, respectively. Code will be available.

</details>


### [47] [Stroke Lesion Segmentation in Clinical Workflows: A Modular, Lightweight, and Deployment-Ready Tool](https://arxiv.org/abs/2510.24378)
*Yann Kerverdo,Florent Leray,Youwan Mahé,Stéphanie Leplaideur,Francesca Galassi*

Main category: cs.CV

TL;DR: StrokeSeg是一个轻量级、模块化的脑卒中病灶分割框架，将研究级模型转化为可部署的临床应用工具，通过ONNX Runtime和量化技术减少模型大小约50%，性能与原PyTorch流水线相当。


<details>
  <summary>Details</summary>
Motivation: 解决nnU-Net等深度学习框架在临床部署中的依赖过重和单体设计问题，使高性能研究模型能够转化为便携、临床可用的工具。

Method: 采用模块化设计，预处理使用Anima工具箱生成BIDS兼容输出，推理使用ONNX Runtime和Float16量化，提供图形和命令行界面，支持Python脚本和Windows可执行文件分发。

Result: 在300名亚急性和慢性卒中患者的测试集上，分割性能与原始PyTorch流水线相当（Dice差异<10^{-3}），模型大小减少约50%。

Conclusion: 研究表明高性能研究流水线可以成功转化为便携、临床可用的分割工具，为深度学习模型在医疗领域的实际部署提供了可行方案。

Abstract: Deep learning frameworks such as nnU-Net achieve state-of-the-art performance
in brain lesion segmentation but remain difficult to deploy clinically due to
heavy dependencies and monolithic design. We introduce \textit{StrokeSeg}, a
modular and lightweight framework that translates research-grade stroke lesion
segmentation models into deployable applications. Preprocessing, inference, and
postprocessing are decoupled: preprocessing relies on the Anima toolbox with
BIDS-compliant outputs, and inference uses ONNX Runtime with \texttt{Float16}
quantisation, reducing model size by about 50\%. \textit{StrokeSeg} provides
both graphical and command-line interfaces and is distributed as Python scripts
and as a standalone Windows executable. On a held-out set of 300 sub-acute and
chronic stroke subjects, segmentation performance was equivalent to the
original PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that
high-performing research pipelines can be transformed into portable, clinically
usable tools.

</details>


### [48] [A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with a Multi-Scene Dataset](https://arxiv.org/abs/2510.24379)
*Zhuangfan Huang,Xiaosong Li,Gao Wang,Tao Ye,Haishu Tan,Huafeng Li*

Main category: cs.CV

TL;DR: 提出了一种亮度感知多尺度网络(MLSN)用于偏振图像融合，通过亮度分支和多尺度空间权重矩阵解决偏振图像固有对比度差异问题，在瓶颈层设计全局-局部特征融合机制，并在解码器阶段引入亮度增强模块实现非线性亮度校正。


<details>
  <summary>Details</summary>
Motivation: 偏振图像融合结合S0和DOLP图像，通过互补的纹理特征揭示表面粗糙度和材料特性，在伪装识别、组织病理分析、表面缺陷检测等领域有重要应用。需要解决复杂光照环境下不同偏振图像互补信息的整合问题。

Method: 编码器阶段：通过亮度分支生成多尺度空间权重矩阵，动态加权注入亮度到特征图；瓶颈层：设计全局-局部特征融合机制，执行窗口化自注意力计算；解码器阶段：提出亮度增强模块，建立亮度分布与纹理特征的映射关系，实现融合结果的非线性亮度校正。

Result: 在MSP、PIF和GAND数据集上的广泛实验验证，提出的MLSN在主观和客观评估中优于最先进方法，MS-SSIM和SD指标分别比其他方法的平均值高出8.57%、60.64%、10.26%、63.53%、22.21%和54.31%。

Conclusion: MLSN网络有效解决了偏振图像融合中的对比度差异和复杂光照适应性问题，提出的MSP数据集填补了高质量偏振图像融合数据集的空白，为相关应用提供了有力工具。

Abstract: Polarization image fusion combines S0 and DOLP images to reveal surface
roughness and material properties through complementary texture features, which
has important applications in camouflage recognition, tissue pathology
analysis, surface defect detection and other fields. To intergrate
coL-Splementary information from different polarized images in complex
luminance environment, we propose a luminance-aware multi-scale network (MLSN).
In the encoder stage, we propose a multi-scale spatial weight matrix through a
brightness-branch , which dynamically weighted inject the luminance into the
feature maps, solving the problem of inherent contrast difference in polarized
images. The global-local feature fusion mechanism is designed at the bottleneck
layer to perform windowed self-attention computation, to balance the global
context and local details through residual linking in the feature dimension
restructuring stage. In the decoder stage, to further improve the adaptability
to complex lighting, we propose a Brightness-Enhancement module, establishing
the mapping relationship between luminance distribution and texture features,
realizing the nonlinear luminance correction of the fusion result. We also
present MSP, an 1000 pairs of polarized images that covers 17 types of indoor
and outdoor complex lighting scenes. MSP provides four-direction polarization
raw maps, solving the scarcity of high-quality datasets in polarization image
fusion. Extensive experiment on MSP, PIF and GAND datasets verify that the
proposed MLSN outperms the state-of-the-art methods in subjective and objective
evaluations, and the MS-SSIM and SD metircs are higher than the average values
of other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,
respectively. The source code and dataset is avalable at
https://github.com/1hzf/MLS-UNet.

</details>


### [49] [When are radiology reports useful for training medical image classifiers?](https://arxiv.org/abs/2510.24385)
*Herman Bergström,Zhongqi Yue,Fredrik D. Johansson*

Main category: cs.CV

TL;DR: 该研究系统分析了放射学报告在医学图像分类训练中的使用时机和方式，发现报告在预训练阶段对文本中明确体现的标签任务有益，但在标签与文本弱相关的任务中可能有害；在微调阶段使用报告能带来显著改进。


<details>
  <summary>Details</summary>
Motivation: 医学图像训练通常依赖放射学报告的专家标注，但实际应用中需要放射科医生手动处理报告。研究旨在明确何时以及如何利用放射学报告来改进仅基于图像的分类模型。

Method: 系统研究放射学报告在预训练和微调阶段的使用，涵盖诊断和预后任务（如12个月再入院），并在不同训练集规模下进行测试。

Result: 发现：（1）当标签在文本中明确体现时，预训练阶段利用报告有益；但当标签与文本弱相关时，显式的图像-文本对齐预训练可能有害；（2）微调阶段使用报告能带来显著改进，在某些情况下比预训练方法影响更大。

Conclusion: 研究为如何利用特权文本数据训练医学图像分类器提供了实用指导，同时指出了当前研究的空白领域。

Abstract: Medical images used to train machine learning models are often accompanied by
radiology reports containing rich expert annotations. However, relying on these
reports as inputs for clinical prediction requires the timely manual work of a
trained radiologist. This raises a natural question: when can radiology reports
be leveraged during training to improve image-only classification? Prior works
are limited to evaluating pre-trained image representations by fine-tuning them
to predict diagnostic labels, often extracted from reports, ignoring tasks with
labels that are weakly associated with the text. To address this gap, we
conduct a systematic study of how radiology reports can be used during both
pre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,
12-month readmission), and under varying training set sizes. Our findings
reveal that: (1) Leveraging reports during pre-training is beneficial for
downstream classification tasks where the label is well-represented in the
text; however, pre-training through explicit image-text alignment can be
detrimental in settings where it's not; (2) Fine-tuning with reports can lead
to significant improvements and even have a larger impact than the pre-training
method in certain settings. These results provide actionable insights into when
and how to leverage privileged text data to train medical image classifiers
while highlighting gaps in current research.

</details>


### [50] [Unsupervised Detection of Post-Stroke Brain Abnormalities](https://arxiv.org/abs/2510.24398)
*Youwan Mahé,Elise Bannier,Stéphanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: 评估REFLECT生成模型在卒中后患者中无监督检测局灶性和非病灶性异常的能力，发现使用健康对照数据训练的模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 卒中后MRI显示的继发性结构变化（如萎缩和脑室扩大）是恢复和预后的影像生物标志物，但监督分割方法难以有效捕捉这些异常。

Method: 使用基于流的生成模型REFLECT，在ATLAS数据上通过双专家中央切片标注进行无监督异常检测，评估对象级性能，比较在卒中患者无病灶切片和健康对照数据上训练的两种模型。

Result: 在ATLAS测试对象上，使用健康对照数据训练的模型在病灶分割（Dice = 0.37 vs 0.27）和非病灶异常敏感性（FROC = 0.62 vs 0.43）方面表现更好。

Conclusion: 在完全健康的解剖结构上训练可以改善正常变异性的建模，从而更广泛、更可靠地检测结构异常。

Abstract: Post-stroke MRI not only delineates focal lesions but also reveals secondary
structural changes, such as atrophy and ventricular enlargement. These
abnormalities, increasingly recognised as imaging biomarkers of recovery and
outcome, remain poorly captured by supervised segmentation methods. We evaluate
REFLECT, a flow-based generative model, for unsupervised detection of both
focal and non-lesional abnormalities in post-stroke patients. Using dual-expert
central-slice annotations on ATLAS data, performance was assessed at the object
level with Free-Response ROC analysis for anomaly maps. Two models were trained
on lesion-free slices from stroke patients (ATLAS) and on healthy controls
(IXI) to test the effect of training data. On ATLAS test subjects, the
IXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and
improved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).
Training on fully healthy anatomy improves the modelling of normal variability,
enabling broader and more reliable detection of structural abnormalities.

</details>


### [51] [GenTrack: A New Generation of Multi-Object Tracking](https://arxiv.org/abs/2510.24399)
*Toan Van Nguyen,Rasmus G. K. Christiansen,Dirk Kraft,Leon Bodenhagen*

Main category: cs.CV

TL;DR: GenTrack是一种新颖的多目标跟踪方法，采用混合跟踪策略结合随机和确定性方式，利用粒子群优化(PSO)和社会交互来增强跟踪性能，特别是在处理遮挡和弱检测器时表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多目标跟踪中目标数量未知且时变、身份一致性维护困难、非线性动态处理以及弱噪声检测器下的鲁棒跟踪问题。

Method: 提出混合跟踪方法，结合随机和确定性策略；使用PSO和提出的适应度度量引导粒子；集成目标间社会交互；构建基于空间一致性、外观、检测置信度、轨迹惩罚和社会分数的综合状态观测模型。

Result: 在标准基准测试和实际场景中，GenTrack相比最先进的跟踪器表现出优越性能，减少了ID切换和轨迹丢失，特别是在遮挡情况下。

Conclusion: GenTrack提供了一种有效的多目标跟踪解决方案，通过混合策略和社会交互增强了跟踪鲁棒性，并提供了公开源代码实现便于复现和比较。

Abstract: This paper introduces a novel multi-object tracking (MOT) method, dubbed
GenTrack, whose main contributions include: a hybrid tracking approach
employing both stochastic and deterministic manners to robustly handle unknown
and time-varying numbers of targets, particularly in maintaining target
identity (ID) consistency and managing nonlinear dynamics, leveraging particle
swarm optimization (PSO) with some proposed fitness measures to guide
stochastic particles toward their target distribution modes, enabling effective
tracking even with weak and noisy object detectors, integration of social
interactions among targets to enhance PSO-guided particles as well as improve
continuous updates of both strong (matched) and weak (unmatched) tracks,
thereby reducing ID switches and track loss, especially during occlusions, a
GenTrack-based redefined visual MOT baseline incorporating a comprehensive
state and observation model based on space consistency, appearance, detection
confidence, track penalties, and social scores for systematic and efficient
target updates, and the first-ever publicly available source-code reference
implementation with minimal dependencies, featuring three variants, including
GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.
Experimental results have shown that GenTrack provides superior performance on
standard benchmarks and real-world scenarios compared to state-of-the-art
trackers, with integrated implementations of baselines for fair comparison.
Potential directions for future work are also discussed. The source-code
reference implementations of both the proposed method and compared-trackers are
provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack

</details>


### [52] [A Hybrid Approach for Visual Multi-Object Tracking](https://arxiv.org/abs/2510.24410)
*Toan Van Nguyen,Rasmus G. K. Christiansen,Dirk Kraft,Leon Bodenhagen*

Main category: cs.CV

TL;DR: 提出一种结合随机和确定性机制的多目标跟踪方法，通过粒子滤波和粒子群优化处理非线性动态，使用确定性关联确保标识一致性，适用于预录制视频和实时摄像头流。


<details>
  <summary>Details</summary>
Motivation: 解决未知和时变目标数量下的标识一致性问题，特别是在非线性动态、目标交互和长时间遮挡等复杂场景中。

Method: 使用随机粒子滤波处理非线性动态和非高斯噪声，结合粒子群优化引导粒子；采用确定性关联机制确保标识一致性；提出平滑更新方案处理弱跟踪；利用历史状态进行速度回归。

Result: 实验结果表明该方法在性能上优于当前最先进的跟踪器。

Conclusion: 提出的混合方法能有效处理复杂跟踪场景，在保持标识一致性的同时提供灵活的视频处理能力。

Abstract: This paper proposes a visual multi-object tracking method that jointly
employs stochastic and deterministic mechanisms to ensure identifier
consistency for unknown and time-varying target numbers under nonlinear
dynamics. A stochastic particle filter addresses nonlinear dynamics and
non-Gaussian noise, with support from particle swarm optimization (PSO) to
guide particles toward state distribution modes and mitigate divergence through
proposed fitness measures incorporating motion consistency, appearance
similarity, and social-interaction cues with neighboring targets. Deterministic
association further enforces identifier consistency via a proposed cost matrix
incorporating spatial consistency between particles and current detections,
detection confidences, and track penalties. Subsequently, a novel scheme is
proposed for the smooth updating of target states while preserving their
identities, particularly for weak tracks during interactions with other targets
and prolonged occlusions. Moreover, velocity regression over past states
provides trend-seed velocities, enhancing particle sampling and state updates.
The proposed tracker is designed to operate flexibly for both pre-recorded
videos and camera live streams, where future frames are unavailable.
Experimental results confirm superior performance compared to state-of-the-art
trackers. The source-code reference implementations of both the proposed method
and compared-trackers are provided on GitHub:
https://github.com/SDU-VelKoTek/GenTrack2

</details>


### [53] [50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon](https://arxiv.org/abs/2510.24413)
*Ali Ahmad Faour,Nabil Amacha,Ali J. Ghandour*

Main category: cs.CV

TL;DR: 提出一种基于卫星影像和机器学习的无传感器方法，用于实时监测黎巴嫩Qaraaoun水库的储水量，通过支持向量回归模型仅凭水面面积估算水库容积，准确率超过98%。


<details>
  <summary>Details</summary>
Motivation: 由于传感器频繁故障和维护能力有限，需要开发不依赖地面测量的可靠方法来监测水库储水量，确保可持续管理。

Method: 整合开源卫星影像（Sentinel-2和Landsat），使用新提出的水体分割指数进行水面提取，基于支持向量回归（SVR）训练机器学习模型，通过网格搜索优化超参数。

Result: 水体分割与真实岸线吻合度超过95%，SVR模型误差低于水库总容量的1.5%，决定系数超过0.98，生成了50年的时间序列数据。

Conclusion: 该方法具有鲁棒性和成本效益，为水库储量的连续、无传感器监测提供了实用解决方案，可推广到其他水体，对气候变化和环境模式研究具有重要价值。

Abstract: The sustainable management of the Qaraaoun Reservoir, the largest surface
water body in Lebanon located in the Bekaa Plain, depends on reliable
monitoring of its storage volume despite frequent sensor malfunctions and
limited maintenance capacity. This study introduces a sensor-free approach that
integrates open-source satellite imagery, advanced water-extent segmentation,
and machine learning to estimate the reservoir surface area and volume in near
real time. Sentinel-2 and Landsat images are processed, where surface water is
delineated using a newly proposed water segmentation index. A machine learning
model based on Support Vector Regression (SVR) is trained on a curated dataset
that includes water surface area, water level, and water volume calculations
using a reservoir bathymetry survey. The model is then able to estimate
reservoir volume relying solely on surface area extracted from satellite
imagery, without the need for ground measurements. Water segmentation using the
proposed index aligns with ground truth for more than 95 percent of the
shoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR
performance with error under 1.5 percent of full reservoir capacity and
coefficients of determination exceeding 0.98. These results demonstrate the
robustness and cost-effectiveness of the method, offering a practical solution
for continuous, sensor-independent monitoring of reservoir storage. The
proposed methodology can be replicated for other water bodies, and the
resulting 50 years of time-series data is valuable for research on climate
change and environmental patterns.

</details>


### [54] [XAI Evaluation Framework for Semantic Segmentation](https://arxiv.org/abs/2510.24414)
*Reem Hammoud,Abdul karim Gizzini,Ali J. Ghandour*

Main category: cs.CV

TL;DR: 提出了一个专门用于评估语义分割中可解释AI方法的系统性框架，该框架考虑了空间和上下文任务复杂性，使用像素级评估策略和精心设计的指标。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在安全关键和高风险领域的应用增加，确保透明度和信任至关重要。虽然可解释AI方法在分类任务中的评估已有进展，但针对语义分割的评估策略仍相对不足。

Method: 开发了一个全面的评估框架，专门用于语义分割中的XAI评估，采用像素级评估策略和精心设计的指标来提供细粒度的可解释性洞察。

Result: 使用最近适应的基于类激活映射的XAI方案进行模拟，证明了所提出方法的高效性、鲁棒性和可靠性。

Conclusion: 这些发现有助于推进透明、可信和可问责的语义分割模型的发展。

Abstract: Ensuring transparency and trust in artificial intelligence (AI) models is
essential, particularly as they are increasingly applied in safety-critical and
high-stakes domains. Explainable AI (XAI) has emerged as a promising approach
to address this challenge, yet the rigorous evaluation of XAI methods remains
crucial for optimizing the trade-offs between model complexity, predictive
performance, and interpretability. While extensive progress has been achieved
in evaluating XAI techniques for classification tasks, evaluation strategies
tailored to semantic segmentation remain relatively underexplored. This work
introduces a comprehensive and systematic evaluation framework specifically
designed for assessing XAI in semantic segmentation, explicitly accounting for
both spatial and contextual task complexities. The framework employs
pixel-level evaluation strategies and carefully designed metrics to provide
fine-grained interpretability insights. Simulation results using recently
adapted class activation mapping (CAM)-based XAI schemes demonstrate the
efficiency, robustness, and reliability of the proposed methodology. These
findings contribute to advancing transparent, trustworthy, and accountable
semantic segmentation models.

</details>


### [55] [Deeply-Conditioned Image Compression via Self-Generated Priors](https://arxiv.org/abs/2510.24437)
*Zhineng Zhao,Zhihai He,Zikun Zhou,Siwei Ma,Yaowei Wang*

Main category: cs.CV

TL;DR: 提出基于功能分解的深度条件图像压缩框架DCIC-sgp，通过自生成先验来分离图像的结构骨架和纹理细节，有效缓解低码率下的几何变形问题。


<details>
  <summary>Details</summary>
Motivation: 现有学习型图像压缩方法难以建模自然图像中复杂的相关性结构，特别是全局结构与局部纹理的纠缠，导致低码率下出现严重几何变形。

Method: 首先编码自生成的先验来捕捉图像结构骨架，然后用该先验深度调节整个压缩流程，特别是分析变换，使其专注于残差的高熵细节。

Result: 视觉分析显示大幅减轻了传统编解码器在低码率下的几何变形伪影；定量评估在Kodak、CLIC和Tecnick数据集上相比VVC测试模型VTM-12.1分别实现了14.4%、15.7%和15.1%的BD-rate降低。

Conclusion: 通过层次化、依赖驱动的方法实现了信息流的有效分离，在保持竞争力的同时显著提升了压缩性能。

Abstract: Learned image compression (LIC) has shown great promise for achieving high
rate-distortion performance. However, current LIC methods are often limited in
their capability to model the complex correlation structures inherent in
natural images, particularly the entanglement of invariant global structures
with transient local textures within a single monolithic representation. This
limitation precipitates severe geometric deformation at low bitrates. To
address this, we introduce a framework predicated on functional decomposition,
which we term Deeply-Conditioned Image Compression via self-generated priors
(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior
to encapsulate the image's structural backbone. This prior is subsequently
utilized not as mere side-information, but to holistically modulate the entire
compression pipeline. This deep conditioning, most critically of the analysis
transform, liberates it to dedicate its representational capacity to the
residual, high-entropy details. This hierarchical, dependency-driven approach
achieves an effective disentanglement of information streams. Our extensive
experiments validate this assertion; visual analysis demonstrates that our
method substantially mitigates the geometric deformation artifacts that plague
conventional codecs at low bitrates. Quantitatively, our framework establishes
highly competitive performance, achieving significant BD-rate reductions of
14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,
and Tecnick datasets.

</details>


### [56] [Rethinking Visual Intelligence: Insights from Video Pretraining](https://arxiv.org/abs/2510.24448)
*Pablo Acuaviva,Aram Davtyan,Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: 视频扩散模型(VDMs)相比大语言模型(LLMs)在视觉领域展现出更强的组合理解能力和数据效率，通过时空数据预训练获得的结构和动态归纳偏置支持广泛的视觉任务适应性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在语言领域表现出色，但在视觉领域仍面临组合理解、样本效率和通用问题解决能力的挑战，需要探索更好的视觉基础模型方法。

Method: 使用预训练的视频扩散模型(VDMs)和大语言模型(LLMs)，配备轻量级适配器，在ARC-AGI、ConceptARC、视觉游戏、路径规划和元胞自动机等基准测试中进行对比评估。

Result: 在多个基准测试中，视频扩散模型比语言模型表现出更高的数据效率，证明了视频预训练提供的归纳偏置对视觉基础模型发展的价值。

Conclusion: 视频预训练为视觉基础模型提供了有益的归纳偏置，是推进视觉智能发展的有前景方向。

Abstract: Large language models (LLMs) have demonstrated that large-scale pretraining
enables systems to adapt rapidly to new problems with little supervision in the
language domain. This success, however, has not translated as effectively to
the visual domain, where models, including LLMs, continue to struggle with
compositional understanding, sample efficiency, and general-purpose
problem-solving. We investigate Video Diffusion Models (VDMs) as a promising
direction for bridging this gap. Pretraining on spatiotemporal data endows
these models with strong inductive biases for structure and dynamics, which we
hypothesize can support broad task adaptability. To test this, we design a
controlled evaluation in which both a pretrained LLM and a pretrained VDM are
equipped with lightweight adapters and presented with tasks in their natural
modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,
route planning, and cellular automata, VDMs demonstrate higher data efficiency
than their language counterparts. Taken together, our results indicate that
video pretraining offers inductive biases that support progress toward visual
foundation models.

</details>


### [57] [A Critical Study towards the Detection of Parkinsons Disease using ML Technologies](https://arxiv.org/abs/2510.24456)
*Vivek Chetia,Abdul Taher Khan,Rahish Gogoi,David Kapsian Khual,Purnendu Bikash,Sajal Saha*

Main category: cs.CV

TL;DR: 该论文提出使用深度学习技术检测和分类茶叶病害，包括红锈病、Helopeltis和红蜘蛛螨三种病害，并评估了SSD MobileNet V2和Faster R-CNN ResNet50 V1两种目标检测模型，其中Faster R-CNN表现更好，mAP达到25%。


<details>
  <summary>Details</summary>
Motivation: 开发能够自动识别茶叶病害并定位受损区域的深度学习系统，以帮助茶农及时检测和处理病害问题。

Method: 使用SSD MobileNet V2和Faster R-CNN ResNet50 V1进行目标检测，同时使用Mask R-CNN进行实例分割，并开发了自定义方法来计算叶片受损区域。

Result: Faster R-CNN ResNet50 V1在IOU 0.50:0.95范围内获得25.2%的精确度和4.4%的召回率，mAP为25%，优于SSD MobileNet V2的20.9% mAP。

Conclusion: Faster R-CNN ResNet50 V1在茶叶病害检测中表现优于SSD MobileNet V2，结合Mask R-CNN的实例分割可以有效地识别病害并量化受损区域。

Abstract: The proposed solution is Deep Learning Technique that will be able classify
three types of tea leaves diseases from which two diseases are caused by the
pests and one due to pathogens (infectious organisms) and environmental
conditions and also show the area damaged by a disease in leaves. Namely Red
Rust, Helopeltis and Red spider mite respectively. In this paper we have
evaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for
the object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU
range of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.
While Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95
and recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than
SSD. Also used Mask R-CNN for Object Instance Segmentation where we have
implemented our custom method to calculate the damaged diseased portion of
leaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red
Spider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.

</details>


### [58] [Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras](https://arxiv.org/abs/2510.24464)
*Charles Javerliat,Pierre Raimbaud,Guillaume Lavoué*

Main category: cs.CV

TL;DR: Kineo是一个无需相机标定的多视角运动捕捉系统，使用消费级RGB相机即可实现自动化的3D人体运动重建，显著提升了标定精度和重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的无标定运动捕捉方法虽然减少了相机标定的需求，但存在计算成本高和重建精度低的问题，限制了非专业用户的使用和在野外环境中的应用。

Method: 利用现成的2D关键点检测器，通过置信度驱动的时空关键点采样策略和图优化方法，同时实现相机标定（包括畸变系数）和3D关键点重建，并引入重投影一致性评分来量化重建可靠性。

Result: 在EgoHumans和Human3.6M数据集上的评估显示，相比现有方法，相机平移误差减少83-85%，相机角度误差减少86-92%，世界平均关节误差减少83-91%。

Conclusion: Kineo提供了一种高效、准确的无标定运动捕捉解决方案，处理速度在实际场景中优于视频时长，代码已开源以促进实际应用。

Abstract: Markerless multiview motion capture is often constrained by the need for
precise camera calibration, limiting accessibility for non-experts and
in-the-wild captures. Existing calibration-free approaches mitigate this
requirement but suffer from high computational cost and reduced reconstruction
accuracy.
  We present Kineo, a fully automatic, calibration-free pipeline for markerless
motion capture from videos captured by unsynchronized, uncalibrated,
consumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf
detectors to simultaneously calibrate cameras, including Brown-Conrady
distortion coefficients, and reconstruct 3D keypoints and dense scene point
maps at metric scale. A confidence-driven spatio-temporal keypoint sampling
strategy, combined with graph-based global optimization, ensures robust
calibration at a fixed computational cost independent of sequence length. We
further introduce a pairwise reprojection consensus score to quantify 3D
reconstruction reliability for downstream tasks.
  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements
over prior calibration-free methods. Compared to previous state-of-the-art
approaches, Kineo reduces camera translation error by approximately 83-85%,
camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by
83-91%.
  Kineo is also efficient in real-world scenarios, processing multi-view
sequences faster than their duration in specific configuration (e.g., 36min to
process 1h20min of footage). The full pipeline and evaluation code are openly
released to promote reproducibility and practical adoption at
https://liris-xr.github.io/kineo/.

</details>


### [59] [Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling](https://arxiv.org/abs/2510.24474)
*Kyungmin Lee,Sihyun Yu,Jinwoo Shin*

Main category: cs.CV

TL;DR: 提出Decoupled MeanFlow方法，将流模型转换为流映射模型，无需架构修改即可实现1-4步高质量图像生成，显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的去噪生成模型（如扩散模型和流模型）需要大量去噪步骤，存在离散化误差问题。流映射方法能减少这种误差并加速采样，但通常需要架构修改，限制了与预训练模型的兼容性。

Method: 提出解耦平均流方法，通过条件化扩散变换器的最终块到后续时间步，将预训练流模型直接转换为流映射模型。结合增强训练技术，实现1-4步高质量生成。

Result: 在ImageNet 256x256和512x512上，1步FID分别达到2.16和2.12，4步FID分别达到1.51和1.68，超越现有方法，同时提供超过100倍的推理加速。

Conclusion: 通过将流模型转换为流映射模型的方法比从头训练流映射更高效有效，实现了高质量图像生成与快速推理的良好平衡。

Abstract: Denoising generative models, such as diffusion and flow-based models, produce
high-quality samples but require many denoising steps due to discretization
error. Flow maps, which estimate the average velocity between timesteps,
mitigate this error and enable faster sampling. However, their training
typically demands architectural changes that limit compatibility with
pretrained flow models. We introduce Decoupled MeanFlow, a simple decoding
strategy that converts flow models into flow map models without architectural
modifications. Our method conditions the final blocks of diffusion transformers
on the subsequent timestep, allowing pretrained flow models to be directly
repurposed as flow maps. Combined with enhanced training techniques, this
design enables high-quality generation in as few as 1 to 4 steps. Notably, we
find that training flow models and subsequently converting them is more
efficient and effective than training flow maps from scratch. On ImageNet
256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,
respectively, surpassing prior art by a large margin. Furthermore, we achieve
FID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the
performance of flow models while delivering over 100x faster inference.

</details>


### [60] [Fast and accurate neural reflectance transformation imaging through knowledge distillation](https://arxiv.org/abs/2510.24486)
*Tinsae G. Dulecha,Leonardo Righetto,Ruggero Pintus,Enrico Gobbetti,Andrea Giachetti*

Main category: cs.CV

TL;DR: 提出DisK-NeuralRTI方法，通过知识蒸馏降低NeuralRTI的计算成本，解决其在高分辨率图像上渲染速度慢的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RTI方法（如PTM和HSH）使用固定基函数，难以准确捕捉复杂反射场，导致伪影。NeuralRTI虽然质量更好，但解码网络参数多，计算成本高，在大图像上无法实时渲染。

Method: 使用知识蒸馏技术，训练一个更小的学生网络来模拟原始大型教师网络的行为，从而降低计算复杂度。

Result: DisK-NeuralRTI在保持与NeuralRTI相近质量的同时，显著降低了计算成本，使其能够在有限硬件上实现高分辨率图像的实时渲染。

Conclusion: 知识蒸馏是降低NeuralRTI计算成本的有效方法，能够在保持高质量的同时实现实时交互式重光照。

Abstract: Reflectance Transformation Imaging (RTI) is very popular for its ability to
visually analyze surfaces by enhancing surface details through interactive
relighting, starting from only a few tens of photographs taken with a fixed
camera and variable illumination. Traditional methods like Polynomial Texture
Maps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle
to accurately capture complex reflectance fields using few per-pixel
coefficients and fixed bases, leading to artifacts, especially in highly
reflective or shadowed areas. The NeuralRTI approach, which exploits a neural
autoencoder to learn a compact function that better approximates the local
reflectance as a function of light directions, has been shown to produce
superior quality at comparable storage cost. However, as it performs
interactive relighting with custom decoder networks with many parameters, the
rendering step is computationally expensive and not feasible at full resolution
for large images on limited hardware. Earlier attempts to reduce costs by
directly training smaller networks have failed to produce valid results. For
this reason, we propose to reduce its computational cost through a novel
solution based on Knowledge Distillation (DisK-NeuralRTI). ...

</details>


### [61] [Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs](https://arxiv.org/abs/2510.24514)
*Huanyu Zhang,Wenshan Wu,Chengzu Li,Ning Shang,Yan Xia,Yangyu Huang,Yifan Zhang,Li Dong,Zhang Zhang,Liang Wang,Tieniu Tan,Furu Wei*

Main category: cs.CV

TL;DR: Latent Sketchpad为多模态大语言模型添加内部视觉草稿本，通过视觉生成增强复杂场景中的推理能力，在保持原有推理性能的同时实现视觉思维。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在需要视觉规划和想象的复杂场景中表现不佳，而人类使用草图作为视觉思维工具来发展和交流想法，因此需要为模型提供类似的视觉思考能力。

Method: 在MLLMs的自回归推理过程中集成视觉生成，交替进行文本推理和视觉潜在表示生成，包含上下文感知视觉头和预训练草图解码器两个组件。

Result: 在MazePlanning数据集上的实验表明，Latent Sketchpad在保持骨干模型推理性能的同时，甚至在某些情况下表现更优，并在Gemma3和Qwen2.5-VL等不同MLLMs上具有良好的泛化性。

Conclusion: 通过将模型的文本推理扩展到视觉思维，该框架为人机交互和更广泛的应用开辟了新机会。

Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding,
they often struggle in complex scenarios that require visual planning and
imagination. Inspired by how humans use sketching as a form of visual thinking
to develop and communicate ideas, we introduce Latent Sketchpad, a framework
that equips MLLMs with an internal visual scratchpad. The internal visual
representations of MLLMs have traditionally been confined to perceptual
understanding. We repurpose them to support generative visual thought without
compromising reasoning ability. Building on frontier MLLMs, our approach
integrates visual generation directly into their native autoregressive
reasoning process. It allows the model to interleave textual reasoning with the
generation of visual latents. These latents guide the internal thought process
and can be translated into sketch images for interpretability. To realize this,
we introduce two components: a Context-Aware Vision Head autoregressively
produces visual representations, and a pretrained Sketch Decoder renders these
into human-interpretable images. We evaluate the framework on our new dataset
MazePlanning. Experiments across various MLLMs show that Latent Sketchpad
delivers comparable or even superior reasoning performance to their backbone.
It further generalizes across distinct frontier MLLMs, including Gemma3 and
Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our
framework opens new opportunities for richer human-computer interaction and
broader applications. More details and resources are available on our project
page: https://latent-sketchpad.github.io/.

</details>


### [62] [OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents](https://arxiv.org/abs/2510.24563)
*Hongrui Jia,Jitong Liao,Xi Zhang,Haiyang Xu,Tianbao Xie,Chaoya Jiang,Ming Yan,Si Liu,Wei Ye,Fei Huang*

Main category: cs.CV

TL;DR: OSWorld-MCP是首个全面公平的基准测试，用于评估计算机使用代理在真实环境中的工具调用、GUI操作和决策能力，填补了现有评估主要关注GUI交互而忽视工具调用能力的空白。


<details>
  <summary>Details</summary>
Motivation: 随着多模态代理决策和推理能力的进步，它们在计算机应用场景中展现出巨大潜力。过去的评估主要关注GUI交互技能，而工具调用能力（如通过模型上下文协议MCP实现）被严重忽视。将集成工具调用的代理与仅评估GUI交互的代理进行比较本质上是不公平的。

Method: 设计了一个新颖的自动化代码生成流水线来创建工具，并结合现有工具的精选集合。通过严格的手动验证产生了158个高质量工具（覆盖7个常见应用），每个工具都验证了功能正确性、实际适用性和多功能性。

Result: 在OSWorld-MCP上对最先进的多模态代理进行广泛评估显示，MCP工具通常能提高任务成功率（例如OpenAI o3在15步时从8.3%提高到20.4%，Claude 4 Sonnet在50步时从40.1%提高到43.3%）。然而，即使是最强的模型工具调用率也相对较低，仅为36.3%，表明还有改进空间。

Conclusion: 通过明确测量MCP工具使用技能，OSWorld-MCP加深了对多模态代理的理解，并为评估复杂工具辅助环境中的性能设立了新标准。

Abstract: With advances in decision-making and reasoning capabilities, multimodal
agents show strong potential in computer application scenarios. Past
evaluations have mainly assessed GUI interaction skills, while tool invocation
abilities, such as those enabled by the Model Context Protocol (MCP), have been
largely overlooked. Comparing agents with integrated tool invocation to those
evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,
the first comprehensive and fair benchmark for assessing computer-use agents'
tool invocation, GUI operation, and decision-making abilities in a real-world
environment. We design a novel automated code-generation pipeline to create
tools and combine them with a curated selection from existing tools. Rigorous
manual validation yields 158 high-quality tools (covering 7 common
applications), each verified for correct functionality, practical
applicability, and versatility. Extensive evaluations of state-of-the-art
multimodal agents on OSWorld-MCP show that MCP tools generally improve task
success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%
to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of
assessing tool invocation capabilities. However, even the strongest models have
relatively low tool invocation rates, Only 36.3%, indicating room for
improvement and highlighting the benchmark's challenge. By explicitly measuring
MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents
and sets a new standard for evaluating performance in complex, tool-assisted
environments. Our code, environment, and data are publicly available at
https://osworld-mcp.github.io.

</details>


### [63] [Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT](https://arxiv.org/abs/2510.24579)
*Xu Jiang,Huiying Pan,Ligen Shi,Jianing Sun,Wenfeng Xu,Xing Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的锥束CT散射伪影校正方法，结合物理先验知识和KAN网络，有效提升图像质量


<details>
  <summary>Details</summary>
Motivation: 锥束CT在数据采集过程中容易受到散射影响，导致CT值偏差和组织对比度降低，从而影响诊断准确性

Method: 利用散射概率分布的旋转对称性，使用高斯径向基函数建模点散射函数，并将其嵌入到Kolmogorov-Arnold Networks层中，结合物理特性和复杂函数映射能力

Result: 在合成和真实扫描实验中验证了方法的有效性，能够有效校正重建图像中的散射伪影，在定量指标上优于现有方法

Conclusion: 该方法通过结合物理先验知识和深度学习，成功提升了锥束CT散射伪影校正的性能

Abstract: Cone-beam CT (CBCT) employs a flat-panel detector to achieve
three-dimensional imaging with high spatial resolution. However, CBCT is
susceptible to scatter during data acquisition, which introduces CT value bias
and reduced tissue contrast in the reconstructed images, ultimately degrading
diagnostic accuracy. To address this issue, we propose a deep learning-based
scatter artifact correction method inspired by physical prior knowledge.
Leveraging the fact that the observed point scatter probability density
distribution exhibits rotational symmetry in the projection domain. The method
uses Gaussian Radial Basis Functions (RBF) to model the point scatter function
and embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides
efficient nonlinear mapping capabilities for learning high-dimensional scatter
features. By incorporating the physical characteristics of the scattered photon
distribution together with the complex function mapping capacity of KAN, the
model improves its ability to accurately represent scatter. The effectiveness
of the method is validated through both synthetic and real-scan experiments.
Experimental results show that the model can effectively correct the scatter
artifacts in the reconstructed images and is superior to the current methods in
terms of quantitative metrics.

</details>


### [64] [A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries](https://arxiv.org/abs/2510.24640)
*Xin Zhang,Yuqi Song,Fei Zuo*

Main category: cs.CV

TL;DR: 提出了一种新颖的双分支卷积神经网络用于人脸伪造检测，结合空间域和频域的互补线索，通过注意力机制融合异构特征，在DiFF基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展使得伪造人脸图像越来越逼真，这对AI安全、数字媒体完整性和公众信任构成严重威胁，迫切需要鲁棒且通用的人脸伪造检测方法。

Method: 采用双分支CNN架构：RGB分支捕获语义信息，频率分支关注高频伪影；引入通道注意力模块自适应融合异构特征；设计统一的FSC损失函数结合焦点损失、监督对比损失和频率中心边界损失。

Result: 在DiFF基准测试中表现优异，涵盖文本到图像、图像到图像、人脸交换和人脸编辑四种代表性伪造方法，超越了平均人类准确率。

Conclusion: 该方法有效证明了模型在保护AI生态系统免受视觉伪造攻击方面的潜力，为AI安全基础设施提供了重要贡献。

Abstract: The rapid advancement of generative AI has enabled the creation of highly
realistic forged facial images, posing significant threats to AI security,
digital media integrity, and public trust. Face forgery techniques, ranging
from face swapping and attribute editing to powerful diffusion-based image
synthesis, are increasingly being used for malicious purposes such as
misinformation, identity fraud, and defamation. This growing challenge
underscores the urgent need for robust and generalizable face forgery detection
methods as a critical component of AI security infrastructure. In this work, we
propose a novel dual-branch convolutional neural network for face forgery
detection that leverages complementary cues from both spatial and frequency
domains. The RGB branch captures semantic information, while the frequency
branch focuses on high-frequency artifacts that are difficult for generative
models to suppress. A channel attention module is introduced to adaptively fuse
these heterogeneous features, highlighting the most informative channels for
forgery discrimination. To guide the network's learning process, we design a
unified loss function, FSC Loss, that combines focal loss, supervised
contrastive loss, and a frequency center margin loss to enhance class
separability and robustness. We evaluate our model on the DiFF benchmark, which
includes forged images generated from four representative methods:
text-to-image, image-to-image, face swap, and face edit. Our method achieves
strong performance across all categories and outperforms average human
accuracy. These results demonstrate the model's effectiveness and its potential
contribution to safeguarding AI ecosystems against visual forgery attacks.

</details>


### [65] [Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology](https://arxiv.org/abs/2510.24653)
*Veronica Thai,Rui Li,Meng Ling,Shuning Jiang,Jeremy Wolfe,Raghu Machiraju,Yan Hu,Zaibo Li,Anil Parwani,Jian Chen*

Main category: cs.CV

TL;DR: PathoGaze1.0是一个全面的行为数据集，记录了病理学家在癌症诊断过程中的视觉搜索和决策过程，包含眼动追踪、鼠标交互等数据。


<details>
  <summary>Details</summary>
Motivation: 病理学家解读千兆像素全切片图像(WSIs)的准确性仅约70%，且增加第二位病理学家无法显著改善决策一致性。该领域缺乏足够的行为数据来解释诊断错误和不一致性。

Method: 开发了PathoGaze1.0数据集，收集了19位病理学家解读397张WSIs的18.69小时数据，包括眼动追踪、鼠标交互、刺激追踪、视口导航和诊断决策数据(EMSVD)。使用名为PTAH的应用基础测试平台确保生态效度。

Result: 共记录了171,909次注视、263,320次扫视和1,867,362次鼠标交互事件。数据集可用于改进病理学家和AI系统的训练。

Conclusion: PathoGaze1.0填补了病理诊断行为数据的空白，为理解诊断错误提供了重要资源，所有实验已预注册，完整数据集和分析代码已公开。

Abstract: Interpretation of giga-pixel whole-slide images (WSIs) is an important but
difficult task for pathologists. Their diagnostic accuracy is estimated to
average around 70%. Adding a second pathologist does not substantially improve
decision consistency. The field lacks adequate behavioral data to explain
diagnostic errors and inconsistencies. To fill in this gap, we present
PathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual
search and decision-making processes of the full diagnostic workflow during
cancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse
interaction, stimulus tracking, viewport navigation, and diagnostic decision
data (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data
collection process emphasizes ecological validity through an
application-grounded testbed, called PTAH. In total, we recorded 171,909
fixations, 263,320 saccades, and 1,867,362 mouse interaction events. In
addition, such data could also be used to improve the training of both
pathologists and AI systems that might support human experts. All experiments
were preregistered at https://osf.io/hj9a7, and the complete dataset along with
analysis code is available at https://go.osu.edu/pathogaze.

</details>


### [66] [Group Relative Attention Guidance for Image Editing](https://arxiv.org/abs/2510.24657)
*Xuanpu Zhang,Xuesong Niu,Ruidong Chen,Dan Song,Jianhao Zeng,Penghui Du,Haoxiang Cao,Kai Wu,An-an Liu*

Main category: cs.CV

TL;DR: 提出Group Relative Attention Guidance方法，通过重新加权注意力机制中的delta值来调节模型对输入图像相对于编辑指令的关注度，实现无需调参的连续精细编辑强度控制。


<details>
  <summary>Details</summary>
Motivation: 现有基于Diffusion-in-Transformer的图像编辑方法缺乏对编辑程度的有效控制，限制了实现更定制化结果的能力。

Method: 分析DiT模型中的MM-Attention机制，发现Query和Key共享仅与层相关的偏置向量。将该偏置解释为模型固有编辑行为，而每个token与其对应偏置之间的delta编码内容特定的编辑信号。提出GRAG方法重新加权不同token的delta值。

Result: 在现有图像编辑框架上的广泛实验表明，GRAG仅需四行代码即可集成，持续提升编辑质量。相比常用的Classifier-Free Guidance，GRAG实现了更平滑和精确的编辑程度控制。

Conclusion: GRAG是一种简单有效的方法，能够实现连续精细的编辑强度控制，无需任何调参，显著提升图像编辑质量。

Abstract: Recently, image editing based on Diffusion-in-Transformer models has
undergone rapid development. However, existing editing methods often lack
effective control over the degree of editing, limiting their ability to achieve
more customized results. To address this limitation, we investigate the
MM-Attention mechanism within the DiT model and observe that the Query and Key
tokens share a bias vector that is only layer-dependent. We interpret this bias
as representing the model's inherent editing behavior, while the delta between
each token and its corresponding bias encodes the content-specific editing
signals. Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning. Extensive experiments conducted on existing image
editing frameworks demonstrate that GRAG can be integrated with as few as four
lines of code, consistently enhancing editing quality. Moreover, compared to
the commonly used Classifier-Free Guidance, GRAG achieves smoother and more
precise control over the degree of editing. Our code will be released at
https://github.com/little-misfit/GRAG-Image-Editing.

</details>


### [67] [SAGE: Structure-Aware Generative Video Transitions between Diverse Clips](https://arxiv.org/abs/2510.24667)
*Mia Kan,Yilin Liu,Niloy Mitra*

Main category: cs.CV

TL;DR: 提出SAGE方法，一种零样本的视频过渡技术，结合结构引导和生成合成，能在不同视频片段间产生平滑、语义一致的过渡效果。


<details>
  <summary>Details</summary>
Motivation: 传统视频过渡方法在处理时间间隔大、语义差异显著的视频片段时存在困难，无法保持视觉连贯性，需要内容感知的过渡解决方案。

Method: 基于艺术工作流程，通过轮廓对齐和显著特征插值保持结构连续性，结合线图和运动流的结构引导与生成合成，实现零样本视频过渡。

Result: 在定量指标和用户研究中，SAGE优于传统方法和生成基线，能有效处理不同视频片段间的过渡。

Conclusion: SAGE方法通过结构感知的生成视频过渡，成功解决了不同视频片段间的平滑过渡问题，无需微调即可产生高质量结果。

Abstract: Video transitions aim to synthesize intermediate frames between two clips,
but naive approaches such as linear blending introduce artifacts that limit
professional use or break temporal coherence. Traditional techniques
(cross-fades, morphing, frame interpolation) and recent generative inbetweening
methods can produce high-quality plausible intermediates, but they struggle
with bridging diverse clips involving large temporal gaps or significant
semantic differences, leaving a gap for content-aware and visually coherent
transitions. We address this challenge by drawing on artistic workflows,
distilling strategies such as aligning silhouettes and interpolating salient
features to preserve structure and perceptual continuity. Building on this, we
propose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot
approach that combines structural guidance, provided via line maps and motion
flow, with generative synthesis, enabling smooth, semantically consistent
transitions without fine-tuning. Extensive experiments and comparison with
current alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate
that SAGE outperforms both classical and generative baselines on quantitative
metrics and user studies for producing transitions between diverse clips. Code
to be released on acceptance.

</details>


### [68] [MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection](https://arxiv.org/abs/2510.24688)
*Yun Zhang,Zhaoliang Zheng,Johnson Liu,Zhiyu Huang,Zewei Zhou,Zonglin Meng,Tianhui Cai,Jiaqi Ma*

Main category: cs.CV

TL;DR: MIC-BEV是一个基于Transformer的鸟瞰图感知框架，用于基础设施多摄像头3D物体检测，支持异构摄像头配置并具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于摄像头的检测模型在基础设施感知场景中表现不佳，面临多视角设置、异构摄像头配置、视觉质量下降和多样化道路布局等挑战。

Method: 提出基于Transformer的BEV感知框架，包含图增强融合模块，利用摄像头与BEV单元间的几何关系和潜在视觉线索，将多视角图像特征整合到BEV空间。

Result: 在M2I合成数据集和RoScenes真实数据集上的实验表明，MIC-BEV在3D物体检测上达到最先进性能，在极端天气和传感器退化等挑战条件下保持鲁棒性。

Conclusion: MIC-BEV展示了在现实世界部署的潜力，为智能交通系统的基础设施感知提供了有效解决方案。

Abstract: Infrastructure-based perception plays a crucial role in intelligent
transportation systems, offering global situational awareness and enabling
cooperative autonomy. However, existing camera-based detection models often
underperform in such scenarios due to challenges such as multi-view
infrastructure setup, diverse camera configurations, degraded visual inputs,
and various road layouts. We introduce MIC-BEV, a Transformer-based
bird's-eye-view (BEV) perception framework for infrastructure-based
multi-camera 3D object detection. MIC-BEV flexibly supports a variable number
of cameras with heterogeneous intrinsic and extrinsic parameters and
demonstrates strong robustness under sensor degradation. The proposed
graph-enhanced fusion module in MIC-BEV integrates multi-view image features
into the BEV space by exploiting geometric relationships between cameras and
BEV cells alongside latent visual cues. To support training and evaluation, we
introduce M2I, a synthetic dataset for infrastructure-based object detection,
featuring diverse camera configurations, road layouts, and environmental
conditions. Extensive experiments on both M2I and the real-world dataset
RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D
object detection. It also remains robust under challenging conditions,
including extreme weather and sensor degradation. These results highlight the
potential of MIC-BEV for real-world deployment. The dataset and source code are
available at: https://github.com/HandsomeYun/MIC-BEV.

</details>


### [69] [Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?](https://arxiv.org/abs/2510.24709)
*Yihao Li,Saeed Salehi,Lyle Ungar,Konrad P. Kording*

Main category: cs.CV

TL;DR: 研究发现预训练的视觉变换器（ViTs）能够自然涌现出物体绑定能力，即识别哪些图像块属于同一物体的能力。这种能力在自监督模型（DINO、MAE、CLIP）中表现突出，但在ImageNet监督模型中较弱，表明这是通过特定预训练目标获得的能力。


<details>
  <summary>Details</summary>
Motivation: 探索视觉变换器是否能够自然涌现出物体绑定能力，即识别哪些图像块属于同一物体的能力，这对于理解ViTs的内部工作机制具有重要意义。

Method: 使用相似性探针从ViT各层的图像块嵌入中解码IsSameObject属性，通过消融实验验证该信号对注意力机制和下游任务的影响。

Result: IsSameObject解码准确率超过90%，该能力在自监督ViTs中可靠涌现，但在监督模型中较弱。IsSameObject编码在物体特征之上的低维子空间中，并主动引导注意力。

Conclusion: ViTs能够自然涌现出物体绑定能力，这种能力服务于预训练目标，挑战了ViTs缺乏物体绑定的观点，展示了符号知识如何在连接主义系统中自然涌现。

Abstract: Object binding, the brain's ability to bind the many features that
collectively represent an object into a coherent whole, is central to human
cognition. It groups low-level perceptual features into high-level object
representations, stores those objects efficiently and compositionally in
memory, and supports human reasoning about individual object instances. While
prior work often imposes object-centric attention (e.g., Slot Attention)
explicitly to probe these benefits, it remains unclear whether this ability
naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they
could: recognizing which patches belong to the same object should be useful for
downstream prediction and thus guide attention. Motivated by the quadratic
nature of self-attention, we hypothesize that ViTs represent whether two
patches belong to the same object, a property we term IsSameObject. We decode
IsSameObject from patch embeddings across ViT layers using a similarity probe,
which reaches over 90% accuracy. Crucially, this object-binding capability
emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker
in ImageNet-supervised models, suggesting that binding is not a trivial
architectural artifact, but an ability acquired through specific pretraining
objectives. We further discover that IsSameObject is encoded in a
low-dimensional subspace on top of object features, and that this signal
actively guides attention. Ablating IsSameObject from model activations
degrades downstream performance and works against the learning objective,
implying that emergent object binding naturally serves the pretraining
objective. Our findings challenge the view that ViTs lack object binding and
highlight how symbolic knowledge of "which parts belong together" emerges
naturally in a connectionist system.

</details>


### [70] [Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance](https://arxiv.org/abs/2510.24711)
*Yujie Wei,Shiwei Zhang,Hangjie Yuan,Yujin Han,Zhekai Chen,Jiayu Wang,Difan Zou,Xihui Liu,Yingya Zhang,Yu Liu,Hongming Shan*

Main category: cs.CV

TL;DR: ProMoE是一个专为视觉任务设计的MoE框架，通过两阶段路由机制（条件路由和原型路由）解决视觉令牌的空间冗余和功能异质性问题，显著提升了扩散变换器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MoE方法在语言模型中表现优异，但在视觉扩散变换器中效果有限，主要原因是语言令牌语义密集且变化显著，而视觉令牌存在空间冗余和功能异质性，阻碍了专家专业化。

Method: 提出两阶段路由器：条件路由根据功能角色将图像令牌划分为条件集和无条件集；原型路由使用可学习原型基于语义内容精炼条件图像令牌的分配。同时引入路由对比损失增强原型路由过程。

Result: 在ImageNet基准测试中，ProMoE在Rectified Flow和DDPM训练目标下均超越了最先进方法。

Conclusion: ProMoE通过显式路由指导和语义引导机制有效解决了视觉MoE中的专家专业化问题，为视觉扩散模型提供了高效的扩展方案。

Abstract: Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model
capacity while preserving computational efficiency. Despite its notable success
in large language models (LLMs), existing attempts to apply MoE to Diffusion
Transformers (DiTs) have yielded limited gains. We attribute this gap to
fundamental differences between language and visual tokens. Language tokens are
semantically dense with pronounced inter-token variation, while visual tokens
exhibit spatial redundancy and functional heterogeneity, hindering expert
specialization in vision MoE. To this end, we present ProMoE, an MoE framework
featuring a two-step router with explicit routing guidance that promotes expert
specialization. Specifically, this guidance encourages the router to partition
image tokens into conditional and unconditional sets via conditional routing
according to their functional roles, and refine the assignments of conditional
image tokens through prototypical routing with learnable prototypes based on
semantic content. Moreover, the similarity-based expert allocation in latent
space enabled by prototypical routing offers a natural mechanism for
incorporating explicit semantic guidance, and we validate that such guidance is
crucial for vision MoE. Building on this, we propose a routing contrastive loss
that explicitly enhances the prototypical routing process, promoting
intra-expert coherence and inter-expert diversity. Extensive experiments on
ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods
under both Rectified Flow and DDPM training objectives. Code and models will be
made publicly available.

</details>


### [71] [Uniform Discrete Diffusion with Metric Path for Video Generation](https://arxiv.org/abs/2510.24717)
*Haoge Deng,Ting Pan,Fan Zhang,Yang Liu,Zhuoyan Luo,Yufeng Cui,Wenxuan Wang,Chunhua Shen,Shiguang Shan,Zhaoxiang Zhang,Xinlong Wang*

Main category: cs.CV

TL;DR: URSA是一种离散视频生成框架，通过全局迭代优化离散时空token，结合线性化度量路径和分辨率相关时间步移机制，实现了高效的高分辨率图像合成和长视频生成。


<details>
  <summary>Details</summary>
Motivation: 连续空间视频生成发展迅速，而离散方法由于误差累积和长上下文不一致问题而落后，需要重新审视离散生成建模。

Method: 将视频生成任务制定为离散时空token的迭代全局优化，采用线性化度量路径和分辨率相关时间步移机制，以及异步时间微调策略。

Result: 在挑战性视频和图像生成基准测试中，URSA持续优于现有离散方法，性能与最先进的连续扩散方法相当。

Conclusion: URSA成功弥合了离散方法与连续方法在可扩展视频生成方面的差距，提供了一种简单而强大的框架。

Abstract: Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA

</details>


### [72] [Generative View Stitching](https://arxiv.org/abs/2510.24718)
*Chonghyuk Song,Michal Stary,Boyuan Chen,George Kopanas,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 提出Generative View Stitching (GVS)方法，通过并行采样整个序列解决自回归视频扩散模型在相机引导生成中的碰撞问题，实现稳定、无碰撞的视频生成。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型虽然能进行长序列生成，但无法利用未来信息来指导当前生成，在预定义相机轨迹的场景中容易发生碰撞并导致生成崩溃。

Method: 基于扩散缝合技术开发采样算法，与任何使用Diffusion Forcing训练的现成视频模型兼容，引入Omni Guidance技术增强时间一致性，并提出闭环机制实现长程连贯性。

Result: GVS实现了稳定、无碰撞、帧间一致且能闭合循环的相机引导视频生成，适用于包括不可能楼梯在内的各种预定义相机路径。

Conclusion: GVS方法有效解决了自回归模型在相机引导视频生成中的局限性，为复杂相机轨迹下的稳定视频生成提供了可行方案。

Abstract: Autoregressive video diffusion models are capable of long rollouts that are
stable and consistent with history, but they are unable to guide the current
generation with conditioning from the future. In camera-guided video generation
with a predefined camera trajectory, this limitation leads to collisions with
the generated scene, after which autoregression quickly collapses. To address
this, we propose Generative View Stitching (GVS), which samples the entire
sequence in parallel such that the generated scene is faithful to every part of
the predefined camera trajectory. Our main contribution is a sampling algorithm
that extends prior work on diffusion stitching for robot planning to video
generation. While such stitching methods usually require a specially trained
model, GVS is compatible with any off-the-shelf video model trained with
Diffusion Forcing, a prevalent sequence diffusion framework that we show
already provides the affordances necessary for stitching. We then introduce
Omni Guidance, a technique that enhances the temporal consistency in stitching
by conditioning on both the past and future, and that enables our proposed
loop-closing mechanism for delivering long-range coherence. Overall, GVS
achieves camera-guided video generation that is stable, collision-free,
frame-to-frame consistent, and closes loops for a variety of predefined camera
paths, including Oscar Reutersv\"ard's Impossible Staircase. Results are best
viewed as videos at https://andrewsonga.github.io/gvs.

</details>
