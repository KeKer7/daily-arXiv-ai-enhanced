<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: LVTINO是首个基于视频一致性模型(VCMs)的零样本视频修复方法，通过利用VCMs的时间因果性来确保视频帧间的时间一致性，显著优于逐帧应用图像LDM的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像修复方法在视频修复中面临时间不一致性问题，逐帧应用图像LDM先验会导致重建结果在时间上不连贯。

Method: 利用视频一致性模型(VCMs)作为先验，通过条件机制绕过自动微分需求，仅需少量神经网络评估即可实现高质量视频重建，同时确保测量一致性和平滑的时间过渡。

Result: 在多种视频逆问题上进行广泛实验，相比当前最先进的逐帧图像LDM方法，在重建保真度和计算效率方面都取得了显著感知改进。

Conclusion: LVTINO为高清晰度视频修复建立了新的基准，在保证时间一致性的同时实现了最先进的重建质量和计算效率。

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [2] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: 提出基于风格提取的三阶段训练图像生成方法，通过风格编码器和投影层将风格表示与文本表示对齐，实现细粒度文本引导的风格化图像生成


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中细粒度风格难以用自然语言精确描述和控制的问题，以及风格参考图像的引导信息难以与传统文本条件直接对齐的挑战

Method: 三阶段训练风格提取方法，使用风格编码器和风格投影层将风格表示与文本表示对齐，构建包含图像、风格标签和文本描述的Style30k-captions数据集

Result: 实现了在不改变下游生成模型结构框架的情况下，从单一风格参考图像获取细粒度风格表示，并注入生成主体

Conclusion: 该方法能够最大化预训练生成模型的生成能力，实现细粒度控制的风格化图像生成

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [3] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: 该研究收集了一个包含61.68小时视频、2,793个视频和5,385个标注时间段的挣扎检测数据集，用于研究技能学习过程中挣扎行为的时间演变。


<details>
  <summary>Details</summary>
Motivation: 现有操作数据集未关注挣扎行为随时间的变化，而理解这种演变对于确定用户当前学习阶段和开发有效辅助系统至关重要。

Method: 收集了76名参与者完成18个任务的视频数据，任务分为四类活动：打结、折纸、七巧板拼图和洗牌。参与者重复相同任务五次以捕捉技能演变。将挣扎检测定义为时间动作定位任务。

Result: 时间动作定位模型能够成功学习检测挣扎线索，在未见任务或活动上的评估显示，跨任务平均mAP为34.56%，跨活动为19.24%。

Conclusion: 挣扎是一个可跨各种基于技能任务转移的概念，但在挣扎检测方面仍需进一步改进。

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [4] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: SPUS是一个紧凑高效的PDE求解基础模型，使用轻量级残差U-Net架构，通过自回归预训练策略学习物理规律，在6个未见PDE任务上实现最先进的泛化性能，参数效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有PDE基础模型主要基于大型复杂Transformer架构，计算和参数开销大，需要探索更轻量高效的架构。

Method: 采用轻量级残差U-Net架构，使用自回归预训练策略模拟数值求解器行为，在多样化流体动力学PDE数据集上预训练。

Result: 在6个挑战性未见下游PDE任务上达到最先进泛化性能，所需参数显著减少，微调数据需求最小。

Conclusion: SPUS展示了作为参数高效PDE求解基础模型的潜力，残差U-Net架构在此领域具有竞争力。

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [5] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: DisCo是一个基于强化学习的框架，通过多样性约束直接优化多人生成中的身份多样性，解决了文本到图像模型在多人提示下重复面孔、合并身份和错误计数的问题。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的文本到图像模型在现实主义方面表现出色，但在多人提示下表现不佳，会出现重复面孔、合并身份和错误计数个体的问题。

Method: DisCo通过组相对策略优化(GRPO)微调流匹配模型，使用组合奖励函数：(i)惩罚图像内面部相似性，(ii)阻止跨样本身份重复，(iii)强制执行准确的人员计数，(iv)通过人类偏好分数保持视觉保真度。采用单阶段课程学习来稳定训练。

Result: 在DiverseHumans测试集上，DisCo实现了98.6%的唯一面孔准确率和接近完美的全局身份分布，超越了开源和专有方法（如Gemini、GPT-Image），同时保持了有竞争力的感知质量。

Conclusion: DisCo作为一个可扩展、无需额外标注的解决方案，解决了生成模型中长期存在的身份危机问题，并为组合多人生成设立了新的基准。

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [6] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: 提出了一种新的视觉地理定位方法，通过将查询图像的视觉表示与学习的地理表示对齐，并使用层次化地理嵌入和语义分割融合来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉地理定位方法在学习地理表示方面仍有改进空间，需要更有效地结合视觉内容和地理信息。

Method: 将世界建模为层次化地理嵌入，并融合查询图像的外观特征和语义分割图来形成鲁棒的视觉表示。

Result: 在5个基准数据集的25个指标中，22个指标达到了新的最佳性能，超越了现有SOTA方法和大型视觉语言模型。

Conclusion: 地理表示和视觉表示的结合是性能提升的主要驱动力，证明了该方法在视觉地理定位任务中的有效性。

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [7] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: XMAS是一种针对大型视觉语言模型的数据高效指令调优方法，通过聚类基于注意力矩阵轨迹的示例来消除训练数据冗余，能在保持性能的同时大幅减少训练数据量。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法在大型视觉语言模型上表现不佳，无法超越随机选择，需要开发专门针对LVLMs的高效数据选择方法。

Method: 基于相似交叉模态注意力矩阵示例具有相似梯度的理论，通过聚类注意力矩阵的顶部奇异值轨迹，从聚类中采样平衡子集来消除冗余。

Result: XMAS能丢弃LLaVA-665k数据集的50%和Vision-Flan数据集的85%，同时完全保持LLaVA-1.5-7B在10个下游基准上的性能，并将训练速度提升1.2倍。

Conclusion: XMAS是首个针对LVLMs的数据高效指令调优原则性方法，能显著减少训练数据冗余并加速训练，同时保持模型性能。

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [8] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception是一种用于矢量量化图像生成的变分流匹配方法，结合了明确的分类监督和连续传输动力学。


<details>
  <summary>Details</summary>
Motivation: 为了在矢量量化图像生成中同时获得连续方法的几何感知能力和分类方法的离散监督优势，解决现有方法在训练效率和不确定性量化方面的局限性。

Method: 通过将变分流匹配适应于矢量量化潜在空间，学习码本索引的分类后验，同时在连续嵌入空间中计算速度场。

Result: 在ImageNet-1k 256x256生成任务上，训练收敛速度比连续流匹配和离散流匹配基线更快，同时达到与最先进模型竞争的FID分数。

Conclusion: 变分流匹配能够有效桥接连续传输和离散监督，提高图像生成的训练效率。

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [9] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 提出了一种统一的深度学习框架，通过条件扩散模型和多任务学习，从非对比CT扫描同时生成合成对比增强CT图像并分割主动脉腔和血栓，避免了多阶段方法的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 传统对比增强CT需要碘对比剂，存在肾毒性、过敏反应和环境危害等风险。现有方法采用多阶段流程，导致误差累积且无法利用共享的解剖结构信息。

Method: 集成条件扩散模型与多任务学习，实现端到端的图像合成和分割联合优化。共享编码器和解码器参数，采用半监督训练策略处理缺失分割标签的临床数据。

Result: 在264名患者队列中表现优异：图像合成PSNR达25.61dB，优于单任务CDM的23.80dB；分割性能提升，腔Dice分数0.89，血栓Dice分数0.53；临床测量更准确，腔直径MAE降至4.19mm。

Conclusion: 该统一框架在减少对比剂使用的同时，通过联合优化显著提升了图像合成和分割性能，为临床AAA评估提供了更安全有效的解决方案。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [10] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: 提出了一个用于多模态内容分析的高效原型框架，将预训练模型与视频数据结合，生成可查询的知识图谱表示。


<details>
  <summary>Details</summary>
Motivation: 多模态内容分析通常计算成本高且工程复杂，现有预训练模型与复杂视频数据的融合具有挑战性。

Method: 设计候选流程，结合预训练模型将视频转换为时序半结构化数据，再转换为可查询的帧级索引知识图谱。

Result: 框架支持持续学习，能够通过交互方式动态整合新的领域特定知识。

Conclusion: 该框架为多模态内容分析提供了一种高效的原型设计方法，支持知识图谱表示和持续学习能力。

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [11] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: WALT框架通过逆向工程将网站功能转化为可重用的工具，让Web代理直接调用高级操作（如搜索、过滤、排序），而不是依赖脆弱的逐步UI交互，从而在浏览器自动化中实现更高的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 当前Web代理方法依赖逐步UI交互和大量LLM推理，在动态布局和长任务中表现脆弱。相比之下，人类通过网站提供的高级功能（如搜索、过滤）更高效地完成任务。

Method: WALT框架逆向工程网站的潜在功能，将其转化为可调用的工具，包括发现（搜索、过滤、排序）、通信（发布、评论、点赞）和内容管理（创建、编辑、删除）等操作。

Result: 在VisualWebArena和WebArena测试中，WALT实现了更高的成功率，使用更少的步骤和更少的LLM依赖推理。

Conclusion: WALT建立了一个稳健且可推广的浏览器自动化范式，将计算负担从脆弱的逐步推理转移到可靠的工具调用。

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [12] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: 提出了一种用于半监督分割的拓扑一致性框架，通过多预测扰动和结构匹配来保持语义结构，特别适用于密集分布的病理图像分割。


<details>
  <summary>Details</summary>
Motivation: 在半监督分割中，从未标记数据中捕获有意义的语义结构至关重要，特别是在对象密集分布的病理图像分析中更具挑战性。

Method: 利用随机dropout和时间训练快照获得多个扰动预测，通过整合空间重叠和全局结构对齐的新匹配策略来强制拓扑一致性。

Result: 大量实验表明该方法有效减少了拓扑错误，产生了更鲁棒和准确的分割结果。

Conclusion: 该方法能够可靠地区分生物学有意义的结构与瞬态噪声伪影，为可靠的下游分析提供了基础。

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [13] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: 提出了Diffusion-LPO框架，将列表式偏好优化应用于扩散模型，通过Plackett-Luce模型扩展DPO目标，在图像生成、编辑和个性化偏好对齐任务中优于成对DPO方法。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法主要依赖成对偏好，但人类对图像的反馈通常包含隐含的排名信息，能更精确地表达偏好。列表式偏好优化在扩散模型中尚未得到充分探索。

Method: 基于Plackett-Luce模型推导列表式DPO目标，给定标题时将用户反馈聚合成图像排名列表，鼓励每个样本优于其所有低排名替代品，确保整个排名的一致性。

Result: Diffusion-LPO在文本到图像生成、图像编辑和个性化偏好对齐等任务中，在视觉质量和偏好对齐方面持续优于成对DPO基线方法。

Conclusion: Diffusion-LPO是一个简单有效的列表式偏好优化框架，能够更好地利用人类反馈中的排名信息，提升扩散模型与人类偏好的对齐效果。

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [14] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: Bridge是一个纯自回归的统一多模态大语言模型，通过混合Transformer架构和语义到像素的离散表示，在单一的下一个token预测框架中同时实现图像理解和生成，在减少训练数据和训练时间的同时达到竞争性的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的统一多模态大语言模型面临挑战：混合方法破坏了自回归范式，而纯自回归方法在语义对齐和像素级保真度之间存在权衡。

Method: 采用混合Transformer架构，将预训练视觉理解模型与生成能力结合；提出语义到像素的离散表示，整合紧凑语义token和细粒度像素token。

Result: 在多个多模态基准测试中，Bridge在理解和生成任务上均达到竞争性或更优的结果，序列长度仅增加7.9%。

Conclusion: Bridge证明了纯自回归方法可以在单一框架中有效统一多模态理解和生成，同时减少训练需求。

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [15] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: 提出了一种结合CNN与贝叶斯深度学习的混合模型，用于小数据集口腔癌分类，通过变分推理量化不确定性，在数据稀缺环境下提高模型可靠性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 口腔癌是全球高发癌症，早期诊断对降低死亡率至关重要。传统深度学习模型依赖大数据集，在医疗资源匮乏地区难以应用，且存在过度自信和可靠性不足的问题。

Method: 使用卷积神经网络与贝叶斯深度学习相结合的混合模型，采用变分推理进行不确定性量化，使用智能手机拍摄的彩色图像进行训练。

Result: 在训练数据分布相似的测试集上达到94%准确率，与传统CNN相当；在真实世界图像数据上表现更优，准确率达88%（传统CNN为72.94%），且对正确分类样本置信度高，错误分类样本置信度低。

Conclusion: 贝叶斯推理在数据稀缺环境下能有效提高口腔癌早期诊断的模型可靠性和泛化能力。

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [16] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: 提出CADTrans方法解决无源域自适应问题，通过构建辅助域模块获取多样化表示，使用一致性策略获得不变特征表示，并采用条件多核最大均值差异策略对齐困难样本。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法主要关注评估目标域中与源域相似的不变特征，但容易受到困难样本和域偏差的影响，且无法充分表示多样性。

Method: 构建辅助域模块从中间聚合全局注意力获取多样化表示；基于辅助域和目标域，通过多一致性策略获得不变特征表示；使用条件多核最大均值差异策略对齐困难样本到对应简单样本。

Result: 在Office-31、Office-Home、VISDA-C和DomainNet-126等多个基准数据集上进行了广泛实验，证明了所提方法带来的显著性能提升。

Conclusion: CADTrans通过构建域一致性的不变特征表示，有效解决了SFDA中的困难样本和域偏差问题，在多个基准数据集上取得了显著性能改进。

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [17] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: 开发了一个系统，利用BLV用户的历史问题来指导多模态大语言模型生成更相关的图像描述，而不是默认的冗长描述。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM应用通常提供全面的冗长描述，而不管上下文，导致用户需要浏览不相关的细节，而不是获得他们可能寻求的具体信息。

Method: 系统从VizWiz-LF数据集中识别相似的过去视觉上下文，并使用相关的问题来指导MLLM生成更符合BLV用户需求的描述。

Result: 评估显示，上下文感知描述在76.1%的情况下预测并回答了用户的问题，并在54.4%的比较中被偏好。

Conclusion: 利用BLV用户的历史问题可以有效地指导MLLM生成更相关和有用的图像描述。

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [18] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: 开发了ImageNet-Think数据集，包含25万张ImageNet21k图像，提供结构化思考标记和答案，用于训练具有显式推理能力的视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 促进具有显式推理能力的视觉语言模型发展，加深对多模态推理机制的理解。

Method: 使用两个最先进的VLM（GLM-4.1V-9B-Thinking和Kimi-VL-A3B-Thinking-2506）生成合成数据集，每张图像配有两对思考-答案序列。

Result: 创建了包含逐步推理过程和最终描述性答案的多模态推理数据集。

Conclusion: 该数据集将公开可用，支持推理/思考多模态VLM的研究发展。

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [19] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 提出了一种新的正则化方法NPN，通过神经网络的非线性投影来约束感知矩阵零空间，提升成像逆问题的重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法忽略了感知矩阵零空间的特定结构，而零空间包含感知过程无法捕获的信息，利用这些信息可以更好地解决逆问题。

Method: 使用神经网络学习感知矩阵零空间的低维投影，将解约束在该投影空间中，而不是在图像域施加结构约束。

Result: 在压缩感知、去模糊、超分辨率、CT和MRI等多种成像逆问题中，NPN先验显著提升了重建保真度，与多种重建框架兼容。

Conclusion: NPN方法通过利用感知矩阵零空间的结构信息，提供了一种可解释且灵活的正则化策略，能有效提升各种成像逆问题的重建性能。

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [20] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: 提出结合混沌游戏表示和概念瓶颈模型的自动化基因组解释模块，将原始DNA序列转化为可解释的医疗决策，在HIV亚型分类中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决基因组数据解释与自动化医疗决策之间的差距，提供可验证的生物证据，提高临床自动化系统的可靠性。

Method: 使用混沌游戏表示(CGR)和概念瓶颈模型(CBM)，结合概念保真度监督、先验一致性对齐、KL分布匹配和不确定性校准，并加入成本感知推荐层。

Result: 在内部和LANL数据集上实现最先进的HIV亚型分类性能，具有优越的概念预测保真度和更好的成本效益权衡。

Conclusion: 该工作为基因组医学中的机器人和临床自动化建立了可靠基础，弥合了可解释基因组建模与自动化决策之间的差距。

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [21] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: VLA-R1是一个增强推理能力的视觉-语言-动作模型，通过强化学习从可验证奖励和群体相对策略优化来系统优化推理和执行能力，在泛化性和真实世界性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型缺乏显式的逐步推理，直接输出最终动作而不考虑可操作性约束或几何关系，且后训练流程很少强化推理质量。

Method: 集成强化学习从可验证奖励(RLVR)与群体相对策略优化(GRPO)，设计基于RLVR的后训练策略，包含区域对齐、轨迹一致性和输出格式化的可验证奖励，并开发了VLA-CoT-13K高质量数据集。

Result: 在领域内、领域外、仿真和真实机器人平台上的广泛评估表明，VLA-R1相比现有VLA方法实现了更优越的泛化性和真实世界性能。

Conclusion: VLA-R1通过系统优化推理和执行能力，显著提升了VLA模型的泛化性和实际应用效果，相关模型、代码和数据集将在论文发表后开源。

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [22] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: 提出了一种针对微距摄影的联合去模糊和3D重建方法，通过可微分渲染自监督优化3D模型和离焦模糊核，从少量多视角模糊图像中实现高质量去模糊和高保真3D重建。


<details>
  <summary>Details</summary>
Motivation: 微距镜头具有高分辨率和大放大倍率的优势，但离焦模糊问题严重阻碍了清晰成像和高质量3D重建。传统去模糊方法需要大量图像和标注，且目前没有针对微距摄影的多视角3D重建方法。

Method: 从多视角模糊图像出发，联合优化物体的清晰3D模型和每个像素的离焦模糊核。整个框架采用可微分渲染方法，自监督优化3D模型和离焦模糊核。

Result: 大量实验表明，从少量多视角图像中，该方法不仅能实现高质量图像去模糊，还能恢复高保真3D外观。

Conclusion: 该方法成功解决了微距摄影中的离焦模糊问题，为小尺寸精细物体的3D建模提供了有效解决方案。

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [23] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff是一种新颖的单步扩散模型，用于高保真图像去模糊。通过将运动模糊重新表述为扩散过程，训练一致性模型实现准确的一步去模糊，在保持高保真度的同时显著减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的去模糊方法存在推理时间过长和保真度不足的问题，限制了其在真实世界工业应用中的潜力。

Method: 将运动模糊重新表述为扩散过程，每个时间步代表逐渐模糊的图像；训练一致性模型使所有时间步对齐到同一清晰图像；集成Kernel ControlNet进行模糊核估计；引入自适应时间步预测。

Result: 在完整参考指标上达到优越性能，超越之前的扩散方法，与其他最先进模型性能相当。

Conclusion: FideDiff为将预训练扩散模型应用于高保真图像恢复任务提供了新方向，为在真实世界工业应用中进一步推进扩散模型建立了稳健基准。

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [24] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: 该论文提出了一个用于青铜器铭文识别的两阶段检测-识别流程，通过LadderMoE架构处理多领域变异性，并在大规模数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 青铜器铭文是早期中国文字的重要阶段，但由于严重的视觉退化、多领域变异性（照片、拓片、摹本）和极端长尾字符分布，自动识别仍然困难。

Method: 开发了两阶段检测-识别流程：先定位铭文区域，再转录单个字符。采用LadderMoE架构，在预训练的CLIP编码器基础上添加阶梯式MoE适配器，实现动态专家专业化和更强的鲁棒性。

Result: 在单字符和全页识别任务上，该方法显著优于最先进的场景文本识别基线，在头部、中部和尾部类别以及所有采集模态上都取得了优异的准确率。

Conclusion: 这些结果为青铜器铭文识别和下游考古分析奠定了坚实基础。

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [25] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA提出了一种通过视觉重编程实现参数高效的领域自适应方法，仅需训练少量参数即可实现跨域适应，避免了传统方法需要为每个源-目标对微调整个骨干网络的问题。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法需要为每个新的源-目标对微调整个骨干网络，导致训练参数和存储需求线性增长，且无法重用训练好的骨干网络参数。

Method: 在骨干网络前添加领域特定的视觉重编程层，生成视觉提示作为纹理偏置来调整输入图像的"风格"，通过优化域内和域间分布差异来训练这些层，而不修改骨干网络参数。

Result: 在Office-31数据集上达到92.8%的平均准确率，仅需1.5M可训练参数，优于PDA方法+1.6%准确率且仅使用其46%参数，相比全骨干微调方法仅需1.7%-2.8%参数。

Conclusion: VirDA实现了参数高效的领域自适应，显著减少了训练参数需求，同时保持了较高的准确率，支持骨干网络在不同域间的重用。

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [26] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: 提出了一种名为离散面部编码（DFE）的无监督方法，通过残差向量量化变分自编码器从3D网格序列中学习紧凑且可解释的面部表情字典，在心理任务中优于FACS和其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有的面部表情编码系统（如FACS）存在覆盖范围有限和手动标注成本高的问题，需要一种数据驱动、无监督的替代方案。

Method: 使用3D可变形模型提取身份不变的表情特征，然后通过残差向量量化变分自编码器将这些特征编码为来自共享码本的离散标记序列，每个标记捕获特定的可重用面部变形模式。

Result: DFE比FACS和其他面部编码方法捕获更精确的面部行为，在压力检测、性格预测和抑郁检测等心理任务中，使用简单的词袋模型就能持续优于FACS基线和强大的图像/视频表示学习模型。

Conclusion: DFE作为FACS的可扩展有效替代方案，在心理学和情感计算应用中具有潜力，能够覆盖更广泛的面部表情。

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [27] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: 提出了一种名为Con-NRSfM的新方法，用于处理保形变形下的非刚性结构恢复问题，能够准确计算局部保形尺度并实现更精确的深度估计。


<details>
  <summary>Details</summary>
Motivation: 现有NRSfM方法依赖严格假设（如局部平面表面或局部线性变形）且无法恢复保形尺度，需要消除这些约束并提高重建精度。

Method: 使用基于图的框架优化2D图像扭曲进行逐点重建，采用并行可分离迭代优化策略，并整合自监督学习框架生成带纹理的密集3D点云。

Result: 在合成和真实数据集上的仿真和实验结果表明，该方法在重建精度和鲁棒性方面优于现有方法。

Conclusion: Con-NRSfM方法成功解决了现有NRSfM方法的局限性，能够准确计算保形尺度并实现更精确的深度估计，为单目视觉可变形SLAM提供了有效的解决方案。

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [28] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: UniVerse是一个统一的鲁棒重建框架，通过将不一致的多视角图像转换为视频，使用视频扩散模型恢复一致性，然后进行3D重建，解决了稀疏观测下的鲁棒重建问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖密集观测来优化模型参数，难以处理稀疏观测下的图像不一致问题。

Method: 将鲁棒重建解耦为恢复和重建两个子任务：首先将不一致图像转换为初始视频，然后用视频扩散模型恢复一致性，最后从恢复的图像重建3D场景。

Result: 在合成和真实数据集上的实验表明，该方法具有强大的泛化能力和优越的鲁棒重建性能，并能控制重建3D场景的风格。

Conclusion: UniVerse通过解耦策略和扩散模型学习的大规模场景先验，有效解决了多视角图像不一致的鲁棒重建问题。

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [29] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: 提出轻量级端到端模板匹配框架，将模板匹配重新定义为联合定位和几何回归，输出中心坐标、旋转角度和独立缩放比例，在复合变换下实现高精度和快速推理。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖穷举角度和尺度导致效率低下，深度学习方法仅估计相似度分数而不显式建模几何姿态，无法满足实际部署需求。

Method: 使用模板感知动态卷积模块动态注入模板特征，集成深度可分离卷积和像素洗牌实现高效匹配，采用旋转-剪切增强策略进行无几何标注训练，通过轻量级精化模块提升精度。

Result: 3.07M参数模型在复合变换下达到高精度和14ms推理速度，在小模板和多目标场景中表现出强鲁棒性。

Conclusion: 该方法适用于实时工业应用部署，在模板匹配任务中实现了高效准确的几何姿态估计。

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [30] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出了首个自适应像素推理框架，通过动态确定必要的像素级操作来提升视觉语言模型的细粒度视觉理解能力，在保持高性能的同时显著减少不必要的视觉操作。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在处理需要精确理解细粒度视觉元素的任务时经常遇到困难，主要由于图像编码过程中的信息丢失或对关键区域关注不足。现有方法虽然通过引入像素级信息有所改进，但往往过度使用这些信息，导致效率低下和无关视觉细节的干扰。

Method: 首先应用操作感知的监督微调建立文本推理和视觉操作的基础能力，然后设计新颖的rollout引导的强化学习框架，基于模型自身响应的反馈来动态决定何时调用像素操作。

Result: 在广泛的多模态推理基准测试中，模型实现了优越性能，同时显著减少了不必要的视觉操作。在HR-Bench 4K上达到73.4%准确率，工具使用率仅为20.1%，相比之前方法在提高准确率的同时减少了66.5%的工具使用。

Conclusion: 该自适应像素推理框架有效解决了视觉语言模型在细粒度视觉理解中的挑战，实现了性能与效率的良好平衡，为多模态推理系统提供了新的设计思路。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [31] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: 提出了ASRS框架，通过测量临床合理旋转下的嵌入变化来识别易出错的胸部X光片病例，提高医疗AI的公平性和安全性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在胸部X光片解读中表现良好，但在患者亚组间存在准确性不均的问题，现有错误检测方法难以处理分布内的细微错误。

Method: 使用ASRS框架，应用临床合理的旋转（±15°/±30°），通过RAD-DINO编码器测量嵌入变化，根据敏感度分数将样本分层到稳定性四分位数。

Result: 高敏感度病例的召回率显著降低（-0.2到-0.3），尽管具有高AUROC和置信度，ASRS为选择性预测和临床医生审查提供了无标签方法。

Conclusion: ASRS框架能够识别易出错病例，提高医疗AI的公平性和安全性，为选择性预测和临床审查提供有效工具。

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [32] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS是一个无需训练的视频风格化框架，通过整合多个风格化参考到预训练的图像到视频模型，生成具有丰富风格细节和强时间一致性的风格化视频。


<details>
  <summary>Details</summary>
Motivation: 传统帧级图像风格化方法会损害时间一致性并降低风格丰富度，而专门的视频风格化模型通常需要配对视频数据且计算成本高。

Method: 整合多个风格化参考到预训练I2V模型，使用高频补偿约束内容布局和运动，结合基于光流的运动线索在低显著性区域保留风格纹理。

Result: FreeViS实现了更高的风格化保真度和优越的时间一致性，超越了现有基线方法并获得强烈的人类偏好。

Conclusion: 这种无需训练的流程为高质量、时间一致性的视频风格化提供了实用且经济的解决方案。

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [33] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: 提出了MedQ-Bench基准，通过感知-推理范式评估多模态大语言模型在医学图像质量评估中的能力，包含感知任务和推理任务，覆盖5种成像模态和40多个质量属性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像质量评估方法受限于标量评分指标，无法反映专家评估中的人类推理过程，需要建立更符合人类推理的评估范式。

Method: 构建包含2,600个感知查询和708个推理评估的基准，定义MedQ-Perception（低层感知能力）和MedQ-Reasoning（无参考和比较推理）两个任务，并提出四维评估协议。

Result: 评估14个最先进MLLMs发现模型表现出初步但不稳定的感知和推理能力，准确性不足以可靠用于临床。

Conclusion: MLLMs在医学图像质量评估中需要针对性优化，MedQ-Bench将促进该领域的进一步探索。

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [34] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: InstaFormer是一种能够从单张RGB图像中预测完整遮挡和深度排序的网络，无需昂贵的输入格式，通过单次前向传播完成所有实例的几何关系预测。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型在理解实例级几何关系时依赖昂贵的输入格式（类别标签、分割掩码）和推理成本（二次方前向传播），需要更高效的解决方案。

Method: InstaFormer通过对象查询与潜在掩码描述符的交互，语义上表示相同对象同时携带互补信息，实现全遮挡和深度排序预测。

Result: 通过全面基准测试和消融实验验证了方法的有效性，代码和模型已开源。

Conclusion: InstaFormer提供了一种高效的单次前向传播方法，能够从RGB图像中预测完整的场景实例几何关系，解决了现有方法的输入和计算成本问题。

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [35] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler是一个基于Transformer的神经风格迁移框架，通过金字塔位置编码和强化学习优化，实现了高效的高分辨率图像风格化。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN和Transformer模型在处理复杂风格和高分辨率输入时效率不足，需要更高效的风格迁移方法。

Method: 提出PyramidStyler框架，采用金字塔位置编码(PPE)捕获多尺度特征，并结合强化学习动态优化风格化过程。

Result: 在4000轮训练后，内容损失降低62.6%至2.07，风格损失降低57.4%至0.86，推理时间1.39秒；使用RL后进一步改善至内容损失2.03，风格损失0.75，速度仅1.40秒。

Conclusion: 该方法实现了实时高质量艺术渲染，在媒体和设计领域具有广泛应用前景。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [36] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS是一个负载平衡的高效3D高斯泼溅框架，解决了大规模场景重建中的负载不均和训练效率问题，实现了2倍加速训练且保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有分治方法在大规模无边界场景中存在负载严重不均和粗到细流水线效率低下的问题，导致训练开销高。

Method: 提出深度感知分区方法、基于优化的负载平衡策略，以及可见性裁剪和选择性稠密化两种轻量技术。

Result: 在大规模城市和户外数据集上，LoBE-GS相比最先进基线实现了2倍端到端训练加速，同时保持重建质量。

Conclusion: LoBE-GS能够扩展到传统3DGS无法处理的场景，为大规模3D场景重建提供了高效解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [37] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 提出了MemoryPack和Direct Forcing两种方法来解决长视频生成中的长期依赖建模和错误累积问题，提升上下文一致性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临双重挑战：需要捕捉长期依赖关系，同时防止自回归解码中固有的错误累积。

Method: 1. MemoryPack：可学习的上下文检索机制，利用文本和图像信息作为全局指导，联合建模短期和长期依赖；2. Direct Forcing：高效的单步近似策略，改善训练-推理对齐以减少推理时的错误传播。

Result: 实现了分钟级的时间一致性，计算效率高且保持线性复杂度，显著提升了长视频生成的上下文一致性和可靠性。

Conclusion: MemoryPack和Direct Forcing共同推进了自回归视频模型的实际可用性，为长视频生成提供了有效的解决方案。

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [38] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schröder,Marius-Raphael Schlüter,Markus Lienkamp*

Main category: cs.CV

TL;DR: 该论文提出了一种用于3D目标检测器分类任务置信度校准的方法，包括评估完整预测置信度分布的新指标和两个正则化损失项，在CenterPoint和PillarNet上实现了最佳校准效果。


<details>
  <summary>Details</summary>
Motivation: 在自主系统中，精确的目标检测和不确定性估计对于自感知和安全操作至关重要，需要关注所有类别的完整预测置信度分布的校准。

Method: 提出了两个辅助正则化损失项：一个用于主导预测的校准，另一个用于完整预测向量的校准；评估了多种后处理和训练时方法，结合完整类别预测校准损失项和等渗回归。

Result: 在CenterPoint和PillarNet上，结合完整类别预测校准损失项和等渗回归的方法实现了主导和次要类别预测的最佳校准；但DSVT-Pillar无法使用相同方法同时校准主导和次要预测。

Conclusion: 提出的完整预测向量校准方法在CenterPoint和PillarNet上有效，但不同检测器可能需要特定的校准策略，DSVT-Pillar的校准需要进一步研究。

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [39] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: DiffPS是一个利用预训练扩散模型进行行人搜索的新框架，通过三个专门模块解决现有方法中检测和重识别任务之间的优化冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有行人搜索方法主要使用ImageNet预训练骨干网络，可能无法有效捕捉复杂空间上下文和细粒度身份线索。同时，它们依赖共享骨干网络特征来处理检测和重识别两个任务，导致特征优化冲突。

Method: 提出DiffPS框架，利用预训练扩散模型并消除两个子任务间的优化冲突。包含三个专门模块：扩散引导区域提议网络(DGRPN)用于增强行人定位，多尺度频率细化网络(MSFRN)缓解形状偏差，语义自适应特征聚合网络(SFAN)利用文本对齐的扩散特征。

Result: DiffPS在CUHK-SYSU和PRW数据集上取得了新的最先进性能。

Conclusion: 扩散先验知识能够有效提升行人搜索性能，通过专门设计的模块可以解决检测和重识别任务间的优化冲突问题。

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [40] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出了FMU网络，首次将流匹配集成到HSI重建中，通过深度展开框架嵌入生成先验，显著提升了重建质量


<details>
  <summary>Details</summary>
Motivation: 高光谱成像成本高且重建困难，现有压缩感知系统如CASSI在重建精度上仍面临严重退化问题和精细光谱细节丢失的挑战

Method: 提出流匹配引导的展开网络(FMU)，将流匹配的生成先验嵌入深度展开框架，并引入平均速度损失来增强流动态的全局一致性

Result: 在模拟和真实数据集上的广泛实验表明，FMU在重建质量上显著优于现有方法

Conclusion: FMU成功结合了基于优化方法的可解释性和流匹配的生成能力，为HSI重建提供了更鲁棒和准确的解决方案

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [41] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: 提出基于深度学习的双列直插封装自动缺陷检测系统，使用ConSinGAN生成训练数据，YOLOv7模型在准确率和检测时间上表现最优。


<details>
  <summary>Details</summary>
Motivation: 传统工业元件缺陷检测耗时耗力，给质检人员带来沉重负担且难以管理产品质量。

Method: 使用数字相机光学和深度学习模型，采用ConSinGAN生成合适大小的数据集，研究四种YOLO模型（v3、v4、v7、v9）及其与ConSinGAN增强的组合。

Result: YOLOv7与ConSinGAN组合在准确率（95.50%）和检测时间（285ms）上优于其他YOLO版本，远超基于阈值的方法。

Conclusion: 所提出的自动缺陷检测系统可轻松应用于多种缺陷类型或缺陷数据不足的情况，并开发了SCADA系统及相关传感器架构。

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [42] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: FoundAD是一个基于基础视觉编码器的少样本异常检测方法，通过学习非线性投影算子来识别图像中的异常区域，在参数更少的情况下实现竞争性性能


<details>
  <summary>Details</summary>
Motivation: 少样本异常检测在工业安全检查中很重要，但有限样本使得正常与异常特征难以区分，特别是在类别无关条件下。基础视觉编码器的大规模预训练有助于学习正常图像的一般分布

Method: 观察到图像中的异常量与学习嵌入的差异直接相关，设计了一个非线性投影算子来投影到自然图像流形上，该算子作为异常检测的有效工具来识别分布外区域

Result: 广泛实验表明该方法支持多类别检测，在使用比先前方法少得多的参数的情况下实现了竞争性性能，并通过多个基础编码器（包括DINOv3）的评估得到验证

Conclusion: 该方法为基础特征的应用提供了新视角，并推动了少样本异常检测领域的发展

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [43] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 提出ClustViT方法，通过可训练的聚类模块和再生器模块，在保持分割精度的同时显著降低Vision Transformer的计算复杂度


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在机器人系统应用中受到二次注意力复杂度的限制，现有token合并方法适合分类但不适合密集预测任务

Method: 在ViT骨干网络中引入可训练的聚类模块，根据分割掩码的伪聚类指导合并相似token，然后通过再生器模块恢复细节

Result: 在三个数据集上实现高达2.18倍GFLOPs减少和1.64倍推理加速，同时保持可比较的分割精度

Conclusion: ClustViT有效解决了ViT在密集预测任务中的计算效率问题，为实际机器人应用提供了可行的解决方案

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [44] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: 提出了Patch-as-Decodable Token (PaDT)方法，通过视觉参考令牌(VRTs)使多模态大语言模型能直接生成文本和视觉输出，解决了现有方法依赖间接表示的限制。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在视觉任务中依赖间接表示（如将坐标生成为文本），这限制了性能并阻碍了密集预测任务如分割。

Method: 引入视觉参考令牌(VRTs)，从查询图像的视觉补丁嵌入中派生，与LLM的输出文本令牌交织。使用轻量级解码器将LLM输出转换为检测、分割和定位预测。

Result: 在四个视觉感知和理解任务上的实证研究表明，PaDT始终达到最先进的性能，甚至与显著更大的MLLM模型相比也是如此。

Conclusion: PaDT提供了一种统一的范式，使MLLMs能够直接生成多样化的视觉输出，在多个视觉任务上实现了优越性能。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [45] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: 该论文针对生鲜电商中的信任缺失问题，提出了'信任金字塔'模型和'三角信任指数'，并设计了可解释AI框架TriAlignXA，通过多目标优化解决农产品分级中的'不可能三角'问题。


<details>
  <summary>Details</summary>
Motivation: 在线生鲜电商存在'信任赤字'，因为数字交易无法提供对产品质量的直接感官感知。传统绝对分级标准在生物特性、时效性和经济可行性方面存在局限性。

Method: 构建'信任金字塔'模型，提出'三角信任指数'量化权衡，设计可解释AI框架TriAlignXA，包含生物自适应引擎、时效优化引擎和经济优化引擎，以及'预映射机制'将过程数据编码为二维码。

Result: 在分级任务实验中，准确率显著高于基线模型。实证证据和理论分析验证了框架在解决'不可能三角'方面的平衡能力。

Conclusion: 该研究为构建可信赖的在线农产品生态系统提供了从理论到实践的全面支持，建立了从算法决策到消费者信任的关键路径。

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [46] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 提出了4DGS-Craft框架，通过4D感知的InstructPix2Pix模型、多视图网格模块和Gaussian选择机制，解决了4D高斯溅射编辑中的视图、时间和非编辑区域一致性问题，并利用LLM模块理解用户意图。


<details>
  <summary>Details</summary>
Motivation: 现有的4D高斯溅射编辑方法在视图一致性、时间一致性、非编辑区域一致性以及处理复杂文本指令方面仍面临挑战。

Method: 1) 引入4D感知的InstructPix2Pix模型，结合4D VGGT几何特征；2) 多视图网格模块迭代优化多视图输入图像；3) Gaussian选择机制识别和优化编辑区域；4) LLM模块分解复杂指令为原子操作序列。

Result: 相比相关工作，该方法实现了更一致和可控的4D场景编辑，能够处理复杂的用户指令。

Conclusion: 4DGS-Craft框架有效解决了4D高斯溅射编辑中的一致性和交互性问题，为复杂4D场景编辑提供了可行方案。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [47] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出了Pure-Pass (PP)像素级掩码机制，通过固定颜色中心点分类像素，免除纯像素的昂贵计算，集成到ATD-light模型中实现更好的超分辨率性能和参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级超分辨率方法如CAMixer存在适应性差、掩码粒度粗和空间灵活性不足等问题，需要更精细的计算优化机制。

Method: Pure-Pass像素级掩码机制，利用固定颜色中心点将像素分类，识别纯像素并免除其昂贵计算，实现细粒度、空间灵活的计算节省。

Result: PP-ATD-light在节省相似计算量的情况下，在重建质量和参数效率方面均优于CAMixer-ATD-light。

Conclusion: Pure-Pass机制通过像素级精细掩码有效提升轻量级超分辨率模型的性能，在保持计算效率的同时获得更好的重建质量。

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [48] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: 使用GPT-4o自动生成颌骨囊肿的影像学发现，通过自校正循环框架(SLSO)提高准确性，相比传统CoT方法在多个评估项目上表现更好。


<details>
  <summary>Details</summary>
Motivation: 利用多模态AI自动生成颌骨囊肿的影像学发现，提高诊断效率和准确性。

Method: 构建自校正循环结构化输出(SLSO)框架，包含10步流程：图像输入分析、结构化数据生成、牙号提取和一致性检查、不一致时迭代再生、发现生成及后续重构和验证。

Result: SLSO框架在牙号、牙齿移动和牙根吸收方面分别提高了66.9%、33.3%和28.6%的准确性，成功案例最多经过5次再生实现结构化输出。

Conclusion: SLSO框架能强制阴性发现描述、抑制幻觉、提高牙号识别准确性，但对跨多牙的大范围病变识别有限，需要进一步改进以实现实用的发现生成系统。

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [49] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja Pérez,Jaime Godoy,Abdulla Al-Kaff,Fernando García*

Main category: cs.CV

TL;DR: 提出了一种名为LiLa-Net的3D自动编码器架构，仅使用LiDAR点云从真实交通环境中编码高效特征，通过简化网络结构和跳过连接实现了良好的重建性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种仅使用LiDAR点云就能从真实交通环境中提取高效特征的3D自动编码器，避免使用资源密集的最先进架构。

Method: 采用跳过连接概念，减少编码器层数并简化跳过连接，在保持高效潜在空间的同时准确重建原始点云。

Result: 在跳过连接携带的信息和潜在编码之间实现了有效平衡，提高了重建质量而不影响性能，模型展现出强大的泛化能力。

Conclusion: LiLa-Net架构能够仅使用LiDAR点云有效编码交通环境特征，并在简化结构下实现高质量重建和良好泛化性能。

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [50] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: 开发了kabr-tools开源工具包，通过无人机视频和机器学习自动监测多物种行为，相比传统方法提高了行为数据的采集效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统野外观察方法范围有限、耗时费力，难以评估跨景观的行为响应，需要可扩展的方法来量化复杂多维行为模式。

Method: 整合无人机视频与机器学习系统，利用目标检测、跟踪和行为分类技术，提取行为、社交和空间指标。

Result: 无人机观测显著改善行为粒度，减少15%的可见性损失，以更高准确性和连续性捕获更多行为转换。通过三个案例研究验证了工具的有效性。

Conclusion: kabr-tools能够实现大规模自动行为监测，为生态系统研究、保护和生态监测提供强大工具。

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [51] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: GaussianMorphing是一个从多视角图像进行语义感知3D形状和纹理形变的新框架，通过网格引导的3D高斯泼溅实现高保真几何和外观建模，无需标注数据即可保持局部细节和全局语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖点云或需要预定义同胚映射来处理无纹理数据，存在局限性。本文旨在克服这些限制，实现高质量的3D形状和纹理形变。

Method: 采用统一变形策略，将3D高斯锚定到重建的网格面片上，确保几何一致变换；利用网格拓扑作为几何先验建立无监督语义对应；通过物理合理的点轨迹保持结构完整性。

Result: 在TexMorph基准测试中，GaussianMorphing显著优于先前的2D/3D方法，颜色一致性误差(ΔE)降低22.2%，EI降低26.2%。

Conclusion: 该框架成功实现了无需标注数据的语义感知3D形变，在保持纹理保真度和结构完整性方面表现出色，为3D内容创作提供了有效工具。

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [52] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 提出InPose方法，使用预训练扩散模型仅基于旋转测量进行姿态估计，通过似然项引导生成与测量位置一致的姿态序列，实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于条件扩散模型的方法泛化性差，主要因为位置测量受用户体型影响大，难以跨用户通用。

Method: 将姿态估计建模为逆问题，使用预训练扩散模型仅以旋转测量为条件，通过位置测量导出的似然项引导生成过程。

Result: 提出的InPose方法能够生成与稀疏体表测量一致的姿态序列，实现零样本泛化。

Conclusion: 通过分离旋转条件和位置引导，InPose方法解决了跨用户泛化问题，为有限传感器下的姿态估计提供了有效解决方案。

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [53] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: 提出VGDM：基于视觉引导扩散模型的脑肿瘤检测与分割框架，结合Transformer和扩散模型提升脑肿瘤分割的准确性和边界精度


<details>
  <summary>Details</summary>
Motivation: 传统卷积架构如U-Net在捕捉长距离依赖关系方面能力有限，限制了在复杂肿瘤结构上的性能。扩散模型在生成高保真医学图像和优化分割边界方面显示出强大潜力

Method: 在扩散过程核心嵌入视觉Transformer，利用全局上下文推理和迭代去噪来增强体积精度和边界精度。Transformer主干能够更有效地建模整个MRI体积的空间关系，而扩散细化则减轻体素级误差并恢复细粒度肿瘤细节

Result: 在MRI脑肿瘤数据集上的实验验证显示，在Dice相似度和Hausdorff距离指标上取得一致提升

Conclusion: 这种混合设计为神经肿瘤学提供了改进鲁棒性和可扩展性的途径，超越了传统的U-Net基线，展示了Transformer引导扩散模型在推进肿瘤分割技术前沿的潜力

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [54] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,Rémi Lemoy*

Main category: cs.CV

TL;DR: 开发了一个可扩展的深度学习管道，从历史地图中提取法国1925-1950年的城市足迹数据，填补了1970年代前全国尺度城市扩张定量分析的空白。


<details>
  <summary>Details</summary>
Motivation: 1970年代前的法国城市扩张定量分析因缺乏全国性数字城市足迹数据而受阻，需要解决这一数据空白。

Method: 采用双通道U-Net方法处理历史地图的高辐射度和风格复杂性，第一通道生成初步地图识别混淆区域，第二通道使用精炼数据集和二值化输出来最小化辐射噪声。

Result: 处理了覆盖整个法国本土的941个高分辨率瓦片，最终镶嵌图的总体准确率达到73%，有效捕捉了多样化的城市模式。

Conclusion: 成功创建了首个开放获取的全国尺度城市足迹数据集，支持长期城市化动态的进一步研究。

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [55] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: 系统分析了腹腔镜胆囊切除术视频中点跟踪的失败模式，发现点跟踪对于手术工具表现良好，但对于解剖目标（如胆囊）表现不佳，主要由于组织相似性和边界模糊导致失败。


<details>
  <summary>Details</summary>
Motivation: SAM2等视频对象分割模型在手术视频中具有零样本跟踪能力，但点跟踪在复杂手术环境中的可靠性和失败情况尚不明确，需要系统分析。

Method: 在腹腔镜胆囊切除术视频中，针对胆囊、抓钳和L型电钩三个手术目标，比较点跟踪与分割掩码初始化的性能表现。

Result: 点跟踪对于手术工具具有竞争力，但对于解剖目标表现持续不佳，主要失败原因是组织相似性和模糊边界。

Conclusion: 通过定性分析揭示了影响跟踪结果的关键因素，并提供了选择与放置跟踪点的实用建议，以改善手术视频分析性能。

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [56] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: 提出FRIEREN框架解决联邦学习中的语义分割跨域挑战，在客户端仅使用未标注数据，利用视觉基础模型的多模态能力提升性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在语义分割任务中面临域偏移挑战，特别是当客户端数据未标注时。现有方法要么不切实际地假设客户端有标注数据，要么未能充分利用现代视觉基础模型的能力。

Method: 使用视觉-语言解码器，基于CLIP文本嵌入提升语义消歧，采用弱到强一致性学习策略在伪标签上进行鲁棒的本地训练。

Result: 在合成到真实和清晰到恶劣天气基准测试中，该方法有效解决了新任务，与现有域泛化和适应方法相比具有竞争力。

Conclusion: FRIEREN框架为联邦学习中的语义分割跨域问题提供了有效解决方案，并为未来研究建立了强基线。

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [57] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: ASK-Hint是一个基于动作知识的结构化提示框架，用于改进冻结视觉语言模型在视频异常检测中的性能，通过细粒度引导问题提供可解释的推理。


<details>
  <summary>Details</summary>
Motivation: 现有提示方法过于抽象，忽略了定义复杂异常所需的人-物交互和动作语义细节。

Method: 将提示组织成语义连贯的组别（如暴力、财产犯罪、公共安全），并制定细粒度的引导问题，使模型预测与判别性视觉线索对齐。

Result: 在UCF-Crime和XD-Violence数据集上的实验显示，ASK-Hint持续提升AUC，相比之前基准方法达到最先进性能。

Conclusion: 该框架强调了提示粒度的重要性，为可解释视频异常检测提供了一个无需训练且可泛化的解决方案。

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [58] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GeoPurify通过学生亲和网络和几何引导池化模块，利用3D自监督教师模型的几何先验来纯化2D VLM生成的3D点特征，有效解决了2D到3D特征传输中的噪声和碎片化问题，仅需约1.5%的训练数据即可达到或超越最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将2D视觉语言模型特征转移到3D语义分割时面临权衡：直接投影会产生噪声和碎片化预测，而强制几何一致性需要昂贵的训练流程和大规模标注3D数据。作者认为这种限制源于主流的'分割-匹配'范式未能调和2D语义与3D几何结构。

Method: 提出GeoPurify方法：使用小型学生亲和网络，利用从3D自监督教师模型提取的几何先验来纯化2D VLM生成的3D点特征；在推理时设计几何引导池化模块进一步去噪点云并确保语义和结构一致性。

Result: 在主要3D基准测试上的广泛实验表明，GeoPurify仅使用约1.5%的训练数据即可达到或超越最先进的性能。

Conclusion: GeoPurify通过利用潜在几何信息和学习到的亲和网络，有效缓解了2D到3D特征传输中的权衡问题，实现了卓越的数据效率。

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [59] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: 提出了一种基于猪耳静脉模式的非侵入性生物识别方法，使用智能手机采集图像，通过计算机视觉和机器学习实现98.12%的识别准确率，为小规模养殖户提供经济有效的动物识别方案。


<details>
  <summary>Details</summary>
Motivation: 传统猪只识别方法（如耳标和微芯片）不可靠、成本高且主要针对纯种猪，对小规模养殖户不实用，需要开发非侵入性、经济有效的替代方案。

Method: 收集20头混种猪的800张耳部图像，使用标准智能手机和简单背光采集；开发多阶段计算机视觉流程增强静脉可见性，提取结构和空间特征生成生物特征签名；使用机器学习模型（SVM）进行分类。

Result: 支持向量机（SVM）在混种猪群体中达到98.12%的识别精度；从图像处理到分类的整个过程平均耗时8.3秒，证明适合实时农场部署。

Conclusion: 通过用永久性生物标记替代易损的物理标识符，该系统为养殖户提供了经济有效且无压力的动物识别方法，证实了耳静脉生物识别在数字化牲畜管理中的实用性，有望将精准农业的益处扩展到资源受限的农业社区。

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [60] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: 提出了一种多类别计数框架，使用Twins金字塔视觉transformer骨干网络和专门的多类别计数头，通过双任务设计和类别聚焦模块减少类别间干扰，在密集场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决密集和遮挡场景中离散检测方法失效的问题，需要开发能够准确估计多类别对象数量的密度图估计方法。

Method: 采用Twins金字塔视觉transformer骨干网络，构建专门的多类别计数头，基于多尺度解码方法，添加基于分割的类别聚焦模块来抑制训练时的类别间干扰。

Result: 在VisDrone和iSAID基准测试中表现优于先前的多类别人群计数方法（MAE降低33%、43%和64%），与YOLOv11的比较证明了在密集场景中人群计数方法的必要性。

Conclusion: 该方法通过区域损失将多类别人群计数扩展到新领域，在生物多样性监测数据集上的应用展示了其在保护工作和可扩展生态洞察方面的潜力。

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [61] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl是一种无需重新训练或额外监督的方法，通过优化交叉注意力图来实现视频生成中的细粒度时间控制，允许用户指定视觉元素在生成序列中的出现时间。


<details>
  <summary>Details</summary>
Motivation: 现有的生成视频模型缺乏细粒度的时间控制，无法让用户指定特定视觉元素在生成序列中的出现时间。

Method: 利用文本到视频扩散模型中的交叉注意力图，通过新颖的优化方法引导概念的时间安排，采用三个互补原则：通过相关性对齐时间形状、通过能量放大可见性区域、通过熵保持空间焦点。

Result: TempoControl能够在确保高质量和多样性的同时，实现对时序的精确控制，在多种视频生成应用中表现出色，包括单对象和多对象的时间重排序，以及动作和音频对齐的生成。

Conclusion: TempoControl提供了一种有效的方法来实现生成视频中的细粒度时间控制，无需重新训练或额外监督，在各种应用中展现出良好的效果。

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [62] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: 提出了RewardMap框架，通过多阶段强化学习和难度感知奖励设计，解决多模态大语言模型在细粒度视觉推理任务中的稀疏奖励和优化不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在结构化信息丰富的场景（如交通地图）中的空间推理能力不足，传统RL方法因稀疏奖励和优化不稳定而受限。

Method: 构建ReasonMap-Plus数据集提供密集奖励信号，提出RewardMap框架：1）难度感知奖励设计包含细节奖励；2）多阶段RL方案从简单感知到复杂推理任务逐步训练。

Result: 在ReasonMap和ReasonMap-Plus上实验表明，RewardMap各组件均带来性能提升，组合使用效果最佳。在6个基准测试中平均提升3.47%，涵盖空间推理、细粒度视觉推理和通用任务。

Conclusion: RewardMap有效提升了MLLMs的视觉理解和推理能力，特别是在细粒度视觉推理任务中表现出色，证明了多阶段RL和密集奖励设计的有效性。

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [63] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow是首个利用FLUX强大先验进行拖拽式图像编辑的框架，通过区域级编辑范式和仿射变换解决了DiT特征结构不足的问题，在拖拽编辑任务中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型从UNet-based DDPMs转向更可扩展的DiT with flow matching（如SD3.5、FLUX），生成先验显著增强，但拖拽式编辑尚未从中受益。传统方法在目标区域存在严重失真问题。

Method: 提出区域级编辑范式，使用仿射变换提供更丰富一致的特征监督；集成预训练开放域个性化适配器增强主体一致性；采用基于梯度掩码的硬约束保持背景保真度；使用多模态大语言模型解决任务歧义。

Result: 在DragBench-DR和ReD Bench上的广泛实验表明，DragFlow超越了基于点和基于区域的基线方法，在拖拽式图像编辑中达到了新的最先进水平。

Conclusion: DragFlow成功利用FLUX的丰富先验进行拖拽编辑，通过区域级方法克服了DiT特征结构不足的限制，为拖拽式图像编辑带来了实质性改进。

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [64] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: 本文提出F2C方法，通过选择关键片段而非孤立关键帧来改进视频理解，同时采用自适应分辨率策略保持固定计算预算。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型面临'大海捞针'问题：原始视频帧产生的大量视觉标记耗尽模型上下文窗口。现有解决方案通过选择稀疏帧集来减少标记数量，但这种帧级选择丢弃了基本的时间动态信息，导致对运动和事件连续性的推理效果不佳。

Method: 提出F2C方法：1）将选择范围从孤立关键帧扩展到关键片段（短时相干片段）；2）采用自适应分辨率策略，动态平衡空间分辨率和片段长度，确保每个视频的标记数量恒定。

Result: 在三个长视频基准测试上的实验表明，这种无需训练的方法在Video-MME、LongVideoBench和MLVU基准上分别比均匀采样高出8.1%、5.6%和10.3%。

Conclusion: 研究结果强调了在帧选择中保持时间一致性的重要性，并为将视频LLM扩展到现实世界视频理解应用提供了实用途径。

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


### [65] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fernández-González,Francisco-Javier Díaz-Pernas,Hichem Saoudi,Javier González-Alonso,Mario Martínez-Zarzuela*

Main category: cs.CV

TL;DR: 该研究比较了基于单目视频的3D人体姿态估计模型与惯性测量单元(IMU)在临床相关日常活动中的性能，发现MotionAGFormer表现最佳，两种技术都适用于实验室外的运动学评估。


<details>
  <summary>Details</summary>
Motivation: 利用机器学习和可穿戴传感器的进步，在真实条件下准确评估人体运动对于远程医疗、运动科学和康复至关重要。

Method: 使用VIDIMU数据集，比较了四种深度学习框架(MotionAGFormer、MotionBERT、MMPose 2D-to-3D姿态提升和NVIDIA BodyTrack)与IMU数据在13种临床相关日常活动中的性能。

Result: MotionAGFormer表现最优，总体RMSE为9.27±4.80度，MAE为7.86±4.18度，Pearson相关系数0.86±0.15，决定系数R²为0.67±0.28。两种技术都适用于实验室外的运动学评估。

Conclusion: 研究阐明了现成视频模型在健康成人中提供临床可行运动学的位置，以及它们落后于IMU估计的位置，为研究人员和临床医生开发稳健、经济高效且用户友好的远程医疗解决方案提供了宝贵指南。

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [66] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: NeuroSwift是一个通过扩散模型重建大脑视觉信息的框架，整合AutoKL和CLIP适配器，实现跨被试的视觉刺激重建，仅需少量参数微调和1小时训练即可达到先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决从fMRI数据重建视觉信息时面临的跨被试变异性挑战和大脑对复杂视觉输入的抽象语义编码问题。

Method: 提出NeuroSwift框架，整合AutoKL适配器处理低级特征和CLIP适配器处理语义特征，通过预训练和仅微调17%参数实现跨被试泛化。

Result: 在轻量级GPU（三块RTX 4090）上仅需每被试1小时训练，性能超越现有方法，实现最先进的跨被试视觉重建。

Conclusion: NeuroSwift通过创新的适配器集成和参数高效微调策略，成功解决了跨被试视觉重建的挑战，为理解视觉神经机制提供了有效工具。

Abstract: Reconstructing visual information from brain activity via computer vision
technology provides an intuitive understanding of visual neural mechanisms.
Despite progress in decoding fMRI data with generative models, achieving
accurate cross-subject reconstruction of visual stimuli remains challenging and
computationally demanding. This difficulty arises from inter-subject
variability in neural representations and the brain's abstract encoding of core
semantic features in complex visual inputs. To address these challenges, we
propose NeuroSwift, which integrates complementary adapters via diffusion:
AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter
is trained on Stable Diffusion generated images paired with COCO captions to
emulate higher visual cortex encoding. For cross-subject generalization, we
pretrain on one subject and then fine-tune only 17 percent of parameters (fully
connected layers) for new subjects, while freezing other components. This
enables state-of-the-art performance with only one hour of training per subject
on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [67] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: microCLIP是一个自训练框架，通过联合优化CLIP的视觉和文本表示来提升细粒度图像分类性能，核心包括Saliency-Oriented Attention Pooling和两阶段LLM分类器，在13个细粒度基准上平均提升2.90%准确率。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在细粒度图像分类任务中表现受限，因为它主要依赖粗粒度的全局特征，而忽略了微观局部线索。现有方法通过LLM描述对齐CLIP的[CLS]令牌，但缺乏空间精度。

Method: 提出Saliency-Oriented Attention Pooling构建细粒度[FG]令牌并与全局[CLS]令牌融合；使用两阶段LLM分类器（冻结和可学习）提供稳定文本先验；开发动态知识聚合结合固定先验和演化logits迭代优化伪标签。

Result: 在13个细粒度基准测试中平均获得2.90%的准确率提升，仅需轻量级适配。

Conclusion: microCLIP成功挖掘了CLIP中潜在的细粒度信号，通过粗-细粒度对齐和稳定适配机制显著提升了细粒度分类性能。

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [68] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: VidGuard-R1是首个通过GRPO微调MLLM的视频真实性检测器，提供高精度判断和可解释推理，在现有基准测试中实现SOTA零样本性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成视频快速发展，需要有效检测工具来减轻错误信息和声誉损害等社会风险，同时检测模型需要提供可解释性以确保监管机构和终端用户的透明度。

Method: 使用GRPO微调多模态大语言模型Qwen-VL，配备两个专门针对时间伪影和生成复杂度的奖励模型，构建包含14万真实和AI生成视频的挑战性数据集。

Result: 在现有基准测试中实现SOTA零样本性能，额外训练后准确率超过95%，案例研究显示模型能产生精确且可解释的预测理由。

Conclusion: VidGuard-R1成功解决了AI生成视频检测的准确性和可解释性挑战，为监管和终端用户提供了透明可靠的检测工具。

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [69] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 提出了一种简单有效的长视频生成方法，通过利用教师模型的丰富知识为自生成长视频中的采样片段提供指导，避免质量退化，可生成长达4分15秒的视频。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面取得了突破性进展，但基于transformer架构的计算成本过高，特别是在生成长视频时。现有自回归方法在超出训练范围时会出现明显的质量退化问题。

Method: 利用教师模型为自生成长视频中的采样片段提供指导，无需长视频教师监督或重新训练长视频数据集，保持时间一致性，避免过曝光和错误累积问题。

Result: 能够生成长达4分15秒的视频，相当于基础模型位置嵌入支持的最大跨度的99.9%，比基线模型长50倍以上，在保真度和一致性方面显著优于基线方法。

Conclusion: 该方法有效缓解了长视频生成中的质量退化问题，无需额外监督或重新训练，在标准基准测试中表现出优越性能。

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [70] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask是一种物理引导的视频生成方法，通过两阶段训练策略实现逼真的刚体控制、交互和效果，显著提升了对象交互的真实性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在物理可信的对象交互方面存在不足，缺乏基于物理的控制机制，限制了其在机器人学和决策模拟中的应用潜力。

Method: 提出两阶段训练策略，通过对象掩码逐步移除未来运动监督，在合成场景上训练视频扩散模型，并结合低级运动控制与高级文本条件。

Result: 在真实场景中显著改善了对象交互效果，在复杂动态现象合成方面表现出色，相比同类规模模型有显著提升。

Conclusion: KineMask通过物理引导的视频生成方法有效解决了对象交互的物理可信性问题，低高级条件在视频扩散模型中具有互补作用。

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [71] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: 提出了一种多模态细粒度动作模拟方法，通过整合本体感觉、动觉、力触觉和肌肉激活等多种感官，提升机器人精细控制的模拟准确性。


<details>
  <summary>Details</summary>
Motivation: 当前视频模型缺乏精细控制能力，无法作为世界模型。通用家用机器人需要实时精细运动控制来处理精细任务和紧急情况。

Method: 开发了特征学习范式来对齐多种感官模态，同时保留每个模态的独特信息；提出正则化方案增强动作轨迹特征在复杂交互动态中的因果性。

Result: 实验表明，整合多模态感官提高了模拟准确性并减少了时间漂移；广泛的消融研究和下游应用证明了方法的有效性和实用性。

Conclusion: 多模态细粒度动作模拟方法能够有效提升机器人精细控制的模拟性能，为通用家用机器人的开发提供了实用解决方案。

Abstract: Current video models fail as world model as they lack fine-graiend control.
General-purpose household robots require real-time fine motor control to handle
delicate tasks and urgent situations. In this work, we introduce fine-grained
multimodal actions to capture such precise control. We consider senses of
proprioception, kinesthesia, force haptics, and muscle activation. Such
multimodal senses naturally enables fine-grained interactions that are
difficult to simulate with text-conditioned generative models. To effectively
simulate fine-grained multisensory actions, we develop a feature learning
paradigm that aligns these modalities while preserving the unique information
each modality provides. We further propose a regularization scheme to enhance
causality of the action trajectory features in representing intricate
interaction dynamics. Experiments show that incorporating multimodal senses
improves simulation accuracy and reduces temporal drift. Extensive ablation
studies and downstream applications demonstrate the effectiveness and
practicality of our work.

</details>


### [72] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: VideoNSA通过原生稀疏注意力机制增强视频语言模型，在216K视频指令数据集上端到端训练Qwen2.5-VL，实现128K令牌的可靠扩展，在长视频理解、时序推理和空间基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多模态语言模型中视频理解受限于上下文长度的问题，模型常错过关键过渡帧且难以在长时间尺度上保持连贯性。

Method: 采用硬件感知的混合注意力方法：文本保持密集注意力，视频使用原生稀疏注意力(NSA)，通过端到端训练适配Qwen2.5-VL模型。

Result: 相比令牌压缩和无训练稀疏基线，VideoNSA在长视频理解、时序推理和空间基准测试中表现更优，能可靠扩展到128K令牌。

Conclusion: 研究发现：可靠扩展到128K令牌；固定预算下存在最优的全局-局部注意力分配；任务依赖的分支使用模式；可学习的组合稀疏注意力有助于诱导动态注意力汇聚点。

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [73] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: NoiseShift是一种无需训练的方法，通过根据分辨率大小重新校准去噪器的噪声水平，解决扩散模型在不同分辨率下生成质量不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率文本到图像生成器无法为不需要高分辨率图像的用户提供开箱即用的经济高效替代方案，因为扩散模型在固定分辨率集上训练后往往无法泛化到其他分辨率。

Method: 提出NoiseShift方法，识别噪声调度器在不同分辨率下具有不等的感知效果，重新校准去噪器的噪声水平以适应分辨率大小，无需修改模型架构或采样计划。

Result: 在Stable Diffusion 3、3.5和Flux-Dev上应用NoiseShift后，低分辨率生成质量显著提升：在LAION-COCO上平均FID分别提高15.89%、8.56%和2.44%；在CelebA上分别提高10.36%、5.19%和3.02%。

Conclusion: NoiseShift能有效缓解分辨率相关的伪影，提升低分辨率图像生成质量，且与现有模型兼容。

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [74] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 该论文研究从视频预测动态物理属性的任务，包括弹性、粘度和动态摩擦，通过收集新数据集并探索三种推断方法，发现视频基础模型表现接近oracle方法，而多模态大语言模型目前表现较差但可通过提示改进。


<details>
  <summary>Details</summary>
Motivation: 研究从视频中预测需要时间信息才能推断的动态物理属性，如弹性、粘度和动态摩擦，填补现有研究的空白。

Method: 收集三个物理属性的新视频数据集，探索三种推断方法：(a) 使用传统计算机视觉技术的oracle方法；(b) 基于预训练视频生成和自监督模型的可训练提示向量交叉注意力机制；(c) 多模态大语言模型的提示策略。

Result: 生成式或自监督训练的视频基础模型性能相似，接近oracle方法但仍有差距；多模态大语言模型目前表现较差，但通过合适的提示可以改进性能。

Conclusion: 视频基础模型在动态物理属性预测任务中表现良好，多模态大语言模型有改进潜力但当前性能有限，合适的提示策略对提升模型性能很重要。

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


### [75] [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](https://arxiv.org/abs/2510.02313)
*Mengyu Yang,Yiming Chen,Haozheng Pei,Siddhant Agarwal,Arun Balajee Vasudevan,James Hays*

Main category: cs.CV

TL;DR: 提出声音物体检测任务，通过多模态对象感知框架从第一人称视角视频中学习，使用分割掩码和槽注意力机制来引导模型关注交互中的关键物体区域。


<details>
  <summary>Details</summary>
Motivation: 日常物体交互产生独特声音，需要评估模型将声音与相关物体关联的能力，受人类感知启发。

Method: 开发自动管道计算参与物体的分割掩码，使用槽注意力视觉编码器强化物体先验，从野外第一人称视频中多模态学习。

Result: 在新任务和现有多模态动作理解任务上达到最先进性能。

Conclusion: 提出的对象感知框架能有效链接声音与相关物体，在声音物体检测任务中表现优异。

Abstract: Can a model distinguish between the sound of a spoon hitting a hardwood floor
versus a carpeted one? Everyday object interactions produce sounds unique to
the objects involved. We introduce the sounding object detection task to
evaluate a model's ability to link these sounds to the objects directly
involved. Inspired by human perception, our multimodal object-aware framework
learns from in-the-wild egocentric videos. To encourage an object-centric
approach, we first develop an automatic pipeline to compute segmentation masks
of the objects involved to guide the model's focus during training towards the
most informative regions of the interaction. A slot attention visual encoder is
used to further enforce an object prior. We demonstrate state of the art
performance on our new task along with existing multimodal action understanding
tasks.

</details>


### [76] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: 本文分析了3D高斯溅射(3DGS)对图像级投毒攻击的脆弱性，提出了一种新颖的密度引导投毒方法，通过在低密度区域注入高斯点来嵌入视角依赖的幻觉物体，同时引入自适应噪声策略破坏多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 随着NeRF和3DGS等3D场景表示方法的普及，解决其安全漏洞变得至关重要。本文旨在分析3DGS对图像级投毒攻击的鲁棒性。

Method: 提出密度引导投毒方法，使用核密度估计识别低密度区域，战略性地注入高斯点嵌入视角依赖的幻觉物体；引入自适应噪声策略破坏多视角一致性；提出基于KDE的评估协议系统评估攻击难度。

Result: 大量实验表明，该方法在性能上优于现有最先进技术，能够从投毒视角清晰显示幻觉物体，同时对无辜视角影响最小。

Conclusion: 该方法有效揭示了3DGS的安全漏洞，为未来研究提供了客观的基准测试框架。

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

</details>


### [77] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: 提出了首个理论框架和优化目标，通过随机最优控制方法解决文本到图像模型在多主体生成中的属性泄漏、身份纠缠和主体遗漏问题。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在单主体提示上表现优秀，但在多主体描述中经常出现属性泄漏、身份纠缠和主体遗漏等问题，需要专门的方法来提升多主体生成质量。

Method: 将流匹配通过随机最优控制视角重新表述，提出了两种架构无关的算法：无需训练测试时控制器和轻量级微调规则Adjoint Matching，并开发了FOCUS方法。

Result: 在Stable Diffusion 3.5、FLUX和Stable Diffusion XL等模型上，两种算法都显著提升了多主体对齐度，同时保持了基础模型的风格，测试时控制可在普通GPU上高效运行。

Conclusion: 该框架为多主体保真度提供了首个专门设计的微调路径，FOCUS方法在各种模型上实现了最先进的多主体保真度，并能泛化到未见过的提示。

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with
multi-subject descriptions, often showing attribute leakage, identity
entanglement, and subject omissions. We introduce the first theoretical
framework with a principled, optimizable objective for steering sampling
dynamics toward multi-subject fidelity. Viewing flow matching (FM) through
stochastic optimal control (SOC), we formulate subject disentanglement as
control over a trained FM sampler. This yields two architecture-agnostic
algorithms: (i) a training-free test-time controller that perturbs the base
velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight
fine-tuning rule that regresses a control network to a backward adjoint signal
while preserving base-model capabilities. The same formulation unifies prior
attention heuristics, extends to diffusion models via a flow-diffusion
correspondence, and provides the first fine-tuning route explicitly designed
for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and
Stable Diffusion XL, both algorithms consistently improve multi-subject
alignment while maintaining base-model style. Test-time control runs
efficiently on commodity GPUs, and fine-tuned controllers trained on limited
prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal
Control for Unentangled Subjects), which achieves state-of-the-art
multi-subject fidelity across models.

</details>
