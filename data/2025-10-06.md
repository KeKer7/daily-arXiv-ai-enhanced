<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 57]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Exploring OCR-augmented Generation for Bilingual VQA](https://arxiv.org/abs/2510.02543)
*JoonHo Lee,Sunho Park*

Main category: cs.CV

TL;DR: 该论文研究了OCR增强的视觉语言模型生成，专注于韩语和英语的双语任务，发布了KLOCR双语OCR基线模型和KOCRBench韩语VQA基准，实验表明OCR提取的文本能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在多语言环境下的OCR增强生成能力，特别是在韩语和英语双语任务中，以支持该领域的研究。

Method: 训练了KLOCR双语OCR基线模型（基于1亿个实例），构建了KOCRBench韩语VQA基准，并分析了不同的提示方法。

Result: 广泛的实验表明，OCR提取的文本显著提升了开源和商业模型的性能。

Conclusion: 该研究为双语VQA的OCR增强生成提供了新的见解，并发布了模型、代码和数据资源。

Abstract: We investigate OCR-augmented generation with Vision Language Models (VLMs),
exploring tasks in Korean and English toward multilingualism. To support
research in this domain, we train and release KLOCR, a strong bilingual OCR
baseline trained on 100M instances to augment VLMs with OCR ability. To
complement existing VQA benchmarks, we curate KOCRBench for Korean VQA, and
analyze different prompting methods. Extensive experiments show that
OCR-extracted text significantly boosts performance across open source and
commercial models. Our work offers new insights into OCR-augmented generation
for bilingual VQA. Model, code, and data are available at
https://github.com/JHLee0513/KLOCR.

</details>


### [2] [Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback](https://arxiv.org/abs/2510.02561)
*Derek Shi,Ruben Glatt,Christine Klymko,Shubham Mohole,Hongjun Choi,Shashank Kushwaha,Sam Sakla,Felipe Leno da Silva*

Main category: cs.CV

TL;DR: 提出了Oracle-RLAIF框架，用通用Oracle排序器替代训练奖励模型，结合新的基于排名的损失函数GRPO_rank，在视频理解任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着视频语言模型参数规模扩大，收集人类反馈成本高昂。现有RLAIF方法依赖专门训练的奖励模型，流程昂贵且受限。

Method: 使用Oracle排序器对候选模型响应进行排序而非评分，提出基于GRPO的GRPO_rank损失函数，直接优化序数反馈。

Result: 在各种视频理解基准测试中，Oracle-RLAIF始终优于使用现有微调方法的领先VLM。

Conclusion: Oracle-RLAIF为使用排名而非分数的强化学习对齐大型多模态视频模型开辟了灵活且数据高效的路径。

Abstract: Recent advances in large video-language models (VLMs) rely on extensive
fine-tuning techniques that strengthen alignment between textual and visual
comprehension. Leading pipelines typically pair supervised fine-tuning (SFT)
with reinforcement learning from preference data to enhance video
comprehension. However, as VLMs scale in parameter size, so does the cost of
gathering enough human feedback. To make fine-tuning more cost-effective,
recent frameworks explore reinforcement learning with AI feedback (RLAIF),
which replace human preference with AI as a judge. Current RLAIF frameworks
rely on a specialized reward model trained with video narratives to create
calibrated scalar rewards-- an expensive and restrictive pipeline. We propose
Oracle-RLAIF, a novel framework that replaces the trained reward model with a
more general Oracle ranker which acts as a drop-in model ranking candidate
model responses rather than scoring them. Alongside Oracle-RLAIF, we introduce
$GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy
Optimization (GRPO) that directly optimizes ordinal feedback with rank-aware
advantages. Empirically, we demonstrate that Oracle-RLAIF consistently
outperforms leading VLMs using existing fine-tuning methods when evaluated
across various video comprehension benchmarks. Oracle-RLAIF paves the path to
creating flexible and data-efficient frameworks for aligning large multi-modal
video models with reinforcement learning from rank rather than score.

</details>


### [3] [PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction](https://arxiv.org/abs/2510.02566)
*Qiao Feng,Yiming Huang,Yufu Wang,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: PhysHMR是一个统一框架，直接从单目视频学习视觉到动作的策略，在物理模拟器中控制人形角色，实现既物理合理又与输入视频视觉对齐的运动重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于运动学的姿态估计，缺乏物理约束导致不真实结果。传统两阶段方法（先运动估计后物理后处理）存在误差累积问题，限制了重建质量。

Method: 1) 使用像素作为射线策略将2D关键点提升为3D空间射线并转换到全局空间；2) 结合局部视觉特征和全局姿态指导；3) 通过蒸馏方案从动捕训练专家转移运动知识，再用物理驱动的强化学习奖励进行精炼。

Result: PhysHMR在多样化场景中产生高保真、物理合理的运动，在视觉精度和物理真实感方面优于先前方法。

Conclusion: 该框架通过统一的视觉到动作学习策略，实现了物理基础和视觉对齐的运动重建，解决了传统两阶段方法的误差累积问题。

Abstract: Reconstructing physically plausible human motion from monocular videos
remains a challenging problem in computer vision and graphics. Existing methods
primarily focus on kinematics-based pose estimation, often leading to
unrealistic results due to the lack of physical constraints. To address such
artifacts, prior methods have typically relied on physics-based post-processing
following the initial kinematics-based motion estimation. However, this
two-stage design introduces error accumulation, ultimately limiting the overall
reconstruction quality. In this paper, we present PhysHMR, a unified framework
that directly learns a visual-to-action policy for humanoid control in a
physics-based simulator, enabling motion reconstruction that is both physically
grounded and visually aligned with the input video. A key component of our
approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial
rays and transforms them into global space. These rays are incorporated as
policy inputs, providing robust global pose guidance without depending on noisy
3D root predictions. This soft global grounding, combined with local visual
features from a pretrained encoder, allows the policy to reason over both
detailed pose and global positioning. To overcome the sample inefficiency of
reinforcement learning, we further introduce a distillation scheme that
transfers motion knowledge from a mocap-trained expert to the
vision-conditioned policy, which is then refined using physically motivated
reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR
produces high-fidelity, physically plausible motion across diverse scenarios,
outperforming prior approaches in both visual accuracy and physical realism.

</details>


### [4] [Unlocking the power of partnership: How humans and machines can work together to improve face recognition](https://arxiv.org/abs/2510.02570)
*P. Jonathon Phillips,Geraldine Jeckeln,Carina A. Hahn,Amy N. Yates,Peter C. Fontana,Alice J. O'Toole*

Main category: cs.CV

TL;DR: 本文研究了人机协作在面部识别中的效果，提出了近端准确率规则（PAR），发现当协作双方的基线准确率差异较小时，协作效果最佳。通过智能人机融合，可以显著提高系统准确率。


<details>
  <summary>Details</summary>
Motivation: 研究人机协作面部识别系统在什么情况下能够提高准确率，探索人类和机器在面部识别中的最佳协作方式。

Method: 使用专家和非专家面部识别者的数据，分析人-人和人-机协作的效果，提出近端准确率规则（PAR），并实施智能人机融合策略。

Result: 协作效益随着协作双方基线准确率差异的减小而增加。智能人机融合比单独使用机器或简单结合所有人类和机器判断更准确。人机协作能更有效地最小化低表现人类对系统准确率的影响。

Conclusion: 人类和机器在确保准确的面部识别中都扮演着重要角色，研究为智能使用AI进行面部识别提供了基于证据的路线图。

Abstract: Human review of consequential decisions by face recognition algorithms
creates a "collaborative" human-machine system. Individual differences between
people and machines, however, affect whether collaboration improves or degrades
accuracy in any given case. We establish the circumstances under which
combining human and machine face identification decisions improves accuracy.
Using data from expert and non-expert face identifiers, we examined the
benefits of human-human and human-machine collaborations. The benefits of
collaboration increased as the difference in baseline accuracy between
collaborators decreased-following the Proximal Accuracy Rule (PAR). This rule
predicted collaborative (fusion) benefit across a wide range of baseline
abilities, from people with no training to those with extensive training. Using
the PAR, we established a critical fusion zone, where humans are less accurate
than the machine, but fusing the two improves system accuracy. This zone was
surprisingly large. We implemented "intelligent human-machine fusion" by
selecting people with the potential to increase the accuracy of a
high-performing machine. Intelligent fusion was more accurate than the machine
operating alone and more accurate than combining all human and machine
judgments. The highest system-wide accuracy achievable with human-only
partnerships was found by graph theory. This fully human system approximated
the average performance achieved by intelligent human-machine collaboration.
However, intelligent human-machine collaboration more effectively minimized the
impact of low-performing humans on system-wide accuracy. The results
demonstrate a meaningful role for both humans and machines in assuring accurate
face identification. This study offers an evidence-based road map for the
intelligent use of AI in face identification.

</details>


### [5] [How Confident are Video Models? Empowering Video Models to Express their Uncertainty](https://arxiv.org/abs/2510.02571)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 提出了首个视频生成模型不确定性量化框架S-QUBED，包含评估指标、黑盒UQ方法和基准数据集，能分解预测不确定性为偶然性和认知性成分。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型像大语言模型一样会产生幻觉，生成看似合理但事实错误的视频，但目前缺乏视频模型的不确定性量化方法，存在安全隐患。

Method: 提出S-QUBED框架：基于稳健秩相关估计的校准评估指标；通过潜在建模分解偶然性和认知性不确定性的黑盒UQ方法；视频模型校准基准数据集。

Result: 在基准视频数据集上的实验表明，S-QUBED能计算与任务准确度负相关的校准总不确定性估计，并有效分解偶然性和认知性成分。

Conclusion: 这是首个视频生成模型不确定性量化工作，S-QUBED框架能可靠量化视频模型的不确定性，为视频生成安全提供重要保障。

Abstract: Generative video models demonstrate impressive text-to-video capabilities,
spurring widespread adoption in many real-world applications. However, like
large language models (LLMs), video generation models tend to hallucinate,
producing plausible videos even when they are factually wrong. Although
uncertainty quantification (UQ) of LLMs has been extensively studied in prior
work, no UQ method for video models exists, raising critical safety concerns.
To our knowledge, this paper represents the first work towards quantifying the
uncertainty of video models. We present a framework for uncertainty
quantification of generative video models, consisting of: (i) a metric for
evaluating the calibration of video models based on robust rank correlation
estimation with no stringent modeling assumptions; (ii) a black-box UQ method
for video models (termed S-QUBED), which leverages latent modeling to
rigorously decompose predictive uncertainty into its aleatoric and epistemic
components; and (iii) a UQ dataset to facilitate benchmarking calibration in
video models. By conditioning the generation task in the latent space, we
disentangle uncertainty arising due to vague task specifications from that
arising from lack of knowledge. Through extensive experiments on benchmark
video datasets, we demonstrate that S-QUBED computes calibrated total
uncertainty estimates that are negatively correlated with the task accuracy and
effectively computes the aleatoric and epistemic constituents.

</details>


### [6] [PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization](https://arxiv.org/abs/2510.02599)
*Hovhannes Margaryan,Bo Wan,Tinne Tuytelaars*

Main category: cs.CV

TL;DR: 提出Prompt Embedding Optimization (PEO)方法，通过优化文本嵌入来提升预训练文本到图像扩散模型生成图像的美学质量，无需额外训练且与骨干模型无关。


<details>
  <summary>Details</summary>
Motivation: 针对预训练文本到图像扩散模型在简单提示词下生成图像美学质量不足的问题，希望在不修改模型参数的情况下提升生成效果。

Method: 使用三重目标函数优化文本嵌入：提升生成图像的美学保真度、确保与优化后文本嵌入的一致性、通过提示词保留项最小化与初始提示词的差异。

Result: 定量和定性评估表明该方法有效，在性能上达到或超过最先进的文本到图像和提示词适应方法。

Conclusion: PEO方法能够有效提升预训练扩散模型在简单提示词下的图像美学质量，且具有训练自由和骨干无关的优势。

Abstract: This paper introduces a novel approach to aesthetic quality improvement in
pre-trained text-to-image diffusion models when given a simple prompt. Our
method, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained
text-to-image diffusion model as a backbone and optimizes the text embedding of
a given simple and uncurated prompt to enhance the visual quality of the
generated image. We achieve this by a tripartite objective function that
improves the aesthetic fidelity of the generated image, ensures adherence to
the optimized text embedding, and minimal divergence from the initial prompt.
The latter is accomplished through a prompt preservation term. Additionally,
PEO is training-free and backbone-independent. Quantitative and qualitative
evaluations confirm the effectiveness of the proposed method, exceeding or
equating the performance of state-of-the-art text-to-image and prompt
adaptation methods.

</details>


### [7] [Ego-Exo 3D Hand Tracking in the Wild with a Mobile Multi-Camera Rig](https://arxiv.org/abs/2510.02601)
*Patrick Rim,Kun He,Kevin Harris,Braden Copple,Shangchen Han,Sizhe An,Ivan Shugurov,Tomas Hodan,He Wen,Xu Xie*

Main category: cs.CV

TL;DR: 提出了一种新颖的无标记多摄像头系统，用于在真实野外条件下捕获精确的3D手部和物体，解决了现有数据集在环境多样性和模型泛化方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有手部跟踪数据集主要在受控实验室环境中采集，限制了环境多样性和模型泛化能力，需要一种能够在真实野外条件下精确跟踪手部及其与物体交互的方法。

Method: 设计了一种轻量级背戴式捕获系统，包含八个外中心摄像头和Meta Quest 3头显提供的两个内中心视图，开发了ego-exo跟踪流程来生成准确的3D手部姿态真值。

Result: 收集了包含同步多视角图像和精确3D手部姿态的标注数据集，证明了该方法能够显著减少环境真实性与3D标注精度之间的权衡。

Conclusion: 该方法为在真实野外条件下进行精确3D手部跟踪提供了可行解决方案，提升了环境多样性与标注精度之间的平衡。

Abstract: Accurate 3D tracking of hands and their interactions with the world in
unconstrained settings remains a significant challenge for egocentric computer
vision. With few exceptions, existing datasets are predominantly captured in
controlled lab setups, limiting environmental diversity and model
generalization. To address this, we introduce a novel marker-less multi-camera
system designed to capture precise 3D hands and objects, which allows for
nearly unconstrained mobility in genuinely in-the-wild conditions. We combine a
lightweight, back-mounted capture rig with eight exocentric cameras, and a
user-worn Meta Quest 3 headset, which contributes two egocentric views. We
design an ego-exo tracking pipeline to generate accurate 3D hand pose ground
truth from this system, and rigorously evaluate its quality. By collecting an
annotated dataset featuring synchronized multi-view images and precise 3D hand
poses, we demonstrate the capability of our approach to significantly reduce
the trade-off between environmental realism and 3D annotation accuracy.

</details>


### [8] [Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation](https://arxiv.org/abs/2510.02617)
*Beijia Lu,Ziyi Chen,Jing Xiao,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 提出了一种新的视频蒸馏方法，通过输入人体姿态条件引导注意力和损失函数，将多步扩散模型蒸馏为少步学生模型，实现实时音频驱动视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的音频驱动视频生成方法由于去噪步骤多和注意力机制成本高，导致速度慢，无法实现实时部署。

Method: 使用输入人体姿态关键点对应关系引导注意力到相关区域（如人脸、手部、上半身），提出输入感知稀疏注意力减少冗余计算；引入输入感知蒸馏损失提升唇部同步和手部运动真实性。

Result: 方法实现了实时性能，相比最近的音频驱动和输入驱动方法具有更好的视觉质量。

Conclusion: 通过整合输入感知稀疏注意力和蒸馏损失，该方法在保持视觉质量的同时显著提升了推理效率，为实时音频驱动视频生成提供了有效解决方案。

Abstract: Diffusion models can synthesize realistic co-speech video from audio for
various applications, such as video creation and virtual agents. However,
existing diffusion-based methods are slow due to numerous denoising steps and
costly attention mechanisms, preventing real-time deployment. In this work, we
distill a many-step diffusion video model into a few-step student model.
Unfortunately, directly applying recent diffusion distillation methods degrades
video quality and falls short of real-time performance. To address these
issues, our new video distillation method leverages input human pose
conditioning for both attention and loss functions. We first propose using
accurate correspondence between input human pose keypoints to guide attention
to relevant regions, such as the speaker's face, hands, and upper body. This
input-aware sparse attention reduces redundant computations and strengthens
temporal correspondences of body parts, improving inference efficiency and
motion coherence. To further enhance visual quality, we introduce an
input-aware distillation loss that improves lip synchronization and hand motion
realism. By integrating our input-aware sparse attention and distillation loss,
our method achieves real-time performance with improved visual quality compared
to recent audio-driven and input-driven methods. We also conduct extensive
experiments showing the effectiveness of our algorithmic design choices.

</details>


### [9] [Deep Generative Continual Learning using Functional LoRA: FunLoRA](https://arxiv.org/abs/2510.02631)
*Victor Enescu,Hichem Sahbi*

Main category: cs.CV

TL;DR: 提出FunLoRA方法，使用基于低秩适应的新型条件机制来解决生成模型持续学习中的灾难性遗忘问题，仅需在当前任务数据上训练，无需依赖合成数据。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型在持续适应过程中面临灾难性遗忘问题，现有方法依赖合成数据进行再训练，存在训练时间不断增加和性能长期下降的局限性。

Method: 设计基于低秩适应的功能LoRA(FunLoRA)，使用仅包含秩1矩阵的动态条件机制，通过精心选择的函数增加重参数化矩阵的秩。

Result: 实验表明FunLoRA在基于流匹配的模型上超越了基于扩散模型的现有最佳方法，达到更高的分类准确率，同时仅需少量内存成本和采样时间。

Conclusion: FunLoRA提供了一种参数高效的微调方法，能有效避免灾难性遗忘，在持续学习任务中表现出色。

Abstract: Continual adaptation of deep generative models holds tremendous potential and
critical importance, given their rapid and expanding usage in text and vision
based applications. Incremental training, however, remains highly challenging
due to catastrophic forgetting phenomenon, which makes it difficult for neural
networks to effectively incorporate new knowledge. A common strategy consists
in retraining the generative model on its own synthetic data in order to
mitigate forgetting. Yet, such an approach faces two major limitations: (i) the
continually increasing training time eventually becomes intractable, and (ii)
reliance on synthetic data inevitably leads to long-term performance
degradation, since synthetic samples lack the richness of real training data.
In this paper, we attenuate these issues by designing a novel and more
expressive conditioning mechanism for generative models based on low rank
adaptation (LoRA), that exclusively employs rank 1 matrices, whose
reparametrized matrix rank is functionally increased using carefully selected
functions -- and dubbed functional LoRA: FunLoRA. Using this dynamic
conditioning, the generative model is guaranteed to avoid catastrophic
forgetting and needs only to be trained on data from the current task.
Extensive experiments using flow-matching based models trained from scratch,
showcase that our proposed parameter-efficient fine-tuning (PEFT) method
surpasses prior state-of-the-art results based on diffusion models, reaching
higher classification accuracy scores, while only requiring a fraction of the
memory cost and sampling time.

</details>


### [10] [Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles](https://arxiv.org/abs/2510.02642)
*Abhishek Joshi,Jahnavi Krishna Koda,Abhishek Phadke*

Main category: cs.CV

TL;DR: 提出了一个双视野、序列保持的鲁棒性框架，用于美国交通灯和标志识别，通过统一的三层防御堆栈（特征压缩、防御蒸馏、基于熵的异常检测）和时序投票机制，显著提升了模型在数字和自然退化条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆中交通灯和标志识别错误直接影响导航和安全，现有方法缺乏对时间连续性、多静态视野感知以及数字和自然退化鲁棒性的考虑。

Method: 基于aiMotive、Udacity、Waymo和自录德克萨斯州视频构建多源数据集，采用双视野、序列保持的鲁棒性框架，包含特征压缩、防御蒸馏、熵基异常检测的三层防御堆栈，并使用时序投票增强。

Result: 统一防御堆栈达到79.8mAP，将攻击成功率降至18.2%，优于YOLOv8、YOLOv9和BEVFormer，同时将高风险误分类降至32%。

Conclusion: 该框架有效提升了交通灯和标志识别在多种操作设计域下的鲁棒性，为自动驾驶安全提供了重要保障。

Abstract: Traffic light and sign recognition are key for Autonomous Vehicles (AVs)
because perception mistakes directly influence navigation and safety. In
addition to digital adversarial attacks, models are vulnerable to existing
perturbations (glare, rain, dirt, or graffiti), which could lead to dangerous
misclassifications. The current work lacks consideration of temporal
continuity, multistatic field-of-view (FoV) sensing, and robustness to both
digital and natural degradation. This study proposes a dual FoV,
sequence-preserving robustness framework for traffic lights and signs in the
USA based on a multi-source dataset built on aiMotive, Udacity, Waymo, and
self-recorded videos from the region of Texas. Mid and long-term sequences of
RGB images are temporally aligned for four operational design domains (ODDs):
highway, night, rainy, and urban. Over a series of experiments on a real-life
application of anomaly detection, this study outlines a unified three-layer
defense stack framework that incorporates feature squeezing, defensive
distillation, and entropy-based anomaly detection, as well as sequence-wise
temporal voting for further enhancement. The evaluation measures included
accuracy, attack success rate (ASR), risk-weighted misclassification severity,
and confidence stability. Physical transferability was confirmed using probes
for recapture. The results showed that the Unified Defense Stack achieved
79.8mAP and reduced the ASR to 18.2%, which is superior to YOLOv8, YOLOv9, and
BEVFormer, while reducing the high-risk misclassification to 32%.

</details>


### [11] [Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models](https://arxiv.org/abs/2510.02654)
*Benjamin Yu,Jackie Liu,Justin Cui*

Main category: cs.CV

TL;DR: Smart-GRPO是一种优化流匹配模型中强化学习噪声扰动的方法，通过迭代搜索策略改进噪声分布，提高奖励优化和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型具有确定性特性，不适合强化学习，而现有方法通过随机噪声扰动效率低且不稳定。

Method: 采用迭代搜索策略：解码候选扰动、用奖励函数评估、向高奖励区域优化噪声分布。

Result: 实验表明Smart-GRPO在奖励优化和视觉质量方面优于基线方法。

Conclusion: 为流匹配框架中的强化学习提供了实用路径，弥合了高效训练与人类对齐生成之间的差距。

Abstract: Recent advancements in flow-matching have enabled high-quality text-to-image
generation. However, the deterministic nature of flow-matching models makes
them poorly suited for reinforcement learning, a key tool for improving image
quality and human alignment. Prior work has introduced stochasticity by
perturbing latents with random noise, but such perturbations are inefficient
and unstable. We propose Smart-GRPO, the first method to optimize noise
perturbations for reinforcement learning in flow-matching models. Smart-GRPO
employs an iterative search strategy that decodes candidate perturbations,
evaluates them with a reward function, and refines the noise distribution
toward higher-reward regions. Experiments demonstrate that Smart-GRPO improves
both reward optimization and visual quality compared to baseline methods. Our
results suggest a practical path toward reinforcement learning in flow-matching
frameworks, bridging the gap between efficient training and human-aligned
generation.

</details>


### [12] [FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min](https://arxiv.org/abs/2510.02691)
*Yibin Zhao,Yihan Pan,Jun Nan,Jianjun Yi*

Main category: cs.CV

TL;DR: FSFSplatter是一种从自由稀疏图像进行快速表面重建的新方法，通过端到端密集高斯初始化、相机参数估计和几何增强场景优化，解决了现有方法需要密集校准视图的问题。


<details>
  <summary>Details</summary>
Motivation: 现有高斯溅射方法大多需要密集、校准的视图，而从自由稀疏图像重建往往由于有限重叠和过拟合导致表面质量差。

Method: 使用大型Transformer编码多视图图像，通过自分裂高斯头生成密集且几何一致的高斯场景初始化，采用贡献度剪枝消除局部漂浮物，利用深度和多视图特征监督结合可微分相机参数缓解过拟合。

Result: FSFSplatter在广泛使用的DTU和Replica数据集上优于当前最先进方法。

Conclusion: 该方法能够从自由稀疏图像实现高质量的表面重建，解决了现有方法的局限性。

Abstract: Gaussian Splatting has become a leading reconstruction technique, known for
its high-quality novel view synthesis and detailed reconstruction. However,
most existing methods require dense, calibrated views. Reconstructing from free
sparse images often leads to poor surface due to limited overlap and
overfitting. We introduce FSFSplatter, a new approach for fast surface
reconstruction from free sparse images. Our method integrates end-to-end dense
Gaussian initialization, camera parameter estimation, and geometry-enhanced
scene optimization. Specifically, FSFSplatter employs a large Transformer to
encode multi-view images and generates a dense and geometrically consistent
Gaussian scene initialization via a self-splitting Gaussian head. It eliminates
local floaters through contribution-based pruning and mitigates overfitting to
limited views by leveraging depth and multi-view feature supervision with
differentiable camera parameters during rapid optimization. FSFSplatter
outperforms current state-of-the-art methods on widely used DTU and Replica.

</details>


### [13] [MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context](https://arxiv.org/abs/2510.02722)
*Junyu Shi,Yong Sun,Zhiyuan Zhang,Lijiang Liu,Zhengjie Zhang,Yuxin He,Qiang Nie*

Main category: cs.CV

TL;DR: MoGIC是一个统一的多模态运动生成框架，通过整合意图建模和视觉先验，解决了现有方法在捕捉动作执行因果逻辑和人类意图方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动运动生成方法将合成视为语言和运动之间的双向映射，但在捕捉动作执行的因果逻辑和驱动行为的人类意图方面存在局限。缺乏视觉基础进一步限制了精度和个性化。

Method: 提出MoGIC框架，通过联合优化多模态条件运动生成和意图预测，揭示潜在的人类目标，利用视觉先验增强生成能力。引入具有自适应范围的混合注意力机制，实现条件标记和运动子序列之间的有效局部对齐。

Result: 微调后，MoGIC在HumanML3D上将FID降低了38.6%，在Mo440H上降低了34.6%。在运动描述任务中超越了基于LLM的方法，并进一步实现了意图预测和视觉条件生成。

Conclusion: MoGIC推进了可控运动合成和意图理解，展示了多模态生成能力，为运动生成领域提供了更精确和个性化的解决方案。

Abstract: Existing text-driven motion generation methods often treat synthesis as a
bidirectional mapping between language and motion, but remain limited in
capturing the causal logic of action execution and the human intentions that
drive behavior. The absence of visual grounding further restricts precision and
personalization, as language alone cannot specify fine-grained spatiotemporal
details. We propose MoGIC, a unified framework that integrates intention
modeling and visual priors into multimodal motion synthesis. By jointly
optimizing multimodal-conditioned motion generation and intention prediction,
MoGIC uncovers latent human goals, leverages visual priors to enhance
generation, and exhibits versatile multimodal generative capability. We further
introduce a mixture-of-attention mechanism with adaptive scope to enable
effective local alignment between conditional tokens and motion subsequences.
To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21
high-quality motion datasets. Experiments show that after finetuning, MoGIC
reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based
methods in motion captioning with a lightweight text head, and further enables
intention prediction and vision-conditioned generation, advancing controllable
motion synthesis and intention understanding. The code is available at
https://github.com/JunyuShi02/MoGIC

</details>


### [14] [From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2510.02732)
*Jianing Chen,Zehao Li,Yujun Cai,Hao Jiang,Shuqin Gao,Honglong Zhao,Tianlu Mao,Yucheng Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于运动自适应的动态3D重建框架，通过语义和运动先验实现控制点密度与运动复杂度的对齐，解决了现有稀疏控制方法在静态区域冗余和动态区域不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏控制方法仅基于几何分配控制点，导致静态区域冗余和动态区域不足，无法有效处理动态3D重建中运动复杂度的变化。

Method: 利用视觉基础模型的语义和运动先验建立patch-token-node对应关系，通过运动自适应压缩在动态区域集中控制点，同时抑制静态背景冗余。采用迭代体素化和运动趋势评分实现表示密度自适应，并使用基于样条的轨迹参数化替代MLP变形场。

Result: 在重建质量和效率方面相比现有最先进方法有显著提升。

Conclusion: 提出的运动自适应框架有效解决了控制点分配与运动复杂度不匹配的问题，实现了更高质量的动态3D重建。

Abstract: Dynamic 3D reconstruction from monocular videos remains difficult due to the
ambiguity inferring 3D motion from limited views and computational demands of
modeling temporally varying scenes. While recent sparse control methods
alleviate computation by reducing millions of Gaussians to thousands of control
points, they suffer from a critical limitation: they allocate points purely by
geometry, leading to static redundancy and dynamic insufficiency. We propose a
motion-adaptive framework that aligns control density with motion complexity.
Leveraging semantic and motion priors from vision foundation models, we
establish patch-token-node correspondences and apply motion-adaptive
compression to concentrate control points in dynamic regions while suppressing
redundancy in static backgrounds. Our approach achieves flexible
representational density adaptation through iterative voxelization and motion
tendency scoring, directly addressing the fundamental mismatch between control
point allocation and motion complexity. To capture temporal evolution, we
introduce spline-based trajectory parameterization initialized by 2D tracklets,
replacing MLP-based deformation fields to achieve smoother motion
representation and more stable optimization. Extensive experiments demonstrate
significant improvements in reconstruction quality and efficiency over existing
state-of-the-art methods.

</details>


### [15] [Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising](https://arxiv.org/abs/2510.02733)
*Weimin Yuan,Cai Meng*

Main category: cs.CV

TL;DR: Net2Net是一种结合无监督DIP和监督预训练DRUNet的混合去噪方法，通过RED正则化实现，无需标记数据即可适应各种真实噪声模式。


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法依赖手工先验，难以处理真实噪声的复杂性；深度学习需要大量标记数据且泛化能力有限。需要一种能结合两者优势的方法。

Method: 结合无监督DIP网络和预训练DRUNet网络，通过RED正则化进行融合。无监督网络适应输入图像特定噪声，预训练网络提供大规模数据集学到的鲁棒表示。

Result: 在基准数据集上的广泛实验表明，该方法在真实世界噪声去除方面具有优越性，特别是在训练数据有限的情况下表现更好。

Conclusion: Net2Net混合框架通过结合无监督和监督网络的优点，有效提升了去噪性能和对不同噪声模式的泛化能力。

Abstract: Traditional denoising methods for noise removal have largely relied on
handcrafted priors, often perform well in controlled environments but struggle
to address the complexity and variability of real noise. In contrast, deep
learning-based approaches have gained prominence for learning noise
characteristics from large datasets, but these methods frequently require
extensive labeled data and may not generalize effectively across diverse noise
types and imaging conditions. In this paper, we present an innovative method,
termed as Net2Net, that combines the strengths of untrained and pre-trained
networks to tackle the challenges of real-world noise removal. The innovation
of Net2Net lies in its combination of unsupervised DIP and supervised
pre-trained model DRUNet by regularization by denoising (RED). The untrained
network adapts to the unique noise characteristics of each input image without
requiring labeled data, while the pre-trained network leverages learned
representations from large-scale datasets to deliver robust denoising
performance. This hybrid framework enhances generalization across varying noise
patterns and improves performance, particularly in scenarios with limited
training data. Extensive experiments on benchmark datasets demonstrate the
superiority of our method for real-world noise removal.

</details>


### [16] [Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient Multimodal Retrieval](https://arxiv.org/abs/2510.02745)
*Lanyun Zhu,Deyi Ji,Tianrun Chen,Haiyang Wu,Shiqi Wang*

Main category: cs.CV

TL;DR: Retrv-R1是首个专为多模态通用检索设计的R1风格MLLM，通过逐步推理提升检索准确性，解决了DeepSeek-R1直接应用于检索任务时的计算成本高和训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: DeepSeek-R1展示了RL增强LLM推理能力的潜力，但直接应用于检索任务存在计算成本高和训练不稳定的问题，需要专门设计来解决这些挑战。

Method: 引入信息压缩模块和细节检查机制降低计算成本，提出新的训练范式包括检索定制的合成CoT数据集激活阶段和带课程奖励的RL训练。

Result: Retrv-R1在多个基准测试和任务中实现了SOTA性能、高效率和强泛化能力。

Conclusion: 通过专门设计的压缩模块和训练范式，Retrv-R1成功将R1风格推理应用于多模态检索任务，取得了优异性能。

Abstract: The success of DeepSeek-R1 demonstrates the immense potential of using
reinforcement learning (RL) to enhance LLMs' reasoning capabilities. This paper
introduces Retrv-R1, the first R1-style MLLM specifically designed for
multimodal universal retrieval, achieving higher performance by employing
step-by-step reasoning to produce more accurate retrieval results. We find that
directly applying the methods of DeepSeek-R1 to retrieval tasks is not
feasible, mainly due to (1) the high computational cost caused by the large
token consumption required for multiple candidates with reasoning processes,
and (2) the instability and suboptimal results when directly applying RL to
train for retrieval tasks. To address these issues, Retrv-R1 introduces an
information compression module with a details inspection mechanism, which
enhances computational efficiency by reducing the number of tokens while
ensuring that critical information for challenging candidates is preserved.
Furthermore, a new training paradigm is proposed, including an activation stage
using a retrieval-tailored synthetic CoT dataset for more effective
optimization, followed by RL with a novel curriculum reward to improve both
performance and efficiency. Incorporating these novel designs, Retrv-R1
achieves SOTA performance, high efficiency, and strong generalization ability,
as demonstrated by experiments across multiple benchmarks and tasks.

</details>


### [17] [Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models](https://arxiv.org/abs/2510.02750)
*Lihua Zhou,Mao Ye,Shuaifeng Li,Nianxin Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: BCA+是一个无需训练、无需反向传播的测试时自适应框架，通过动态缓存机制和贝叶斯推理，同时适应物体识别和检测任务中的分布偏移。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时自适应方法要么依赖计算昂贵的反向传播，要么只关注似然适应而忽略了先验的重要作用。BCA+旨在解决这些问题，为实时部署提供高效解决方案。

Method: 引入动态缓存机制，自适应存储和更新类别嵌入、空间尺度（用于检测）以及从历史预测中推导的自适应类别先验。将自适应建模为贝叶斯推理问题，通过融合初始VLM输出和基于缓存的预测来生成最终预测。

Result: 在识别和检测基准测试中，BCA+实现了最先进的性能，同时保持高效性。

Conclusion: BCA+通过双适应机制和不确定性引导的融合，有效纠正模型的语义理解和上下文置信度，为视觉语言模型在真实世界分布偏移下的部署提供了高效解决方案。

Abstract: Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved
remarkable success in object recognition and detection. However, their
performance often degrades under real-world distribution shifts. Test-time
adaptation (TTA) aims to mitigate this issue by adapting models during
inference. Existing methods either rely on computationally expensive
backpropagation, which hinders real-time deployment, or focus solely on
likelihood adaptation, which overlooks the critical role of the prior. Our
prior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for
object recognition by introducing a training-free framework that incorporates
adaptive priors. Building upon this foundation, we now present Bayesian Class
Adaptation plus (BCA+), a unified, training-free framework for TTA for both
object recognition and detection. BCA+ introduces a dynamic cache that
adaptively stores and updates class embeddings, spatial scales (for detection),
and, crucially, adaptive class priors derived from historical predictions. We
formulate adaptation as a Bayesian inference problem, where final predictions
are generated by fusing the initial VLM output with a cache-based prediction.
This cache-based prediction combines a dynamically updated likelihood
(measuring feature and scale similarity) and a prior (reflecting the evolving
class distribution). This dual-adaptation mechanism, coupled with
uncertainty-guided fusion, enables BCA+ to correct both the model's semantic
understanding and its contextual confidence. As a training-free method
requiring no backpropagation, BCA+ is highly efficient. Extensive experiments
demonstrate that BCA+ achieves state-of-the-art performance on both recognition
and detection benchmarks.

</details>


### [18] [Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology](https://arxiv.org/abs/2510.02760)
*Matthias Perkonigg,Patrick Rockenschaub,Georg Göbel,Adelheid Wöhrer*

Main category: cs.CV

TL;DR: 提出了一种用于脑肿瘤分类的层次化广义类别发现方法（HGCD-BT），通过结合层次聚类和对比学习，能够同时识别已知和未知的肿瘤类别，在OpenSRH数据集上相比现有GCD方法准确率提升28%。


<details>
  <summary>Details</summary>
Motivation: 现有脑肿瘤分类方法局限于预定义的固定类别，无法识别训练时未见的肿瘤类型。无监督学习缺乏利用标注数据先验知识的能力，而半监督方法假设所有潜在类别都在标注数据中。需要一种能够同时分类已知和未知类别的方法。

Method: HGCD-BT将层次聚类与对比学习相结合，扩展了基于对比学习的GCD方法，引入了半监督层次聚类损失函数，能够捕捉脑肿瘤分类学的层次结构。

Result: 在OpenSRH数据集上，HGCD-BT在图像块级别分类中比最先进的GCD方法准确率提高28%，特别是在识别未见过的肿瘤类别方面表现优异。在Digital Brain Tumor Atlas的H&E染色全玻片图像上验证了方法的跨模态通用性。

Conclusion: HGCD-BT是一种有效的脑肿瘤分类方法，能够同时识别已知和未知肿瘤类别，在多种成像模态上都表现出良好的性能，为术中决策提供了更全面的肿瘤分类能力。

Abstract: Accurate brain tumor classification is critical for intra-operative decision
making in neuro-oncological surgery. However, existing approaches are
restricted to a fixed set of predefined classes and are therefore unable to
capture patterns of tumor types not available during training. Unsupervised
learning can extract general-purpose features, but it lacks the ability to
incorporate prior knowledge from labelled data, and semi-supervised methods
often assume that all potential classes are represented in the labelled data.
Generalized Category Discovery (GCD) aims to bridge this gap by categorizing
both known and unknown classes within unlabelled data. To reflect the
hierarchical structure of brain tumor taxonomies, in this work, we introduce
Hierarchical Generalized Category Discovery for Brain Tumor Classification
(HGCD-BT), a novel approach that integrates hierarchical clustering with
contrastive learning. Our method extends contrastive learning based GCD by
incorporating a novel semi-supervised hierarchical clustering loss. We evaluate
HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images,
achieving a +28% improvement in accuracy over state-of-the-art GCD methods for
patch-level classification, particularly in identifying previously unseen tumor
categories. Furthermore, we demonstrate the generalizability of HGCD-BT on
slide-level classification of hematoxylin and eosin stained whole-slide images
from the Digital Brain Tumor Atlas, confirming its utility across imaging
modalities.

</details>


### [19] [AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding](https://arxiv.org/abs/2510.02778)
*Xian Zhang,Zexi Wu,Zinuo Li,Hongming Xu,Luqi Gong,Farid Boussaid,Naoufel Werghi,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: AdaRD-Key是一种无需训练的关键帧采样模块，通过统一的相关性-多样性目标函数，为长视频理解选择信息丰富且非冗余的关键帧。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型依赖均匀采样，往往忽略关键时刻；而现有关键帧选择方法要么采用严格的时间间隔导致错过精细线索，要么强调视觉多样性但忽略查询相关性。

Method: 提出最大化统一的相关性-多样性最大体积目标函数，结合查询条件相关性评分和对数行列式多样性组件，并采用轻量级相关性感知门控机制处理弱对齐查询。

Result: 在LongVideoBench和Video-MME上的广泛实验展示了最先进的性能，特别是在长视频上表现优异，且计算高效（在单GPU上实时运行）。

Conclusion: AdaRD-Key是一种无需训练、计算高效且与现有视觉语言模型即插即用的关键帧采样方法，显著提升了长视频理解能力。

Abstract: Understanding long-form videos remains a significant challenge for
vision--language models (VLMs) due to their extensive temporal length and high
information density. Most current multimodal large language models (MLLMs) rely
on uniform sampling, which often overlooks critical moments, leading to
incorrect responses to queries. In parallel, many keyframe selection approaches
impose rigid temporal spacing: once a frame is chosen, an exclusion window
suppresses adjacent timestamps to reduce redundancy. While effective at
limiting overlap, this strategy frequently misses short, fine-grained cues near
important events. Other methods instead emphasize visual diversity but neglect
query relevance. We propose AdaRD-Key, a training-free keyframe sampling module
for query-driven long-form video understanding. AdaRD-Key maximizes a unified
Relevance--Diversity Max-Volume (RD-MV) objective, combining a
query-conditioned relevance score with a log-determinant diversity component to
yield informative yet non-redundant frames. To handle broad queries with weak
alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating
mechanism; when the relevance distribution indicates weak alignment, the method
seamlessly shifts into a diversity-only mode, enhancing coverage without
additional supervision. Our pipeline is training-free, computationally
efficient (running in real time on a single GPU), and compatible with existing
VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and
Video-MME demonstrate state-of-the-art performance, particularly on long-form
videos. Code available at https://github.com/Xian867/AdaRD-Key.

</details>


### [20] [Reasoning Riddles: How Explainability Reveals Cognitive Limits in Vision-Language Models](https://arxiv.org/abs/2510.02780)
*Prahitha Movva*

Main category: cs.CV

TL;DR: 该研究通过可解释性分析揭示视觉语言模型在解决复杂横向思维挑战（如字谜游戏）时的认知过程和失败模式，发现推理质量在不同谜题类别间差异显著，提示策略对认知方法和问题解决效果有重要影响。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多模态任务中表现出色，但其在复杂横向思维挑战（如字谜游戏）上的认知过程仍不透明，且现有研究主要关注性能指标而非底层推理过程。

Method: 构建了包含221个字谜的系统标注数据集，涵盖六个认知类别，并开发了将推理质量与答案正确性分离的评估框架，研究了三种提示策略以引出不同类型的解释过程。

Result: 发现推理质量在不同谜题类别间差异显著，模型在视觉组合方面表现出系统性优势，但在缺失解释和文化象征方面存在根本性限制；提示策略显著影响认知方法和问题解决效果。

Conclusion: 可解释性是模型性能的组成部分而非事后考虑，提示策略对认知过程和问题解决效果有重要影响，揭示了视觉语言模型在复杂横向思维任务中的系统优势和局限性。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet their
cognitive processes remain opaque on complex lateral thinking challenges like
rebus puzzles. While recent work has demonstrated these models struggle
significantly with rebus puzzle solving, the underlying reasoning processes and
failure patterns remain largely unexplored. We address this gap through a
comprehensive explainability analysis that moves beyond performance metrics to
understand how VLMs approach these complex lateral thinking challenges. Our
study contributes a systematically annotated dataset of 221 rebus puzzles
across six cognitive categories, paired with an evaluation framework that
separates reasoning quality from answer correctness. We investigate three
prompting strategies designed to elicit different types of explanatory
processes and reveal critical insights into VLM cognitive processes. Our
findings demonstrate that reasoning quality varies dramatically across puzzle
categories, with models showing systematic strengths in visual composition
while exhibiting fundamental limitations in absence interpretation and cultural
symbolism. We also discover that prompting strategy substantially influences
both cognitive approach and problem-solving effectiveness, establishing
explainability as an integral component of model performance rather than a
post-hoc consideration.

</details>


### [21] [OTR: Synthesizing Overlay Text Dataset for Text Removal](https://arxiv.org/abs/2510.02787)
*Jan Zdenek,Wataru Shimoda,Kota Yamaguchi*

Main category: cs.CV

TL;DR: 提出了一种合成文本移除基准数据集的方法，适用于场景文本以外的领域，解决现有数据集在跨域泛化和准确评估方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有文本移除研究主要关注自然图像中的场景文本，但当前数据集存在地面真值人工编辑伪影、文本背景过于简单、评估指标无法捕捉生成结果质量等问题，限制了跨域泛化能力。

Method: 通过对象感知布局和视觉语言模型生成内容，在复杂背景上渲染文本，合成具有清洁地面真值和挑战性文本移除场景的数据集。

Result: 创建了一个适用于场景文本以外领域的文本移除基准数据集，解决了现有数据集的局限性，数据集已在HuggingFace平台公开。

Conclusion: 提出的合成方法能够生成高质量的文本移除基准数据集，为跨域文本移除任务提供了更好的评估基础。

Abstract: Text removal is a crucial task in computer vision with applications such as
privacy preservation, image editing, and media reuse. While existing research
has primarily focused on scene text removal in natural images, limitations in
current datasets hinder out-of-domain generalization or accurate evaluation. In
particular, widely used benchmarks such as SCUT-EnsText suffer from ground
truth artifacts due to manual editing, overly simplistic text backgrounds, and
evaluation metrics that do not capture the quality of generated results. To
address these issues, we introduce an approach to synthesizing a text removal
benchmark applicable to domains other than scene texts. Our dataset features
text rendered on complex backgrounds using object-aware placement and
vision-language model-generated content, ensuring clean ground truth and
challenging text removal scenarios. The dataset is available at
https://huggingface.co/datasets/cyberagent/OTR .

</details>


### [22] [Align Your Query: Representation Alignment for Multimodality Medical Object Detection](https://arxiv.org/abs/2510.02789)
*Ara Seo,Bryan Sangwoo Kim,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出了一种简单、检测器无关的框架，通过模态上下文注意力(MoCA)和查询表示对齐(QueryREPA)来对齐DETR风格对象查询与模态上下文，解决多模态医学目标检测中的统计异质性和表示空间不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 医学目标检测在单一检测器训练混合医学模态(如CXR、CT、MRI)时表现不佳，因为存在异构统计特性和不连续的表示空间。

Method: 1. 定义模态标记：紧凑的文本衍生嵌入编码成像模态；2. 通过多模态上下文注意力(MoCA)将模态标记集成到检测过程中；3. 引入QueryREPA预训练阶段，使用任务特定对比目标对齐查询表示与模态标记。

Result: 该方法在多种模态联合训练时持续提高AP，开销最小且无需架构修改，为鲁棒的多模态医学目标检测提供了实用路径。

Conclusion: MoCA和QueryREPA共同产生模态感知、类别忠实的查询，能有效迁移到下游训练，显著提升多模态医学目标检测性能。

Abstract: Medical object detection suffers when a single detector is trained on mixed
medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and
disjoint representation spaces. To address this challenge, we turn to
representation alignment, an approach that has proven effective for bringing
features from different sources into a shared space. Specifically, we target
the representations of DETR-style object queries and propose a simple,
detector-agnostic framework to align them with modality context. First, we
define modality tokens: compact, text-derived embeddings encoding imaging
modality that are lightweight and require no extra annotations. We integrate
the modality tokens into the detection process via Multimodality Context
Attention (MoCA), mixing object-query representations via self-attention to
propagate modality context within the query set. This preserves DETR-style
architectures and adds negligible latency while injecting modality cues into
object queries. We further introduce QueryREPA, a short pretraining stage that
aligns query representations to their modality tokens using a task-specific
contrastive objective with modality-balanced batches. Together, MoCA and
QueryREPA produce modality-aware, class-faithful queries that transfer
effectively to downstream training. Across diverse modalities trained
altogether, the proposed approach consistently improves AP with minimal
overhead and no architectural modifications, offering a practical path toward
robust multimodality medical object detection. Project page:
https://araseo.github.io/alignyourquery/.

</details>


### [23] [MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding](https://arxiv.org/abs/2510.02790)
*Jingyuan Deng,Yujiu Yang*

Main category: cs.CV

TL;DR: 提出了一种名为MaskCD的方法，通过掩码LVLMs中的"图像头"来构建对比样本，有效缓解大视觉语言模型的幻觉问题，同时保持模型的一般能力。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型存在幻觉问题，即生成与输入视觉和文本内容相矛盾的内容。现有的对比解码方法难以构建合适的对比样本，注意力操作方法缺乏稳定性。

Method: 利用LVLMs中的"图像头"，通过掩码这些头来构建对比样本进行对比解码。

Result: 在LLaVA-1.5-7b和Qwen-VL-7b上的评估显示，MaskCD在CHAIR、POPE、AMBER和MME等基准测试中有效缓解了幻觉现象。

Conclusion: MaskCD能够有效缓解LVLMs的幻觉问题，同时保持模型的通用能力。

Abstract: Large vision-language models (LVLMs) have shown remarkable performance in
visual-language understanding for downstream multimodal tasks. While their
capabilities are improving, problems emerge simultaneously. Among those
problems, the hallucinations have attracted much attention, which stands for
the phenomenon where LVLMs generate contradictory content to their input visual
and text contents. Many approaches have been proposed to deal with this issue,
such as contrastive decoding and attention manipulation. However, contrastive
decoding methods struggle in constructing appropriate contrastive samples, and
attention manipulation methods are highly sensitive, lacking stability. In this
work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and
Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The
results demonstrate that MaskCD effectively alleviates the phenomenon of
hallucinations and retains the general capabilities of LVLMs. Corresponding
resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .

</details>


### [24] [VERNIER: an open-source software pushing marker pose estimation down to the micrometer and nanometer scales](https://arxiv.org/abs/2510.02791)
*Patrick Sandoz,Antoine N. André,Guillaume J. Laurent*

Main category: cs.CV

TL;DR: VERNIER是一个开源相位处理软件，用于基于伪周期模式提供快速可靠的姿态测量，特别适用于纳米级和微弧度分辨率的微小尺度姿态估计。


<details>
  <summary>Details</summary>
Motivation: 解决在微小尺度下进行6自由度姿态估计的挑战，特别是在纳米级和微弧度分辨率下对相对大范围进行精确测量的需求。

Method: 采用基于相位的局部阈值算法处理伪周期模式，通过相位处理步骤实现姿态测量，软件对噪声、散焦和遮挡具有鲁棒性。

Result: 软件在合成和实验图像上得到验证，能够实现厘米级范围和纳米级分辨率的姿态测量，提供了选择合适模式设计和显微镜放大镜头的指南。

Conclusion: VERNIER软件为微小尺度姿态估计提供了可靠解决方案，通过相位处理伪周期模式实现了高精度测量，并提供了实际应用指导。

Abstract: Pose estimation is still a challenge at the small scales. Few solutions exist
to capture the 6 degrees of freedom of an object with nanometric and
microradians resolutions over relatively large ranges. Over the years, we have
proposed several fiducial marker and pattern designs to achieve reliable
performance for various microscopy applications. Centimeter ranges are possible
using pattern encoding methods, while nanometer resolutions can be achieved
using phase processing of the periodic frames. This paper presents VERNIER, an
open source phase processing software designed to provide fast and reliable
pose measurement based on pseudo-periodic patterns. Thanks to a phase-based
local thresholding algorithm, the software has proven to be particularly robust
to noise, defocus and occlusion. The successive steps of the phase processing
are presented, as well as the different types of patterns that address
different application needs. The implementation procedure is illustrated with
synthetic and experimental images. Finally, guidelines are given for selecting
the appropriate pattern design and microscope magnification lenses as a
function of the desired performance.

</details>


### [25] [Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis](https://arxiv.org/abs/2510.02815)
*Feng Yuan,Yifan Gao,Yuehua Ye,Haoyue Li,Xin Gao*

Main category: cs.CV

TL;DR: 提出Med-K2N方法解决K到N医学图像合成中的三个关键挑战：异质模态贡献建模、融合质量控制、模态身份一致性保持。通过三个协作模块和因果模态身份模块实现自适应权重学习和渐进增强。


<details>
  <summary>Details</summary>
Motivation: 临床诊断需要灵活的重建缺失成像模态，但面临三个挑战：不同模态对目标任务的异质贡献建模、融合质量控制防止噪声信息退化、多输出生成中的模态身份一致性保持。

Method: 将多模态医学数据视为具有质量驱动选择机制的序列帧，设计三个模块：PreWeightNet（全局贡献评估）、ThresholdNet（自适应过滤）、EffiWeightNet（有效权重计算），以及CMIM模块通过视觉语言建模建立生成图像与目标模态描述间的因果约束。

Result: 在多个基准测试中，Med-K2N显著优于现有最先进方法。

Conclusion: 通过渐进增强和质量控制机制，Med-K2N有效解决了K到N医学图像合成中的关键挑战，为临床诊断提供了可靠的跨模态重建能力。

Abstract: Cross-modal medical image synthesis research focuses on reconstructing
missing imaging modalities from available ones to support clinical diagnosis.
Driven by clinical necessities for flexible modality reconstruction, we explore
K to N medical generation, where three critical challenges emerge: How can we
model the heterogeneous contributions of different modalities to various target
tasks? How can we ensure fusion quality control to prevent degradation from
noisy information? How can we maintain modality identity consistency in
multi-output generation? Driven by these clinical necessities, and drawing
inspiration from SAM2's sequential frame paradigm and clinicians' progressive
workflow of incrementally adding and selectively integrating multi-modal
information, we treat multi-modal medical data as sequential frames with
quality-driven selection mechanisms. Our key idea is to "learn" adaptive
weights for each modality-task pair and "memorize" beneficial fusion patterns
through progressive enhancement. To achieve this, we design three collaborative
modules: PreWeightNet for global contribution assessment, ThresholdNet for
adaptive filtering, and EffiWeightNet for effective weight computation.
Meanwhile, to maintain modality identity consistency, we propose the Causal
Modality Identity Module (CMIM) that establishes causal constraints between
generated images and target modality descriptions using vision-language
modeling. Extensive experimental results demonstrate that our proposed Med-K2N
outperforms state-of-the-art methods by significant margins on multiple
benchmarks. Source code is available.

</details>


### [26] [ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment](https://arxiv.org/abs/2510.02876)
*Md Zahim Hassan,Md. Osama,Muhammad Ashad Kabir,Md. Saiful Islam,Zannatul Naim*

Main category: cs.CV

TL;DR: ELMF4EggQ是一个集成学习框架，通过融合图像、形状和重量等多模态外部特征来无损评估鸡蛋等级和新鲜度，准确率分别达到86.57%和70.83%。


<details>
  <summary>Details</summary>
Motivation: 需要准确、无损的鸡蛋质量评估方法来确保食品安全、维持产品标准和提高商业家禽生产效率。传统方法需要破坏性内部测量，而本研究旨在仅使用外部特征进行质量评估。

Method: 构建包含186个褐壳鸡蛋的公开数据集，使用预训练CNN模型提取图像特征，结合形状和重量特征，通过PCA降维、SMOTE增强和集成投票机制进行分类。

Result: 多模态方法显著优于仅使用图像或仅使用表格特征的基线方法，在等级分类和新鲜度预测上分别达到86.57%和70.83%的准确率。

Conclusion: ELMF4EggQ证明了仅使用外部非侵入性特征就能有效评估鸡蛋内部质量的可行性，为食品质量检测提供了新的无损评估方案。

Abstract: Accurate, non-destructive assessment of egg quality is critical for ensuring
food safety, maintaining product standards, and operational efficiency in
commercial poultry production. This paper introduces ELMF4EggQ, an ensemble
learning framework that employs multimodal feature fusion to classify egg grade
and freshness using only external attributes - image, shape, and weight. A
novel, publicly available dataset of 186 brown-shelled eggs was constructed,
with egg grade and freshness levels determined through laboratory-based expert
assessments involving internal quality measurements, such as yolk index and
Haugh unit. To the best of our knowledge, this is the first study to apply
machine learning methods for internal egg quality assessment using only
external, non-invasive features, and the first to release a corresponding
labeled dataset. The proposed framework integrates deep features extracted from
external egg images with structural characteristics such as egg shape and
weight, enabling a comprehensive representation of each egg. Image feature
extraction is performed using top-performing pre-trained CNN models (ResNet152,
DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction,
SMOTE augmentation, and classification using multiple machine learning
algorithms. An ensemble voting mechanism combines predictions from the
best-performing classifiers to enhance overall accuracy. Experimental results
demonstrate that the multimodal approach significantly outperforms image-only
and tabular (shape and weight) only baselines, with the multimodal ensemble
approach achieving 86.57% accuracy in grade classification and 70.83% in
freshness prediction. All code and data are publicly available at
https://github.com/Kenshin-Keeps/Egg_Quality_Prediction_ELMF4EggQ, promoting
transparency, reproducibility, and further research in this domain.

</details>


### [27] [One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework](https://arxiv.org/abs/2510.02898)
*Lorenzo Bianchi,Giacomo Pacini,Fabio Carrara,Nicola Messina,Giuseppe Amato,Fabrizio Falchi*

Main category: cs.CV

TL;DR: 提出了一个统一的零样本图像描述框架，从图像中心转向补丁中心范式，能够描述任意图像区域而无需区域级监督。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本描述模型局限于全局表示和整图描述，无法灵活描述图像中的特定区域。

Method: 将单个图像补丁作为原子描述单元，通过聚合补丁特征来描述任意区域（从单个补丁到非连续区域和整图）。

Result: 使用DINO等生成有意义的密集视觉特征的主干网络，在多个基于区域的描述任务中达到最先进性能。

Conclusion: 补丁级语义表示对于可扩展的描述生成非常有效，在密集描述、区域集描述和轨迹描述任务中表现优异。

Abstract: Zero-shot captioners are recently proposed models that utilize common-space
vision-language representations to caption images without relying on paired
image-text data. To caption an image, they proceed by textually decoding a
text-aligned image feature, but they limit their scope to global
representations and whole-image captions. We present \frameworkName{}, a
unified framework for zero-shot captioning that shifts from an image-centric to
a patch-centric paradigm, enabling the captioning of arbitrary regions without
the need of region-level supervision. Instead of relying on global image
representations, we treat individual patches as atomic captioning units and
aggregate them to describe arbitrary regions, from single patches to
non-contiguous areas and entire images. We analyze the key ingredients that
enable current latent captioners to work in our novel proposed framework.
Experiments demonstrate that backbones producing meaningful, dense visual
features, such as DINO, are key to achieving state-of-the-art performance in
multiple region-based captioning tasks. Compared to other baselines and
state-of-the-art competitors, our models achieve better performance on
zero-shot dense, region-set, and a newly introduced trace captioning task,
highlighting the effectiveness of patch-wise semantic representations for
scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .

</details>


### [28] [Training-Free Out-Of-Distribution Segmentation With Foundation Models](https://arxiv.org/abs/2510.02909)
*Laith Nayal,Hadi Salloum,Ahmad Taha,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.CV

TL;DR: 提出了一种无需训练的OoD检测方法，利用InternImage骨干网络特征和K-Means聚类结合置信度阈值，在RoadAnomaly和ADE-OoD基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在语义分割中检测未知对象的能力，特别是在没有异常监督的情况下区分分布内和分布外区域。

Method: 使用InternImage骨干网络特征，应用K-Means聚类和原始解码器logits的置信度阈值来识别OoD聚类，无需训练。

Result: 在RoadAnomaly基准测试上达到50.02平均精度，在ADE-OoD基准测试上达到48.77平均精度，超越了多个监督和无监督基线方法。

Conclusion: 该方法为通用OoD分割方法提供了一个有前景的方向，需要最少的假设或额外数据。

Abstract: Detecting unknown objects in semantic segmentation is crucial for
safety-critical applications such as autonomous driving. Large vision
foundation models, includ- ing DINOv2, InternImage, and CLIP, have advanced
visual representation learn- ing by providing rich features that generalize
well across diverse tasks. While their strength in closed-set semantic tasks is
established, their capability to detect out- of-distribution (OoD) regions in
semantic segmentation remains underexplored. In this work, we investigate
whether foundation models fine-tuned on segmen- tation datasets can inherently
distinguish in-distribution (ID) from OoD regions without any outlier
supervision. We propose a simple, training-free approach that utilizes features
from the InternImage backbone and applies K-Means clustering alongside
confidence thresholding on raw decoder logits to identify OoD clusters. Our
method achieves 50.02 Average Precision on the RoadAnomaly benchmark and 48.77
on the benchmark of ADE-OoD with InternImage-L, surpassing several supervised
and unsupervised baselines. These results suggest a promising direc- tion for
generic OoD segmentation methods that require minimal assumptions or additional
data.

</details>


### [29] [Don't Just Chase "Highlighted Tokens" in MLLMs: Revisiting Visual Holistic Context Retention](https://arxiv.org/abs/2510.02912)
*Xin Zou,Di Lu,Yizhou Wang,Yibo Yan,Yuanhuiyi Lyu,Xu Zheng,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: HoloV是一个简单有效的视觉token剪枝框架，通过从全局视角重新思考token保留策略，在高效推理中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型依赖大量视觉token导致计算开销巨大，现有基于注意力的剪枝方法在高剪枝率下性能下降严重。

Method: 提出HoloV框架，通过自适应地在不同空间裁剪中分配剪枝预算，确保保留的token捕获全局视觉上下文而非孤立显著特征。

Result: HoloV在各种任务、MLLM架构和剪枝率下均优于SOTA方法，例如LLaVA1.5在剪枝88.9%视觉token后仍保持95.8%原始性能。

Conclusion: HoloV通过整体视角的token保留策略，最小化表征崩溃并保持任务相关信息，实现了优越的效率-准确性权衡。

Abstract: Despite their powerful capabilities, Multimodal Large Language Models (MLLMs)
suffer from considerable computational overhead due to their reliance on
massive visual tokens. Recent studies have explored token pruning to alleviate
this problem, which typically uses text-vision cross-attention or
[\texttt{CLS}] attention to assess and discard redundant visual tokens. In this
work, we identify a critical limitation of such attention-first pruning
approaches, i.e., they tend to preserve semantically similar tokens, resulting
in pronounced performance drops under high pruning ratios. To this end, we
propose {HoloV}, a simple yet effective, plug-and-play visual token pruning
framework for efficient inference. Distinct from previous attention-first
schemes, HoloV rethinks token retention from a holistic perspective. By
adaptively distributing the pruning budget across different spatial crops,
HoloV ensures that the retained tokens capture the global visual context rather
than isolated salient features. This strategy minimizes representational
collapse and maintains task-relevant information even under aggressive pruning.
Experimental results demonstrate that our HoloV achieves superior performance
across various tasks, MLLM architectures, and pruning ratios compared to SOTA
methods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\% of the
original performance after pruning 88.9\% of visual tokens, achieving superior
efficiency-accuracy trade-offs.

</details>


### [30] [Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting](https://arxiv.org/abs/2510.02913)
*Nikoo Naghavian,Mostafa Tavassolipour*

Main category: cs.CV

TL;DR: 提出CAW方法增强视觉语言模型的零样本鲁棒性，通过置信度感知损失和特征对齐正则化，在不牺牲泛化能力的情况下提升干净和对抗样本的准确率。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型如CLIP在零样本泛化方面表现出色，但对对抗攻击高度脆弱，需要提升其鲁棒性。

Method: CAW包含两个组件：(1)置信度感知损失，通过缩放干净和对抗预测间的KL散度来优先处理不确定的对抗样本；(2)特征对齐正则化，通过最小化冻结和微调图像编码器在对抗输入上的特征距离来保持语义一致性。

Result: 在TinyImageNet和14个额外数据集上的广泛实验表明，CAW在AutoAttack等强攻击下优于PMG-AFT和TGA-ZSR等最新方法，且内存使用更少。

Conclusion: CAW能有效提升视觉语言模型的零样本鲁棒性，在保持泛化能力的同时改善干净和对抗准确率。

Abstract: Vision-language models like CLIP demonstrate impressive zero-shot
generalization but remain highly vulnerable to adversarial attacks. In this
work, we propose Confidence-Aware Weighting (CAW) to enhance zero-shot
robustness in vision-language models. CAW consists of two components: (1) a
Confidence-Aware loss that prioritizes uncertain adversarial examples by
scaling the KL divergence between clean and adversarial predictions, and (2) a
feature alignment regularization that preserves semantic consistency by
minimizing the distance between frozen and fine-tuned image encoder features on
adversarial inputs. These components work jointly to improve both clean and
robust accuracy without sacrificing generalization. Extensive experiments on
TinyImageNet and 14 additional datasets show that CAW outperforms recent
methods such as PMG-AFT and TGA-ZSR under strong attacks like AutoAttack, while
using less memory.

</details>


### [31] [Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights](https://arxiv.org/abs/2510.02922)
*Daphne Tsolissou,Theofanis Ganitidis,Konstantinos Mitsis,Stergios CHristodoulidis,Maria Vakalopoulou,Konstantina Nikita*

Main category: cs.CV

TL;DR: 该研究评估了大型视觉语言模型在颈动脉粥样硬化疾病风险评估中的应用，通过整合超声成像与临床数据，发现现有模型在风险分类方面表现不佳，但通过领域适应和多模态整合可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 颈动脉粥样硬化疾病的风险评估需要整合多样化的临床和影像信息，且需要透明和可解释的方法，现有方法难以满足这一需求。

Method: 提出一个模拟真实诊断场景的框架，比较多种开源LVLMs，包括通用和医学专用模型，使用零样本实验，并对LLaVa-NeXT-Vicuna进行超声领域的低秩适应(LoRA)微调。

Result: 零样本实验显示多数LVLMs无法准确识别成像模态和解剖结构，风险分类表现差；经过领域适应后，卒中风险分层性能显著提升；整合多模态表格数据进一步提高了特异性和平衡准确率。

Conclusion: LVLMs在超声心血管风险预测中既有潜力也有局限性，多模态整合、模型校准和领域适应对临床转化至关重要。

Abstract: Reliable risk assessment for carotid atheromatous disease remains a major
clinical challenge, as it requires integrating diverse clinical and imaging
information in a manner that is transparent and interpretable to clinicians.
This study investigates the potential of state-of-the-art and recent large
vision-language models (LVLMs) for multimodal carotid plaque assessment by
integrating ultrasound imaging (USI) with structured clinical, demographic,
laboratory, and protein biomarker data. A framework that simulates realistic
diagnostic scenarios through interview-style question sequences is proposed,
comparing a range of open-source LVLMs, including both general-purpose and
medically tuned models. Zero-shot experiments reveal that even if they are very
powerful, not all LVLMs can accurately identify imaging modality and anatomy,
while all of them perform poorly in accurate risk classification. To address
this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using
low-rank adaptation (LoRA), resulting in substantial improvements in stroke
risk stratification. The integration of multimodal tabular data in the form of
text further enhances specificity and balanced accuracy, yielding competitive
performance compared to prior convolutional neural network (CNN) baselines
trained on the same dataset. Our findings highlight both the promise and
limitations of LVLMs in ultrasound-based cardiovascular risk prediction,
underscoring the importance of multimodal integration, model calibration, and
domain adaptation for clinical translation.

</details>


### [32] [Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis](https://arxiv.org/abs/2510.02970)
*Xiaoyan Kui,Qianmu Xiao,Qqinsong Li,Zexin Ji,JIelin Zhang,Beiji Zou*

Main category: cs.CV

TL;DR: 提出FDA-VAE模型，一种轻量级特征解耦的变分自编码器，用于多期相增强MRI合成，通过对称潜在分布分离共享和独立特征，显著减少参数和推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有多期相增强MRI合成方法使用深度自编码器生成器参数效率低，缺乏可解释的训练策略，需要改进特征分离能力。

Method: 使用Flip Distribution Alignment变分自编码器，将输入和目标图像编码为关于标准正态分布对称的两个潜在分布，通过Y形双向训练策略增强特征分离的可解释性。

Result: 相比现有基于深度自编码器的端到端合成方法，FDA-VAE显著减少模型参数和推理时间，同时有效提高合成质量。

Conclusion: FDA-VAE为多期相增强MRI合成提供了一种轻量级且可解释的特征解耦解决方案，在参数效率和合成质量方面均有显著改进。

Abstract: Separating shared and independent features is crucial for multi-phase
contrast-enhanced (CE) MRI synthesis. However, existing methods use deep
autoencoder generators with low parameter efficiency and lack interpretable
training strategies. In this paper, we propose Flip Distribution Alignment
Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model
for multi-phase CE MRI synthesis. Our method encodes input and target images
into two latent distributions that are symmetric concerning a standard normal
distribution, effectively separating shared and independent features. The
Y-shaped bidirectional training strategy further enhances the interpretability
of feature separation. Experimental results show that compared to existing deep
autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces
model parameters and inference time while effectively improving synthesis
quality. The source code is publicly available at
https://github.com/QianMuXiao/FDA-VAE.

</details>


### [33] [TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency](https://arxiv.org/abs/2510.02987)
*Juntong Wang,Huiyu Duan,Jiarui Wang,Ziheng Jia,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 提出了LPG-Bench基准测试，包含200个平均长度超过250词的长提示，用于评估文本到图像生成模型的长提示理解能力。同时开发了TIT评估指标，通过文本-图像-文本一致性来量化生成图像与长提示的对齐程度。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型在短提示下能生成高质量图像，但在理解和遵循长而详细的提示时表现不佳，生成结果不一致。需要专门的基准测试和评估方法来衡量和改进模型的长提示理解能力。

Method: 1) 构建LPG-Bench基准：200个精心设计的长提示，平均长度超过250词；2) 使用13个最先进模型生成2600张图像并进行人工排名标注；3) 提出TIT评估框架：通过比较原始提示与LMM对生成图像描述的文本一致性来评估对齐度，包括TIT-Score和TIT-Score-LLM两种实现方式。

Result: 实验表明现有T2I对齐评估指标与人类偏好一致性较差。TIT框架在人类判断对齐方面表现优异，TIT-Score-LLM在成对准确率上比最强基线提高了7.31%。

Conclusion: LPG-Bench和TIT方法共同为T2I模型的基准测试和发展提供了更深入的视角，有助于推动长提示文本到图像生成技术的发展。

Abstract: With the rapid advancement of large multimodal models (LMMs), recent
text-to-image (T2I) models can generate high-quality images and demonstrate
great alignment to short prompts. However, they still struggle to effectively
understand and follow long and detailed prompts, displaying inconsistent
generation. To address this challenge, we introduce LPG-Bench, a comprehensive
benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench
features 200 meticulously crafted prompts with an average length of over 250
words, approaching the input capacity of several leading commercial models.
Using these prompts, we generate 2,600 images from 13 state-of-the-art models
and further perform comprehensive human-ranked annotations. Based on LPG-Bench,
we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor
consistency with human preferences on long-prompt-based image generation. To
address the gap, we introduce a novel zero-shot metric based on
text-to-image-to-text consistency, termed TIT, for evaluating
long-prompt-generated images. The core concept of TIT is to quantify T2I
alignment by directly comparing the consistency between the raw prompt and the
LMM-produced description on the generated image, which includes an efficient
score-based instantiation TIT-Score and a large-language-model (LLM) based
instantiation TIT-Score-LLM. Extensive experiments demonstrate that our
framework achieves superior alignment with human judgment compared to
CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute
improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT
methods together offer a deeper perspective to benchmark and foster the
development of T2I models. All resources will be made publicly available.

</details>


### [34] [Towards Scalable and Consistent 3D Editing](https://arxiv.org/abs/2510.02994)
*Ruihao Xia,Yang Tang,Pan Zhou*

Main category: cs.CV

TL;DR: 提出了3DEditVerse数据集和3DEditFormer模型，通过大规模配对数据和结构保持的Transformer架构，实现了无需3D掩码的高质量3D编辑。


<details>
  <summary>Details</summary>
Motivation: 解决3D编辑中跨视图一致性、结构保真度和细粒度控制的挑战，克服现有方法速度慢、几何失真和依赖手动3D掩码的问题。

Method: 构建3DEditVerse数据集（116,309训练对和1,500测试对），并提出3DEditFormer模型，使用双引导注意力和时间自适应门控来分离可编辑区域和保留结构。

Result: 在定量和定性评估中均优于现有最先进方法，实现了精确且一致的3D编辑，无需辅助3D掩码。

Conclusion: 为实用和可扩展的3D编辑设定了新标准，数据集和代码将公开发布。

Abstract: 3D editing - the task of locally modifying the geometry or appearance of a 3D
asset - has wide applications in immersive content creation, digital
entertainment, and AR/VR. However, unlike 2D editing, it remains challenging
due to the need for cross-view consistency, structural fidelity, and
fine-grained controllability. Existing approaches are often slow, prone to
geometric distortions, or dependent on manual and accurate 3D masks that are
error-prone and impractical. To address these challenges, we advance both the
data and model fronts. On the data side, we introduce 3DEditVerse, the largest
paired 3D editing benchmark to date, comprising 116,309 high-quality training
pairs and 1,500 curated test pairs. Built through complementary pipelines of
pose-driven geometric edits and foundation model-guided appearance edits,
3DEditVerse ensures edit locality, multi-view consistency, and semantic
alignment. On the model side, we propose 3DEditFormer, a
3D-structure-preserving conditional transformer. By enhancing image-to-3D
generation with dual-guidance attention and time-adaptive gating, 3DEditFormer
disentangles editable regions from preserved structure, enabling precise and
consistent edits without requiring auxiliary 3D masks. Extensive experiments
demonstrate that our framework outperforms state-of-the-art baselines both
quantitatively and qualitatively, establishing a new standard for practical and
scalable 3D editing. Dataset and code will be released. Project:
https://www.lv-lab.org/3DEditFormer/

</details>


### [35] [Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources](https://arxiv.org/abs/2510.03006)
*Sara Mobsite,Renaud Hostache,Laure Berti Equille,Emmanuel Roux,Joris Guerin*

Main category: cs.CV

TL;DR: 该论文提出了一种云注入算法来模拟真实云覆盖，并开发了轻量级方法将归一化差异指数注入解码层，以提升云覆盖条件下Sentinel-1和Sentinel-2数据融合的土地覆盖语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有Sentinel-2数据集多为无云图像，在热带多云地区实用性有限，需要解决云遮挡问题并改善深度网络中下采样导致的空间和光谱细节丢失问题。

Method: 开发云注入算法模拟真实云覆盖；提出轻量级方法将归一化差异指数注入最终解码层，以保留关键空间特征；结合Sentinel-1雷达数据填补光学图像云遮挡空隙。

Result: 在DFC2020数据集上，NDI注入使U-Net性能提升1.99%，DeepLabV3提升2.78%；在云覆盖条件下，融合Sentinel-1数据相比仅使用光学数据带来显著性能提升。

Conclusion: 雷达-光学数据融合在挑战性大气场景中具有显著效果，提出的方法能有效提升多云条件下的土地覆盖分割性能。

Abstract: Supervised deep learning for land cover semantic segmentation (LCS) relies on
labeled satellite data. However, most existing Sentinel-2 datasets are
cloud-free, which limits their usefulness in tropical regions where clouds are
common. To properly evaluate the extent of this problem, we developed a cloud
injection algorithm that simulates realistic cloud cover, allowing us to test
how Sentinel-1 radar data can fill in the gaps caused by cloud-obstructed
optical imagery. We also tackle the issue of losing spatial and/or spectral
details during encoder downsampling in deep networks. To mitigate this loss, we
propose a lightweight method that injects Normalized Difference Indices (NDIs)
into the final decoding layers, enabling the model to retain key spatial
features with minimal additional computation. Injecting NDIs enhanced land
cover segmentation performance on the DFC2020 dataset, yielding improvements of
1.99% for U-Net and 2.78% for DeepLabV3 on cloud-free imagery. Under
cloud-covered conditions, incorporating Sentinel-1 data led to significant
performance gains across all models compared to using optical data alone,
highlighting the effectiveness of radar-optical fusion in challenging
atmospheric scenarios.

</details>


### [36] [PocketSR: The Super-Resolution Expert in Your Pocket Mobiles](https://arxiv.org/abs/2510.03012)
*Haoze Sun,Linfeng Jiang,Fan Li,Renjing Pei,Zhixin Wang,Yong Guo,Jiaqi Xu,Haoyu Chen,Jin Han,Fenglong Song,Yujiu Yang,Wenbo Li*

Main category: cs.CV

TL;DR: PocketSR是一个超轻量级的单步图像超分辨率模型，通过高效的LiteED编码器和在线退火剪枝技术，在保持生成质量的同时大幅降低计算成本，适合边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于大生成模型的RealSR方法计算成本高、延迟大，难以在边缘设备上实际部署，需要开发更高效的解决方案。

Method: 设计了LiteED作为VAE的高效替代，参数减少97.5%；提出在线退火剪枝技术，逐步将生成先验从重模块转移到轻模块；使用多层特征蒸馏损失来缓解剪枝过程中的知识损失。

Result: 模型参数量仅146M，处理4K图像仅需0.8秒，性能与最先进的单步和多步RealSR模型相当。

Conclusion: PocketSR在保持高质量的同时实现了显著的效率提升，为边缘设备上的RealSR应用提供了实用解决方案。

Abstract: Real-world image super-resolution (RealSR) aims to enhance the visual quality
of in-the-wild images, such as those captured by mobile phones. While existing
methods leveraging large generative models demonstrate impressive results, the
high computational cost and latency make them impractical for edge deployment.
In this paper, we introduce PocketSR, an ultra-lightweight, single-step model
that brings generative modeling capabilities to RealSR while maintaining high
fidelity. To achieve this, we design LiteED, a highly efficient alternative to
the original computationally intensive VAE in SD, reducing parameters by 97.5%
while preserving high-quality encoding and decoding. Additionally, we propose
online annealing pruning for the U-Net, which progressively shifts generative
priors from heavy modules to lightweight counterparts, ensuring effective
knowledge transfer and further optimizing efficiency. To mitigate the loss of
prior knowledge during pruning, we incorporate a multi-layer feature
distillation loss. Through an in-depth analysis of each design component, we
provide valuable insights for future research. PocketSR, with a model size of
146M parameters, processes 4K images in just 0.8 seconds, achieving a
remarkable speedup over previous methods. Notably, it delivers performance on
par with state-of-the-art single-step and even multi-step RealSR models, making
it a highly practical solution for edge-device applications.

</details>


### [37] [When and Where do Events Switch in Multi-Event Video Generation?](https://arxiv.org/abs/2510.03049)
*Ruotong Liao,Guowen Huang,Qing Cheng,Thomas Seidl,Daniel Cremers,Volker Tresp*

Main category: cs.CV

TL;DR: 该论文提出了MEve评估套件，系统研究了多事件文本到视频生成中事件转换的控制时机和位置，发现早期去噪步骤和块级模型层对多事件视频生成至关重要。


<details>
  <summary>Details</summary>
Motivation: 解决多事件文本到视频生成中的时序一致性和内容控制问题，探究多事件提示在何时何地控制事件转换的内在因素。

Method: 引入MEve自建提示套件，对OpenSora和CogVideoX两个代表性模型家族进行系统研究，通过广泛实验分析去噪步骤和模型层的影响。

Result: 实验表明早期干预去噪步骤和块级模型层对多事件视频生成至关重要，揭示了多事件条件化的关键因素。

Conclusion: 研究为未来模型的多事件条件化提供了可能性，强调了在生成过程中早期阶段进行干预的重要性。

Abstract: Text-to-video (T2V) generation has surged in response to challenging
questions, especially when a long video must depict multiple sequential events
with temporal coherence and controllable content. Existing methods that extend
to multi-event generation omit an inspection of the intrinsic factor in event
shifting. The paper aims to answer the central question: When and where
multi-event prompts control event transition during T2V generation. This work
introduces MEve, a self-curated prompt suite for evaluating multi-event
text-to-video (T2V) generation, and conducts a systematic study of two
representative model families, i.e., OpenSora and CogVideoX. Extensive
experiments demonstrate the importance of early intervention in denoising steps
and block-wise model layers, revealing the essential factor for multi-event
video generation and highlighting the possibilities for multi-event
conditioning in future models.

</details>


### [38] [InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition](https://arxiv.org/abs/2510.03066)
*Ahsan Farabi,Israt Khandaker,Ibrahim Khalil Shanto,Md Abdul Ahad Minhaz,Tanisha Zaman*

Main category: cs.CV

TL;DR: InsideOut是一个基于EfficientNetV2-S的可复现面部表情识别框架，通过迁移学习、数据增强和类别不平衡优化，在FER2013数据集上达到62.8%准确率和0.590宏平均F1分数。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在情感计算中至关重要，但由于遮挡、光照变化、姿势变化、类内细微差异以及数据集不平衡等问题，特别是对少数情绪类别的识别仍然具有挑战性。

Method: 基于EfficientNetV2-S构建框架，采用迁移学习、强数据增强和分层分割，使用类别加权损失函数处理不平衡分布，微调轻量级分类头。

Result: 在FER2013数据集上达到62.8%准确率和0.590宏平均F1分数，与传统CNN基线相比具有竞争力。

Conclusion: 研究表明，高效架构结合定制化的不平衡处理策略，能够提供实用、透明且可复现的面部表情识别解决方案。

Abstract: Facial Emotion Recognition (FER) is a key task in affective computing,
enabling applications in human-computer interaction, e-learning, healthcare,
and safety systems. Despite advances in deep learning, FER remains challenging
due to occlusions, illumination and pose variations, subtle intra-class
differences, and dataset imbalance that hinders recognition of minority
emotions. We present InsideOut, a reproducible FER framework built on
EfficientNetV2-S with transfer learning, strong data augmentation, and
imbalance-aware optimization. The approach standardizes FER2013 images, applies
stratified splitting and augmentation, and fine-tunes a lightweight
classification head with class-weighted loss to address skewed distributions.
InsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,
showing competitive results compared to conventional CNN baselines. The novelty
lies in demonstrating that efficient architectures, combined with tailored
imbalance handling, can provide practical, transparent, and reproducible FER
solutions.

</details>


### [39] [What Drives Compositional Generalization in Visual Generative Models?](https://arxiv.org/abs/2510.03075)
*Karim Farid,Rajat Sahay,Yumna Ali Alnaggar,Simon Schrodi,Volker Fischer,Cordelia Schmid,Thomas Brox*

Main category: cs.CV

TL;DR: 该论文系统研究了影响视觉生成模型组合泛化能力的设计选择，发现训练目标的离散/连续分布特性以及条件信息提供程度是关键因素，并提出通过添加连续JEPA目标来改进离散模型MaskGIT的组合性能。


<details>
  <summary>Details</summary>
Motivation: 组合泛化是视觉生成模型的关键能力，但目前对其促进或抑制机制的理解尚不充分。本研究旨在系统探索不同设计选择如何影响图像和视频生成中的组合泛化能力。

Method: 通过受控实验研究各种设计选择的影响，重点关注训练目标的离散/连续分布特性以及条件信息提供程度。基于这些发现，提出在MaskGIT离散损失基础上添加辅助的连续JEPA目标。

Result: 识别出两个关键因素：(1)训练目标操作于离散还是连续分布；(2)训练期间条件信息对组成概念的提供程度。实验表明，通过辅助连续JEPA目标可以改进离散模型MaskGIT的组合性能。

Conclusion: 训练目标的分布特性和条件信息的提供程度是影响组合泛化能力的关键因素，通过结合离散和连续目标可以提升模型的组合泛化性能。

Abstract: Compositional generalization, the ability to generate novel combinations of
known concepts, is a key ingredient for visual generative models. Yet, not all
mechanisms that enable or inhibit it are fully understood. In this work, we
conduct a systematic study of how various design choices influence
compositional generalization in image and video generation in a positive or
negative way. Through controlled experiments, we identify two key factors: (i)
whether the training objective operates on a discrete or continuous
distribution, and (ii) to what extent conditioning provides information about
the constituent concepts during training. Building on these insights, we show
that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based
objective can improve compositional performance in discrete models like
MaskGIT.

</details>


### [40] [Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations](https://arxiv.org/abs/2510.03089)
*Naresh Kumar Devulapally,Shruti Agarwal,Tejas Gokhale,Vishnu Suresh Lokhande*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型潜在空间的图像扰动方法，通过改变去噪轨迹的起点来生成不可学习的训练样本，既保持视觉保真度又防止下游生成模型的未经授权使用。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型在个性化应用中引发的数据隐私、知识产权保护和未经授权使用问题，现有基于像素空间的图像污染方法存在噪声和伪影问题。

Method: 在扩散模型的潜在空间中操作，交替进行去噪和反转过程，修改去噪轨迹的起点，实现轨迹偏移采样。

Result: 在四个基准数据集上验证，相比现有方法在感知指标（PSNR、SSIM、FID）上提升约8%-10%，在五种对抗设置下平均鲁棒性提升约10%。

Conclusion: 该方法将不可学习性集成到潜在扩散模型框架中，提供了一种实用且难以察觉的防御机制，有效保护敏感数据免受未经授权的模型适应。

Abstract: Text-to-image diffusion models have demonstrated remarkable effectiveness in
rapid and high-fidelity personalization, even when provided with only a few
user images. However, the effectiveness of personalization techniques has lead
to concerns regarding data privacy, intellectual property protection, and
unauthorized usage. To mitigate such unauthorized usage and model replication,
the idea of generating ``unlearnable'' training samples utilizing image
poisoning techniques has emerged. Existing methods for this have limited
imperceptibility as they operate in the pixel space which results in images
with noise and artifacts. In this work, we propose a novel model-based
perturbation strategy that operates within the latent space of diffusion
models. Our method alternates between denoising and inversion while modifying
the starting point of the denoising trajectory: of diffusion models. This
trajectory-shifted sampling ensures that the perturbed images maintain high
visual fidelity to the original inputs while being resistant to inversion and
personalization by downstream generative models. This approach integrates
unlearnability into the framework of Latent Diffusion Models (LDMs), enabling a
practical and imperceptible defense against unauthorized model adaptation. We
validate our approach on four benchmark datasets to demonstrate robustness
against state-of-the-art inversion attacks. Results demonstrate that our method
achieves significant improvements in imperceptibility ($\sim 8 \% -10\%$ on
perceptual metrics including PSNR, SSIM, and FID) and robustness ( $\sim 10\%$
on average across five adversarial settings), highlighting its effectiveness in
safeguarding sensitive data.

</details>


### [41] [Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields](https://arxiv.org/abs/2510.03104)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 该论文研究了在辐射场语义蒸馏中几何基础特征与纯视觉特征的对比，发现几何基础特征包含更多几何细节但降低了姿态估计精度，而纯视觉特征在更广泛的下游任务中更具优势。


<details>
  <summary>Details</summary>
Motivation: 探索几何基础语义特征在蒸馏辐射场中的潜在优势，特别是在空间任务如姿态估计中的表现，以填补现有研究在几何基础语义特征有效性方面的空白。

Method: 提出SPINE框架进行无初始猜测的辐射场反演，包含基于蒸馏语义的粗反演和基于光度优化的细反演两个核心组件，并对比几何基础特征与纯视觉特征在多个任务中的表现。

Result: 几何基础特征包含更精细的结构细节，但在语义目标定位任务中无显著差异，在姿态估计任务中精度反而下降；纯视觉特征在更广泛的下游任务中表现更优。

Conclusion: 纯视觉特征在通用性方面更具优势，而几何基础特征虽包含更多几何细节但降低了任务性能，未来需要研究更有效的几何基础策略来增强预训练语义特征的多功能性和性能。

Abstract: Semantic distillation in radiance fields has spurred significant advances in
open-vocabulary robot policies, e.g., in manipulation and navigation, founded
on pretrained semantics from large vision models. While prior work has
demonstrated the effectiveness of visual-only semantic features (e.g., DINO and
CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit
of geometry-grounding in distilled fields remains an open question. In
principle, visual-geometry features seem very promising for spatial tasks such
as pose estimation, prompting the question: Do geometry-grounded semantic
features offer an edge in distilled fields? Specifically, we ask three critical
questions: First, does spatial-grounding produce higher-fidelity geometry-aware
semantic features? We find that image features from geometry-grounded backbones
contain finer structural details compared to their counterparts. Secondly, does
geometry-grounding improve semantic object localization? We observe no
significant difference in this task. Thirdly, does geometry-grounding enable
higher-accuracy radiance field inversion? Given the limitations of prior work
and their lack of semantics integration, we propose a novel framework SPINE for
inverting radiance fields without an initial guess, consisting of two core
components: coarse inversion using distilled semantics, and fine inversion
using photometric-based optimization. Surprisingly, we find that the pose
estimation accuracy decreases with geometry-grounded features. Our results
suggest that visual-only features offer greater versatility for a broader range
of downstream tasks, although geometry-grounded features contain more geometric
detail. Notably, our findings underscore the necessity of future research on
effective strategies for geometry-grounding that augment the versatility and
performance of pretrained semantic features.

</details>


### [42] [GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion](https://arxiv.org/abs/2510.03110)
*Beibei Lin,Tingting Chen,Robby T. Tan*

Main category: cs.CV

TL;DR: GeoComplete是一个基于3D结构引导的参考驱动图像补全框架，通过双分支扩散架构和几何感知机制，在目标视图与参考图像差异较大时仍能保持几何一致性。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法仅依赖扩散先验，缺乏几何线索（如相机位姿或深度），导致补全内容错位或不合理。当目标视图与参考图像差异显著时，这个问题尤为突出。

Method: 提出双分支扩散架构：一个分支从掩码目标合成缺失区域，另一个分支从投影点云提取几何特征；采用联合自注意力确保一致性；引入目标感知掩码策略，在训练时掩蔽参考图像中的遮挡区域。

Result: 实验显示GeoComplete相比最先进方法PSNR提升17.1，显著提高了几何准确性，同时保持高视觉质量。

Conclusion: 通过整合几何感知的双分支扩散架构和目标感知掩码策略，GeoComplete为几何条件图像补全提供了统一且鲁棒的解决方案。

Abstract: Reference-driven image completion, which restores missing regions in a target
view using additional images, is particularly challenging when the target view
differs significantly from the references. Existing generative methods rely
solely on diffusion priors and, without geometric cues such as camera pose or
depth, often produce misaligned or implausible content. We propose GeoComplete,
a novel framework that incorporates explicit 3D structural guidance to enforce
geometric consistency in the completed regions, setting it apart from prior
image-only approaches. GeoComplete introduces two key ideas: conditioning the
diffusion process on projected point clouds to infuse geometric information,
and applying target-aware masking to guide the model toward relevant reference
cues. The framework features a dual-branch diffusion architecture. One branch
synthesizes the missing regions from the masked target, while the other
extracts geometric features from the projected point cloud. Joint
self-attention across branches ensures coherent and accurate completion. To
address regions visible in references but absent in the target, we project the
target view into each reference to detect occluded areas, which are then masked
during training. This target-aware masking directs the model to focus on useful
cues, enhancing performance in difficult scenarios. By integrating a
geometry-aware dual-branch diffusion architecture with a target-aware masking
strategy, GeoComplete offers a unified and robust solution for
geometry-conditioned image completion. Experiments show that GeoComplete
achieves a 17.1 PSNR improvement over state-of-the-art methods, significantly
boosting geometric accuracy while maintaining high visual quality.

</details>


### [43] [Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction](https://arxiv.org/abs/2510.03117)
*Kaisi Guan,Xihua Wang,Zhengfeng Lai,Xin Cheng,Peng Zhang,XiaoJiang Liu,Ruihua Song,Meng Cao*

Main category: cs.CV

TL;DR: 提出了一种新的文本到音视频生成方法，通过分层视觉引导字幕框架生成解耦的视频和音频字幕，并采用双塔扩散变换器实现跨模态特征交互，解决了模态干扰和同步问题。


<details>
  <summary>Details</summary>
Motivation: 解决文本到音视频生成中的两个关键挑战：(1)共享文本字幕导致的模态干扰问题，(2)跨模态特征交互机制不明确的问题。

Method: 提出分层视觉引导字幕框架生成解耦的视频和音频字幕，并设计BridgeDiT双塔扩散变换器，采用双交叉注意力机制实现双向信息交换。

Result: 在三个基准数据集上的实验表明，该方法在大多数指标上达到了最先进水平，人类评估也证实了其有效性。

Conclusion: 该方法有效解决了文本到音视频生成中的模态干扰和同步问题，为未来研究提供了重要见解。

Abstract: This study focuses on a challenging yet promising task,
Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with
synchronized audio from text conditions, meanwhile ensuring both modalities are
aligned with text. Despite progress in joint audio-video training, two critical
challenges still remain unaddressed: (1) a single, shared text caption where
the text for video is equal to the text for audio often creates modal
interference, confusing the pretrained backbones, and (2) the optimal mechanism
for cross-modal feature interaction remains unclear. To address these
challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)
framework that generates pairs of disentangled captions, a video caption, and
an audio caption, eliminating interference at the conditioning stage. Based on
HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,
which employs a Dual CrossAttention (DCA) mechanism that acts as a robust
``bridge" to enable a symmetric, bidirectional exchange of information,
achieving both semantic and temporal synchronization. Extensive experiments on
three benchmark datasets, supported by human evaluations, demonstrate that our
method achieves state-of-the-art results on most metrics. Comprehensive
ablation studies further validate the effectiveness of our contributions,
offering key insights for the future T2SV task. All the codes and checkpoints
will be publicly released.

</details>


### [44] [HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion](https://arxiv.org/abs/2510.03122)
*Shiyi Zhang,Dong Liang,Hairong Zheng,Yihang Zhou*

Main category: cs.CV

TL;DR: HAVIR模型通过分离视觉皮层的两个层次区域提取不同特征，结合结构生成器和语义提取器，使用Versatile Diffusion模型合成图像，在复杂场景中提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在恢复复杂视觉刺激时面临挑战，因为自然场景的低级特征具有异质性，高级特征由于上下文重叠而存在语义纠缠。

Method: 将视觉皮层分为两个层次区域：结构生成器从空间处理体素提取结构信息并转换为潜在扩散先验，语义提取器将语义处理体素转换为CLIP嵌入，通过Versatile Diffusion模型集成合成最终图像。

Result: 实验结果表明HAVIR在复杂场景中提升了重建的结构和语义质量，优于现有模型。

Conclusion: 基于视觉皮层层次表示理论的HAVIR模型能有效解决复杂视觉刺激的重建问题，在结构和语义质量上都取得了显著提升。

Abstract: The reconstruction of visual information from brain activity fosters
interdisciplinary integration between neuroscience and computer vision.
However, existing methods still face challenges in accurately recovering highly
complex visual stimuli. This difficulty stems from the characteristics of
natural scenes: low-level features exhibit heterogeneity, while high-level
features show semantic entanglement due to contextual overlaps. Inspired by the
hierarchical representation theory of the visual cortex, we propose the HAVIR
model, which separates the visual cortex into two hierarchical regions and
extracts distinct features from each. Specifically, the Structural Generator
extracts structural information from spatial processing voxels and converts it
into latent diffusion priors, while the Semantic Extractor converts semantic
processing voxels into CLIP embeddings. These components are integrated via the
Versatile Diffusion model to synthesize the final image. Experimental results
demonstrate that HAVIR enhances both the structural and semantic quality of
reconstructions, even in complex scenes, and outperforms existing models.

</details>


### [45] [Mask2IV: Interaction-Centric Video Generation via Mask Trajectories](https://arxiv.org/abs/2510.03135)
*Gen Li,Bo Zhao,Jianfei Yang,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: Mask2IV是一个用于生成交互中心视频的两阶段框架，无需密集掩码输入，通过预测动作和物体的运动轨迹来生成高质量交互视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以建模复杂的动态交互，而获取密集精确的掩码标注在现实应用中具有挑战性。

Method: 采用解耦的两阶段流程：首先预测动作和物体的合理运动轨迹，然后基于这些轨迹生成视频。

Result: 在人类-物体交互和机器人操作场景的基准测试中，该方法在视觉真实性和可控性方面优于现有基线方法。

Conclusion: Mask2IV框架在无需密集掩码输入的情况下，实现了高质量的交互中心视频生成，并支持直观的控制方式。

Abstract: Generating interaction-centric videos, such as those depicting humans or
robots interacting with objects, is crucial for embodied intelligence, as they
provide rich and diverse visual priors for robot learning, manipulation policy
training, and affordance reasoning. However, existing methods often struggle to
model such complex and dynamic interactions. While recent studies show that
masks can serve as effective control signals and enhance generation quality,
obtaining dense and precise mask annotations remains a major challenge for
real-world use. To overcome this limitation, we introduce Mask2IV, a novel
framework specifically designed for interaction-centric video generation. It
adopts a decoupled two-stage pipeline that first predicts plausible motion
trajectories for both actor and object, then generates a video conditioned on
these trajectories. This design eliminates the need for dense mask inputs from
users while preserving the flexibility to manipulate the interaction process.
Furthermore, Mask2IV supports versatile and intuitive control, allowing users
to specify the target object of interaction and guide the motion trajectory
through action descriptions or spatial position cues. To support systematic
training and evaluation, we curate two benchmarks covering diverse action and
object categories across both human-object interaction and robotic manipulation
scenarios. Extensive experiments demonstrate that our method achieves superior
visual realism and controllability compared to existing baselines.

</details>


### [46] [ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories](https://arxiv.org/abs/2510.03152)
*Anantajit Subrahmanya,Chandrakanth Gudavalli,Connor Levenson,Umang Garg,B. S. Manjunath*

Main category: cs.CV

TL;DR: 提出Markovian Reeb Graphs框架，用于模拟保留基线数据中生活模式的时空轨迹，在保持数据效率和计算效率的同时实现高保真度。


<details>
  <summary>Details</summary>
Motivation: 准确建模人类移动性对城市规划、流行病学和交通管理至关重要，需要生成既保持生活模式一致性又包含变异性的真实轨迹。

Method: 结合个体和群体层面的移动结构，在概率拓扑模型中构建Markovian Reeb Graphs框架来模拟时空轨迹。

Result: 在Urban Anomalies数据集（亚特兰大和柏林子集）上评估，使用Jensen-Shannon Divergence在群体和个体层面指标上显示强保真度。

Conclusion: Markovian Reeb Graphs是一个可扩展的轨迹模拟框架，适用于多种城市环境。

Abstract: Accurately modeling human mobility is critical for urban planning,
epidemiology, and traffic management. In this work, we introduce Markovian Reeb
Graphs, a novel framework for simulating spatiotemporal trajectories that
preserve Patterns of Life (PoLs) learned from baseline data. By combining
individual- and population-level mobility structures within a probabilistic
topological model, our approach generates realistic future trajectories that
capture both consistency and variability in daily life. Evaluations on the
Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon
Divergence (JSD) across population- and agent-level metrics demonstrate that
the proposed method achieves strong fidelity while remaining data- and
compute-efficient. These results position Markovian Reeb Graphs as a scalable
framework for trajectory simulation with broad applicability across diverse
urban environments.

</details>


### [47] [SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus](https://arxiv.org/abs/2510.03160)
*Ming Zhao,Wenhui Dong,Yang Zhang,Xiang Zheng,Zhonghao Zhang,Zian Zhou,Yunzhi Guan,Liukun Xu,Wei Peng,Zhaoyang Gong,Zhicheng Zhang,Dachuan Li,Xiaosheng Ma,Yuli Ma,Jianing Ni,Changjiang Jiang,Lixia Tian,Qixin Chen,Kaishun Xia,Pingping Liu,Tongshun Zhang,Zhiqiang Liu,Zhongan Bi,Chenyang Si,Tiansheng Sun,Caifeng Shan*

Main category: cs.CV

TL;DR: SpineMed是一个脊柱医学AI诊断生态系统，包含SpineMed-450k大规模数据集和SpineBench评估框架，解决了脊柱疾病诊断中缺乏模态感知、椎体级别推理数据的问题。


<details>
  <summary>Details</summary>
Motivation: 脊柱疾病影响全球6.19亿人，是主要致残原因，但AI辅助诊断因缺乏椎体级别感知的多模态数据集而受限。临床决策需要跨X光、CT和MRI在特定椎体级别进行复杂推理。

Method: 与执业脊柱外科医生共同设计，采用两阶段LLM生成方法（草稿和修订）构建SpineMed-450k数据集，包含45万条指令实例，来自教科书、指南、开放数据集和约1000个医院病例。

Result: 在SpineBench评估中，现有大型视觉语言模型在细粒度、椎体级别推理方面存在系统性弱点，而基于SpineMed-450k微调的模型在所有任务上均表现出显著改进。

Conclusion: 临床医生评估确认了模型输出的诊断清晰度和实用性，SpineMed为脊柱疾病AI诊断提供了高质量、可追溯的数据和标准化评估框架。

Abstract: Spine disorders affect 619 million people globally and are a leading cause of
disability, yet AI-assisted diagnosis remains limited by the lack of
level-aware, multimodal datasets. Clinical decision-making for spine disorders
requires sophisticated reasoning across X-ray, CT, and MRI at specific
vertebral levels. However, progress has been constrained by the absence of
traceable, clinically-grounded instruction data and standardized,
spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem
co-designed with practicing spine surgeons. It features SpineMed-450k, the
first large-scale dataset explicitly designed for vertebral-level reasoning
across imaging modalities with over 450,000 instruction instances, and
SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is
curated from diverse sources, including textbooks, guidelines, open datasets,
and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline
with a two-stage LLM generation method (draft and revision) to ensure
high-quality, traceable data for question-answering, multi-turn consultations,
and report generation. SpineBench evaluates models on clinically salient axes,
including level identification, pathology assessment, and surgical planning.
Our comprehensive evaluation of several recently advanced large vision-language
models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,
level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k
demonstrates consistent and significant improvements across all tasks.
Clinician assessments confirm the diagnostic clarity and practical utility of
our model's outputs.

</details>


### [48] [UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization](https://arxiv.org/abs/2510.03161)
*Qing Huang,Zhipei Xu,Xuanyu Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: 提出了UniShield系统，这是一个基于多智能体的统一伪造图像检测和定位框架，能够跨多个领域检测和定位图像伪造，包括图像篡改、文档篡改、DeepFake和AI生成图像。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的快速发展，合成图像变得越来越逼真，带来了严重的社会风险，如虚假信息和欺诈。现有的领域特定检测方法虽然性能出色，但实际应用受限，主要因为其专业化程度过高、跨领域泛化能力差，以及缺乏集成自适应框架。

Method: UniShield创新性地集成了感知智能体和检测智能体。感知智能体智能分析图像特征以动态选择合适的检测模型，检测智能体将各种专家检测器整合到统一框架中并生成可解释的报告。

Result: 大量实验表明，UniShield实现了最先进的结果，超越了现有的统一方法和领域特定检测器，突显了其卓越的实用性、自适应性和可扩展性。

Conclusion: UniShield为解决伪造图像检测和定位的实际挑战提供了一个有效的统一解决方案，具有重要的社会安全意义。

Abstract: With the rapid advancements in image generation, synthetic images have become
increasingly realistic, posing significant societal risks, such as
misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus
emerges as essential for maintaining information integrity and societal
security. Despite impressive performances by existing domain-specific detection
methods, their practical applicability remains limited, primarily due to their
narrow specialization, poor cross-domain generalization, and the absence of an
integrated adaptive framework. To address these issues, we propose UniShield,
the novel multi-agent-based unified system capable of detecting and localizing
image forgeries across diverse domains, including image manipulation, document
manipulation, DeepFake, and AI-generated images. UniShield innovatively
integrates a perception agent with a detection agent. The perception agent
intelligently analyzes image features to dynamically select suitable detection
models, while the detection agent consolidates various expert detectors into a
unified framework and generates interpretable reports. Extensive experiments
show that UniShield achieves state-of-the-art results, surpassing both existing
unified approaches and domain-specific detectors, highlighting its superior
practicality, adaptiveness, and scalability.

</details>


### [49] [ROGR: Relightable 3D Objects using Generative Relighting](https://arxiv.org/abs/2510.03163)
*Jiapeng Tang,Matthew Lavine,Dor Verbin,Stephan J. Garbin,Matthias Nießner,Ricardo Martin Brualla,Pratul P. Srinivasan,Philipp Henzler*

Main category: cs.CV

TL;DR: ROGR提出了一种新颖的可重光照3D重建方法，通过生成式重光照模型从多视角图像中重建物体的可重光照3D模型，能够在任意环境光照下进行高效前向重光照。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法在任意环境光照下进行重光照时需要针对每种光照进行优化或进行复杂光传输模拟的问题，开发一种能够高效处理任意环境光照的重光照方法。

Method: 使用生成式重光照模型采样物体在多种光照环境下的外观，构建训练数据集，训练具有双分支架构的照明条件神经辐射场（NeRF），分别编码一般光照效果和镜面反射。

Result: 在TensoIR和Stanford-ORB数据集上的评估表明，该方法在大多数指标上优于现有最先进方法，并在真实物体捕获上展示了良好效果。

Conclusion: ROGR方法通过照明条件NeRF实现了无需针对每种光照进行优化或光传输模拟的高效前向重光照，在多个数据集上取得了优于现有方法的性能。

Abstract: We introduce ROGR, a novel approach that reconstructs a relightable 3D model
of an object captured from multiple views, driven by a generative relighting
model that simulates the effects of placing the object under novel environment
illuminations. Our method samples the appearance of the object under multiple
lighting environments, creating a dataset that is used to train a
lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's
appearance under any input environmental lighting. The lighting-conditioned
NeRF uses a novel dual-branch architecture to encode the general lighting
effects and specularities separately. The optimized lighting-conditioned NeRF
enables efficient feed-forward relighting under arbitrary environment maps
without requiring per-illumination optimization or light transport simulation.
We evaluate our approach on the established TensoIR and Stanford-ORB datasets,
where it improves upon the state-of-the-art on most metrics, and showcase our
approach on real-world object captures.

</details>


### [50] [Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training](https://arxiv.org/abs/2510.03189)
*Tidiane Camaret Ndir,Alexander Pfefferle,Robin Tibor Schirrmeister*

Main category: cs.CV

TL;DR: 提出一种结合动态体积提示生成和内容感知自适应裁剪的训练策略，用于优化3D生物医学图像交互式分割，在单GPU上解决序列细化反馈的计算挑战。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型要么缺乏体积感知能力，要么交互能力有限，需要高效的交互式3D生物医学图像分割模型。

Method: 使用动态体积提示生成和内容感知自适应裁剪的训练策略，基于nnInteractive分割模型的公开权重初始化网络，模拟真实用户交互模式。

Result: 在3D生物医学图像分割竞赛中表现优异，平均Dice得分0.6385，标准化表面距离0.6614，AUC指标分别为2.4799（Dice）和2.5671（NSD）。

Conclusion: 该方法有效解决了3D生物医学图像交互式分割的计算挑战，在保持高效训练的同时实现了良好的分割性能。

Abstract: Interactive 3D biomedical image segmentation requires efficient models that
can iteratively refine predictions based on user prompts. Current foundation
models either lack volumetric awareness or suffer from limited interactive
capabilities. We propose a training strategy that combines dynamic volumetric
prompt generation with content-aware adaptive cropping to optimize the use of
the image encoder. Our method simulates realistic user interaction patterns
during training while addressing the computational challenges of learning from
sequential refinement feedback on a single GPU. For efficient training, we
initialize our network using the publicly available weights from the
nnInteractive segmentation model. Evaluation on the \textbf{Foundation Models
for Interactive 3D Biomedical Image Segmentation} competition demonstrates
strong performance with an average final Dice score of 0.6385, normalized
surface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice)
and 2.5671 (NSD).

</details>


### [51] [Product-Quantised Image Representation for High-Quality Image Synthesis](https://arxiv.org/abs/2510.03191)
*Denis Zavadski,Nikita Philip Tatsch,Carsten Rother*

Main category: cs.CV

TL;DR: PQGAN将产品量化(PQ)集成到VQGAN框架中，显著提升了图像重建性能，在PSNR、FID、LPIPS和CMMD等指标上大幅超越现有方法，并能无缝集成到预训练扩散模型中实现更高效的图像生成。


<details>
  <summary>Details</summary>
Motivation: 产品量化(PQ)在可扩展向量编码中表现优异，但在高保真图像生成的潜在表示中应用有限，本文旨在将PQ集成到VQGAN框架以提升性能。

Method: 在VQGAN的向量量化框架中集成产品量化，通过深入分析码本大小、嵌入维度和子空间分解之间的相互作用，将向量量化和标量量化作为特例进行研究。

Result: PSNR达到37dB（先前工作为27dB），FID、LPIPS和CMMD分数降低高达96%，发现VQ和PQ在扩展嵌入维度时性能表现相反，为超参数选择提供指导。

Conclusion: PQGAN展示了产品量化在图像合成中作为离散潜在表示的强大扩展能力，能够实现更快速、计算效率更高的生成，或在无额外成本下将输出分辨率加倍。

Abstract: Product quantisation (PQ) is a classical method for scalable vector encoding,
yet it has seen limited usage for latent representations in high-fidelity image
generation. In this work, we introduce PQGAN, a quantised image autoencoder
that integrates PQ into the well-known vector quantisation (VQ) framework of
VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in
terms of reconstruction performance, including both quantisation methods and
their continuous counterparts. We achieve a PSNR score of 37dB, where prior
work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up
to 96%. Our key to success is a thorough analysis of the interaction between
codebook size, embedding dimensionality, and subspace factorisation, with
vector and scalar quantisation as special cases. We obtain novel findings, such
that the performance of VQ and PQ behaves in opposite ways when scaling the
embedding dimension. Furthermore, our analysis shows performance trends for PQ
that help guide optimal hyperparameter selection. Finally, we demonstrate that
PQGAN can be seamlessly integrated into pre-trained diffusion models. This
enables either a significantly faster and more compute-efficient generation, or
a doubling of the output resolution at no additional cost, positioning PQ as a
strong extension for discrete latent representation in image synthesis.

</details>


### [52] [Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft](https://arxiv.org/abs/2510.03198)
*Junchao Huang,Xinting Hu,Boyao Han,Shaoshuai Shi,Zhuotao Tian,Tianyu He,Li Jiang*

Main category: cs.CV

TL;DR: 提出了Memory Forcing学习框架，通过几何索引空间记忆和混合训练协议，解决视频扩散模型在有限计算预算下长期空间一致性与新场景生成质量之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在模拟游戏玩法时需要同时生成自然的新场景内容，并在重新访问已探索区域时保持空间一致性。有限上下文窗口下，仅使用时间记忆缺乏长期空间一致性，而添加空间记忆又可能导致模型过度依赖不充分的上下文而降低新场景生成质量。

Method: 1. 混合训练：暴露不同的游戏玩法机制，引导模型在探索时依赖时间记忆，在重新访问时结合空间记忆；2. 链式前向训练：通过模型滚动扩展自回归训练，链式预测创建更大的姿态变化，鼓励依赖空间记忆保持一致性；3. 点到帧检索：通过将当前可见点映射到源帧来高效检索历史；4. 增量3D重建：维护和更新显式3D缓存。

Result: 大量实验表明，Memory Forcing在多样化环境中实现了优越的长期空间一致性和生成质量，同时在扩展序列中保持了计算效率。

Conclusion: Memory Forcing框架通过精心设计的训练协议和空间记忆机制，有效平衡了长期空间一致性与新场景生成质量，为自回归视频扩散模型的世界建模和交互场景生成提供了高效解决方案。

Abstract: Autoregressive video diffusion models have proved effective for world
modeling and interactive scene generation, with Minecraft gameplay as a
representative application. To faithfully simulate play, a model must generate
natural content while exploring new scenes and preserve spatial consistency
when revisiting explored areas. Under limited computation budgets, it must
compress and exploit historical cues within a finite context window, which
exposes a trade-off: Temporal-only memory lacks long-term spatial consistency,
whereas adding spatial memory strengthens consistency but may degrade new scene
generation quality when the model over-relies on insufficient spatial context.
We present Memory Forcing, a learning framework that pairs training protocols
with a geometry-indexed spatial memory. Hybrid Training exposes distinct
gameplay regimes, guiding the model to rely on temporal memory during
exploration and incorporate spatial memory for revisits. Chained Forward
Training extends autoregressive training with model rollouts, where chained
predictions create larger pose variations and encourage reliance on spatial
memory for maintaining consistency. Point-to-Frame Retrieval efficiently
retrieves history by mapping currently visible points to their source frames,
while Incremental 3D Reconstruction maintains and updates an explicit 3D cache.
Extensive experiments demonstrate that Memory Forcing achieves superior
long-term spatial consistency and generative quality across diverse
environments, while maintaining computational efficiency for extended
sequences.

</details>


### [53] [MonSTeR: a Unified Model for Motion, Scene, Text Retrieval](https://arxiv.org/abs/2510.03200)
*Luca Collorone,Matteo Gioia,Massimiliano Pappa,Paolo Leoni,Giovanni Ficarra,Or Litany,Indro Spinelli,Fabio Galasso*

Main category: cs.CV

TL;DR: MonSTeR是首个运动-场景-文本检索模型，构建统一潜在空间来捕捉多模态间复杂依赖关系，在多种任务中实现灵活而鲁棒的检索。


<details>
  <summary>Details</summary>
Motivation: 人类运动由意图驱动，但只能在支持该意图的环境中发生。现有研究缺乏评估骨骼运动、意图和周围环境对齐关系的工具。

Method: 利用单模态和跨模态表示构建统一潜在空间，通过建模高阶关系来捕捉模态间复杂依赖。

Result: MonSTeR优于仅依赖单模态表示的三模态模型，并通过用户研究验证检索分数与人类偏好的一致性。

Conclusion: MonSTeR的潜在空间具有多功能性，可应用于零样本场景内物体放置和运动描述等任务。

Abstract: Intention drives human movement in complex environments, but such movement
can only happen if the surrounding context supports it. Despite the intuitive
nature of this mechanism, existing research has not yet provided tools to
evaluate the alignment between skeletal movement (motion), intention (text),
and the surrounding context (scene). In this work, we introduce MonSTeR, the
first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of
higher-order relations, MonSTeR constructs a unified latent space by leveraging
unimodal and cross-modal representations. This allows MonSTeR to capture the
intricate dependencies between modalities, enabling flexible but robust
retrieval across various tasks. Our results show that MonSTeR outperforms
trimodal models that rely solely on unimodal representations. Furthermore, we
validate the alignment of our retrieval scores with human preferences through a
dedicated user study. We demonstrate the versatility of MonSTeR's latent space
on zero-shot in-Scene Object Placement and Motion Captioning. Code and
pre-trained models are available at github.com/colloroneluca/MonSTeR.

</details>


### [54] [Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles](https://arxiv.org/abs/2510.03224)
*Dong Lao,Yuxiang Zhang,Haniyeh Ehsani Oskouie,Yangchao Wu,Alex Wong,Stefano Soatto*

Main category: cs.CV

TL;DR: 提出一种基于随机共振的测试时防御机制，通过引入小的平移扰动来对抗对抗性攻击，无需训练且适用于多种网络架构和任务。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法依赖特征过滤或平滑，会导致信息损失。本文旨在通过"以噪声对抗噪声"的方式，在保持信息完整性的同时增强模型鲁棒性。

Method: 对输入图像引入小的平移扰动，对齐变换后的特征嵌入并聚合，然后映射回原始参考图像。该方法采用闭式公式实现，无需额外网络模块或针对特定攻击的微调。

Result: 在图像分类上恢复68.1%的准确率损失，立体匹配恢复71.9%，光流恢复29.2%，在各种对抗攻击下表现出最先进的鲁棒性。

Conclusion: 该方法首次为密集预测任务建立了通用的测试时防御机制，具有训练无关、架构无关和攻击无关的特点，展现了方法的通用性和实用性。

Abstract: We propose a test-time defense mechanism against adversarial attacks:
imperceptible image perturbations that significantly alter the predictions of a
model. Unlike existing methods that rely on feature filtering or smoothing,
which can lead to information loss, we propose to "combat noise with noise" by
leveraging stochastic resonance to enhance robustness while minimizing
information loss. Our approach introduces small translational perturbations to
the input image, aligns the transformed feature embeddings, and aggregates them
before mapping back to the original reference image. This can be expressed in a
closed-form formula, which can be deployed on diverse existing network
architectures without introducing additional network modules or fine-tuning for
specific attack types. The resulting method is entirely training-free,
architecture-agnostic, and attack-agnostic. Empirical results show
state-of-the-art robustness on image classification and, for the first time,
establish a generic test-time defense for dense prediction tasks, including
stereo matching and optical flow, highlighting the method's versatility and
practicality. Specifically, relative to clean (unperturbed) performance, our
method recovers up to 68.1% of the accuracy loss on image classification, 71.9%
on stereo matching, and 29.2% on optical flow under various types of
adversarial attacks.

</details>


### [55] [MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition](https://arxiv.org/abs/2510.03228)
*Ricardo T. Fares,Lucas C. Ribas*

Main category: cs.CV

TL;DR: 提出Mixer，一种新颖的随机神经网络，用于纹理表示学习，通过超球面随机嵌入和双分支学习模块捕获纹理关系。


<details>
  <summary>Details</summary>
Motivation: 现有随机神经网络方法主要关注改进跨信息预测，未对整体网络架构进行重大改进，需要更有效的纹理表示学习方法。

Method: 使用超球面随机嵌入和双分支学习模块来捕获通道内和通道间关系，并通过新制定的优化问题构建丰富的纹理表示。

Result: 在多个具有不同特征和挑战的纯纹理基准测试中取得了有趣的结果。

Conclusion: Mixer方法在纹理识别任务中表现出色，源代码将在发表后提供。

Abstract: Randomized neural networks for representation learning have consistently
achieved prominent results in texture recognition tasks, effectively combining
the advantages of both traditional techniques and learning-based approaches.
However, existing approaches have so far focused mainly on improving
cross-information prediction, without introducing significant advancements to
the overall randomized network architecture. In this paper, we propose Mixer, a
novel randomized neural network for texture representation learning. At its
core, the method leverages hyperspherical random embeddings coupled with a
dual-branch learning module to capture both intra- and inter-channel
relationships, further enhanced by a newly formulated optimization problem for
building rich texture representations. Experimental results have shown the
interesting results of the proposed approach across several pure texture
benchmarks, each with distinct characteristics and challenges. The source code
will be available upon publication.

</details>


### [56] [Improving GUI Grounding with Explicit Position-to-Coordinate Mapping](https://arxiv.org/abs/2510.03230)
*Suyuchen Wang,Tianyu Zhang,Ahmed Masry,Christopher Pal,Spandana Gella,Bang Liu,Perouz Taslakian*

Main category: cs.CV

TL;DR: 提出了RULER令牌和I-MRoPE两种创新方法来解决GUI接地任务中的像素坐标映射问题，显著提高了在高分辨率显示界面上的定位精度。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在GUI接地任务中面临核心瓶颈：可靠的补丁到像素映射在训练未见的高分辨率显示上失效。现有方法直接从视觉特征生成坐标文本令牌，迫使模型隐式推断复杂的位置到像素映射，导致精度下降和失败增加。

Method: 1. RULER令牌作为显式坐标标记，让模型像地图上的网格线一样参考位置，而不是从头生成坐标；2. 交错MRoPE（I-MRoPE）改进空间编码，确保宽度和高度维度得到平等表示，解决标准位置方案的不对称性。

Result: 在ScreenSpot、ScreenSpot-V2和ScreenSpot-Pro数据集上的实验显示接地精度持续提升，在高分辨率界面上的改进最大。

Conclusion: 通过提供显式空间指导而非依赖隐式学习，该方法能够在不同分辨率和平台上实现更可靠的GUI自动化。

Abstract: GUI grounding, the task of mapping natural-language instructions to pixel
coordinates, is crucial for autonomous agents, yet remains difficult for
current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which
breaks when extrapolating to high-resolution displays unseen during training.
Current approaches generate coordinates as text tokens directly from visual
features, forcing the model to infer complex position-to-pixel mappings
implicitly; as a result, accuracy degrades and failures proliferate on new
resolutions. We address this with two complementary innovations. First, RULER
tokens serve as explicit coordinate markers, letting the model reference
positions similar to gridlines on a map and adjust rather than generate
coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial
encoding by ensuring that width and height dimensions are represented equally,
addressing the asymmetry of standard positional schemes. Experiments on
ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in
grounding accuracy, with the largest improvements on high-resolution
interfaces. By providing explicit spatial guidance rather than relying on
implicit learning, our approach enables more reliable GUI automation across
diverse resolutions and platforms.

</details>


### [57] [LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models](https://arxiv.org/abs/2510.03232)
*Ci-Siang Lin,Min-Hung Chen,Yu-Yang Sheng,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: LEAML是一个标签高效的MLLM适应框架，通过生成领域相关的伪问答对和选择性神经元更新，在有限标注数据下提升多模态大语言模型在专业领域的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在通用视觉基准上表现良好，但在医学影像等专业领域的分布外任务中表现不佳，这些领域标注数据有限且昂贵。

Method: 利用稀缺的标注VQA样本和丰富的未标注图像，通过经过标题蒸馏正则化的QA生成器为未标注数据生成领域相关伪问答对，并选择性更新与问答最相关的神经元。

Result: 在胃肠道内窥镜和体育VQA任务上的实验表明，LEAML在最小监督下始终优于标准微调方法。

Conclusion: LEAML框架在标签稀缺的专业领域任务中表现出有效性，为多模态大语言模型的领域适应提供了高效解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance on
general visual benchmarks but struggle with out-of-distribution (OOD) tasks in
specialized domains such as medical imaging, where labeled data is limited and
expensive. We introduce LEAML, a label-efficient adaptation framework that
leverages both scarce labeled VQA samples and abundant unlabeled images. Our
approach generates domain-relevant pseudo question-answer pairs for unlabeled
data using a QA generator regularized by caption distillation. Importantly, we
selectively update only those neurons most relevant to question-answering,
enabling the QA Generator to efficiently acquire domain-specific knowledge
during distillation. Experiments on gastrointestinal endoscopy and sports VQA
demonstrate that LEAML consistently outperforms standard fine-tuning under
minimal supervision, highlighting the effectiveness of our proposed LEAML
framework.

</details>
