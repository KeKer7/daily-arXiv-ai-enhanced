<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 56]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Bi-Orthogonal Factor Decomposition for Vision Transformers](https://arxiv.org/abs/2601.05328)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

TL;DR: BFD框架揭示Vision Transformer注意力机制中位置与内容信息的交互方式，发现内容-内容交互主导注意力，DINOv2在内容-位置耦合上分配更多能量，且注意力头存在功能分化。


<details>
  <summary>Details</summary>
Motivation: 传统注意力图仅显示权重集中位置，无法揭示查询和键之间交换的是位置信息、内容信息还是两者兼有。需要一种原则性方法来理解注意力机制中信息交换的本质。

Method: 提出双正交因子分解(BFD)框架：第一阶段使用ANOVA分解将token激活统计解耦为正交的位置和内容因子；第二阶段对查询-键交互矩阵QK^T进行SVD，揭示这些因子如何介导通信的双正交模式。

Result: 1) 注意力主要通过内容运作，内容-内容交互主导注意力能量，其次是内容-位置耦合；2) 注意力机制存在专业化：头部分化为内容-内容、内容-位置和位置-位置算子；3) DINOv2的优越整体形状处理源于中间层同时保持位置结构并丰富语义内容。

Conclusion: BFD揭示了token如何通过注意力交互以及哪些信息因子（位置或语义）介导它们的通信，为理解视觉Transformer机制提供了实用见解。

Abstract: Self-attention is the central computational primitive of Vision Transformers, yet we lack a principled understanding of what information attention mechanisms exchange between tokens. Attention maps describe where weight mass concentrates; they do not reveal whether queries and keys trade position, content, or both. We introduce Bi-orthogonal Factor Decomposition (BFD), a two-stage analytical framework: first, an ANOVA-based decomposition statistically disentangles token activations into orthogonal positional and content factors; second, SVD of the query-key interaction matrix QK^T exposes bi-orthogonal modes that reveal how these factors mediate communication. After validating proper isolation of position and content, we apply BFD to state-of-the-art vision models and uncover three phenomena.(i) Attention operates primarily through content. Content-content interactions dominate attention energy, followed by content-position coupling. DINOv2 allocates more energy to content-position than supervised models and distributes computation across a richer mode spectrum. (ii) Attention mechanisms exhibit specialization: heads differentiate into content-content, content-position, and position-position operators, while singular modes within heads show analogous specialization. (iii) DINOv2's superior holistic shape processing emerges from intermediate layers that simultaneously preserve positional structure while contextually enriching semantic content.
  Overall, BFD exposes how tokens interact through attention and which informational factors - positional or semantic - mediate their communication, yielding practical insights into vision transformer mechanisms.

</details>


### [2] [Coding the Visual World: From Image to Simulation Using Vision Language Models](https://arxiv.org/abs/2601.05344)
*Sagi Eppel*

Main category: cs.CV

TL;DR: VLMs能够通过Im2Sim方法理解图像中的复杂系统并生成模拟代码，但在细节复制方面能力有限，表现出高层次理解与低层次感知的不对称性。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型是否能够像人类一样构建图像中描绘系统的心理模型，理解其机制并进行模拟。

Method: 使用Im2Sim方法：给VLM输入真实世界系统的自然图像，要求其描述系统并编写模拟代码，执行代码生成合成图像，然后与原图比较。

Result: 领先的VLMs（GPT、Gemini）能够理解和建模跨多个抽象层次和广泛领域的复杂多组件系统，但在复制图像中的精细细节和低层次模式排列方面能力有限。

Conclusion: VLMs表现出有趣的不对称性：结合了高层次、深度的视觉理解能力，但对精细细节的感知有限，这揭示了当前视觉理解模型的优势和局限性。

Abstract: The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) demonstrate the capacity to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.

</details>


### [3] [STResNet & STYOLO : A New Family of Compact Classification and Object Detection Models for MCUs](https://arxiv.org/abs/2601.05364)
*Sudhakar Sah,Ravish Kumar*

Main category: cs.CV

TL;DR: 该论文提出了STResNet和STYOLO两个轻量级神经网络系列，分别用于图像分类和目标检测，在资源受限的边缘设备上实现了精度、效率和内存占用的联合优化。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级网络通常在精度和延迟之间进行权衡，限制了在微控制器和神经处理单元等边缘硬件上的应用。需要开发在资源受限平台上同时优化精度、效率和内存占用的模型。

Method: 提出了两个模型系列：用于图像分类的STResNet（包括Nano到Tiny变体）和用于目标检测的STYOLO（包括Micro和Milli变体）。这些模型专门针对资源受限平台进行了联合优化。

Result: STResNetMilli在仅300万参数下达到70.0%的ImageNet Top-1准确率，优于MobileNetV1和ShuffleNetV2。STYOLOMicro和STYOLOMilli在MS COCO数据集上分别达到30.5%和33.6%的mAP，超越了YOLOv5n和YOLOX Nano。

Conclusion: 提出的STResNet和STYOLO系列在资源受限的边缘设备上实现了精度、效率和内存占用的良好平衡，为边缘计算应用提供了有效的轻量级解决方案。

Abstract: Recent advancements in lightweight neural networks have significantly improved the efficiency of deploying deep learning models on edge hardware. However, most existing architectures still trade accuracy for latency, which limits their applicability on microcontroller and neural processing unit based devices. In this work, we introduce two new model families, STResNet for image classification and STYOLO for object detection, jointly optimized for accuracy, efficiency, and memory footprint on resource constrained platforms. The proposed STResNet series, ranging from Nano to Tiny variants, achieves competitive ImageNet 1K accuracy within a four million parameter budget. Specifically, STResNetMilli attains 70.0 percent Top 1 accuracy with only three million parameters, outperforming MobileNetV1 and ShuffleNetV2 at comparable computational complexity. For object detection, STYOLOMicro and STYOLOMilli achieve 30.5 percent and 33.6 percent mean average precision, respectively, on the MS COCO dataset, surpassing YOLOv5n and YOLOX Nano in both accuracy and efficiency. Furthermore, when STResNetMilli is used as a backbone with the Ultralytics training environment.

</details>


### [4] [MOSAIC-GS: Monocular Scene Reconstruction via Advanced Initialization for Complex Dynamic Environments](https://arxiv.org/abs/2601.05368)
*Svitlana Morkva,Maximum Wilder-Smith,Michael Oechsle,Alessio Tonioni,Marco Hutter,Vaishakh Patil*

Main category: cs.CV

TL;DR: MOSAIC-GS：一种基于高斯泼溅的单目视频动态场景重建方法，通过几何线索和运动约束实现高效高质量重建


<details>
  <summary>Details</summary>
Motivation: 单目重建由于缺乏多视角约束而具有病态性，难以准确恢复物体几何和时间一致性。现有方法主要依赖视觉外观进行运动推断，这在单目设置中常常存在歧义。

Method: 1. 利用深度、光流、动态物体分割和点跟踪等多种几何线索；2. 结合基于刚性的运动约束，在初始化阶段估计初步的3D场景动态；3. 将场景分解为静态和动态组件；4. 动态部分的高斯使用时间相关的Poly-Fourier曲线表示轨迹，实现参数高效的运动编码。

Result: MOSAIC-GS在标准单目动态场景基准测试中，相比现有方法实现了显著更快的优化和渲染速度，同时保持了与最先进方法相当的重建质量。

Conclusion: 该方法通过先验动态恢复和高效运动表示，解决了单目动态场景重建的挑战，实现了高质量、高效率的重建效果。

Abstract: We present MOSAIC-GS, a novel, fully explicit, and computationally efficient approach for high-fidelity dynamic scene reconstruction from monocular videos using Gaussian Splatting. Monocular reconstruction is inherently ill-posed due to the lack of sufficient multiview constraints, making accurate recovery of object geometry and temporal coherence particularly challenging. To address this, we leverage multiple geometric cues, such as depth, optical flow, dynamic object segmentation, and point tracking. Combined with rigidity-based motion constraints, these cues allow us to estimate preliminary 3D scene dynamics during an initialization stage. Recovering scene dynamics prior to the photometric optimization reduces reliance on motion inference from visual appearance alone, which is often ambiguous in monocular settings. To enable compact representations, fast training, and real-time rendering while supporting non-rigid deformations, the scene is decomposed into static and dynamic components. Each Gaussian in the dynamic part of the scene is assigned a trajectory represented as time-dependent Poly-Fourier curve for parameter-efficient motion encoding. We demonstrate that MOSAIC-GS achieves substantially faster optimization and rendering compared to existing methods, while maintaining reconstruction quality on par with state-of-the-art approaches across standard monocular dynamic scene benchmarks.

</details>


### [5] [Ensemble of radiomics and ConvNeXt for breast cancer diagnosis](https://arxiv.org/abs/2601.05373)
*Jorge Alberto Garza-Abdala,Gerardo Alejandro Fumagal-González,Beatriz A. Bosques-Palomo,Mario Alexis Monsivais Molina,Daly Avedano,Servando Cardona-Huerta,José Gerardo Tamez-Pena*

Main category: cs.CV

TL;DR: 集成深度学习和影像组学的方法在乳腺癌筛查中表现最佳，AUC达0.87，优于单独的深度学习方法（0.83）和影像组学方法（0.80）。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌早期诊断对提高生存率至关重要。虽然影像组学和深度学习在辅助放射科医生早期癌症检测方面显示出潜力，但需要评估这些方法在筛查乳腺X光片中的性能表现。

Method: 使用两个独立数据集：RSNA 2023乳腺癌检测挑战赛（11,913名患者）和墨西哥TecSalud队列（19,400名患者）。训练ConvNeXtV1-small深度学习模型于RSNA数据集并在TecSalud数据集验证；影像组学模型基于TecSalud数据集开发，采用留一年交叉验证；集成方法通过相同方法学一致地组合和校准预测。

Result: 集成方法获得最高AUC（0.87），优于ConvNeXtV1-small深度学习模型（0.83）和影像组学模型（0.80）。

Conclusion: 结合深度学习和影像组学预测的集成方法能显著提升乳腺X光片的乳腺癌诊断性能。

Abstract: Early diagnosis of breast cancer is crucial for improving survival rates. Radiomics and deep learning (DL) have shown significant potential in assisting radiologists with early cancer detection. This paper aims to critically assess the performance of radiomics, DL, and ensemble techniques in detecting cancer from screening mammograms. Two independent datasets were used: the RSNA 2023 Breast Cancer Detection Challenge (11,913 patients) and a Mexican cohort from the TecSalud dataset (19,400 patients). The ConvNeXtV1-small DL model was trained on the RSNA dataset and validated on the TecSalud dataset, while radiomics models were developed using the TecSalud dataset and validated with a leave-one-year-out approach. The ensemble method consistently combined and calibrated predictions using the same methodology. Results showed that the ensemble approach achieved the highest area under the curve (AUC) of 0.87, compared to 0.83 for ConvNeXtV1-small and 0.80 for radiomics. In conclusion, ensemble methods combining DL and radiomics predictions significantly enhance breast cancer diagnosis from mammograms.

</details>


### [6] [EdgeLDR: Quaternion Low-Displacement Rank Neural Networks for Edge-Efficient Deep Learning](https://arxiv.org/abs/2601.05379)
*Vladimir Frants,Sos Agaian,Karen Panetta*

Main category: cs.CV

TL;DR: EdgeLDR框架将四元数神经网络与块循环矩阵结构结合，通过FFT加速计算，在边缘设备上实现高效压缩和低延迟推理。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署深度神经网络常受限于内存带宽和密集线性算子的计算成本。四元数神经网络通过哈密顿积耦合多个通道提高参数效率，但通常保留非结构化密集权重；而结构化矩阵虽能实现快速计算，但通常应用于实数域。

Method: 提出EdgeLDR框架，结合四元数通道混合与块循环参数结构，通过复数伴随表示实现基于FFT的评估。提供EdgeLDR层的参考实现，比较FFT计算与朴素空间域实现的性能差异。

Result: FFT评估相比朴素实现获得显著加速，且延迟随块大小增加保持稳定，使更大压缩因子计算可行。在CIFAR-10/100、SVHN等数据集上，EdgeLDR层在紧凑CNN和Transformer骨干中实现显著压缩同时保持竞争力精度。

Conclusion: EdgeLDR框架通过结合四元数神经网络与块循环结构，在边缘设备上实现了显著压缩和高效推理，为资源受限环境提供了实用的解决方案。

Abstract: Deploying deep neural networks on edge devices is often limited by the memory traffic and compute cost of dense linear operators. While quaternion neural networks improve parameter efficiency by coupling multiple channels through Hamilton products, they typically retain unstructured dense weights; conversely, structured matrices enable fast computation but are usually applied in the real domain. This paper introduces EdgeLDR, a practical framework for quaternion block-circulant linear and convolutional layers that combines quaternion channel mixing with block-circulant parameter structure and enables FFT-based evaluation through the complex adjoint representation. We present reference implementations of EdgeLDR layers and compare FFT-based computation against a naive spatial-domain realization of quaternion circulant products. FFT evaluation yields large empirical speedups over the naive implementation and keeps latency stable as block size increases, making larger compression factors computationally viable. We further integrate EdgeLDR layers into compact CNN and Transformer backbones and evaluate accuracy-compression trade-offs on 32x32 RGB classification (CIFAR-10/100, SVHN) and hyperspectral image classification (Houston 2013, Pavia University), reporting parameter counts and CPU/GPU latency. The results show that EdgeLDR layers provide significant compression with competitive accuracy.

</details>


### [7] [Sketch&Patch++: Efficient Structure-Aware 3D Gaussian Representation](https://arxiv.org/abs/2601.05394)
*Yuang Shi,Simone Gasparini,Géraldine Morin,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: 提出基于高斯混合模型的3D场景分层表示方法，将高斯分为描绘高频特征的Sketch Gaussians和覆盖低频平滑区域的Patch Gaussians，实现渐进式流式传输和高效压缩。


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯表示在带宽受限网络和资源有限设备上效率低下，需要一种能够根据场景结构特征进行自适应压缩和流式传输的方法。

Method: 提出分层自适应分类框架，通过多准则密度聚类和自适应质量驱动优化，将高斯分为Sketch Gaussians（高频特征）和Patch Gaussians（低频区域），实现结构感知的渐进式流式传输。

Result: 在多样化场景（人造和自然环境）中，相比均匀剪枝基线，PSNR提升达1.74dB，SSIM提升6.7%，LPIPS提升41.4%；室内场景仅需原始模型大小的0.5%即可保持视觉质量。

Conclusion: 提出的结构感知表示方法能够实现高效存储、自适应流式传输和渲染，特别适用于带宽受限网络和资源有限设备的高保真3D内容应用。

Abstract: We observe that Gaussians exhibit distinct roles and characteristics analogous to traditional artistic techniques -- like how artists first sketch outlines before filling in broader areas with color, some Gaussians capture high-frequency features such as edges and contours, while others represent broader, smoother regions analogous to brush strokes that add volume and depth. Based on this observation, we propose a hybrid representation that categorizes Gaussians into (i) Sketch Gaussians, which represent high-frequency, boundary-defining features, and (ii) Patch Gaussians, which cover low-frequency, smooth regions. This semantic separation naturally enables layered progressive streaming, where the compact Sketch Gaussians establish the structural skeleton before Patch Gaussians incrementally refine volumetric detail.
  In this work, we extend our previous method to arbitrary 3D scenes by proposing a novel hierarchical adaptive categorization framework that operates directly on the 3DGS representation. Our approach employs multi-criteria density-based clustering, combined with adaptive quality-driven refinement. This method eliminates dependency on external 3D line primitives while ensuring optimal parametric encoding effectiveness. Our comprehensive evaluation across diverse scenes, including both man-made and natural environments, demonstrates that our method achieves up to 1.74 dB improvement in PSNR, 6.7% in SSIM, and 41.4% in LPIPS at equivalent model sizes compared to uniform pruning baselines. For indoor scenes, our method can maintain visual quality with only 0.5\% of the original model size. This structure-aware representation enables efficient storage, adaptive streaming, and rendering of high-fidelity 3D content across bandwidth-constrained networks and resource-limited devices.

</details>


### [8] [Multi-task Cross-modal Learning for Chest X-ray Image Retrieval](https://arxiv.org/abs/2601.05399)
*Zhaohui Liang,Sivaramakrishnan Rajaraman,Niccolo Marini,Zhiyun Xue,Sameer Antani*

Main category: cs.CV

TL;DR: 该论文提出一个多任务学习框架来微调BiomedCLIP，以改进胸部X光图像-文本检索性能，通过结合分类、对比和CLIP损失实现更平衡的临床相关检索结果。


<details>
  <summary>Details</summary>
Motivation: CLIP和BiomedCLIP虽然提供强大的跨模态嵌入，但未针对细粒度医学检索任务（如使用胸部X光图像查询检索临床相关放射学报告）进行优化，需要专门的方法来提升医学领域的检索性能。

Method: 以BiomedCLIP为骨干网络，加入轻量级MLP投影头，采用多任务复合损失函数：1) 区分正常/异常CXR研究的二元交叉熵损失；2) 增强类内一致性的监督对比损失；3) 保持跨模态对齐的CLIP损失。

Result: 微调后的模型在图像到文本和文本到图像检索任务上相比预训练的BiomedCLIP和通用CLIP模型，实现了更平衡且临床意义更强的性能。t-SNE可视化显示正常和异常病例的语义聚类更清晰，表明模型诊断敏感性增强。

Conclusion: 领域自适应的多任务学习对于推进生物医学应用中的跨模态检索具有重要价值，能够显著提升医学图像-文本检索的临床相关性和诊断敏感性。

Abstract: CLIP and BiomedCLIP are examples of vision-language foundation models and offer strong cross-modal embeddings; however, they are not optimized for fine-grained medical retrieval tasks, such as retrieving clinically relevant radiology reports using chest X-ray (CXR) image queries. To address this shortcoming, we propose a multi-task learning framework to fine-tune BiomedCLIP and evaluate improvements to CXR image-text retrieval. Using BiomedCLIP as the backbone, we incorporate a lightweight MLP projector head trained with a multi-task composite loss function that includes: (1) a binary cross-entropy loss to distinguish normal from abnormal CXR studies, (2) a supervised contrastive loss to reinforce intra-class consistency, and (3) a CLIP loss to maintain cross-modal alignment. Experimental results demonstrate that the fine-tuned model achieves more balanced and clinically meaningful performance across both image-to-text and text-to-image retrieval tasks compared to the pretrained BiomedCLIP and general-purpose CLIP models. Furthermore, t-SNE visualizations reveal clearer semantic clustering of normal and abnormal cases, demonstrating the model's enhanced diagnostic sensitivity. These findings highlight the value of domain-adaptive, multi-task learning for advancing cross-modal retrieval in biomedical applications.

</details>


### [9] [Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization](https://arxiv.org/abs/2601.05432)
*Yuxiang Ji,Yong Wang,Ziyu Ma,Yiming Hu,Hailang Huang,Xuecai Hu,Guanhua Chen,Liaoni Wu,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 论文提出了一种结合地图思维的图像地理定位方法，通过代理在地图中的循环推理和两阶段优化方案，显著提升了定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型方法虽然利用了世界知识、思维链推理和代理能力，但忽略了人类常用的地图使用策略。论文旨在让模型具备"地图思维"能力，以更有效地进行地理定位。

Method: 1) 提出"地图思维"能力，构建代理在地图中的循环推理框架；2) 开发两阶段优化方案：代理强化学习提升采样效率，并行测试时缩放使模型能探索多个候选路径；3) 创建MAPBench基准数据集，包含真实世界图像。

Result: 方法在大多数指标上优于现有开源和闭源模型，特别是将Acc@500m从Gemini-3-Pro的8.0%提升到22.1%，提升幅度显著。

Conclusion: 通过引入地图思维和两阶段优化策略，论文显著提升了图像地理定位的性能，证明了结合地图推理的重要性。

Abstract: The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model \textit{Thinking with Map} ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\% to 22.1\% compared to \textit{Gemini-3-Pro} with Google Search/Map grounded mode.

</details>


### [10] [TAPM-Net: Trajectory-Aware Perturbation Modeling for Infrared Small Target Detection](https://arxiv.org/abs/2601.05446)
*Hongyang Xie,Hongyang He,Victor Sanchez*

Main category: cs.CV

TL;DR: TAPM-Net提出了一种轨迹感知的Mamba传播网络，通过建模目标引起的特征扰动空间扩散行为来改进红外小目标检测，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测面临信号对比度弱、空间范围有限、背景杂乱等挑战。现有CNN和ViT模型缺乏追踪小目标在特征空间中引发方向性、层间扰动的机制，而这对于区分红外场景中的信号和结构化噪声至关重要。

Method: 提出TAPM-Net，包含两个核心组件：扰动引导路径模块（PGM）和轨迹感知状态块（TASB）。PGM从多级特征构建扰动能量场，提取梯度跟随的特征轨迹；TASB是基于Mamba的状态空间单元，沿轨迹建模动态传播，结合速度约束扩散和语义对齐的特征融合。

Result: 在NUAA-SIRST和IRSTD-1K数据集上的实验表明，TAPM-Net在红外小目标检测任务中达到了最先进的性能。

Conclusion: TAPM-Net通过显式建模目标引起的特征扰动空间扩散行为，实现了各向异性、上下文敏感的状态转移，同时保持全局一致性且计算成本低，为红外小目标检测提供了有效解决方案。

Abstract: Infrared small target detection (ISTD) remains a long-standing challenge due to weak signal contrast, limited spatial extent, and cluttered backgrounds. Despite performance improvements from convolutional neural networks (CNNs) and Vision Transformers (ViTs), current models lack a mechanism to trace how small targets trigger directional, layer-wise perturbations in the feature space, which is an essential cue for distinguishing signal from structured noise in infrared scenes. To address this limitation, we propose the Trajectory-Aware Mamba Propagation Network (TAPM-Net), which explicitly models the spatial diffusion behavior of target-induced feature disturbances. TAPM-Net is built upon two novel components: a Perturbation-guided Path Module (PGM) and a Trajectory-Aware State Block (TASB). The PGM constructs perturbation energy fields from multi-level features and extracts gradient-following feature trajectories that reflect the directionality of local responses. The resulting feature trajectories are fed into the TASB, a Mamba-based state-space unit that models dynamic propagation along each trajectory while incorporating velocity-constrained diffusion and semantically aligned feature fusion from word-level and sentence-level embeddings. Unlike existing attention-based methods, TAPM-Net enables anisotropic, context-sensitive state transitions along spatial trajectories while maintaining global coherence at low computational cost. Experiments on NUAA-SIRST and IRSTD-1K demonstrate that TAPM-Net achieves state-of-the-art performance in ISTD.

</details>


### [11] [ROAP: A Reading-Order and Attention-Prior Pipeline for Optimizing Layout Transformers in Key Information Extraction](https://arxiv.org/abs/2601.05470)
*Tingwei Xie,Jinxin He,Yonghong Song*

Main category: cs.CV

TL;DR: ROAP是一个轻量级、架构无关的流程，通过自适应XY间隙树提取阅读序列，结合阅读顺序感知的相对位置偏置和文本令牌子块注意力先验，优化Layout Transformers的注意力分布，提升视觉丰富文档理解性能。


<details>
  <summary>Details</summary>
Motivation: 多模态Transformer在视觉丰富文档理解中存在两个关键限制：缺乏对逻辑阅读顺序的显式建模，以及视觉令牌干扰稀释了文本语义的注意力。

Method: 1. 使用自适应XY间隙树从复杂布局中提取层次化阅读序列；2. 通过阅读顺序感知的相对位置偏置将序列集成到注意力机制；3. 引入文本令牌子块注意力先验自适应抑制视觉噪声并增强细粒度文本-文本交互。

Result: 在FUNSD和CORD基准测试上的广泛实验表明，ROAP能持续提升代表性骨干网络（包括LayoutLMv3和GeoLayoutLM）的性能。

Conclusion: 显式建模阅读逻辑和调节模态干扰对于稳健的文档理解至关重要，为复杂布局分析提供了可扩展的解决方案。

Abstract: The efficacy of Multimodal Transformers in visually-rich document understanding (VrDU) is critically constrained by two inherent limitations: the lack of explicit modeling for logical reading order and the interference of visual tokens that dilutes attention on textual semantics.
  To address these challenges, this paper presents ROAP, a lightweight and architecture-agnostic pipeline designed to optimize attention distributions in Layout Transformers without altering their pre-trained backbones.
  The proposed pipeline first employs an Adaptive-XY-Gap (AXG-Tree) to robustly extract hierarchical reading sequences from complex layouts. These sequences are then integrated into the attention mechanism via a Reading-Order-Aware Relative Position Bias (RO-RPB). Furthermore, a Textual-Token Sub-block Attention Prior (TT-Prior) is introduced to adaptively suppress visual noise and enhance fine-grained text-text interactions.
  Extensive experiments on the FUNSD and CORD benchmarks demonstrate that ROAP consistently improves the performance of representative backbones, including LayoutLMv3 and GeoLayoutLM.
  These findings confirm that explicitly modeling reading logic and regulating modality interference are critical for robust document understanding, offering a scalable solution for complex layout analysis. The implementation code will be released at https://github.com/KevinYuLei/ROAP.

</details>


### [12] [Multi-Image Super Resolution Framework for Detection and Analysis of Plant Roots](https://arxiv.org/abs/2601.05482)
*Shubham Agarwal,Ofek Nourian,Michael Sidorov,Sharon Chemweno,Ofer Hadar,Naftali Lazarovitch,Jhonathan E. Ephrath*

Main category: cs.CV

TL;DR: 提出一种用于地下植物根系成像的多图像超分辨率深度学习框架，通过合成数据集训练，利用多视角空间冗余重建高分辨率图像，提升根系可见性和细节，实现根系性状的准确量化。


<details>
  <summary>Details</summary>
Motivation: 地下植物根系成像面临遮挡、土壤湿度变化和低对比度等挑战，传统视觉方法效果有限，需要新方法来提升根系可见性和细节，以支持土壤-植物相互作用、养分吸收和植物健康研究。

Method: 提出一种地下成像系统，捕获植物根系的多重叠视图，并集成基于深度学习的多图像超分辨率框架。构建合成数据集模拟真实地下成像场景，包含影响图像质量的环境因素。MISR算法利用视图间的空间冗余重建高分辨率图像。

Result: 定量评估显示该方法优于最先进的超分辨率基线，BRISQUE降低2.3%，CLIP-IQA得分相同，表明图像质量改善。能够实现根系性状（如根毛数量和密度）的准确估计。

Conclusion: 该框架为农业和生态研究中的稳健自动地下植物根系成像和性状量化提供了有前景的方向，有助于增强根系表型分析。

Abstract: Understanding plant root systems is critical for advancing research in soil-plant interactions, nutrient uptake, and overall plant health. However, accurate imaging of roots in subterranean environments remains a persistent challenge due to adverse conditions such as occlusion, varying soil moisture, and inherently low contrast, which limit the effectiveness of conventional vision-based approaches. In this work, we propose a novel underground imaging system that captures multiple overlapping views of plant roots and integrates a deep learning-based Multi-Image Super Resolution (MISR) framework designed to enhance root visibility and detail. To train and evaluate our approach, we construct a synthetic dataset that simulates realistic underground imaging scenarios, incorporating key environmental factors that affect image quality. Our proposed MISR algorithm leverages spatial redundancy across views to reconstruct high-resolution images with improved structural fidelity and visual clarity. Quantitative evaluations show that our approach outperforms state-of-the-art super resolution baselines, achieving a 2.3 percent reduction in BRISQUE, indicating improved image quality with the same CLIP-IQA score, thereby enabling enhanced phenotypic analysis of root systems. This, in turn, facilitates accurate estimation of critical root traits, including root hair count and root hair density. The proposed framework presents a promising direction for robust automatic underground plant root imaging and trait quantification for agricultural and ecological research.

</details>


### [13] [Hippocampal Atrophy Patterns Across the Alzheimer's Disease Spectrum: A Voxel-Based Morphometry Analysis](https://arxiv.org/abs/2601.05494)
*Trishna Niraula*

Main category: cs.CV

TL;DR: 该研究使用CAT12/SPM12体素形态测量分析ADNI数据，发现AD患者海马萎缩显著，海马体积对MCI向AD转化有中等预测价值，APOE4状态对海马体积无显著影响。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病和轻度认知障碍与灰质丢失相关，特别是在内侧颞叶结构。研究旨在通过体素形态测量分析灰质体积变化，探索AD进展的生物标志物和遗传影响。

Method: 使用CAT12/SPM12对249名ADNI参与者（CN=90，MCI=129，AD=30）的基线T1加权MRI进行体素形态测量分析。采用一般线性模型分析灰质体积，以诊断组为主要预测因子，年龄和总颅内体积为协变量。统计图阈值设为p<0.001（体素水平），使用家族误差校正进行多重比较校正（p<0.05）。

Result: AD患者相对于CN和MCI表现出显著的海马萎缩（Cohen's d分别为2.03和1.61）。海马体积对MCI向AD转化具有中等预测价值（AUC=0.66）。按APOE4状态分层未发现遗传因素对横断面海马体积的显著影响。

Conclusion: 内侧颞叶变性是AD进展的关键特征，海马体积可作为AD进展的预测生物标志物，但APOE4状态对海马体积无显著影响。

Abstract: Alzheimer's disease (AD) and mild cognitive impairment (MCI) are associated with progressive gray matter loss, particularly in medial temporal structures. In this study, CAT12/SPM12 voxel-based morphometry was applied to baseline T1-weighted MRI scans from 249 ADNI participants (CN = 90, MCI = 129, AD = 30). Gray matter volume was analyzed using a general linear model, with the diagnostic group as primary predictor and age and total intracranial volume as covariates. Statistical maps were thresholded at p < 0.001 (voxelwise) and corrected for multiple comparisons at the cluster level using family-wise error (FWE) correction (p < 0.05). Significant hippocampal atrophy was observed in AD relative to CN and MCI (Cohen's d = 2.03 and 1.61, respectively). Hippocampal volume demonstrated moderate predictive value for conversion from MCI to AD (AUC = 0.66). Stratification by APOE4 status did not reveal significant genetic effects on cross-sectional hippocampal volume. These results support medial temporal degeneration as a key feature of AD progression and provide insights into predictive biomarkers and genetic influences.

</details>


### [14] [MMViR: A Multi-Modal and Multi-Granularity Representation for Long-range Video Understanding](https://arxiv.org/abs/2601.05495)
*Zizhong Li,Haopeng Zhang,Jiawei Zhang*

Main category: cs.CV

TL;DR: MMViR提出了一种多模态多粒度结构化表示方法，用于解决长视频理解中的计算复杂性和内容冗余问题，通过关键转折点分割和三层次描述实现高效检索和理解。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在处理长视频时面临挑战：直接编码计算成本过高，简单的视频转文本方法会产生冗余或碎片化内容，难以处理复杂事件、多样场景和长程依赖关系。

Method: MMViR通过识别关键转折点来分割视频，构建包含全局叙事和细粒度视觉细节的三层次描述结构，支持基于查询的高效检索，并具有良好的泛化能力。

Result: 在问答、摘要和检索三个任务上的广泛评估显示，MMViR优于先前最强方法，在小时级视频理解上实现了19.67%的提升，同时将处理延迟降低到原来的45.4%。

Conclusion: MMViR为长视频理解提供了一种有效的多模态结构化表示方法，显著提升了处理效率和理解性能，为长视频分析开辟了新途径。

Abstract: Long videos, ranging from minutes to hours, present significant challenges for current Multi-modal Large Language Models (MLLMs) due to their complex events, diverse scenes, and long-range dependencies. Direct encoding of such videos is computationally too expensive, while simple video-to-text conversion often results in redundant or fragmented content. To address these limitations, we introduce MMViR, a novel multi-modal, multi-grained structured representation for long video understanding. MMViR identifies key turning points to segment the video and constructs a three-level description that couples global narratives with fine-grained visual details. This design supports efficient query-based retrieval and generalizes well across various scenarios. Extensive evaluations across three tasks, including QA, summarization, and retrieval, show that MMViR outperforms the prior strongest method, achieving a 19.67% improvement in hour-long video understanding while reducing processing latency to 45.4% of the original.

</details>


### [15] [Prompt-Free SAM-Based Multi-Task Framework for Breast Ultrasound Lesion Segmentation and Classification](https://arxiv.org/abs/2601.05498)
*Samuel E. Johnny,Bernes L. Atabonfack,Israel Alagbe,Assane Gueye*

Main category: cs.CV

TL;DR: 本文提出了一种基于SAM视觉编码器的多任务深度学习框架，用于乳腺超声图像中的病灶分割和诊断分类，通过掩码引导注意力机制提升分类性能，在PRECISE 2025数据集上取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 乳腺超声图像存在对比度低、斑点噪声和病灶形态多样等问题，导致准确的肿瘤分割和分类具有挑战性。需要开发能够同时处理分割和分类任务的鲁棒方法。

Method: 提出多任务深度学习框架，利用SAM视觉编码器的嵌入特征，通过轻量级卷积头或UNet风格解码器进行像素级分割。分类分支采用掩码引导注意力机制，聚焦病灶相关特征并抑制背景伪影。采用无提示的全监督适应方式。

Result: 在PRECISE 2025乳腺超声数据集上（按类别80%训练，20%测试），Dice相似系数达到0.887，准确率达到92.3%，在PRECISE挑战排行榜中名列前茅。

Conclusion: SAM基础表示结合分割引导学习能显著改善乳腺超声图像中的病灶描绘和诊断预测，为临床辅助诊断提供了有效解决方案。

Abstract: Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.

</details>


### [16] [Enabling Stroke-Level Structural Analysis of Hieroglyphic Scripts without Language-Specific Priors](https://arxiv.org/abs/2601.05508)
*Fuwen Luo,Zihao Wan,Ziyue Wang,Yaluo Liu,Pau Tong Lin Xu,Xuanjia Qiao,Xiaolong Wang,Peng Li,Yang Liu*

Main category: cs.CV

TL;DR: HieroSA是一个新颖的框架，使多模态大语言模型能够从字符位图自动推导笔画级结构，无需手工标注数据，实现跨语言泛化。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型和多模态大语言模型在处理象形文字时存在结构盲区：LLMs将字符视为文本标记，MLLMs将其视为原始像素网格，两者都无法建模字符笔画的内在逻辑。现有结构分析方法通常是特定于文字的且劳动密集型。

Method: 提出Hieroglyphic Stroke Analyzer (HieroSA)框架，将现代表意文字和古代象形文字字符图像转换为标准化坐标空间中的显式、可解释的线段表示，实现跨语言泛化。

Result: 大量实验表明，HieroSA能有效捕捉字符内部结构和语义，无需语言特定的先验知识。实验结果凸显了该工作作为字形分析工具的潜力，有助于更深入理解象形文字。

Conclusion: HieroSA为多模态大语言模型提供了一种通用框架，能够自动分析字符的笔画级结构，克服了现有方法的局限性，为象形文字研究提供了新的分析工具。

Abstract: Hieroglyphs, as logographic writing systems, encode rich semantic and cultural information within their internal structural composition. Yet, current advanced Large Language Models (LLMs) and Multimodal LLMs (MLLMs) usually remain structurally blind to this information. LLMs process characters as textual tokens, while MLLMs additionally view them as raw pixel grids. Both fall short to model the underlying logic of character strokes. Furthermore, existing structural analysis methods are often script-specific and labor-intensive. In this paper, we propose Hieroglyphic Stroke Analyzer (HieroSA), a novel and generalizable framework that enables MLLMs to automatically derive stroke-level structures from character bitmaps without handcrafted data. It transforms modern logographic and ancient hieroglyphs character images into explicit, interpretable line-segment representations in a normalized coordinate space, allowing for cross-lingual generalization. Extensive experiments demonstrate that HieroSA effectively captures character-internal structures and semantics, bypassing the need for language-specific priors. Experimental results highlight the potential of our work as a graphematics analysis tool for a deeper understanding of hieroglyphic scripts. View our code at https://github.com/THUNLP-MT/HieroSA.

</details>


### [17] [GaussianSwap: Animatable Video Face Swapping with 3D Gaussian Splatting](https://arxiv.org/abs/2601.05511)
*Xuan Cheng,Jiahao Rao,Chengyang Li,Wenhao Wang,Weilin Chen,Lvqing Yang*

Main category: cs.CV

TL;DR: GaussianSwap：基于3D高斯溅射的视频人脸交换框架，从目标视频构建3D人脸化身，将源图像身份转移到化身中，实现高保真、可动画控制的人脸交换


<details>
  <summary>Details</summary>
Motivation: 传统视频人脸交换方法仅限于生成基于像素的面部表示，结果只是无结构像素集合，缺乏动画或交互操控能力。需要从传统像素级视频生成转向创建高保真、可交互的人脸化身

Method: 1) 预处理目标视频提取FLAME参数、相机姿态和分割掩码；2) 将3D高斯溅射绑定到跨帧的FLAME模型，实现动态面部控制；3) 提出复合身份嵌入（基于三种最先进人脸识别模型）用于化身微调；4) 将交换后的面部化身渲染到背景帧上

Result: 实验结果表明，GaussianSwap在身份保持、视觉清晰度和时间一致性方面表现优异，同时实现了以往无法达到的交互应用

Conclusion: GaussianSwap实现了从传统像素级视频生成到高保真、可交互人脸化身创建的范式转变，为视频人脸交换提供了新的解决方案

Abstract: We introduce GaussianSwap, a novel video face swapping framework that constructs a 3D Gaussian Splatting based face avatar from a target video while transferring identity from a source image to the avatar. Conventional video swapping frameworks are limited to generating facial representations in pixel-based formats. The resulting swapped faces exist merely as a set of unstructured pixels without any capacity for animation or interactive manipulation. Our work introduces a paradigm shift from conventional pixel-based video generation to the creation of high-fidelity avatar with swapped faces. The framework first preprocesses target video to extract FLAME parameters, camera poses and segmentation masks, and then rigs 3D Gaussian splats to the FLAME model across frames, enabling dynamic facial control. To ensure identity preserving, we propose an compound identity embedding constructed from three state-of-the-art face recognition models for avatar finetuning. Finally, we render the face-swapped avatar on the background frames to obtain the face-swapped video. Experimental results demonstrate that GaussianSwap achieves superior identity preservation, visual clarity and temporal consistency, while enabling previously unattainable interactive applications.

</details>


### [18] [SAS-VPReID: A Scale-Adaptive Framework with Shape Priors for Video-based Person Re-Identification at Extreme Far Distances](https://arxiv.org/abs/2601.05535)
*Qiwei Yang,Pingping Zhang,Yuhao Wang,Zijing Gong*

Main category: cs.CV

TL;DR: 提出SAS-VPReID框架，通过记忆增强视觉主干、多粒度时序建模和先验正则化形状动态三个模块，解决远距离视频行人重识别中的分辨率退化、视角变化和外观噪声问题。


<details>
  <summary>Details</summary>
Motivation: 远距离视频行人重识别面临严重分辨率退化、剧烈视角变化和不可避免的外观噪声等挑战，需要更鲁棒的特征表示方法。

Method: 提出三模块框架：1) 记忆增强视觉主干(MEVB)，结合CLIP视觉编码器和多代理记忆；2) 多粒度时序建模(MGTM)，构建多时间粒度序列并自适应强调运动线索；3) 先验正则化形状动态(PRSD)，捕捉身体结构动态。

Result: 在VReID-XFD基准测试中验证了各模块有效性，最终框架在VReID-XFD挑战排行榜上排名第一。

Conclusion: SAS-VPReID框架通过三个互补模块获得了更具判别性的特征表示，有效解决了远距离视频行人重识别的挑战。

Abstract: Video-based Person Re-IDentification (VPReID) aims to retrieve the same person from videos captured by non-overlapping cameras. At extreme far distances, VPReID is highly challenging due to severe resolution degradation, drastic viewpoint variation and inevitable appearance noise. To address these issues, we propose a Scale-Adaptive framework with Shape Priors for VPReID, named SAS-VPReID. The framework is built upon three complementary modules. First, we deploy a Memory-Enhanced Visual Backbone (MEVB) to extract discriminative feature representations, which leverages the CLIP vision encoder and multi-proxy memory. Second, we propose a Multi-Granularity Temporal Modeling (MGTM) to construct sequences at multiple temporal granularities and adaptively emphasize motion cues across scales. Third, we incorporate Prior-Regularized Shape Dynamics (PRSD) to capture body structure dynamics. With these modules, our framework can obtain more discriminative feature representations. Experiments on the VReID-XFD benchmark demonstrate the effectiveness of each module and our final framework ranks the first on the VReID-XFD challenge leaderboard. The source code is available at https://github.com/YangQiWei3/SAS-VPReID.

</details>


### [19] [DIFF-MF: A Difference-Driven Channel-Spatial State Space Model for Multi-Modal Image Fusion](https://arxiv.org/abs/2601.05538)
*Yiming Sun,Zifan Ye,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: DIFF-MF：基于差异驱动的通道-空间状态空间模型的多模态图像融合方法，通过特征差异图引导特征提取，在通道和空间维度进行融合，在保持线性计算复杂度的同时提升融合质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于状态空间模型的图像融合方法存在两个问题：要么过度优先红外强度而牺牲可见光细节，要么保留可见光结构但降低热目标显著性。需要克服这些挑战，实现更好的多模态图像融合。

Method: 提出DIFF-MF模型：1）利用模态间特征差异图引导特征提取；2）通道维度：通过交叉注意力双状态空间建模的通道交换模块增强通道交互；3）空间维度：通过跨模态状态空间扫描的空间交换模块实现全面空间融合；4）在保持线性计算复杂度的同时捕获全局依赖关系。

Result: 在驾驶场景和低空无人机数据集上的实验表明，该方法在视觉质量和定量评估方面均优于现有方法。

Conclusion: DIFF-MF通过差异驱动的通道-空间状态空间模型，有效解决了现有方法在红外和可见光特征平衡方面的问题，实现了高质量的多模态图像融合。

Abstract: Multi-modal image fusion aims to integrate complementary information from multiple source images to produce high-quality fused images with enriched content. Although existing approaches based on state space model have achieved satisfied performance with high computational efficiency, they tend to either over-prioritize infrared intensity at the cost of visible details, or conversely, preserve visible structure while diminishing thermal target salience. To overcome these challenges, we propose DIFF-MF, a novel difference-driven channel-spatial state space model for multi-modal image fusion. Our approach leverages feature discrepancy maps between modalities to guide feature extraction, followed by a fusion process across both channel and spatial dimensions. In the channel dimension, a channel-exchange module enhances channel-wise interaction through cross-attention dual state space modeling, enabling adaptive feature reweighting. In the spatial dimension, a spatial-exchange module employs cross-modal state space scanning to achieve comprehensive spatial fusion. By efficiently capturing global dependencies while maintaining linear computational complexity, DIFF-MF effectively integrates complementary multi-modal features. Experimental results on the driving scenarios and low-altitude UAV datasets demonstrate that our method outperforms existing approaches in both visual quality and quantitative evaluation.

</details>


### [20] [MoGen: A Unified Collaborative Framework for Controllable Multi-Object Image Generation](https://arxiv.org/abs/2601.05546)
*Yanfeng Li,Yue Sun,Keren Fu,Sio-Kei Im,Xiaoming Liu,Guangtao Zhai,Xiaohong Liu,Tao Tan*

Main category: cs.CV

TL;DR: MoGen提出了一种用户友好的多目标图像生成方法，通过区域语义锚定模块和自适应多模态引导模块，实现了对多目标数量和属性的精确控制，无需依赖固定的外部控制信号。


<details>
  <summary>Details</summary>
Motivation: 现有多目标图像生成方法难以实现语言描述与图像区域的精确对齐，经常导致目标数量不一致和属性混淆。主流方法依赖外部控制信号来约束空间布局和属性，但这种强依赖性使得输入格式僵化，无法适应用户不同的资源条件和约束需求。

Method: 1. 设计区域语义锚定（RSA）模块，在生成过程中将语言描述中的短语单元精确锚定到对应的图像区域，实现遵循多目标数量规格的文本到图像生成。2. 引入自适应多模态引导（AMG）模块，自适应解析和整合多源控制信号的组合，形成结构化意图，指导对场景布局和目标属性的选择性约束。

Result: 实验结果表明，MoGen在生成质量、数量一致性和细粒度控制方面显著优于现有方法，同时展现出更好的可访问性和控制灵活性。

Conclusion: MoGen通过创新的区域语义锚定和自适应多模态引导机制，解决了多目标图像生成中的对齐和控制问题，提供了一种更灵活、更精确的生成方法。

Abstract: Existing multi-object image generation methods face difficulties in achieving precise alignment between localized image generation regions and their corresponding semantics based on language descriptions, frequently resulting in inconsistent object quantities and attribute aliasing. To mitigate this limitation, mainstream approaches typically rely on external control signals to explicitly constrain the spatial layout, local semantic and visual attributes of images. However, this strong dependency makes the input format rigid, rendering it incompatible with the heterogeneous resource conditions of users and diverse constraint requirements. To address these challenges, we propose MoGen, a user-friendly multi-object image generation method. First, we design a Regional Semantic Anchor (RSA) module that precisely anchors phrase units in language descriptions to their corresponding image regions during the generation process, enabling text-to-image generation that follows quantity specifications for multiple objects. Building upon this foundation, we further introduce an Adaptive Multi-modal Guidance (AMG) module, which adaptively parses and integrates various combinations of multi-source control signals to formulate corresponding structured intent. This intent subsequently guides selective constraints on scene layouts and object attributes, achieving dynamic fine-grained control. Experimental results demonstrate that MoGen significantly outperforms existing methods in generation quality, quantity consistency, and fine-grained control, while exhibiting superior accessibility and control flexibility. Code is available at: https://github.com/Tear-kitty/MoGen/tree/master.

</details>


### [21] [VIB-Probe: Detecting and Mitigating Hallucinations in Vision-Language Models via Variational Information Bottleneck](https://arxiv.org/abs/2601.05547)
*Feiran Zhang,Yixin Wu,Zhenghua Wang,Xiaohua Wang,Changze Lv,Xuanjing Huang,Xiaoqing Zheng*

Main category: cs.CV

TL;DR: 提出VIB-Probe框架，利用变分信息瓶颈理论检测和缓解视觉语言模型的幻觉问题，通过分析注意力头并过滤语义噪声，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型存在幻觉问题（生成文本偏离视觉内容），现有检测方法主要依赖输出logits或外部验证工具，忽略了内部机制分析。

Method: 提出VIB-Probe框架：1）利用变分信息瓶颈理论提取跨层和跨头的判别模式，过滤语义噪声；2）通过VIB探针梯度识别对幻觉有强因果影响的注意力头；3）提出推理时干预策略缓解幻觉。

Result: 在多个基准测试上的广泛实验表明，VIB-Probe在幻觉检测和缓解两方面均显著优于现有基线方法。

Conclusion: 通过分析内部注意力机制并应用信息瓶颈原理，可以有效检测和缓解视觉语言模型的幻觉问题，为理解模型内部工作机制提供了新视角。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal tasks, but remain susceptible to hallucinations, where generated text deviates from the underlying visual content. Existing hallucination detection methods primarily rely on output logits or external verification tools, often overlooking their internal mechanisms. In this work, we investigate the outputs of internal attention heads, postulating that specific heads carry the primary signals for truthful generation.However, directly probing these high-dimensional states is challenging due to the entanglement of visual-linguistic syntax and noise. To address this, we propose VIB-Probe, a novel hallucination detection and mitigation framework leveraging the Variational Information Bottleneck (VIB) theory. Our method extracts discriminative patterns across layers and heads while filtering out semantic nuisances through the information bottleneck principle. Furthermore, by leveraging the gradients of our VIB probe, we identify attention heads with strong causal influence on hallucinations and introduce an inference-time intervention strategy for hallucination mitigation. Extensive experiments across diverse benchmarks demonstrate that VIB-Probe significantly outperforms existing baselines in both settings. Our code will be made publicly available.

</details>


### [22] [One Language-Free Foundation Model Is Enough for Universal Vision Anomaly Detection](https://arxiv.org/abs/2601.05552)
*Bin-Bin Gao,Chengjie Wang*

Main category: cs.CV

TL;DR: 提出UniADet框架，通过解耦分类与分割任务权重，实现简单高效的通用视觉异常检测，无需复杂提示工程或适配模块，在14个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言基础模型的异常检测方法存在复杂提示工程、精心设计的适配模块和训练策略等问题，限制了方法的灵活性和通用性。需要重新思考视觉语言模型在异常检测中的基本机制。

Method: 发现语言编码器在异常检测中主要用于生成决策权重，提出完全解耦分类和分割任务，以及解耦跨层级特征的方法。仅学习解耦权重（0.002M可学习参数），无需复杂适配。

Result: 在14个真实世界异常检测基准（工业和医疗领域）中，大幅超越现有零样本/少样本方法，首次超越全样本方法。参数高效，仅0.002M可学习参数。

Conclusion: UniADet展示了简单解耦方法的有效性，为通用视觉异常检测提供了新的思路，具有高度简单、参数高效、通用性强和效果显著的特点。

Abstract: Universal visual anomaly detection (AD) aims to identify anomaly images and segment anomaly regions towards open and dynamic scenarios, following zero- and few-shot paradigms without any dataset-specific fine-tuning. We have witnessed significant progress in widely use of visual-language foundational models in recent approaches. However, current methods often struggle with complex prompt engineering, elaborate adaptation modules, and challenging training strategies, ultimately limiting their flexibility and generality. To address these issues, this paper rethinks the fundamental mechanism behind visual-language models for AD and presents an embarrassingly simple, general, and effective framework for Universal vision Anomaly Detection (UniADet). Specifically, we first find language encoder is used to derive decision weights for anomaly classification and segmentation, and then demonstrate that it is unnecessary for universal AD. Second, we propose an embarrassingly simple method to completely decouple classification and segmentation, and decouple cross-level features, i.e., learning independent weights for different tasks and hierarchical features. UniADet is highly simple (learning only decoupled weights), parameter-efficient (only 0.002M learnable parameters), general (adapting a variety of foundation models), and effective (surpassing state-of-the-art zero-/few-shot by a large margin and even full-shot AD methods for the first time) on 14 real-world AD benchmarks covering both industrial and medical domains. We will make the code and model of UniADet available at https://github.com/gaobb/UniADet.

</details>


### [23] [Semi-Supervised Facial Expression Recognition based on Dynamic Threshold and Negative Learning](https://arxiv.org/abs/2601.05556)
*Zhongpeng Cai,Jun Yu,Wei Xu,Tianyu Liu,Jianqing Sun,Jiaen Liang*

Main category: cs.CV

TL;DR: 提出基于动态阈值调整和选择性负学习的半监督面部表情识别算法，在RAF-DB和AffectNet数据集上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 面部表情识别是人机交互和情感计算的关键任务，但获取大量标注数据成本高昂，因此需要设计能够充分利用标注和未标注数据的半监督算法

Method: 1) 特征提取阶段采用局部注意力增强和特征图随机丢弃策略；2) 动态阈值调整方法适应半监督学习框架需求；3) 选择性负学习策略从互补标签中挖掘有用表情信息，充分利用低置信度未标注样本

Result: 在RAF-DB和AffectNet数据集上取得了最先进的性能，即使不使用完整数据集也超越了全监督方法，证明了方法的有效性

Conclusion: 提出的DTA-SNL半监督面部表情识别算法通过动态阈值调整和选择性负学习策略，有效解决了标注数据稀缺问题，在表情识别任务中表现出色

Abstract: Facial expression recognition is a key task in human-computer interaction and affective computing. However, acquiring a large amount of labeled facial expression data is often costly. Therefore, it is particularly important to design a semi-supervised facial expression recognition algorithm that makes full use of both labeled and unlabeled data. In this paper, we propose a semi-supervised facial expression recognition algorithm based on Dynamic Threshold Adjustment (DTA) and Selective Negative Learning (SNL). Initially, we designed strategies for local attention enhancement and random dropout of feature maps during feature extraction, which strengthen the representation of local features while ensuring the model does not overfit to any specific local area. Furthermore, this study introduces a dynamic thresholding method to adapt to the requirements of the semi-supervised learning framework for facial expression recognition tasks, and through a selective negative learning strategy, it fully utilizes unlabeled samples with low confidence by mining useful expression information from complementary labels, achieving impressive results. We have achieved state-of-the-art performance on the RAF-DB and AffectNet datasets. Our method surpasses fully supervised methods even without using the entire dataset, which proves the effectiveness of our approach.

</details>


### [24] [What's Left Unsaid? Detecting and Correcting Misleading Omissions in Multimodal News Previews](https://arxiv.org/abs/2601.05563)
*Fanxiao Li,Jiaying Wu,Tingchao Fu,Dayang Li,Herun Wan,Wei Zhou,Min-Yen Kan*

Main category: cs.CV

TL;DR: 该论文针对社交媒体新闻预览（图片-标题对）中的"解释偏移"问题，开发了MM-Misleading基准测试，提出了OMGuard解决方案，通过多模态误导性检测和修正来减少选择性省略关键背景导致的误导。


<details>
  <summary>Details</summary>
Motivation: 社交媒体新闻预览即使事实正确，也可能通过选择性省略关键背景导致"解释偏移"，使读者形成与完整文章不同的判断。这种隐性危害比明确错误信息更难检测，但目前研究不足。

Method: 开发了多阶段流程，解构并模拟预览与完整背景的理解差异，构建MM-Misleading基准。提出OMGuard框架，包含：1）解释感知微调改进多模态误导性检测；2）基于推理的误导内容修正，使用明确推理指导标题重写。

Result: OMGuard将8B模型的检测准确率提升至与235B LVLM相当，并在端到端修正方面表现显著更强。分析显示误导性通常源于局部叙事变化（如缺失背景）而非全局框架改变，并识别出仅文本修正失败的图像驱动场景。

Conclusion: 选择性省略导致的解释偏移是社交媒体新闻预览中的重要隐性危害。OMGuard框架有效提升了多模态误导性检测和修正能力，揭示了视觉干预的必要性，为解决这一未充分探索的问题提供了系统方法。

Abstract: Even when factually correct, social-media news previews (image-headline pairs) can induce interpretation drift: by selectively omitting crucial context, they lead readers to form judgments that diverge from what the full article conveys. This covert harm is harder to detect than explicit misinformation yet remains underexplored. To address this gap, we develop a multi-stage pipeline that disentangles and simulates preview-based versus context-based understanding, enabling construction of the MM-Misleading benchmark. Using this benchmark, we systematically evaluate open-source LVLMs and uncover pronounced blind spots to omission-based misleadingness detection. We further propose OMGuard, which integrates (1) Interpretation-Aware Fine-Tuning, which used to improve multimodal misleadingness detection and (2) Rationale-Guided Misleading Content Correction, which uses explicit rationales to guide headline rewriting and reduce misleading impressions. Experiments show that OMGuard lifts an 8B model's detection accuracy to match a 235B LVLM and delivers markedly stronger end-to-end correction. Further analysis reveals that misleadingness typically stems from local narrative shifts (e.g., missing background) rather than global frame changes, and identifies image-driven scenarios where text-only correction fails, highlighting the necessity of visual interventions.

</details>


### [25] [Towards Generalized Multi-Image Editing for Unified Multimodal Models](https://arxiv.org/abs/2601.05572)
*Pengcheng Xu,Peng Tang,Donghao Luo,Xiaobin Hu,Weichu Cui,Qingdong He,Zhennan Chen,Jiangning Zhang,Charles Ling,Boyu Wang*

Main category: cs.CV

TL;DR: 提出一个可扩展的多图像编辑框架，通过可学习的潜在分离器和正弦索引编码来区分图像身份，实现多图像编辑中的视觉一致性和细节区分。


<details>
  <summary>Details</summary>
Motivation: 现有的统一多模态模型在多图像编辑中存在视觉一致性不足和视觉线索歧义的问题，难以在多个输入图像之间保持细节一致性。

Method: 1) 可学习的潜在分离器：在潜在空间中显式区分每个参考图像，实现准确解耦的条件控制；2) 正弦索引编码：为同一图像的视觉标记分配连续的正弦索引嵌入，提供显式的图像身份标识，支持可变数量输入的泛化和外推。

Result: 实验表明，在多样化的多图像编辑任务中，相比现有基线方法，在语义一致性、视觉保真度和跨图像整合方面都有明显改进。

Conclusion: 提出的框架通过显式区分图像身份，有效解决了多图像编辑中的视觉一致性和泛化问题，为统一多模态模型的多图像编辑能力提供了可扩展的解决方案。

Abstract: Unified Multimodal Models (UMMs) integrate multimodal understanding and generation, yet they are limited to maintaining visual consistency and disambiguating visual cues when referencing details across multiple input images. In this work, we propose a scalable multi-image editing framework for UMMs that explicitly distinguishes image identities and generalizes to variable input counts. Algorithmically, we introduce two innovations: 1) The learnable latent separators explicitly differentiate each reference image in the latent space, enabling accurate and disentangled conditioning. 2) The sinusoidal index encoding assigns visual tokens from the same image a continuous sinusoidal index embedding, which provides explicit image identity while allowing generalization and extrapolation on a variable number of inputs. To facilitate training and evaluation, we establish a high-fidelity benchmark using an inverse dataset construction methodology to guarantee artifact-free, achievable outputs. Experiments show clear improvements in semantic consistency, visual fidelity, and cross-image integration over prior baselines on diverse multi-image editing tasks, validating our advantages on consistency and generalization ability.

</details>


### [26] [Orient Anything V2: Unifying Orientation and Rotation Understanding](https://arxiv.org/abs/2601.05573)
*Zehan Wang,Ziang Zhang,Jiayang Xu,Jialei Wang,Tianyu Pang,Chao Du,HengShuang Zhao,Zhou Zhao*

Main category: cs.CV

TL;DR: Orient Anything V2是一个增强的基础模型，用于从单张或成对图像中统一理解物体的3D方向和旋转，相比V1版本扩展了处理旋转对称性和直接估计相对旋转的能力。


<details>
  <summary>Details</summary>
Motivation: 现有方向估计方法通常假设物体具有单一独特的前向面，无法处理具有不同旋转对称性的物体，且不能直接估计相对旋转。需要更通用的方向理解模型来支持多样化的下游任务。

Method: 通过四个关键创新：1) 使用生成模型合成可扩展的3D资产；2) 模型在环的高效标注系统识别0到N个有效前向面；3) 对称感知的周期分布拟合目标；4) 直接预测相对物体旋转的多帧架构。

Result: 在11个广泛使用的基准测试中，Orient Anything V2在方向估计、6DoF姿态估计和物体对称性识别方面实现了最先进的零样本性能，展现出强大的泛化能力。

Conclusion: 该模型显著扩展了方向估计在多样化下游任务中的适用性，为统一的物体方向和旋转理解提供了强大的基础模型。

Abstract: This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.

</details>


### [27] [Generalizable and Adaptive Continual Learning Framework for AI-generated Image Detection](https://arxiv.org/abs/2601.05580)
*Hanyi Wang,Jun Lan,Yaoyu Kang,Huijia Zhu,Weiqiang Wang,Zhuosheng Zhang,Shilin Wang*

Main category: cs.CV

TL;DR: 提出三阶段领域持续学习框架，用于持续适应不断演化的AI生成图像检测，在27个生成模型基准上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: AI生成图像的恶意滥用和广泛传播威胁在线信息真实性。现有检测方法难以泛化到未见过的生成模型，且生成技术快速演化使检测模型面临失效风险。

Method: 三阶段框架：1) 参数高效微调构建可迁移离线检测模型；2) 持续学习整合未见数据流，使用渐进复杂度数据增强链和K-FAC方法缓解灾难性遗忘；3) 基于线性模式连接的线性插值策略捕捉生成模型共性。

Result: 离线检测器在平均精度上超越最佳基线+5.51%；持续学习策略平均准确率达92.20%，优于现有方法。

Conclusion: 提出的三阶段持续学习框架能有效适应不断演化的生成模型，为AI生成图像检测提供了鲁棒且可扩展的解决方案。

Abstract: The malicious misuse and widespread dissemination of AI-generated images pose a significant threat to the authenticity of online information. Current detection methods often struggle to generalize to unseen generative models, and the rapid evolution of generative techniques continuously exacerbates this challenge. Without adaptability, detection models risk becoming ineffective in real-world applications. To address this critical issue, we propose a novel three-stage domain continual learning framework designed for continuous adaptation to evolving generative models. In the first stage, we employ a strategic parameter-efficient fine-tuning approach to develop a transferable offline detection model with strong generalization capabilities. Building upon this foundation, the second stage integrates unseen data streams into a continual learning process. To efficiently learn from limited samples of novel generated models and mitigate overfitting, we design a data augmentation chain with progressively increasing complexity. Furthermore, we leverage the Kronecker-Factored Approximate Curvature (K-FAC) method to approximate the Hessian and alleviate catastrophic forgetting. Finally, the third stage utilizes a linear interpolation strategy based on Linear Mode Connectivity, effectively capturing commonalities across diverse generative models and further enhancing overall performance. We establish a comprehensive benchmark of 27 generative models, including GANs, deepfakes, and diffusion models, chronologically structured up to August 2024 to simulate real-world scenarios. Extensive experiments demonstrate that our initial offline detectors surpass the leading baseline by +5.51% in terms of mean average precision. Our continual learning strategy achieves an average accuracy of 92.20%, outperforming state-of-the-art methods.

</details>


### [28] [GS-DMSR: Dynamic Sensitive Multi-scale Manifold Enhancement for Accelerated High-Quality 3D Gaussian Splatting](https://arxiv.org/abs/2601.05584)
*Nengbo Lu,Minghua Pan,Shaohua Sun,Yizhou Liang*

Main category: cs.CV

TL;DR: GS-DMSR方法通过自适应梯度聚焦和多尺度流形增强模块，在3D动态场景重建中平衡收敛速度与渲染质量，实现96 FPS的高帧率渲染。


<details>
  <summary>Details</summary>
Motivation: 3D动态场景重建领域长期面临模型收敛速度与渲染质量难以平衡的挑战，特别是在复杂动态运动场景的高精度建模中，这一问题尤为突出。

Method: 提出GS-DMSR方法：1）通过量化分析高斯属性动态演化过程实现自适应梯度聚焦，动态识别高斯模型运动状态差异并应用差异化优化策略；2）集成多尺度流形增强模块，利用隐式非线性解码器和显式变形场的协同优化来增强复杂变形场景建模效率。

Result: 在合成数据集上达到96 FPS的帧率，同时有效降低了存储开销和训练时间。

Conclusion: 该方法成功解决了3D动态场景重建中收敛速度与渲染质量的平衡问题，为复杂动态运动场景的高效建模提供了有效解决方案。

Abstract: In the field of 3D dynamic scene reconstruction, how to balance model convergence rate and rendering quality has long been a critical challenge that urgently needs to be addressed, particularly in high-precision modeling of scenes with complex dynamic motions. To tackle this issue, this study proposes the GS-DMSR method. By quantitatively analyzing the dynamic evolution process of Gaussian attributes, this mechanism achieves adaptive gradient focusing, enabling it to dynamically identify significant differences in the motion states of Gaussian models. It then applies differentiated optimization strategies to Gaussian models with varying degrees of significance, thereby significantly improving the model convergence rate. Additionally, this research integrates a multi-scale manifold enhancement module, which leverages the collaborative optimization of an implicit nonlinear decoder and an explicit deformation field to enhance the modeling efficiency for complex deformation scenes. Experimental results demonstrate that this method achieves a frame rate of up to 96 FPS on synthetic datasets, while effectively reducing both storage overhead and training time.Our code and data are available at https://anonymous.4open.science/r/GS-DMSR-2212.

</details>


### [29] [Quantifying and Inducing Shape Bias in CNNs via Max-Pool Dilation](https://arxiv.org/abs/2601.05599)
*Takito Sawada,Akinori Iwata,Masahiro Okuda*

Main category: cs.CV

TL;DR: 提出一个量化数据集形状-纹理平衡的指标，并基于此设计一种计算高效的形状偏置适应方法，通过调整最大池化扩张率来提升形状主导数据集的分类性能。


<details>
  <summary>Details</summary>
Motivation: CNN存在强烈的纹理偏置，偏好局部模式而非全局形状信息。虽然已有研究提出形状偏置模型来缓解此问题，但缺乏量化指标来确定哪些数据集真正需要这种修改。

Method: 1) 提出数据驱动的形状-纹理平衡度量指标：计算每个图像亮度通道与其L0平滑版本之间的结构相似性指数(SSIM)；2) 基于该指标，引入计算高效的适应方法：通过修改最大池化操作的扩张率来促进形状偏置，同时保持卷积权重冻结。

Result: 实验结果表明，该方法在形状主导数据集上持续提升分类准确率，特别是在数据量少的场景下，只需训练最后的分类层，而无需完整微调。

Conclusion: 提出的形状-纹理平衡度量指标能有效识别需要形状偏置的数据集，而基于最大池化扩张率调整的适应方法为形状主导数据提供了计算高效的解决方案。

Abstract: Convolutional Neural Networks (CNNs) are known to exhibit a strong texture bias, favoring local patterns over global shape information--a tendency inherent to their convolutional architecture. While this bias is beneficial for texture-rich natural images, it often degrades performance on shape-dominant data such as illustrations and sketches. Although prior work has proposed shape-biased models to mitigate this issue, these approaches lack a quantitative metric for identifying which datasets would actually benefit from such modifications. To address this gap, we propose a data-driven metric that quantifies the shape-texture balance of a dataset by computing the Structural Similarity Index (SSIM) between each image's luminance channel and its L0-smoothed counterpart. Building on this metric, we further introduce a computationally efficient adaptation method that promotes shape bias by modifying the dilation of max-pooling operations while keeping convolutional weights frozen. Experimental results show that this approach consistently improves classification accuracy on shape-dominant datasets, particularly in low-data regimes where full fine-tuning is impractical, requiring training only the final classification layer.

</details>


### [30] [SceneAlign: Aligning Multimodal Reasoning to Scene Graphs in Complex Visual Scenes](https://arxiv.org/abs/2601.05600)
*Chuhan Wang,Xintong Li,Jennifer Yuntong Zhang,Junda Wu,Chengkai Huang,Lina Yao,Julian McAuley,Jingbo Shang*

Main category: cs.CV

TL;DR: SceneAlign：利用场景图进行可控结构干预的多模态推理对齐框架，通过构造对比样本提升视觉推理的忠实性和准确性


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在复杂视觉场景中经常出现推理不忠实的问题，包括幻觉实体、错误接地关系、跳过步骤和过度指定推理。现有基于偏好的方法依赖文本扰动或答案条件化理由，允许模型利用语言先验绕过视觉接地，无法解决这一挑战。

Method: 提出SceneAlign框架，利用场景图作为结构化视觉信息进行可控结构干预。通过识别推理关键节点，采用四种针对典型接地失败的扰动策略，构造语言上合理但视觉事实不准确的硬负样本理由。使用这些对比样本进行直接偏好优化，引导模型进行细粒度、结构忠实的推理。

Result: 在七个视觉推理基准测试中，SceneAlign持续提高了答案准确性和推理忠实性，证明了接地感知对齐对多模态推理的有效性。

Conclusion: SceneAlign通过场景图的结构化干预和对比学习，有效解决了多模态推理中的忠实性问题，为视觉接地感知的对齐提供了有效框架。

Abstract: Multimodal large language models often struggle with faithful reasoning in complex visual scenes, where intricate entities and relations require precise visual grounding at each step. This reasoning unfaithfulness frequently manifests as hallucinated entities, mis-grounded relations, skipped steps, and over-specified reasoning. Existing preference-based approaches, typically relying on textual perturbations or answer-conditioned rationales, fail to address this challenge as they allow models to exploit language priors to bypass visual grounding. To address this, we propose SceneAlign, a framework that leverages scene graphs as structured visual information to perform controllable structural interventions. By identifying reasoning-critical nodes and perturbing them through four targeted strategies that mimic typical grounding failures, SceneAlign constructs hard negative rationales that remain linguistically plausible but are grounded in inaccurate visual facts. These contrastive pairs are used in Direct Preference Optimization to steer models toward fine-grained, structure-faithful reasoning. Across seven visual reasoning benchmarks, SceneAlign consistently improves answer accuracy and reasoning faithfulness, highlighting the effectiveness of grounding-aware alignment for multimodal reasoning.

</details>


### [31] [Learning Geometric Invariance for Gait Recognition](https://arxiv.org/abs/2601.05604)
*Zengbin Wang,Junjie Li,Saihui Hou,Xu Liu,Chunshui Cao,Yongzhen Huang,Muyi Sun,Siye Wang,Man Zhang*

Main category: cs.CV

TL;DR: 该论文提出RRS-Gait框架，将不同步态条件的变化视为几何变换的组合，通过实现反射、旋转、缩放三种几何变换的不变性学习来提升步态识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别方法主要依赖数据驱动方式隐式学习不同步态条件下的共同特征，但较少显式探索不同步态条件之间的内在关系。作者认为不同步态条件的变化可以近似看作几何变换的组合，通过实现几何不变性自然获得身份不变性。

Method: 提出RRS-Gait框架，探索反射、旋转、缩放三种常见几何变换。首先根据具体几何变换灵活调整卷积核以实现近似特征等变性，然后将这三种等变性感知特征分别输入全局池化操作进行最终的不变性学习。

Result: 在四个流行步态数据集（Gait3D、GREW、CCPG、SUSTech1K）上的大量实验显示，该方法在各种步态条件下都表现出优越性能。

Conclusion: 通过将步态变化建模为几何变换并实现几何不变性学习，能够有效提升步态识别性能，为步态识别提供了新的视角和方法框架。

Abstract: The goal of gait recognition is to extract identity-invariant features of an individual under various gait conditions, e.g., cross-view and cross-clothing. Most gait models strive to implicitly learn the common traits across different gait conditions in a data-driven manner to pull different gait conditions closer for recognition. However, relatively few studies have explicitly explored the inherent relations between different gait conditions. For this purpose, we attempt to establish connections among different gait conditions and propose a new perspective to achieve gait recognition: variations in different gait conditions can be approximately viewed as a combination of geometric transformations. In this case, all we need is to determine the types of geometric transformations and achieve geometric invariance, then identity invariance naturally follows. As an initial attempt, we explore three common geometric transformations (i.e., Reflect, Rotate, and Scale) and design a $\mathcal{R}$eflect-$\mathcal{R}$otate-$\mathcal{S}$cale invariance learning framework, named ${\mathcal{RRS}}$-Gait. Specifically, it first flexibly adjusts the convolution kernel based on the specific geometric transformations to achieve approximate feature equivariance. Then these three equivariant-aware features are respectively fed into a global pooling operation for final invariance-aware learning. Extensive experiments on four popular gait datasets (Gait3D, GREW, CCPG, SUSTech1K) show superior performance across various gait conditions.

</details>


### [32] [LatentVLA: Efficient Vision-Language Models for Autonomous Driving via Latent Action Prediction](https://arxiv.org/abs/2601.05611)
*Chengen Xie,Bin Sun,Tianyu Li,Junjie Wu,Zhihui Hao,XianPeng Lang,Hongyang Li*

Main category: cs.CV

TL;DR: LatentVLA：一种无需语言标注的自监督潜在动作预测框架，解决VLA模型在自动驾驶中的数值不精确、语言依赖和计算效率问题，实现实时高效驾驶


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶模型在常见场景表现良好，但在长尾罕见场景中表现不佳；现有VLA模型存在数值不精确、依赖语言标注引入语言偏见、多步推理计算效率低等问题，难以实时部署

Method: 提出LatentVLA框架：1）使用自监督潜在动作预测训练VLA模型，无需语言标注；2）通过知识蒸馏将VLA模型的泛化能力转移到高效视觉网络中；3）从无标注轨迹数据中学习丰富驾驶表示

Result: 在NAVSIM基准测试中达到92.4的PDMS分数，创下新SOTA；在nuScenes基准测试中展示强大的零样本泛化能力；实现鲁棒性能和实时效率

Conclusion: LatentVLA通过消除语言依赖和计算瓶颈，为自动驾驶提供了既具有强大泛化能力又满足实时部署要求的高效解决方案

Abstract: End-to-end autonomous driving models trained on largescale datasets perform well in common scenarios but struggle with rare, long-tail situations due to limited scenario diversity. Recent Vision-Language-Action (VLA) models leverage broad knowledge from pre-trained visionlanguage models to address this limitation, yet face critical challenges: (1) numerical imprecision in trajectory prediction due to discrete tokenization, (2) heavy reliance on language annotations that introduce linguistic bias and annotation burden, and (3) computational inefficiency from multi-step chain-of-thought reasoning hinders real-time deployment. We propose LatentVLA, a novel framework that employs self-supervised latent action prediction to train VLA models without language annotations, eliminating linguistic bias while learning rich driving representations from unlabeled trajectory data. Through knowledge distillation, LatentVLA transfers the generalization capabilities of VLA models to efficient vision-based networks, achieving both robust performance and real-time efficiency. LatentVLA establishes a new state-of-the-art on the NAVSIM benchmark with a PDMS score of 92.4 and demonstrates strong zeroshot generalization on the nuScenes benchmark.

</details>


### [33] [Compressing image encoders via latent distillation](https://arxiv.org/abs/2601.05639)
*Caroline Mazini Rodrigues,Nicolas Keriven,Thomas Maugey*

Main category: cs.CV

TL;DR: 提出一种通过简化知识蒸馏来压缩深度学习图像压缩模型编码器的方法，在保持重建质量的同时减少模型复杂度和计算需求


<details>
  <summary>Details</summary>
Motivation: 深度学习图像压缩模型虽然重建质量高，但通常复杂、重量级，需要大量训练数据和计算资源，在硬件受限的应用中面临实际限制

Method: 使用简化的知识蒸馏策略，用较少数据和较短训练时间来近似原始模型的潜在空间，从重量级编码器生成轻量级编码器

Result: 在两个不同架构上评估，实验表明该方法比使用原始损失训练轻量级编码器更好地保持重建质量和统计保真度

Conclusion: 该方法为资源受限环境提供了一种实用的解决方案，能够在保持性能的同时显著减少模型复杂度和计算需求

Abstract: Deep learning models for image compression often face practical limitations in hardware-constrained applications. Although these models achieve high-quality reconstructions, they are typically complex, heavyweight, and require substantial training data and computational resources. We propose a methodology to partially compress these networks by reducing the size of their encoders. Our approach uses a simplified knowledge distillation strategy to approximate the latent space of the original models with less data and shorter training, yielding lightweight encoders from heavyweight ones. We evaluate the resulting lightweight encoders across two different architectures on the image compression task. Experiments show that our method preserves reconstruction quality and statistical fidelity better than training lightweight encoders with the original loss, making it practical for resource-limited environments.

</details>


### [34] [SGDrive: Scene-to-Goal Hierarchical World Cognition for Autonomous Driving](https://arxiv.org/abs/2601.05640)
*Jingyu Li,Junjie Wu,Dongnan Hu,Xiangkai Huang,Bin Sun,Zhihui Hao,Xianpeng Lang,Xiatian Zhu,Li Zhang*

Main category: cs.CV

TL;DR: SGDrive提出了一种基于视觉语言模型的分层结构化表示学习框架，通过场景-智能体-目标层次结构增强自动驾驶规划能力，在NAVSIM基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型是通用模型，缺乏对自动驾驶特定3D时空推理的专业理解，难以建立捕捉几何关系、场景上下文和运动模式的结构化时空表示，这限制了它们在复杂驾驶场景中的规划能力。

Method: SGDrive基于预训练的VLM骨干网络，将驾驶理解分解为场景-智能体-目标层次结构，模仿人类驾驶认知：首先感知整体环境（场景上下文），然后关注安全关键智能体及其行为，最后制定短期目标再执行动作。这种分层分解提供了通用VLM所缺乏的结构化时空表示。

Result: 在NAVSIM基准测试中，SGDrive在仅使用摄像头的方法中，在PDMS和EPDMS指标上都达到了最先进的性能，验证了分层知识结构化在将通用VLM适应自动驾驶方面的有效性。

Conclusion: 通过将驾驶理解分解为场景-智能体-目标层次结构，SGDrive成功地为通用视觉语言模型提供了结构化时空表示，显著提升了自动驾驶规划能力，证明了分层知识结构化在专业领域适应中的价值。

Abstract: Recent end-to-end autonomous driving approaches have leveraged Vision-Language Models (VLMs) to enhance planning capabilities in complex driving scenarios. However, VLMs are inherently trained as generalist models, lacking specialized understanding of driving-specific reasoning in 3D space and time. When applied to autonomous driving, these models struggle to establish structured spatial-temporal representations that capture geometric relationships, scene context, and motion patterns critical for safe trajectory planning. To address these limitations, we propose SGDrive, a novel framework that explicitly structures the VLM's representation learning around driving-specific knowledge hierarchies. Built upon a pre-trained VLM backbone, SGDrive decomposes driving understanding into a scene-agent-goal hierarchy that mirrors human driving cognition: drivers first perceive the overall environment (scene context), then attend to safety-critical agents and their behaviors, and finally formulate short-term goals before executing actions. This hierarchical decomposition provides the structured spatial-temporal representation that generalist VLMs lack, integrating multi-level information into a compact yet comprehensive format for trajectory planning. Extensive experiments on the NAVSIM benchmark demonstrate that SGDrive achieves state-of-the-art performance among camera-only methods on both PDMS and EPDMS, validating the effectiveness of hierarchical knowledge structuring for adapting generalist VLMs to autonomous driving.

</details>


### [35] [SketchVL: Policy Optimization via Fine-Grained Credit Assignment for Chart Understanding and More](https://arxiv.org/abs/2601.05688)
*Muye Huang,Lingling Zhang,Yifei Li,Yaqiang Wu,Jun Liu*

Main category: cs.CV

TL;DR: SketchVL是一个通过FinePO算法进行细粒度信用分配的多模态大语言模型，通过在图像上绘制中间推理步骤作为标记，并使用FinePRM对每个绘图动作评分，显著提升了图表理解等复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在图表理解等复杂视觉推理任务中面临挑战，特别是基于强化学习的模型存在信用分配问题，无法区分单个生成响应中正确与错误的推理步骤。

Method: 提出SketchVL模型，采用FinePO强化学习算法，通过在图像上绘制中间推理步骤作为标记并反馈给自身，创建多步推理过程。FinePO利用FinePRM对轨迹中的每个绘图动作评分，实现细粒度信用分配。

Result: SketchVL在图表数据集、自然图像数据集和数学任务上平均性能提升7.23%，能够将步骤级行为与FinePRM对齐，为训练强大推理模型提供了新方向。

Conclusion: SketchVL通过细粒度信用分配机制有效解决了MLLMs在复杂推理任务中的信用分配问题，显著提升了模型性能，为训练具有强大推理能力的多模态模型提供了有前景的新方法。

Abstract: Charts are high-density visual carriers of complex data and medium for information extraction and analysis. Due to the need for precise and complex visual reasoning, automated chart understanding poses a significant challenge to existing Multimodal Large Language Models (MLLMs). Many MLLMs trained with reinforcement learning (RL) face the challenge of credit assignment. Their advantage estimation, typically performed at the trajectory level, cannot distinguish between correct and incorrect reasoning steps within a single generated response. To address this limitation, we introduce SketchVL, a novel MLLM that optimized with FinePO, a new RL algorithm designed for fine-grained credit assignment within each trajectory. SketchVL's methodology involves drawing its intermediate reasoning steps as markers on the image and feeding the annotated image back to itself, creating a robust, multi-step reasoning process. During training, the FinePO algorithm leverages a Fine-grained Process Reward Model (FinePRM) to score each drawing action within a trajectory, thereby precisely assigning credit for each step. This mechanism allows FinePO to more strongly reward correct tokens when a trajectory is globally successful, and more heavily penalize incorrect tokens when the trajectory is globally suboptimal, thus achieving fine-grained reinforcement signals. Experiments show that SketchVL learns to align its step-level behavior with the FinePRM, achieving an average performance gain of 7.23\% over its base model across chart datasets, natural image datasets, and mathematics, providing a promising new direction for training powerful reasoning models.

</details>


### [36] [Rotate Your Character: Revisiting Video Diffusion Models for High-Quality 3D Character Generation](https://arxiv.org/abs/2601.05722)
*Jin Wang,Jianxiang Lu,Comi Chen,Guangzheng Xu,Haoyu Yang,Peng Chen,Na Zhang,Yifan Xu,Longhuang Wu,Shuai Shao,Qinglin Lu,Ping Luo*

Main category: cs.CV

TL;DR: RCM是一个先进的图像到视频扩散框架，专门用于高质量的新视角合成和3D角色生成，能够处理复杂姿态的角色并将其转换到规范姿态，支持高分辨率轨道视频生成和多视图条件输入。


<details>
  <summary>Details</summary>
Motivation: 从单张图像生成高质量的3D角色在数字内容创作中仍然是一个重大挑战，特别是由于复杂的身体姿态和自遮挡问题。现有方法在处理这些复杂情况时存在局限性。

Method: RCM是一个图像到视频扩散框架，具有以下关键技术：1) 将任意复杂姿态的角色转换到规范姿态；2) 支持1024x1024分辨率的高分辨率轨道视频生成；3) 提供可控的观察位置；4) 支持最多4张输入图像的多视图条件输入。

Result: 广泛的实验表明，RCM在新视角合成和3D生成质量方面均优于最先进的方法。

Conclusion: RCM通过将复杂姿态的角色转换到规范姿态，实现了高质量的新视角合成和3D角色生成，为数字内容创作提供了有效的解决方案。

Abstract: Generating high-quality 3D characters from single images remains a significant challenge in digital content creation, particularly due to complex body poses and self-occlusion. In this paper, we present RCM (Rotate your Character Model), an advanced image-to-video diffusion framework tailored for high-quality novel view synthesis (NVS) and 3D character generation. Compared to existing diffusion-based approaches, RCM offers several key advantages: (1) transferring characters with any complex poses into a canonical pose, enabling consistent novel view synthesis across the entire viewing orbit, (2) high-resolution orbital video generation at 1024x1024 resolution, (3) controllable observation positions given different initial camera poses, and (4) multi-view conditioning supporting up to 4 input images, accommodating diverse user scenarios. Extensive experiments demonstrate that RCM outperforms state-of-the-art methods in both novel view synthesis and 3D generation quality.

</details>


### [37] [TAGRPO: Boosting GRPO on Image-to-Video Generation with Direct Trajectory Alignment](https://arxiv.org/abs/2601.05729)
*Jin Wang,Jianxiang Lu,Guangzheng Xu,Comi Chen,Haoyu Yang,Linqing Wang,Peng Chen,Mingtao Chen,Zhichao Hu,Longhuang Wu,Shuai Shao,Qinglin Lu,Ping Luo*

Main category: cs.CV

TL;DR: TAGRPO：一种基于对比学习的图像到视频生成后训练框架，通过改进GRPO在中间潜在空间的应用，显著提升I2V生成质量


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在文本到图像/视频生成中有效，但直接应用于图像到视频(I2V)模型时奖励提升不一致，需要更鲁棒的优化框架

Method: 提出TAGRPO框架：1) 利用相同初始噪声生成的rollout视频作为优化指导；2) 在中间潜在空间应用新型GRPO损失，对齐高奖励轨迹并远离低奖励轨迹；3) 引入rollout视频记忆库增强多样性和降低计算开销

Result: TAGRPO在I2V生成中显著优于DanceGRPO，实现了明显的质量提升

Conclusion: TAGRPO为I2V模型提供了一个简单而有效的后训练框架，通过对比学习思想和中间潜在空间优化解决了现有GRPO方法的局限性

Abstract: Recent studies have demonstrated the efficacy of integrating Group Relative Policy Optimization (GRPO) into flow matching models, particularly for text-to-image and text-to-video generation. However, we find that directly applying these techniques to image-to-video (I2V) models often fails to yield consistent reward improvements. To address this limitation, we present TAGRPO, a robust post-training framework for I2V models inspired by contrastive learning. Our approach is grounded in the observation that rollout videos generated from identical initial noise provide superior guidance for optimization. Leveraging this insight, we propose a novel GRPO loss applied to intermediate latents, encouraging direct alignment with high-reward trajectories while maximizing distance from low-reward counterparts. Furthermore, we introduce a memory bank for rollout videos to enhance diversity and reduce computational overhead. Despite its simplicity, TAGRPO achieves significant improvements over DanceGRPO in I2V generation.

</details>


### [38] [FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time](https://arxiv.org/abs/2601.05738)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 提出实时跟踪SLAM系统，结合3D高斯泼溅进行特征丰富的地图构建，通过密集特征栅格化实现超越RGB-D输入的语义理解，在保持实时性能的同时提升跟踪稳定性和地图保真度。


<details>
  <summary>Details</summary>
Motivation: 传统语义SLAM方法通常依赖于预定义的类别标签，限制了其应用范围。需要一种能够实现开放集分割、支持新下游任务，同时提升跟踪和地图构建性能的实时SLAM系统。

Method: 将密集特征栅格化整合到新颖视图合成中，与视觉基础模型对齐。使用3D高斯泼溅进行特征丰富的地图构建，实现实时相机跟踪与逼真地图生成的统一。

Result: 在标准基准测试中实现实时跟踪，与最先进系统性能相当，同时提升跟踪稳定性和地图保真度。相比固定集SLAM基线，姿态误差降低9%，地图精度提高8%。

Conclusion: 实时特征嵌入SLAM不仅能够支持新的下游应用，还能提升底层跟踪和地图构建子系统的性能，提供与离线3DGS模型相当的语义和语言掩码结果，同时保持最先进的跟踪、深度和RGB渲染性能。

Abstract: We present a real-time tracking SLAM system that unifies efficient camera tracking with photorealistic feature-enriched mapping using 3D Gaussian Splatting (3DGS). Our main contribution is integrating dense feature rasterization into the novel-view synthesis, aligned with a visual foundation model. This yields strong semantics, going beyond basic RGB-D input, aiding both tracking and mapping accuracy. Unlike previous semantic SLAM approaches (which embed pre-defined class labels) FeatureSLAM enables entirely new downstream tasks via free-viewpoint, open-set segmentation. Across standard benchmarks, our method achieves real-time tracking, on par with state-of-the-art systems while improving tracking stability and map fidelity without prohibitive compute. Quantitatively, we obtain 9\% lower pose error and 8\% higher mapping accuracy compared to recent fixed-set SLAM baselines. Our results confirm that real-time feature-embedded SLAM, is not only valuable for enabling new downstream applications. It also improves the performance of the underlying tracking and mapping subsystems, providing semantic and language masking results that are on-par with offline 3DGS models, alongside state-of-the-art tracking, depth and RGB rendering.

</details>


### [39] [ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers](https://arxiv.org/abs/2601.05741)
*Guray Ozgur,Eduarda Caldeira,Tahar Chettaoui,Jan Niklas Kolf,Marco Huber,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 提出ViTNT-FIQA方法，通过分析Vision Transformer中间层块间patch嵌入的演化稳定性来评估人脸图像质量，无需训练且仅需单次前向传播。


<details>
  <summary>Details</summary>
Motivation: 现有FIQA方法主要利用最终层表示，而无需训练的方法需要多次前向传播或反向传播，计算成本高。需要一种更高效的无训练方法。

Method: 通过计算Vision Transformer连续块间L2归一化patch嵌入的欧氏距离，测量特征演化稳定性。高质量图像的特征轨迹稳定，而退化图像则变化剧烈。将块间距离聚合成图像级质量分数。

Result: 在八个基准测试（LFW、AgeDB-30、CFP-FP、CALFW、Adience、CPLFW、XQLFW、IJB-C）上表现出与最先进方法竞争的性能，同时保持计算效率。

Conclusion: ViTNT-FIQA是一种高效的无训练FIQA方法，仅需单次前向传播，无需反向传播或架构修改，可直接应用于任何预训练的ViT人脸识别模型。

Abstract: Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only a single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model.

</details>


### [40] [FlyPose: Towards Robust Human Pose Estimation From Aerial Views](https://arxiv.org/abs/2601.05747)
*Hassaan Farooq,Marvin Brenner,Peter St\ütz*

Main category: cs.CV

TL;DR: FlyPose是一个轻量级自上而下的人体姿态估计流水线，专门针对无人机视角，通过多数据集训练提升检测和姿态估计性能，并在无人机上实现实时部署。


<details>
  <summary>Details</summary>
Motivation: 无人机在人类密集环境中的应用（如包裹递送、交通监控等）需要准确感知人体姿态和动作，但现有方法面临低分辨率、陡峭视角和遮挡等挑战，特别是需要实时可行模型的应用场景。

Method: 训练和部署FlyPose——一个轻量级自上而下的人体姿态估计流水线，通过多数据集训练提升性能，并在Jetson Orin AGX开发套件上实现约20毫秒的推理延迟，最终在四旋翼无人机上进行飞行实验部署。

Result: 在Manipal-UAV、VisDrone、HIT-UAV等测试集上平均提升6.8 mAP的人体检测性能；在具有挑战性的UAV-Human数据集上提升16.3 mAP的2D人体姿态估计性能；同时发布了FlyPose-104数据集。

Conclusion: FlyPose能够有效解决无人机视角下人体姿态估计的挑战，实现实时性能并成功在无人机上部署，为无人机在人类密集环境中的安全可靠操作提供了重要技术支持。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.

</details>


### [41] [Adaptive Disentangled Representation Learning for Incomplete Multi-View Multi-Label Classification](https://arxiv.org/abs/2601.05785)
*Quanjiang Li,Zhiming Liu,Tianxiang Xu,Tingjin Luo,Chenping Hou*

Main category: cs.CV

TL;DR: ADRL是一种自适应解耦表示学习方法，用于解决多视图多标签学习中的特征缺失和标注不完整问题，通过特征补全、表示解耦和标签语义建模实现鲁棒学习。


<details>
  <summary>Details</summary>
Motivation: 多视图多标签学习经常面临特征缺失和标注不完整的问题，这源于数据采集困难和监督成本高昂。现有方法在特征恢复、表示解耦和标签语义建模方面存在局限性，需要一种更有效的解决方案。

Method: ADRL采用自适应解耦表示学习方法：1) 通过跨模态特征级亲和性传播实现鲁棒视图补全；2) 利用随机掩码策略增强重建效果；3) 通过类别级关联传播优化分布参数；4) 基于互信息的目标函数促进共享表示一致性；5) 原型特定特征选择和伪标签生成；6) 利用伪标签空间结构指导视图融合。

Result: 在公共数据集和实际应用中的广泛实验表明，ADRL表现出优越的性能，能够有效处理特征缺失和标注不完整问题。

Conclusion: ADRL通过创新的自适应解耦表示学习框架，成功解决了多视图多标签学习中的关键挑战，在特征补全、表示解耦和标签语义建模方面取得了显著进展。

Abstract: Multi-view multi-label learning frequently suffers from simultaneous feature absence and incomplete annotations, due to challenges in data acquisition and cost-intensive supervision. To tackle the complex yet highly practical problem while overcoming the existing limitations of feature recovery, representation disentanglement, and label semantics modeling, we propose an Adaptive Disentangled Representation Learning method (ADRL). ADRL achieves robust view completion by propagating feature-level affinity across modalities with neighborhood awareness, and reinforces reconstruction effectiveness by leveraging a stochastic masking strategy. Through disseminating category-level association across label distributions, ADRL refines distribution parameters for capturing interdependent label prototypes. Besides, we formulate a mutual-information-based objective to promote consistency among shared representations and suppress information overlap between view-specific representation and other modalities. Theoretically, we derive the tractable bounds to train the dual-channel network. Moreover, ADRL performs prototype-specific feature selection by enabling independent interactions between label embeddings and view representations, accompanied by the generation of pseudo-labels for each category. The structural characteristics of the pseudo-label space are then exploited to guide a discriminative trade-off during view fusion. Finally, extensive experiments on public datasets and real-world applications demonstrate the superior performance of ADRL.

</details>


### [42] [SceneFoundry: Generating Interactive Infinite 3D Worlds](https://arxiv.org/abs/2601.05810)
*ChunTeng Chen,YiChen Hsu,YiWen Liu,WeiFang Sun,TsaiChing Ni,ChunYi Lee,Min Sun,YuanFu Yang*

Main category: cs.CV

TL;DR: SceneFoundry：一个语言引导的扩散框架，用于生成公寓规模的3D世界，包含功能性的铰接式家具和语义多样的布局，用于机器人训练。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法往往无法捕捉真实室内环境的功能复杂性，特别是包含对机器人物体操作和导航至关重要的可移动部件的铰接式物体。需要能够自动生成大规模、交互式且物理真实的3D环境来推进机器人学习和具身智能。

Method: 使用语言引导的扩散框架：1）LLM模块根据自然语言提示控制楼层布局生成；2）基于扩散的后验采样从大规模3D资源库中高效填充场景中的铰接式资产；3）使用可微分指导函数确保物理可用性，包括调节物体数量、防止铰接碰撞、保持足够的机器人可行走空间。

Result: 实验表明，该框架能够生成结构有效、语义连贯且功能交互的环境，适用于多种场景类型和条件，支持可扩展的具身AI研究。

Conclusion: SceneFoundry能够生成公寓规模的3D世界，包含功能性的铰接式家具和语义多样的布局，为机器人训练提供物理真实且功能复杂的交互环境，推动了具身AI研究的可扩展性。

Abstract: The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research.

</details>


### [43] [Boosting Latent Diffusion Models via Disentangled Representation Alignment](https://arxiv.org/abs/2601.05823)
*John Page,Xuesong Niu,Kai Wu,Kun Gai*

Main category: cs.CV

TL;DR: Send-VAE是一种语义解缠的变分自编码器，通过将VAE潜在空间与预训练视觉基础模型的语义层次对齐，实现属性级信息的结构化编码，从而提升图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用相同的对齐目标同时优化VAE和LDM，忽略了它们根本不同的表示需求。LDM需要保留高层语义概念的潜在表示，而VAE应该擅长语义解缠，能够以结构化方式编码属性级信息。

Method: 提出语义解缠VAE（Send-VAE），通过非线性映射网络将VAE潜在空间与预训练视觉基础模型的语义层次对齐，弥合属性级解缠与高层语义之间的差距，为VAE学习提供有效指导。

Result: 通过属性预测任务的线性探测评估语义解缠，显示与改进的生成性能强相关。使用Send-VAE训练基于流的变换器SiTs，显著加速训练并在ImageNet 256x256上达到SOTA的FID分数（有/无分类器自由引导分别为1.21和1.75）。

Conclusion: Send-VAE通过专门针对语义解缠优化VAE，解决了VAE和LDM表示需求不匹配的问题，在图像生成任务中实现了显著的性能提升和训练加速。

Abstract: Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.

</details>


### [44] [GeoSurDepth: Spatial Geometry-Consistent Self-Supervised Depth Estimation for Surround-View Cameras](https://arxiv.org/abs/2601.05839)
*Weimin Liu,Wenjun Wang,Joshua H. Meng*

Main category: cs.CV

TL;DR: GeoSurDepth：利用几何一致性作为环视深度估计主要线索的自监督框架，通过基础模型提供几何先验，结合2D-3D提升和自适应运动学习，在DDAD和nuScenes数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有环视深度估计方法主要关注光度级约束，很少显式利用单目和环视设置中固有的丰富几何结构。需要更充分利用几何一致性来提高深度估计的准确性和鲁棒性。

Method: 1) 利用基础模型作为伪几何先验和特征表示增强工具，保持空间3D空间中的表面法线一致性，正则化2D中的物体和纹理一致性深度估计；2) 提出新颖的视图合成流程，通过空间扭曲重建密集深度实现2D-3D提升，增加跨时间、空间和时空上下文的光度监督；3) 提出自适应联合运动学习策略，自适应强调信息性空间几何线索以改进运动推理。

Result: 在DDAD和nuScenes数据集上的大量实验表明，GeoSurDepth实现了最先进的性能，验证了方法的有效性。

Conclusion: 该框架强调了利用几何相干性和一致性对于鲁棒的自监督多视图深度估计的重要性，为激光雷达传感器提供了有竞争力的替代方案。

Abstract: Accurate surround-view depth estimation provides a competitive alternative to laser-based sensors and is essential for 3D scene understanding in autonomous driving. While prior studies have proposed various approaches that primarily focus on enforcing cross-view constraints at the photometric level, few explicitly exploit the rich geometric structure inherent in both monocular and surround-view setting. In this work, we propose GeoSurDepth, a framework that leverages geometry consistency as the primary cue for surround-view depth estimation. Concretely, we utilize foundation models as a pseudo geometry prior and feature representation enhancement tool to guide the network to maintain surface normal consistency in spatial 3D space and regularize object- and texture-consistent depth estimation in 2D. In addition, we introduce a novel view synthesis pipeline where 2D-3D lifting is achieved with dense depth reconstructed via spatial warping, encouraging additional photometric supervision across temporal, spatial, and spatial-temporal contexts, and compensating for the limitations of single-view image reconstruction. Finally, a newly-proposed adaptive joint motion learning strategy enables the network to adaptively emphasize informative spatial geometry cues for improved motion reasoning. Extensive experiments on DDAD and nuScenes demonstrate that GeoSurDepth achieves state-of-the-art performance, validating the effectiveness of our approach. Our framework highlights the importance of exploiting geometry coherence and consistency for robust self-supervised multi-view depth estimation.

</details>


### [45] [Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals](https://arxiv.org/abs/2601.05848)
*Nate Gillman,Yinghua Zhou,Zitian Tang,Evan Luo,Arjan Chakravarthy,Daksh Aggarwal,Michael Freeman,Charles Herrmann,Chen Sun*

Main category: cs.CV

TL;DR: Goal Force：通过力向量和中间动力学定义目标的视频生成框架，将视频生成基于基本物理交互，实现零样本泛化到复杂现实场景


<details>
  <summary>Details</summary>
Motivation: 现有世界模型难以精确指定目标：文本指令过于抽象无法捕捉物理细节，目标图像对于动态任务通常不可行。需要一种更符合人类物理任务概念化的目标定义方式。

Method: 引入Goal Force框架，允许用户通过明确的力向量和中间动力学定义目标。在合成因果原语数据集上训练视频生成模型，教模型在时间和空间中传播力。

Result: 尽管在简单物理数据上训练，模型展现出对复杂现实场景的显著零样本泛化能力，包括工具操作和多对象因果链。模型可作为隐式神经物理模拟器。

Conclusion: 通过将视频生成基于基本物理交互，模型可以作为隐式神经物理模拟器，实现精确的物理感知规划，无需依赖外部引擎。这为机器人学和规划提供了新方法。

Abstract: Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.

</details>


### [46] [Kidney Cancer Detection Using 3D-Based Latent Diffusion Models](https://arxiv.org/abs/2601.05852)
*Jen Dusseljee,Sarah de Boer,Alessa Hering*

Main category: cs.CV

TL;DR: 提出基于潜在扩散的3D肾脏异常检测方法，结合DDPM、DDIM和VQ-GAN，使用病例级伪标签进行弱监督，直接处理图像体积而非切片。


<details>
  <summary>Details</summary>
Motivation: 解决传统肾脏异常检测方法需要大量标注数据的问题，探索在弱监督条件下使用生成模型进行3D医学图像异常检测的可行性。

Method: 结合DDPM、DDIM和VQ-GAN构建潜在扩散管道，直接处理3D图像体积，仅使用病例级伪标签进行弱监督训练。

Result: 方法在对比增强腹部CT上可行，但性能尚未达到全监督基线水平，揭示了改进重建保真度和病变定位的关键方向。

Conclusion: 该研究为复杂腹部解剖的标注高效生成建模迈出了重要一步，展示了3D潜在扩散在弱监督异常检测中的潜力。

Abstract: In this work, we present a novel latent diffusion-based pipeline for 3D kidney anomaly detection on contrast-enhanced abdominal CT. The method combines Denoising Diffusion Probabilistic Models (DDPMs), Denoising Diffusion Implicit Models (DDIMs), and Vector-Quantized Generative Adversarial Networks (VQ-GANs). Unlike prior slice-wise approaches, our method operates directly on an image volume and leverages weak supervision with only case-level pseudo-labels. We benchmark our approach against state-of-the-art supervised segmentation and detection models. This study demonstrates the feasibility and promise of 3D latent diffusion for weakly supervised anomaly detection. While the current results do not yet match supervised baselines, they reveal key directions for improving reconstruction fidelity and lesion localization. Our findings provide an important step toward annotation-efficient, generative modeling of complex abdominal anatomy.

</details>


### [47] [LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting](https://arxiv.org/abs/2601.05853)
*Yinghan Xu,John Dingliana*

Main category: cs.CV

TL;DR: 提出LayerGS框架，将任意姿态的人体分解为可动画的多层3D人体化身，分离身体和服装，使用2D高斯表示层，通过扩散模型修复遮挡区域，实现高质量渲染和虚拟试穿。


<details>
  <summary>Details</summary>
Motivation: 传统单层重建方法将服装锁定在一个身份上，而现有的多层方法在处理遮挡区域时存在困难。需要一种能够准确分离身体和服装、处理遮挡区域、支持动画和虚拟试穿的多层3D人体重建方法。

Method: 1) 使用2D高斯集合编码每个层，实现精确几何和逼真渲染；2) 通过预训练的2D扩散模型和分数蒸馏采样(SDS)修复遮挡区域；3) 三阶段训练策略：首先通过单层重建恢复粗略规范服装，然后进行多层训练联合恢复内层身体和外层服装细节。

Result: 在两个3D人体基准数据集(4D-Dress, Thuman2.0)上，该方法在渲染质量、层分解和重组方面优于现有最先进方法，能够在新视角和姿态下实现逼真的虚拟试穿。

Conclusion: LayerGS框架成功解决了多层3D人体重建中的遮挡问题，实现了高质量的身体-服装分离，为沉浸式应用中的高保真3D人体资产创建提供了实用解决方案。

Abstract: We propose a novel framework for decomposing arbitrarily posed humans into animatable multi-layered 3D human avatars, separating the body and garments. Conventional single-layer reconstruction methods lock clothing to one identity, while prior multi-layer approaches struggle with occluded regions. We overcome both limitations by encoding each layer as a set of 2D Gaussians for accurate geometry and photorealistic rendering, and inpainting hidden regions with a pretrained 2D diffusion model via score-distillation sampling (SDS). Our three-stage training strategy first reconstructs the coarse canonical garment via single-layer reconstruction, followed by multi-layer training to jointly recover the inner-layer body and outer-layer garment details. Experiments on two 3D human benchmark datasets (4D-Dress, Thuman2.0) show that our approach achieves better rendering quality and layer decomposition and recomposition than the previous state-of-the-art, enabling realistic virtual try-on under novel viewpoints and poses, and advancing practical creation of high-fidelity 3D human assets for immersive applications. Our code is available at https://github.com/RockyXu66/LayerGS

</details>


### [48] [Bidirectional Channel-selective Semantic Interaction for Semi-Supervised Medical Segmentation](https://arxiv.org/abs/2601.05855)
*Kaiwen Huang,Yizhe Zhang,Yi Zhou,Tianyang Xu,Tao Zhou*

Main category: cs.CV

TL;DR: 提出BCSI框架用于半监督医学图像分割，通过语义-空间扰动、通道选择路由器和双向通道交互解决现有方法错误累积和模型复杂性问题


<details>
  <summary>Details</summary>
Motivation: 现有半监督医学图像分割方法（如mean teacher和双流一致性学习）存在错误累积、模型结构复杂以及忽略标注与未标注数据流间交互的问题

Method: 1. 语义-空间扰动(SSP)：使用两种强增强扰动数据，利用弱增强的伪标签进行无监督学习，并对两种强增强的预测进行一致性约束；2. 通道选择路由器(CR)：动态选择最相关通道进行信息交换，减少噪声；3. 双向通道交互(BCI)：补充语义信息并增强重要通道表示

Result: 在多个3D医学数据集上的实验结果表明，该方法优于现有的半监督方法

Conclusion: 提出的BCSI框架通过创新的扰动机制、通道选择和双向交互策略，有效解决了半监督医学图像分割中的关键问题，取得了优越性能

Abstract: Semi-supervised medical image segmentation is an effective method for addressing scenarios with limited labeled data. Existing methods mainly rely on frameworks such as mean teacher and dual-stream consistency learning. These approaches often face issues like error accumulation and model structural complexity, while also neglecting the interaction between labeled and unlabeled data streams. To overcome these challenges, we propose a Bidirectional Channel-selective Semantic Interaction~(BCSI) framework for semi-supervised medical image segmentation. First, we propose a Semantic-Spatial Perturbation~(SSP) mechanism, which disturbs the data using two strong augmentation operations and leverages unsupervised learning with pseudo-labels from weak augmentations. Additionally, we employ consistency on the predictions from the two strong augmentations to further improve model stability and robustness. Second, to reduce noise during the interaction between labeled and unlabeled data, we propose a Channel-selective Router~(CR) component, which dynamically selects the most relevant channels for information exchange. This mechanism ensures that only highly relevant features are activated, minimizing unnecessary interference. Finally, the Bidirectional Channel-wise Interaction~(BCI) strategy is employed to supplement additional semantic information and enhance the representation of important channels. Experimental results on multiple benchmarking 3D medical datasets demonstrate that the proposed method outperforms existing semi-supervised approaches.

</details>


### [49] [Phase4DFD: Multi-Domain Phase-Aware Attention for Deepfake Detection](https://arxiv.org/abs/2601.05861)
*Zhen-Xin Lin,Shang-Kuan Chen*

Main category: cs.CV

TL;DR: Phase4DFD：一种相位感知的频域深度伪造检测框架，通过可学习的注意力机制显式建模相位-幅度交互，在CIFAKE和DFFD数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法主要依赖频域幅度信息，忽视了相位信息的作用。相位不连续性通常由合成生成引入，可以提供幅度信息之外的互补检测线索

Method: 1) 将标准RGB输入与FFT幅度和LBP表示结合；2) 引入输入级相位感知注意力模块，利用相位不连续性引导模型关注最具指示性的频率模式；3) 使用高效的BNext M主干网络处理多域表示，可选通道空间注意力进行语义特征细化

Result: 在CIFAKE和DFFD数据集上，Phase4DFD超越了最先进的基于空间和频率的检测器，同时保持较低的计算开销。消融研究证实显式相位建模提供了幅度信息之外的互补非冗余信息

Conclusion: 相位信息在深度伪造检测中具有重要价值，显式建模相位-幅度交互可以显著提升检测性能，为频域检测方法提供了新的方向

Abstract: Recent deepfake detection methods have increasingly explored frequency domain representations to reveal manipulation artifacts that are difficult to detect in the spatial domain. However, most existing approaches rely primarily on spectral magnitude, implicitly under exploring the role of phase information. In this work, we propose Phase4DFD, a phase aware frequency domain deepfake detection framework that explicitly models phase magnitude interactions via a learnable attention mechanism. Our approach augments standard RGB input with Fast Fourier Transform (FFT) magnitude and local binary pattern (LBP) representations to expose subtle synthesis artifacts that remain indistinguishable under spatial analysis alone. Crucially, we introduce an input level phase aware attention module that uses phase discontinuities commonly introduced by synthetic generation to guide the model toward frequency patterns that are most indicative of manipulation before backbone feature extraction. The attended multi domain representation is processed by an efficient BNext M backbone, with optional channel spatial attention applied for semantic feature refinement. Extensive experiments on the CIFAKE and DFFD datasets demonstrate that our proposed model Phase4DFD outperforms state of the art spatial and frequency-based detectors while maintaining low computational overhead. Comprehensive ablation studies further confirm that explicit phase modeling provides complementary and non-redundant information beyond magnitude-only frequency representations.

</details>


### [50] [Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens](https://arxiv.org/abs/2601.05927)
*Yohann Perron,Vladyslav Sydorov,Christophe Pottier,Loic Landrieu*

Main category: cs.CV

TL;DR: 提出Relay Tokens方法，通过并行处理局部高分辨率和全局低分辨率图像，在保持细节的同时引入多尺度推理，显著提升超高分辨率图像分割性能


<details>
  <summary>Details</summary>
Motivation: 现有超高分辨率图像分割方法存在局限：滑动窗口会丢弃全局上下文，下采样会丢失细节。需要同时保持局部细节和全局感知能力

Method: 并行处理局部尺度（高分辨率小裁剪）和全局尺度（低分辨率大裁剪），使用少量可学习的relay tokens在两条分支间聚合和传播特征，可直接集成到标准Transformer骨干网络

Result: 在Archaeoscape、URUR、Gleason三个超高分辨率分割基准和Cityscapes数据集上均取得一致提升，相对mIoU最高提升15%，仅增加不到2%参数

Conclusion: Relay Tokens方法简单有效，通过显式多尺度推理同时保持局部细节和全局感知，显著提升超高分辨率图像分割性能

Abstract: Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .

</details>


### [51] [Performance of a Deep Learning-Based Segmentation Model for Pancreatic Tumors on Public Endoscopic Ultrasound Datasets](https://arxiv.org/abs/2601.05937)
*Pankaj Gupta,Priya Mudgil,Niharika Dutta,Kartik Bose,Nitish Kumar,Anupam Kumar,Jimil Shah,Vaneet Jearth,Jayanta Samanta,Vishal Sharma,Harshal Mandavdhare,Surinder Rana,Saroj K Sinha,Usha Dutta*

Main category: cs.CV

TL;DR: 本研究开发了一种基于Vision Transformer的深度学习分割模型，用于胰腺肿瘤的超声内镜图像分割，在外部验证中取得了DSC 0.657的良好性能。


<details>
  <summary>Details</summary>
Motivation: 胰腺癌是一种侵袭性极强的癌症，生存率低。超声内镜是关键的诊断方式，但其效果受操作者主观性限制。需要开发自动化的深度学习模型来提高胰腺肿瘤分割的客观性和准确性。

Method: 使用基于Vision Transformer的USFM框架构建分割模型，在17,367张EUS图像（来自两个公共数据集）上进行5折交叉验证训练。在独立的350张EUS图像测试集上进行外部验证，图像经过放射科医生手动分割标注。预处理包括灰度转换、裁剪和调整至512x512像素。

Result: 5折交叉验证中，模型平均DSC为0.651±0.738，IoU为0.579±0.658，敏感性69.8%，特异性98.8%，准确率97.5%。外部验证中，DSC为0.657（95% CI: 0.634-0.769），IoU为0.614（95% CI: 0.590-0.689），敏感性71.8%，特异性97.7%。但9.7%的病例出现错误的多重预测。

Conclusion: 基于Vision Transformer的模型在EUS图像胰腺肿瘤分割中表现出色。然而，数据集异质性和有限的外部验证表明需要进一步优化、标准化和前瞻性研究。

Abstract: Background: Pancreatic cancer is one of the most aggressive cancers, with poor survival rates. Endoscopic ultrasound (EUS) is a key diagnostic modality, but its effectiveness is constrained by operator subjectivity. This study evaluates a Vision Transformer-based deep learning segmentation model for pancreatic tumors. Methods: A segmentation model using the USFM framework with a Vision Transformer backbone was trained and validated with 17,367 EUS images (from two public datasets) in 5-fold cross-validation. The model was tested on an independent dataset of 350 EUS images from another public dataset, manually segmented by radiologists. Preprocessing included grayscale conversion, cropping, and resizing to 512x512 pixels. Metrics included Dice similarity coefficient (DSC), intersection over union (IoU), sensitivity, specificity, and accuracy. Results: In 5-fold cross-validation, the model achieved a mean DSC of 0.651 +/- 0.738, IoU of 0.579 +/- 0.658, sensitivity of 69.8%, specificity of 98.8%, and accuracy of 97.5%. For the external validation set, the model achieved a DSC of 0.657 (95% CI: 0.634-0.769), IoU of 0.614 (95% CI: 0.590-0.689), sensitivity of 71.8%, and specificity of 97.7%. Results were consistent, but 9.7% of cases exhibited erroneous multiple predictions. Conclusions: The Vision Transformer-based model demonstrated strong performance for pancreatic tumor segmentation in EUS images. However, dataset heterogeneity and limited external validation highlight the need for further refinement, standardization, and prospective studies.

</details>


### [52] [Context-Aware Decoding for Faithful Vision-Language Generation](https://arxiv.org/abs/2601.05939)
*Mehrdad Fazli,Bowen Wei,Ziwei Zhu*

Main category: cs.CV

TL;DR: 本文通过分析LVLM层间生成动态，发现真实token比幻觉token更早累积概率质量，提出无需训练的上下文嵌入注入方法，有效减少视觉语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在开放任务（如图像描述、视觉推理）中经常产生与视觉输入不一致的幻觉响应，这是当前LVLM的一个关键限制。需要深入理解幻觉产生的机制并提出有效的缓解策略。

Method: 使用Logit Lens分析LVLM在解码器各层构建下一个token分布的动态过程，发现"承诺深度差距"现象。基于此提出上下文嵌入注入方法，利用最后一个输入token的隐藏状态作为接地信号，在解码过程中保持视觉保真度。

Result: 在CHAIR、AMBER和MMHal-Bench基准测试中（最大token长度为512），CEI方法在三个LVLM上均优于现有最先进基线，其动态变体实现了最低的整体幻觉率。

Conclusion: 通过结合新颖的机制洞察和可扩展的干预措施，这项工作推进了LVLM中幻觉问题的缓解，为理解幻觉产生机制和开发有效缓解策略提供了新思路。

Abstract: Hallucinations, generating responses inconsistent with the visual input, remain a critical limitation of large vision-language models (LVLMs), especially in open-ended tasks such as image captioning and visual reasoning. In this work, we probe the layer-wise generation dynamics that drive hallucinations and propose a training-free mitigation strategy. Employing the Logit Lens, we examine how LVLMs construct next-token distributions across decoder layers, uncovering a pronounced commitment-depth gap: truthful tokens accumulate probability mass on their final candidates earlier than hallucinatory ones. Drawing on this discovery, we introduce Context Embedding Injection (CEI), a lightweight method that harnesses the hidden state of the last input token-the context embedding-as a grounding signal to maintain visual fidelity throughout decoding and curb hallucinations. Evaluated on the CHAIR, AMBER, and MMHal-Bench benchmarks (with a maximum token length of 512), CEI outperforms state-of-the-art baselines across three LVLMs, with its dynamic variant yielding the lowest overall hallucination rates. By integrating novel mechanistic insights with a scalable intervention, this work advances the mitigation of hallucinations in LVLMs.

</details>


### [53] [WaveRNet: Wavelet-Guided Frequency Learning for Multi-Source Domain-Generalized Retinal Vessel Segmentation](https://arxiv.org/abs/2601.05942)
*Chanchan Wang,Yuanfang Wang,Qing Xu,Guanxin Chen*

Main category: cs.CV

TL;DR: WaveRNet：基于小波引导频率学习的多源域泛化视网膜血管分割框架，通过频谱引导域调制器、频率自适应域融合和分层掩码提示细化器解决光照对比度变化下的血管细节保留问题。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割面临光照不均匀和对比度变化导致的域偏移挑战，现有SAM方法忽略频率域信息且直接上采样会丢失细血管细节，需要更鲁棒的域泛化方法。

Method: 1) 频谱引导域调制器(SDM)：小波分解与可学习域令牌结合，分离光照鲁棒的低频结构与高频血管边界；2) 频率自适应域融合(FADF)：基于小波频率相似度的智能测试时域选择与软加权融合；3) 分层掩码提示细化器(HMPR)：通过长程依赖建模实现从粗到细的细化，克服SAM上采样限制。

Result: 在四个公共视网膜数据集上采用Leave-One-Domain-Out协议进行实验，WaveRNet实现了最先进的泛化性能。

Conclusion: WaveRNet通过小波引导的频率学习框架有效解决了视网膜血管分割中的域泛化问题，在光照和对比度变化下表现出优越的鲁棒性和细血管结构保留能力。

Abstract: Domain-generalized retinal vessel segmentation is critical for automated ophthalmic diagnosis, yet faces significant challenges from domain shift induced by non-uniform illumination and varying contrast, compounded by the difficulty of preserving fine vessel structures. While the Segment Anything Model (SAM) exhibits remarkable zero-shot capabilities, existing SAM-based methods rely on simple adapter fine-tuning while overlooking frequency-domain information that encodes domain-invariant features, resulting in degraded generalization under illumination and contrast variations. Furthermore, SAM's direct upsampling inevitably loses fine vessel details. To address these limitations, we propose WaveRNet, a wavelet-guided frequency learning framework for robust multi-source domain-generalized retinal vessel segmentation. Specifically, we devise a Spectral-guided Domain Modulator (SDM) that integrates wavelet decomposition with learnable domain tokens, enabling the separation of illumination-robust low-frequency structures from high-frequency vessel boundaries while facilitating domain-specific feature generation. Furthermore, we introduce a Frequency-Adaptive Domain Fusion (FADF) module that performs intelligent test-time domain selection through wavelet-based frequency similarity and soft-weighted fusion. Finally, we present a Hierarchical Mask-Prompt Refiner (HMPR) that overcomes SAM's upsampling limitation through coarse-to-fine refinement with long-range dependency modeling. Extensive experiments under the Leave-One-Domain-Out protocol on four public retinal datasets demonstrate that WaveRNet achieves state-of-the-art generalization performance. The source code is available at https://github.com/Chanchan-Wang/WaveRNet.

</details>


### [54] [VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction](https://arxiv.org/abs/2601.05966)
*Longbin Ji,Xiaoxiong Liu,Junyuan Shang,Shuohuan Wang,Yu Sun,Hua Wu,Haifeng Wang*

Main category: cs.CV

TL;DR: VideoAR：首个大规模视觉自回归视频生成框架，通过多尺度下一帧预测和自回归建模，在保持高质量的同时显著提升计算效率，缩小了自回归与扩散模型之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成主要依赖扩散和流匹配模型，虽然质量高但计算密集且难以扩展。需要开发更高效、可扩展的视频生成方法，同时保持高质量和长期一致性。

Method: 1. 结合多尺度下一帧预测与自回归建模的VAR框架；2. 通过3D多尺度分词器高效编码时空动态；3. 提出多尺度时序RoPE、跨帧错误校正和随机帧掩码来提升长期一致性；4. 采用多阶段预训练流程逐步对齐空间和时间学习。

Result: 1. 在自回归模型中达到SOTA：UCF-101上FVD从99.5提升到88.6；2. 推理步骤减少10倍以上；3. VBench得分81.74，与规模大一个数量级的扩散模型相当；4. 显著缩小自回归与扩散模型的性能差距。

Conclusion: VideoAR为视频生成提供了一个可扩展、高效且时序一致的基础框架，证明了自回归方法在视频生成领域的潜力，为未来研究开辟了新方向。

Abstract: Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.

</details>


### [55] [Adaptive Conditional Contrast-Agnostic Deformable Image Registration with Uncertainty Estimation](https://arxiv.org/abs/2601.05981)
*Yinsong Wang,Xinzhe Luo,Siyi Du,Chen Qin*

Main category: cs.CV

TL;DR: 提出AC-CAR框架，通过随机卷积对比度增强和自适应特征调制，实现无需训练即可泛化到任意成像对比度的可变形图像配准


<details>
  <summary>Details</summary>
Motivation: 多对比度图像配准面临复杂非线性强度关系的挑战。传统方法耗时，现有学习方法泛化性有限，只能处理训练中见过的特定对比度

Method: 基于随机卷积对比度增强方案，提出自适应条件对比度无关可变形图像配准框架(AC-CAR)。包含自适应条件特征调制器(ACFM)来调节特征，对比度不变潜在正则化确保特征一致性，集成方差网络提供配准不确定性估计

Result: AC-CAR在配准精度上优于基线方法，对未见过的成像对比度表现出优越的泛化能力

Conclusion: 提出的AC-CAR框架能够实现对比度无关的图像配准，具有更好的泛化性和可靠性，代码已开源

Abstract: Deformable multi-contrast image registration is a challenging yet crucial task due to the complex, non-linear intensity relationships across different imaging contrasts. Conventional registration methods typically rely on iterative optimization of the deformation field, which is time-consuming. Although recent learning-based approaches enable fast and accurate registration during inference, their generalizability remains limited to the specific contrasts observed during training. In this work, we propose an adaptive conditional contrast-agnostic deformable image registration framework (AC-CAR) based on a random convolution-based contrast augmentation scheme. AC-CAR can generalize to arbitrary imaging contrasts without observing them during training. To encourage contrast-invariant feature learning, we propose an adaptive conditional feature modulator (ACFM) that adaptively modulates the features and the contrast-invariant latent regularization to enforce the consistency of the learned feature across different imaging contrasts. Additionally, we enable our framework to provide contrast-agnostic registration uncertainty by integrating a variance network that leverages the contrast-agnostic registration encoder to improve the trustworthiness and reliability of AC-CAR. Experimental results demonstrate that AC-CAR outperforms baseline methods in registration accuracy and exhibits superior generalization to unseen imaging contrasts. Code is available at https://github.com/Yinsong0510/AC-CAR.

</details>


### [56] [Deepfake detectors are DUMB: A benchmark to assess adversarial training robustness under transferability constraints](https://arxiv.org/abs/2601.05986)
*Adrian Serrano,Erwan Umlil,Ronan Thomas*

Main category: cs.CV

TL;DR: 该研究将DUMB/DUMBer方法扩展到深度伪造检测，评估了在迁移性和跨数据集配置下检测器对抗攻击的鲁棒性，发现对抗训练在分布内情况下增强鲁棒性，但在跨数据集配置下可能降低鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的深度伪造检测系统面临对抗性攻击威胁，现有对抗训练防御在攻击者知识有限和数据分布不匹配的真实条件下有效性尚未充分探索。

Method: 扩展DUMB/DUMBer方法到深度伪造检测，评估5个SOTA检测器(RECCE、SRM、XCeption、UCF、SPSL)在3种攻击(PGD、FGSM、FPBA)和2个数据集(FaceForensics++、Celeb-DF-V2)下的鲁棒性，分析攻击者和防御者视角在迁移性和跨数据集配置下的表现。

Result: 对抗训练策略在分布内情况下增强鲁棒性，但在跨数据集配置下可能降低鲁棒性，具体效果取决于采用的策略。不同检测器对攻击的敏感性存在差异。

Conclusion: 现实应用中需要针对具体场景的防御策略，对抗训练在真实世界条件下的有效性需要更细致的评估，不能一概而论。

Abstract: Deepfake detection systems deployed in real-world environments are subject to adversaries capable of crafting imperceptible perturbations that degrade model performance. While adversarial training is a widely adopted defense, its effectiveness under realistic conditions -- where attackers operate with limited knowledge and mismatched data distributions - remains underexplored. In this work, we extend the DUMB -- Dataset soUrces, Model architecture and Balance - and DUMBer methodology to deepfake detection. We evaluate detectors robustness against adversarial attacks under transferability constraints and cross-dataset configuration to extract real-world insights. Our study spans five state-of-the-art detectors (RECCE, SRM, XCeption, UCF, SPSL), three attacks (PGD, FGSM, FPBA), and two datasets (FaceForensics++ and Celeb-DF-V2). We analyze both attacker and defender perspectives mapping results to mismatch scenarios. Experiments show that adversarial training strategies reinforce robustness in the in-distribution cases but can also degrade it under cross-dataset configuration depending on the strategy adopted. These findings highlight the need for case-aware defense strategies in real-world applications exposed to adversarial attacks.

</details>
