<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 53]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GR3EN: Generative Relighting for 3D Environments](https://arxiv.org/abs/2601.16272)
*Xiaoyan Xing,Philipp Henzler,Junhwa Hur,Runze Li,Jonathan T. Barron,Pratul P. Srinivasan,Dor Verbin*

Main category: cs.CV

TL;DR: 提出一种通过将视频到视频重光照扩散模型的输出蒸馏到3D重建中，实现房间尺度环境3D重光照的方法，避免了困难的逆渲染问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景重光照方法需要解决欠定或病态的逆渲染问题，难以在复杂真实场景中产生高质量结果；而最近的生成式图像/视频扩散模型方法仅限于2D图像/视频重光照或单个物体的3D重光照。

Method: 通过将视频到视频重光照扩散模型的输出蒸馏到3D重建中，实现可控的房间尺度场景3D重光照，避免了直接解决困难的逆渲染问题。

Result: 在合成和真实世界数据集上验证了该方法能够忠实渲染新光照条件下的场景新视角，实现了复杂真实世界场景的3D重建重光照。

Conclusion: 该方法通过将视频重光照扩散模型蒸馏到3D重建中，成功实现了房间尺度复杂真实场景的高质量可控3D重光照，避免了传统逆渲染方法的困难。

Abstract: We present a method for relighting 3D reconstructions of large room-scale environments. Existing solutions for 3D scene relighting often require solving under-determined or ill-conditioned inverse rendering problems, and are as such unable to produce high-quality results on complex real-world scenes. Though recent progress in using generative image and video diffusion models for relighting has been promising, these techniques are either limited to 2D image and video relighting or 3D relighting of individual objects. Our approach enables controllable 3D relighting of room-scale scenes by distilling the outputs of a video-to-video relighting diffusion model into a 3D reconstruction. This side-steps the need to solve a difficult inverse rendering problem, and results in a flexible system that can relight 3D reconstructions of complex real-world scenes. We validate our approach on both synthetic and real-world datasets to show that it can faithfully render novel views of scenes under new lighting conditions.

</details>


### [2] [Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory](https://arxiv.org/abs/2601.16296)
*Dohun Lee,Chun-Hao Paul Huang,Xuelin Chen,Jong Chul Ye,Duygu Ceylan,Hyeonho Jeong*

Main category: cs.CV

TL;DR: Memory-V2V是一个增强现有视频到视频扩散模型的框架，通过显式记忆机制解决多轮视频编辑中的跨一致性维护问题，在保持任务性能的同时显著提升一致性并减少30%计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前视频编辑模型在多轮交互编辑过程中难以保持跨一致性，而真实世界的视频编辑通常是迭代过程，需要多次交互优化。现有方法缺乏有效机制来维护连续编辑之间的视觉一致性。

Method: 提出Memory-V2V框架，通过外部缓存存储先前编辑的视频，采用精确检索和动态标记化策略将当前编辑步骤条件化于先前结果。在DiT骨干网络中引入可学习的标记压缩器，压缩冗余条件标记同时保留关键视觉线索。

Result: 在视频新视角合成和文本条件长视频编辑等挑战性任务上，Memory-V2V生成的视频在跨一致性方面显著优于现有方法，同时保持或提升任务特定性能，总体计算速度提升30%。

Conclusion: Memory-V2V首次解决了多轮视频编辑中的跨一致性问题，通过显式记忆机制有效维护连续编辑间的视觉一致性，为迭代式视频编辑提供了实用解决方案。

Abstract: Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V

</details>


### [3] [FeTTL: Federated Template and Task Learning for Multi-Institutional Medical Imaging](https://arxiv.org/abs/2601.16302)
*Abhijeet Parida,Antonia Alomar,Zhifan Jiang,Pooneh Roshanitabrizi,Austin Tapp,Ziyue Xu,Syed Muhammad Anwar,Maria J. Ledesma-Carbayo,Holger R. Roth,Marius George Linguraru*

Main category: cs.CV

TL;DR: FeTTL是一个联邦学习框架，通过联合学习全局模板和任务模型来对齐多机构医疗影像数据分布，显著提升了视网膜视盘分割和病理转移分类的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗影像应用中面临数据分布偏移和异质性问题，如采集协议、扫描仪类型和患者群体的差异，导致模型性能下降。

Method: 提出Federated Template and Task Learning (FeTTL)框架，联合学习全局模板和任务模型，对齐客户端间的数据分布。

Result: 在视网膜视盘分割和病理转移分类两个任务上，FeTTL显著优于现有联邦学习方法（p值<0.002），证明了联合学习模板和任务的重要性。

Conclusion: FeTTL为缓解联邦学习中的数据分布偏移提供了原则性和可扩展的解决方案，支持真实世界多机构环境中的稳健模型部署。

Abstract: Federated learning enables collaborative model training across geographically distributed medical centers while preserving data privacy. However, domain shifts and heterogeneity in data often lead to a degradation in model performance. Medical imaging applications are particularly affected by variations in acquisition protocols, scanner types, and patient populations. To address these issues, we introduce Federated Template and Task Learning (FeTTL), a novel framework designed to harmonize multi-institutional medical imaging data in federated environments. FeTTL learns a global template together with a task model to align data distributions among clients. We evaluated FeTTL on two challenging and diverse multi-institutional medical imaging tasks: retinal fundus optical disc segmentation and histopathological metastasis classification. Experimental results show that FeTTL significantly outperforms the state-of-the-art federated learning baselines (p-values <0.002) for optical disc segmentation and classification of metastases from multi-institutional data. Our experiments further highlight the importance of jointly learning the template and the task. These findings suggest that FeTTL offers a principled and extensible solution for mitigating distribution shifts in federated learning, supporting robust model deployment in real-world, multi-institutional environments.

</details>


### [4] [Where is the multimodal goal post? On the Ability of Foundation Models to Recognize Contextually Important Moments](https://arxiv.org/abs/2601.16333)
*Aditya K Surikuchi,Raquel Fernández,Sandro Pezzelle*

Main category: cs.CV

TL;DR: 该研究评估了多模态模型在足球视频中识别重要子事件的能力，发现现有模型表现接近随机水平，主要依赖单一模态且多模态融合效果不佳。


<details>
  <summary>Details</summary>
Motivation: 现实应用中需要从时序多模态事件生成语言描述，而识别视频中的重要子事件是叙述或总结多模态事件的基本前提。研究聚焦足球比赛，旨在评估模型区分比赛中重要与非重要子事件的能力。

Method: 利用足球比赛精彩集锦中隐含的人类重要性偏好构建新数据集，无需额外标注成本。使用该数据集比较多个最先进的多模态模型，并通过超越标准评估指标的分析揭示模型行为模式。

Result: 多模态模型表现接近随机水平，分析显示模型倾向于依赖单一主导模态，且在从多个来源合成必要信息方面效果不佳。

Conclusion: 需要能够处理多模态数据样本级异质性的模块化架构，以及能够最大化跨模态协同作用的补充训练程序。研究构建的数据集将公开提供给社区。

Abstract: Foundation models are used for many real-world applications involving language generation from temporally-ordered multimodal events. In this work, we study the ability of models to identify the most important sub-events in a video, which is a fundamental prerequisite for narrating or summarizing multimodal events. Specifically, we focus on football games and evaluate models on their ability to distinguish between important and non-important sub-events in a game. To this end, we construct a new dataset by leveraging human preferences for importance implicit in football game highlight reels, without any additional annotation costs. Using our dataset, which we will publicly release to the community, we compare several state-of-the-art multimodal models and show that they are not far from chance level performance. Analyses of models beyond standard evaluation metrics reveal their tendency to rely on a single dominant modality and their ineffectiveness in synthesizing necessary information from multiple sources. Our findings underline the importance of modular architectures that can handle sample-level heterogeneity in multimodal data and the need for complementary training procedures that can maximize cross-modal synergy.

</details>


### [5] [Coarse-to-Fine Non-rigid Multi-modal Image Registration for Historical Panel Paintings based on Crack Structures](https://arxiv.org/abs/2601.16348)
*Aline Sindel,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 提出一种基于裂纹特征的非刚性多模态图像配准方法，用于历史油画的多模态图像对齐，采用粗到细策略和神经网络进行关键点检测与匹配。


<details>
  <summary>Details</summary>
Motivation: 历史油画的多模态图像分析需要像素级对齐，目前主要依赖手动操作，耗时且精度有限。多模态图像配准面临分辨率差异、图像尺寸大、非刚性变形和模态依赖内容等挑战。

Method: 提出粗到细非刚性多模态配准方法：1) 利用油画裂纹作为特征，使用CNN进行联合关键点检测和描述；2) 使用GNN进行基于块的描述符匹配；3) 基于局部单应性重投影误差过滤匹配；4) 引入多级关键点细化方法处理混合分辨率图像。

Result: 创建了带大量关键点标注的多模态油画数据集和包含五个多模态域及不同分辨率的测试集。消融研究表明所有模块都有效，相比竞争方法取得了最佳配准结果。

Conclusion: 该方法能有效减少历史油画多模态图像配准的手动工作，提高速度和精度，为艺术技术分析提供可靠工具。

Abstract: Art technological investigations of historical panel paintings rely on acquiring multi-modal image data, including visual light photography, infrared reflectography, ultraviolet fluorescence photography, x-radiography, and macro photography. For a comprehensive analysis, the multi-modal images require pixel-wise alignment, which is still often performed manually. Multi-modal image registration can reduce this laborious manual work, is substantially faster, and enables higher precision. Due to varying image resolutions, huge image sizes, non-rigid distortions, and modality-dependent image content, registration is challenging. Therefore, we propose a coarse-to-fine non-rigid multi-modal registration method efficiently relying on sparse keypoints and thin-plate-splines. Historical paintings exhibit a fine crack pattern, called craquelure, on the paint layer, which is captured by all image systems and is well-suited as a feature for registration. In our one-stage non-rigid registration approach, we employ a convolutional neural network for joint keypoint detection and description based on the craquelure and a graph neural network for descriptor matching in a patch-based manner, and filter matches based on homography reprojection errors in local areas. For coarse-to-fine registration, we introduce a novel multi-level keypoint refinement approach to register mixed-resolution images up to the highest resolution. We created a multi-modal dataset of panel paintings with a high number of keypoint annotations, and a large test set comprising five multi-modal domains and varying image resolutions. The ablation study demonstrates the effectiveness of all modules of our refinement method. Our proposed approaches achieve the best registration results compared to competing keypoint and dense matching methods and refinement methods.

</details>


### [6] [Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models](https://arxiv.org/abs/2601.16378)
*Bridget Leonard,Scott O. Murray*

Main category: cs.CV

TL;DR: 论文提出使用视角令牌（perspective tokens）来增强多模态语言模型的空间推理能力，通过编码方向信息来解决现有模型的自我中心偏见问题


<details>
  <summary>Details</summary>
Motivation: 当前多模态语言模型在语义视觉-语言任务上表现良好，但在需要采用其他智能体视觉视角的空间推理任务上失败。这些错误反映了持续的自我中心偏见，并引发了对当前模型是否支持他中心推理的疑问。

Method: 引入视角令牌——专门化的嵌入，通过两种方式编码方向信息：(1) 基于身体关键点的具身线索；(2) 支持心理旋转的抽象表示。将这些令牌集成到LLaVA-1.5-13B模型中。

Result: 在合成和自然基准测试（Isle Bricks V2, COCO, 3DSRBench）中，视角令牌提高了准确性，其中基于旋转的令牌能够泛化到非人类参考智能体。表示分析显示微调增强了基础模型中已有的潜在方向敏感性。

Conclusion: 将认知基础的空间结构直接嵌入到令牌空间中，为视角采择和更类似人类的空间推理提供了一种轻量级、模型无关的机制。多模态语言模型包含他中心推理的前兆，但缺乏适当的内部结构。

Abstract: Multimodal language models (MLMs) perform well on semantic vision-language tasks but fail at spatial reasoning that requires adopting another agent's visual perspective. These errors reflect a persistent egocentric bias and raise questions about whether current models support allocentric reasoning. Inspired by human spatial cognition, we introduce perspective tokens, specialized embeddings that encode orientation through either (1) embodied body-keypoint cues or (2) abstract representations supporting mental rotation. Integrating these tokens into LLaVA-1.5-13B yields performance on level-2 visual perspective-taking tasks. Across synthetic and naturalistic benchmarks (Isle Bricks V2, COCO, 3DSRBench), perspective tokens improve accuracy, with rotation-based tokens generalizing to non-human reference agents. Representational analyses reveal that fine-tuning enhances latent orientation sensitivity already present in the base model, suggesting that MLMs contain precursors of allocentric reasoning but lack appropriate internal structure. Overall, embedding cognitively grounded spatial structure directly into token space provides a lightweight, model-agnostic mechanism for perspective-taking and more human-like spatial reasoning.

</details>


### [7] [VTFusion: A Vision-Text Multimodal Fusion Network for Few-Shot Anomaly Detection](https://arxiv.org/abs/2601.16381)
*Yuxin Jiang,Yunkang Cao,Yuqi Cheng,Yiheng Zhang,Weiming Shen*

Main category: cs.CV

TL;DR: VTFusion是一个针对少样本异常检测的视觉-文本多模态融合框架，通过自适应特征提取器和多模态预测融合模块，在工业检测场景中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本异常检测方法主要依赖自然场景预训练特征，忽略了工业检测所需的领域特定语义；同时，现有的多模态融合策略多为简单的特征拼接，未能解决视觉和文本模态之间的语义不对齐问题，导致对跨模态干扰的鲁棒性不足。

Method: 1. 为图像和文本模态引入自适应特征提取器，学习任务特定的表示，弥合预训练模型与工业数据之间的领域差距；通过生成多样化的合成异常来增强特征判别能力。
2. 开发专门的多模态预测融合模块，包括促进丰富跨模态信息交换的融合块，以及在多模态指导下生成精细化像素级异常图的分割网络。

Result: 在2-shot场景下，VTFusion在MVTec AD数据集上达到96.8%的图像级AUROC，在VisA数据集上达到86.2%；在本文引入的工业汽车塑料部件真实数据集上达到93.5%的AUPRO，展示了其在严苛工业场景中的实际应用能力。

Conclusion: VTFusion通过有效的视觉-文本多模态融合，显著提升了少样本异常检测在工业场景中的性能，解决了现有方法在领域适应性和模态对齐方面的不足，具有重要的实际应用价值。

Abstract: Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios.

</details>


### [8] [ResAgent: Entropy-based Prior Point Discovery and Visual Reasoning for Referring Expression Segmentation](https://arxiv.org/abs/2601.16394)
*Yihao Wang,Jusheng Zhang,Ziyi Tang,Keze Wang,Meng Yang*

Main category: cs.CV

TL;DR: 提出EBD-VBR框架，通过熵基点发现和视觉基推理解决RES任务中MLLM方法存在的两个关键问题：粗边界框导致的冗余点提示和文本坐标推理不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型（MLLM）的RES方法存在两个主要限制：1）MLLM生成的粗边界框导致冗余或非区分性的点提示；2）依赖文本坐标推理不可靠，无法区分视觉相似的目标和干扰物。

Method: 提出EBD-VBR框架，包含熵基点发现（EBD）和视觉基推理（VBR）。EBD通过建模粗边界框内的空间不确定性，将点选择视为信息最大化过程；VBR通过联合视觉-语义对齐验证点正确性，放弃纯文本坐标推理。采用从粗到细的工作流程：边界框初始化、熵引导点发现、视觉基验证和掩码解码。

Result: 在四个基准数据集（RefCOCO、RefCOCO+、RefCOCOg、ReasonSeg）上的广泛评估表明，EBD-VBR在所有四个基准上都达到了新的最先进性能。

Conclusion: EBD-VBR框架通过熵基点发现和视觉基推理，能够生成准确且语义基础的细分掩码，使用最少的提示，有效解决了现有RES方法的局限性。

Abstract: Referring Expression Segmentation (RES) is a core vision-language segmentation task that enables pixel-level understanding of targets via free-form linguistic expressions, supporting critical applications such as human-robot interaction and augmented reality. Despite the progress of Multimodal Large Language Model (MLLM)-based approaches, existing RES methods still suffer from two key limitations: first, the coarse bounding boxes from MLLMs lead to redundant or non-discriminative point prompts; second, the prevalent reliance on textual coordinate reasoning is unreliable, as it fails to distinguish targets from visually similar distractors. To address these issues, we propose \textbf{\model}, a novel RES framework integrating \textbf{E}ntropy-\textbf{B}ased Point \textbf{D}iscovery (\textbf{EBD}) and \textbf{V}ision-\textbf{B}ased \textbf{R}easoning (\textbf{VBR}). Specifically, EBD identifies high-information candidate points by modeling spatial uncertainty within coarse bounding boxes, treating point selection as an information maximization process. VBR verifies point correctness through joint visual-semantic alignment, abandoning text-only coordinate inference for more robust validation. Built on these components, \model implements a coarse-to-fine workflow: bounding box initialization, entropy-guided point discovery, vision-based validation, and mask decoding. Extensive evaluations on four benchmark datasets (RefCOCO, RefCOCO+, RefCOCOg, and ReasonSeg) demonstrate that \model achieves new state-of-the-art performance across all four benchmarks, highlighting its effectiveness in generating accurate and semantically grounded segmentation masks with minimal prompts.

</details>


### [9] [A Cosine Network for Image Super-Resolution](https://arxiv.org/abs/2601.16413)
*Chunwei Tian,Chengyuan Zhang,Bob Zhang,Zhiwu Li,C. L. Philip Chen,David Zhang*

Main category: cs.CV

TL;DR: 提出CSRNet用于图像超分辨率，通过设计奇偶异构块提取互补同源结构信息，结合线性与非线性信息增强鲁棒性，并使用余弦退火机制优化训练


<details>
  <summary>Details</summary>
Motivation: 深度卷积神经网络能提取层次结构信息恢复高质量图像，但在图像超分辨率中保持所获结构信息的有效性很重要。现有方法在同源信息处理上存在不足，需要改进网络架构和训练策略

Method: 1. 设计奇偶异构块来扩大架构差异，提取互补同源结构信息；2. 结合线性与非线性结构信息克服同源信息缺点；3. 使用余弦退火机制优化训练，通过热重启和调整学习率避免梯度下降的局部最小值

Result: 实验结果表明，提出的CSRNet在图像超分辨率任务中与最先进方法相比具有竞争力

Conclusion: 通过改进网络架构和训练策略，CSRNet能有效提取和保持结构信息，在图像超分辨率中取得良好性能

Abstract: Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution.

</details>


### [10] [DCCS-Det: Directional Context and Cross-Scale-Aware Detector for Infrared Small Target](https://arxiv.org/abs/2601.16428)
*Shuying Li,Qiang Ma,San Zhang,Chuang Yang*

Main category: cs.CV

TL;DR: 提出DCCS-Det红外小目标检测器，通过双流显著性增强块和潜在感知语义提取聚合模块，解决现有方法在局部-全局特征联合建模不足以及特征冗余等问题，在多个数据集上达到SOTA精度。


<details>
  <summary>Details</summary>
Motivation: 现有红外小目标检测方法存在两个主要问题：1) 局部-全局特征联合建模不足，影响目标-背景区分能力；2) 特征冗余和语义稀释，降低目标表示质量。需要设计新方法解决这些问题。

Method: 提出DCCS-Det检测器，包含两个核心组件：1) 双流显著性增强(DSE)块，整合局部感知和方向感知上下文聚合，捕获长距离空间依赖和局部细节；2) 潜在感知语义提取聚合(LaSEA)模块，通过跨尺度特征提取和随机池化采样策略，缓解特征退化，增强判别性特征并抑制噪声。

Result: 在多个数据集上的广泛实验表明，DCCS-Det实现了最先进的检测精度，同时保持有竞争力的效率。消融研究进一步验证了DSE和LaSEA在复杂场景下改善目标感知和特征表示的有效性。

Conclusion: DCCS-Det通过创新的DSE块和LaSEA模块，有效解决了红外小目标检测中的特征建模和表示问题，在精度和效率方面都表现出色，为红外小目标检测提供了新的解决方案。

Abstract: Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. \href{https://huggingface.co/InPeerReview/InfraredSmallTargetDetection-IRSTD.DCCS}{DCCS-Det Official Code is Available Here!}

</details>


### [11] [AlphaFace: High Fidelity and Real-time Face Swapper Robust to Facial Pose](https://arxiv.org/abs/2601.16429)
*Jongmin Yu,Hyeontaek Oh,Zhongtian Sun,Angelica I Aviles-Rivero,Moongu Jeon,Jinhong Yang*

Main category: cs.CV

TL;DR: AlphaFace：利用视觉语言模型和CLIP嵌入，通过视觉与文本语义对比损失实现更鲁棒的人脸交换，在极端姿态下超越现有方法并保持实时性能


<details>
  <summary>Details</summary>
Motivation: 现有的人脸交换方法在受限场景下表现良好，但在极端面部姿态下质量显著下降。基于几何特征的方法增加了额外依赖和计算成本，而基于扩散的方法虽然效果显著但无法实时处理。

Method: 利用开源视觉语言模型和CLIP图像与文本嵌入，应用新颖的视觉和文本语义对比损失，实现更强的身份表示和更精确的属性保留，同时保持实时性能。

Result: 在FF++、MPIE和LPFF数据集上的综合实验表明，AlphaFace在姿态挑战性案例中超越了最先进的方法，项目已在GitHub公开。

Conclusion: AlphaFace通过结合视觉语言模型和对比学习，在保持实时性能的同时，显著提升了极端面部姿态下的人脸交换质量，解决了现有方法的局限性。

Abstract: Existing face-swapping methods often deliver competitive results in constrained settings but exhibit substantial quality degradation when handling extreme facial poses. To improve facial pose robustness, explicit geometric features are applied, but this approach remains problematic since it introduces additional dependencies and increases computational cost. Diffusion-based methods have achieved remarkable results; however, they are impractical for real-time processing. We introduce AlphaFace, which leverages an open-source vision-language model and CLIP image and text embeddings to apply novel visual and textual semantic contrastive losses. AlphaFace enables stronger identity representation and more precise attribute preservation, all while maintaining real-time performance. Comprehensive experiments across FF++, MPIE, and LPFF demonstrate that AlphaFace surpasses state-of-the-art methods in pose-challenging cases. The project is publicly available on `https://github.com/andrewyu90/Alphaface_Official.git'.

</details>


### [12] [MDAFNet: Multiscale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection](https://arxiv.org/abs/2601.16434)
*Shuying Li,Qiang Ma,San Zhang,Wuwei Wang,Chuang Yang*

Main category: cs.CV

TL;DR: 提出MDAFNet网络，通过多尺度差分边缘模块和双域自适应特征增强模块，解决红外小目标检测中边缘信息退化及频率干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有红外小目标检测方法面临两个主要问题：1）随着网络层数增加，目标边缘像素逐渐退化；2）传统卷积在特征提取时难以区分频率分量，导致低频背景干扰高频目标，高频噪声引发误检。

Method: 提出MDAFNet网络，包含两个核心模块：MSDE模块通过多尺度边缘提取和增强机制，补偿下采样过程中的目标边缘信息损失；DAFE模块结合频域处理机制和空间域模拟频率分解融合机制，自适应增强高频目标并选择性抑制高频噪声。

Result: 在多个数据集上的实验结果表明，MDAFNet具有优越的检测性能。

Conclusion: MDAFNet通过有效处理边缘信息退化和频率干扰问题，显著提升了红外小目标检测的性能。

Abstract: Infrared small target detection (IRSTD) plays a crucial role in numerous military and civilian applications. However, existing methods often face the gradual degradation of target edge pixels as the number of network layers increases, and traditional convolution struggles to differentiate between frequency components during feature extraction, leading to low-frequency backgrounds interfering with high-frequency targets and high-frequency noise triggering false detections. To address these limitations, we propose MDAFNet (Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection), which integrates the Multi-Scale Differential Edge (MSDE) module and Dual-Domain Adaptive Feature Enhancement (DAFE) module. The MSDE module, through a multi-scale edge extraction and enhancement mechanism, effectively compensates for the cumulative loss of target edge information during downsampling. The DAFE module combines frequency domain processing mechanisms with simulated frequency decomposition and fusion mechanisms in the spatial domain to effectively improve the network's capability to adaptively enhance high-frequency targets and selectively suppress high-frequency noise. Experimental results on multiple datasets demonstrate the superior detection performance of MDAFNet.

</details>


### [13] [Masked Face Recognition under Different Backbones](https://arxiv.org/abs/2601.16440)
*Bo Zhang,Ming Zhang,Kun Wu,Lei Bian,Yi Lin*

Main category: cs.CV

TL;DR: 该论文对多种人脸识别骨干网络在戴口罩和无口罩情况下的性能进行了全面评估，发现r100系列在标准测试中表现最佳，而r100_mask_v2在戴口罩测试中领先，ViT-Small/Tiny在戴口罩场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 后疫情时代大量民航乘客在安检时佩戴口罩，这对传统人脸识别模型构成重大挑战。需要评估不同骨干网络在戴口罩和无口罩情况下的性能差异，为实际部署提供指导。

Method: 通过广泛的对比实验，对多个核心骨干网络进行综合评估，包括r100系列、r50、r34_mask_v1、r100_mask_v2、r50_mask_v3以及ViT-Small/Tiny等模型。

Result: 标准测试中：r100系列表现最佳（人脸比对在0.01% FAR下准确率98%+，搜索任务top1/top5高）；r50排名第二；r34_mask_v1落后。戴口罩测试中：r100_mask_v2领先（90.07%准确率）；r50_mask_v3在r50中最佳但仍落后于r100；ViT-Small/Tiny在戴口罩场景下表现出色且有效果提升。

Conclusion: 不同骨干网络在戴口罩和无口罩场景下表现差异显著，r100_mask_v2在戴口罩识别中表现最优，ViT架构在戴口罩场景下具有优势，为实际部署提供了具体的模型选择建议。

Abstract: Erratum to the paper (Zhang et al., 2025): corrections to Table IV and the data in Page 3, Section A. In the post-pandemic era, a high proportion of civil aviation passengers wear masks during security checks, posing significant challenges to traditional face recognition models. The backbone network serves as the core component of face recognition models. In standard tests, r100 series models excelled (98%+ accuracy at 0.01% FAR in face comparison, high top1/top5 in search). r50 ranked second, r34_mask_v1 lagged. In masked tests, r100_mask_v2 led (90.07% accuracy), r50_mask_v3 performed best among r50 but trailed r100. Vit-Small/Tiny showed strong masked performance with gains in effectiveness. Through extensive comparative experiments, this paper conducts a comprehensive evaluation of several core backbone networks, aiming to reveal the impacts of different models on face recognition with and without masks, and provide specific deployment recommendations.

</details>


### [14] [Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding](https://arxiv.org/abs/2601.16449)
*Xiaojiang Peng,Jingyi Chen,Zebang Cheng,Bao Peng,Fengyi Wu,Yifei Dong,Shuyuan Tu,Qiyu Hu,Huiting Huang,Yuxiang Lin,Jun-Yan He,Kai Wang,Zheng Lian,Zhi-Qi Cheng*

Main category: cs.CV

TL;DR: Emotion-LLaMAv2是一个端到端的多模态情感推理框架，结合MMEVerse基准测试，通过多视图编码器、注意力预融合模块和感知到认知的课程指令调优，显著提升了情感识别和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在情感推理方面能力有限，缺乏大规模高质量标注数据集和标准化评估基准。现有的Emotion-LLaMA框架受到人脸检测器限制、隐式融合策略和低质量训练数据的制约。

Method: 1. 端到端多视图编码器：消除外部人脸检测，通过丰富的空间和时间多视图token捕捉细微情感线索
2. Conv Attention预融合模块：在LLM主干外部实现局部和全局多模态特征交互
3. 感知到认知课程指令调优：在LLaMA2主干中统一情感识别和自由形式情感推理
4. MMEVerse基准：整合12个公开情感数据集，通过多智能体管道重新标注，形成统一的多模态指令格式

Result: 构建了包含130k训练片段和36k测试片段的MMEVerse基准，涵盖18个评估基准，为大规模训练和可重复评估提供了标准化数据集和评估设置。

Conclusion: Emotion-LLaMAv2和MMEVerse基准建立了一个完整的端到端情感识别和推理管道，解决了现有方法的局限性，为多模态情感计算领域提供了标准化评估框架和大规模训练数据。

Abstract: Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks.

</details>


### [15] [VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology](https://arxiv.org/abs/2601.16451)
*Peixian Liang,Songhao Li,Shunsuke Koga,Yutong Li,Zahra Alipour,Yucheng Tang,Daguang Xu,Zhi Huang*

Main category: cs.CV

TL;DR: VISTA-PATH是一个交互式、类别感知的病理分割基础模型，通过结合视觉上下文、语义组织描述和专家空间提示，实现精确的多类别病理图像分割，并支持人机交互细化，显著提升临床病理分析效果。


<details>
  <summary>Details</summary>
Motivation: 当前的分割基础模型虽然通过大规模预训练提高了泛化能力，但将分割视为静态视觉预测任务，与病理学需求不匹配。病理图像分割需要处理异质结构、融入专家反馈，并产生对临床解释有直接意义的像素级分割。

Method: 提出VISTA-PATH模型，联合视觉上下文、语义组织描述和可选专家空间提示进行分割。构建VISTA-PATH Data数据集，包含160万图像-掩码-文本三元组，涵盖9个器官和93个组织类别。支持通过稀疏的patch级边界框注释反馈进行全切片分割的动态人机交互细化。

Result: 在广泛的保留测试集和外部基准测试中，VISTA-PATH持续优于现有分割基础模型。通过提出的肿瘤相互作用评分（TIS）改善组织微环境分析，TIS与患者生存率显示出强且显著的关联。模型产生的高保真、类别感知分割是计算病理学的优选模型。

Conclusion: VISTA-PATH将病理图像分割从静态预测提升为交互式且临床基础的表征，建立了病理分割基础模型的新范式，为数字病理学提供了更有效的工具。

Abstract: Accurate semantic segmentation for histopathology image is crucial for quantitative tissue analysis and downstream clinical modeling. Recent segmentation foundation models have improved generalization through large-scale pretraining, yet remain poorly aligned with pathology because they treat segmentation as a static visual prediction task. Here we present VISTA-PATH, an interactive, class-aware pathology segmentation foundation model designed to resolve heterogeneous structures, incorporate expert feedback, and produce pixel-level segmentation that are directly meaningful for clinical interpretation. VISTA-PATH jointly conditions segmentation on visual context, semantic tissue descriptions, and optional expert-provided spatial prompts, enabling precise multi-class segmentation across heterogeneous pathology images. To support this paradigm, we curate VISTA-PATH Data, a large-scale pathology segmentation corpus comprising over 1.6 million image-mask-text triplets spanning 9 organs and 93 tissue classes. Across extensive held-out and external benchmarks, VISTA-PATH consistently outperforms existing segmentation foundation models. Importantly, VISTA-PATH supports dynamic human-in-the-loop refinement by propagating sparse, patch-level bounding-box annotation feedback into whole-slide segmentation. Finally, we show that the high-fidelity, class-aware segmentation produced by VISTA-PATH is a preferred model for computational pathology. It improve tissue microenvironment analysis through proposed Tumor Interaction Score (TIS), which exhibits strong and significant associations with patient survival. Together, these results establish VISTA-PATH as a foundation model that elevates pathology image segmentation from a static prediction to an interactive and clinically grounded representation for digital pathology. Source code and demo can be found at https://github.com/zhihuanglab/VISTA-PATH.

</details>


### [16] [Order from Chaos: Physical World Understanding from Glitchy Gameplay Videos](https://arxiv.org/abs/2601.16471)
*Meng Cao,Haoran Tang,Haoze Zhao,Mingfei Han,Ruyang Liu,Qiang Sun,Xiaojun Chang,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: 利用游戏视频中的视觉异常（glitches）作为物理世界理解的监督信号，提出PhysGame数据集和GameBench基准，显著提升多模态模型的物理推理能力


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在物理原理理解上仍达不到人类水平，现有物理推理数据集要么依赖高成本的真实视频标注，要么使用有限真实性和多样性的合成模拟。需要寻找更丰富、可扩展的监督来源

Method: 提出利用游戏视频中的视觉异常（违反预设物理定律的glitches）作为物理理解的监督源。构建PhysGame数据集（140,057个glitch相关的问答对，覆盖5个物理领域和16个细分类别），使用游戏元数据（标题和描述）引导高质量QA生成。同时构建GameBench基准（880个专家标注的glitch视频）用于评估

Result: PhysGame显著提升了Game2Real可迁移性（在PhysBench上提升Qwen2.5VL 2.5%）和Game2General可迁移性（在MVBench上提升1.9%）。PhysGame调优的模型在GameBench上获得3.7%的绝对提升，显示出更强的物理不合理性检测鲁棒性

Conclusion: 从游戏异常中学习为推进多模态智能的物理世界理解提供了一条可扩展且有效的途径。游戏glitches作为监督信号能够显著提升模型的物理推理能力

Abstract: Understanding the physical world, including object dynamics, material properties, and causal interactions, remains a core challenge in artificial intelligence. Although recent multi-modal large language models (MLLMs) have demonstrated impressive general reasoning capabilities, they still fall short of achieving human-level understanding of physical principles. Existing datasets for physical reasoning either rely on real-world videos, which incur high annotation costs, or on synthetic simulations, which suffer from limited realism and diversity. In this paper, we propose a novel paradigm that leverages glitches in gameplay videos, referring to visual anomalies that violate predefined physical laws, as a rich and scalable supervision source for physical world understanding. We introduce PhysGame, an meta information guided instruction-tuning dataset containing 140,057 glitch-centric question-answer pairs across five physical domains and sixteen fine-grained categories. To ensure data accuracy, we design a prompting strategy that utilizes gameplay metadata such as titles and descriptions to guide high-quality QA generation. Complementing PhysGame, we construct GameBench, an expert-annotated benchmark with 880 glitch-identified gameplay videos designed to evaluate physical reasoning capabilities. Extensive experiments show that PhysGame significantly enhances both Game2Real transferability, improving the real world physical reasoning performance of Qwen2.5VL by 2.5% on PhysBench, and Game2General transferability, yielding a 1.9% gain on the MVBench benchmark. Moreover, PhysGame-tuned models achieve a 3.7% absolute improvement on GameBench, demonstrating enhanced robustness in detecting physical implausibilities. These results indicate that learning from gameplay anomalies offers a scalable and effective pathway toward advancing physical world understanding in multimodal intelligence.

</details>


### [17] [Multi-View Consistent Wound Segmentation With Neural Fields](https://arxiv.org/abs/2601.16487)
*Remi Chierchia,Léo Lebrat,David Ahmedt-Aristizabal,Yulia Arzhaeva,Olivier Salvado,Clinton Fookes,Rodrigo Santa Cruz*

Main category: cs.CV

TL;DR: WoundNeRF：一种基于NeRF SDF的方法，用于从自动生成的标注中估计稳健的伤口分割，相比现有方法能恢复更准确的分割结果。


<details>
  <summary>Details</summary>
Motivation: 伤口护理面临经济和后勤负担，计算机视觉和机器学习算法可为医疗专业人员提供支持。伤口分割能提供快速自动的组织评估，但现有方法在从2D图像推断多视角一致的3D结构方面仍存在挑战。

Method: 提出WoundNeRF方法，基于NeRF SDF（神经辐射场符号距离函数）框架，从自动生成的标注中估计稳健的伤口分割，实现多视角一致的3D结构重建。

Result: 与最先进的Vision Transformer网络和传统基于光栅化的算法相比，WoundNeRF在恢复准确分割方面表现出潜力，能够实现更精确的伤口分割。

Conclusion: WoundNeRF展示了从2D图像恢复准确伤口分割的潜力，代码将开源以促进这一有前景范式的发展，有助于更完整和精确的伤口愈合进展跟踪。

Abstract: Wound care is often challenged by the economic and logistical burdens that consistently afflict patients and hospitals worldwide. In recent decades, healthcare professionals have sought support from computer vision and machine learning algorithms. In particular, wound segmentation has gained interest due to its ability to provide professionals with fast, automatic tissue assessment from standard RGB images. Some approaches have extended segmentation to 3D, enabling more complete and precise healing progress tracking. However, inferring multi-view consistent 3D structures from 2D images remains a challenge. In this paper, we evaluate WoundNeRF, a NeRF SDF-based method for estimating robust wound segmentations from automatically generated annotations. We demonstrate the potential of this paradigm in recovering accurate segmentations by comparing it against state-of-the-art Vision Transformer networks and conventional rasterisation-based algorithms. The code will be released to facilitate further development in this promising paradigm.

</details>


### [18] [Expert Knowledge-Guided Decision Calibration for Accurate Fine-Grained Tree Species Classification](https://arxiv.org/abs/2601.16498)
*Chen Long,Dian Chen,Ruifei Ding,Zhe Chen,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TL;DR: 提出EKDC-Net框架，通过引入外部"领域专家"来校准分类决策，解决细粒度树种分类中的长尾分布和类间相似性问题，在三个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度树种分类方法主要设计复杂架构来拟合局部数据分布，但忽视了有限数据中固有的长尾分布和高类间相似性，难以区分少样本或易混淆类别。受人类知识传播过程中寻求专家帮助的启发，引入外部"领域专家"来克服这些挑战。

Method: 提出专家知识引导的分类决策校准网络(EKDC-Net)，包含两个核心模块：1) 局部先验引导的知识提取模块(LPKEM)，利用类激活图(CAM)分析指导领域专家专注于分类关键判别特征；2) 不确定性引导的决策校准模块(UDCM)，基于整体类别不确定性和实例级预测不确定性动态校正局部模型决策。还构建了包含102个树种的大规模数据集CU-Tree102。

Result: 在三个基准数据集上实现了最先进的性能。作为轻量级即插即用模块，仅增加0.08M可学习参数，使骨干网络准确率提升6.42%，精度提升11.46%。

Conclusion: EKDC-Net通过引入外部领域专家知识有效解决了细粒度树种分类中的长尾分布和类间相似性问题，在提升性能的同时保持了轻量级特性，为相关领域提供了新的解决方案。

Abstract: Accurate fine-grained tree species classification is critical for forest inventory and biodiversity monitoring. Existing methods predominantly focus on designing complex architectures to fit local data distributions. However, they often overlook the long-tailed distributions and high inter-class similarity inherent in limited data, thereby struggling to distinguish between few-shot or confusing categories. In the process of knowledge dissemination in the human world, individuals will actively seek expert assistance to transcend the limitations of local thinking. Inspired by this, we introduce an external "Domain Expert" and propose an Expert Knowledge-Guided Classification Decision Calibration Network (EKDC-Net) to overcome these challenges. Our framework addresses two core issues: expert knowledge extraction and utilization. Specifically, we first develop a Local Prior Guided Knowledge Extraction Module (LPKEM). By leveraging Class Activation Map (CAM) analysis, LPKEM guides the domain expert to focus exclusively on discriminative features essential for classification. Subsequently, to effectively integrate this knowledge, we design an Uncertainty-Guided Decision Calibration Module (UDCM). This module dynamically corrects the local model's decisions by considering both overall category uncertainty and instance-level prediction uncertainty. Furthermore, we present a large-scale classification dataset covering 102 tree species, named CU-Tree102 to address the issue of scarce diversity in current benchmarks. Experiments on three benchmark datasets demonstrate that our approach achieves state-of-the-art performance. Crucially, as a lightweight plug-and-play module, EKDC-Net improves backbone accuracy by 6.42% and precision by 11.46% using only 0.08M additional learnable parameters. The dataset, code, and pre-trained models are available at https://github.com/WHU-USI3DV/TreeCLS.

</details>


### [19] [SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer](https://arxiv.org/abs/2601.16515)
*Tongcheng Fang,Hanling Zhang,Ruiqi Xie,Zhuo Han,Xin Tao,Tianchen Zhao,Pengfei Wan,Wenbo Ding,Wanli Ouyang,Xuefei Ning,Yu Wang*

Main category: cs.CV

TL;DR: SALAD：一种用于视频扩散Transformer的轻量级混合注意力机制，通过并行稀疏注意力与线性注意力分支，结合输入相关门控，实现90%稀疏度和1.72倍推理加速，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频生成中的扩散Transformer由于全注意力的二次复杂度导致高计算延迟。现有稀疏注意力方法存在局限性：无需训练的方法稀疏度有限，加速效果有限；基于训练的方法能达到更高稀疏度但需要大量数据和计算资源进行训练。

Method: 提出SALAD方法，在稀疏注意力旁并行引入轻量级线性注意力分支，通过输入相关的门控机制精细平衡两个分支，实现高稀疏度同时保持模型表达能力。

Result: 达到90%稀疏度和1.72倍推理加速，生成质量与全注意力基线相当。微调过程高效，仅需2000个视频样本和1600个训练步骤（批量大小为8）。

Conclusion: SALAD在稀疏注意力与线性注意力之间找到了有效平衡，实现了高稀疏度和显著加速，同时保持了生成质量，且训练成本低，为视频扩散Transformer的高效部署提供了实用解决方案。

Abstract: Diffusion Transformers have recently demonstrated remarkable performance in video generation. However, the long input sequences result in high computational latency due to the quadratic complexity of full attention. Various sparse attention mechanisms have been proposed. Training-free sparse attention is constrained by limited sparsity and thus offers modest acceleration, whereas training-based methods can reach much higher sparsity but demand substantial data and computation for training. In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72x inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8.

</details>


### [20] [TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning](https://arxiv.org/abs/2601.16520)
*Daixian Liu,Jiayi Kuang,Yinghui Li,Yangning Li,Di Yin,Haoyu Cao,Xing Sun,Ying Shen,Hai-Tao Zheng,Liang Lin,Philip S. Yu*

Main category: cs.CV

TL;DR: 论文提出了TangramPuzzle基准测试，用于评估多模态大语言模型在组合空间推理方面的能力，发现现有模型倾向于匹配目标轮廓而忽略几何约束


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在精确组合空间推理方面能力不足，现有基准测试任务简单、依赖语义近似或粗略相对定位，缺乏严格的数学公式化评估指标

Method: 引入TangramPuzzle基准测试，提出Tangram构造表达式(TCE)符号几何框架，设计轮廓预测和端到端代码生成两个互补任务

Result: 对先进开源和专有模型进行广泛评估，发现MLLMs倾向于匹配目标轮廓而忽略几何约束，导致部件变形或扭曲

Conclusion: TangramPuzzle基准测试填补了MLLMs在组合空间推理评估方面的空白，揭示了模型在几何约束理解方面的局限性

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces.

</details>


### [21] [AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding](https://arxiv.org/abs/2601.16532)
*Runmao Yao,Junsheng Zhou,Zhen Dong,Yu-Shen Liu*

Main category: cs.CV

TL;DR: AnchoredDream：通过外观-几何相互促进机制，基于高保真几何锚定360°场景生成的零样本方法


<details>
  <summary>Details</summary>
Motivation: 单视图室内场景生成在现实应用中很重要，但从单张图像生成完整的360°场景仍然是一个高度不适定且具有挑战性的问题。现有方法虽然利用扩散模型和深度估计网络取得进展，但在大视角变化下仍难以保持外观一致性和几何合理性，限制了全场景生成的效果。

Method: 提出AnchoredDream，一种新颖的零样本流程，通过外观-几何相互促进机制将360°场景生成锚定在高保真几何上。方法首先进行外观引导的几何生成以构建可靠的3D场景布局，然后通过一系列模块逐步生成完整场景：warp-and-inpaint、warp-and-refine、后优化以及新颖的Grouting Block，确保输入视图与生成区域之间的无缝过渡。

Result: 大量实验表明，AnchoredDream在外观一致性和几何合理性方面大幅优于现有方法，且完全以零样本方式实现。

Conclusion: 研究结果突显了几何锚定在高质量、零样本单视图场景生成中的潜力。

Abstract: Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation.

</details>


### [22] [OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding](https://arxiv.org/abs/2601.16538)
*Zixian Liu,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: OnlineSI框架使多模态大语言模型具备持续空间理解能力，通过有限空间记忆和3D点云语义融合，支持在视频流中持续改进环境认知，适用于真实世界具身系统部署。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM方法缺乏在动态变化世界中持续工作的能力，且难以部署到真实环境具身系统中。需要让MLLM具备持续空间理解和推理能力，适应不断变化的环境。

Method: 提出OnlineSI框架：1）维护有限空间记忆保留历史观测，确保推理计算量不随输入累积而增加；2）融合3D点云信息与语义信息，帮助MLLM更好地定位和识别场景中的物体。

Result: 在两个代表性数据集上测试，引入Fuzzy F1-Score缓解模糊性，实验证明了方法的有效性，为真实世界具身系统铺平了道路。

Conclusion: OnlineSI框架使MLLM能够持续改进对周围环境的空间理解，通过有限记忆和3D语义融合解决了现有方法的局限性，为实际部署到动态环境中的具身系统提供了可行方案。

Abstract: In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems.

</details>


### [23] [Semi-Supervised Hierarchical Open-Set Classification](https://arxiv.org/abs/2601.16541)
*Erik Wallin,Fredrik Kahl,Lars Hammarstrand*

Main category: cs.CV

TL;DR: 提出半监督层次开放集分类框架，通过子树伪标签和年龄门控机制，利用未标注数据提升分类性能


<details>
  <summary>Details</summary>
Motivation: 现有层次开放集分类方法需要大量标注数据，而实际应用中常遇到包含已知和未知类别的未标注混合数据集。如何有效利用这些未标注数据来提升层次开放集分类性能是一个重要问题。

Method: 提出基于伪标签的师生框架，包含两个关键组件：1) 子树伪标签 - 在未知数据存在时提供可靠监督；2) 年龄门控机制 - 缓解伪标签过度自信问题

Result: 在iNaturalist19基准测试中，该方法优于自监督预训练+监督适应方法，当每类仅使用20个标注样本时，性能甚至能与完全监督方法相媲美

Conclusion: 该半监督层次开放集分类框架能有效利用未标注混合数据集，显著提升分类性能，为实际应用提供了实用解决方案

Abstract: Hierarchical open-set classification handles previously unseen classes by assigning them to the most appropriate high-level category in a class taxonomy. We extend this paradigm to the semi-supervised setting, enabling the use of large-scale, uncurated datasets containing a mixture of known and unknown classes to improve the hierarchical open-set performance. To this end, we propose a teacher-student framework based on pseudo-labeling. Two key components are introduced: 1) subtree pseudo-labels, which provide reliable supervision in the presence of unknown data, and 2) age-gating, a mechanism that mitigates overconfidence in pseudo-labels. Experiments show that our framework outperforms self-supervised pretraining followed by supervised adaptation, and even matches the fully supervised counterpart when using only 20 labeled samples per class on the iNaturalist19 benchmark. Our code is available at https://github.com/walline/semihoc.

</details>


### [24] [HA2F: Dual-module Collaboration-Guided Hierarchical Adaptive Aggregation Framework for Remote Sensing Change Detection](https://arxiv.org/abs/2601.16573)
*Shuying Li,Yuchen Wang,San Zhang,Chuang Yang*

Main category: cs.CV

TL;DR: HA2F是一个用于遥感变化检测的双模块协作引导分层自适应聚合框架，通过动态分层特征校准和噪声自适应特征精炼来解决多时相特征对齐偏差和噪声干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化检测方法要么从局部斑块提取特征，要么整体处理整幅图像，导致跨时相特征匹配偏差，并对辐射和几何噪声敏感。需要解决多时相特征对齐偏差和噪声干扰问题。

Method: 提出HA2F框架，包含两个模块：1) 动态分层特征校准模块(DHFCM)：通过感知特征选择动态融合相邻层级特征，抑制无关差异以解决多时相特征对齐偏差；2) 噪声自适应特征精炼模块(NAFRM)：利用双重特征选择机制突出变化敏感区域并生成空间掩码，抑制无关区域或阴影的干扰。

Result: 在LEVIR-CD、WHU-CD和SYSU-CD数据集上实现了最先进的性能，在精度指标和计算效率方面均超越了现有对比方法。消融实验验证了DHFCM和NAFRM的有效性。

Conclusion: HA2F框架通过双模块协作有效解决了遥感变化检测中的多时相特征对齐偏差和噪声干扰问题，在多个数据集上表现出优越的性能和效率。

Abstract: Remote sensing change detection (RSCD) aims to identify the spatio-temporal changes of land cover, providing critical support for multi-disciplinary applications (e.g., environmental monitoring, disaster assessment, and climate change studies). Existing methods focus either on extracting features from localized patches, or pursue processing entire images holistically, which leads to the cross temporal feature matching deviation and exhibiting sensitivity to radiometric and geometric noise. Following the above issues, we propose a dual-module collaboration guided hierarchical adaptive aggregation framework, namely HA2F, which consists of dynamic hierarchical feature calibration module (DHFCM) and noise-adaptive feature refinement module (NAFRM). The former dynamically fuses adjacent-level features through perceptual feature selection, suppressing irrelevant discrepancies to address multi-temporal feature alignment deviations. The NAFRM utilizes the dual feature selection mechanism to highlight the change sensitive regions and generate spatial masks, suppressing the interference of irrelevant regions or shadows. Extensive experiments verify the effectiveness of the proposed HA2F, which achieves state-of-the-art performance on LEVIR-CD, WHU-CD, and SYSU-CD datasets, surpassing existing comparative methods in terms of both precision metrics and computational efficiency. In addition, ablation experiments show that DHFCM and NAFRM are effective. \href{https://huggingface.co/InPeerReview/RemoteSensingChangeDetection-RSCD.HA2F}{HA2F Official Code is Available Here!}

</details>


### [25] [X-Aligner: Composed Visual Retrieval without the Bells and Whistles](https://arxiv.org/abs/2601.16582)
*Yuqian Zheng,Mariana-Iuliana Georgescu*

Main category: cs.CV

TL;DR: 提出基于视觉语言模型的新型组合视频检索框架，引入跨注意力模块X-Aligner和视觉查询字幕增强，通过两阶段训练实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有组合视频检索框架通常单阶段融合多模态输入，性能提升有限，需要更有效的多模态融合和对齐方法

Method: 基于BLIP架构，引入跨注意力模块X-Aligner渐进融合视觉文本输入并对齐目标视频表示，加入视觉查询字幕增强，采用两阶段训练策略

Result: 在Webvid-CoVR-Test上Recall@1达到63.93%的SOTA性能，在CIRCO和Fashion-IQ上展示强大的零样本泛化能力

Conclusion: 提出的框架通过渐进融合和对齐策略有效提升组合视频检索性能，并具备良好的跨任务泛化能力

Abstract: Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries. However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline. To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs). Our framework incorporates a novel cross-attention module X-Aligner, composed of cross-attention layers that progressively fuse visual and textual inputs and align their multimodal representation with that of the target video. To further enhance the representation of the multimodal query, we incorporate the caption of the visual query as an additional input. The framework is trained in two stages to preserve the pretrained VLM representation. In the first stage, only the newly introduced module is trained, while in the second stage, the textual query encoder is also fine-tuned. We implement our framework on top of BLIP-family architecture, namely BLIP and BLIP-2, and train it on the Webvid-CoVR data set. In addition to in-domain evaluation on Webvid-CoVR-Test, we perform zero-shot evaluations on the Composed Image Retrieval (CIR) data sets CIRCO and Fashion-IQ. Our framework achieves state-of-the-art performance on CoVR obtaining a Recall@1 of 63.93% on Webvid-CoVR-Test, and demonstrates strong zero-shot generalization on CIR tasks.

</details>


### [26] [A Lightweight Medical Image Classification Framework via Self-Supervised Contrastive Learning and Quantum-Enhanced Feature Modeling](https://arxiv.org/abs/2601.16608)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 提出轻量级医学图像分类框架，结合自监督对比学习与量子增强特征建模，在资源受限环境下实现高性能AI诊断


<details>
  <summary>Details</summary>
Motivation: 医学图像分析面临标注稀缺、计算资源有限、模型泛化能力不足等挑战，需要开发轻量高效且性能优越的解决方案

Method: 使用MobileNetV2作为轻量骨干网络，通过SimCLR式自监督预训练，嵌入参数化量子电路作为量子特征增强模块，形成混合经典-量子架构，最后在有限标注数据上进行微调

Result: 仅需约2-3百万参数和低计算成本，在准确率、AUC和F1分数上均优于传统基线方法，特征可视化显示判别性和表示稳定性得到改善

Conclusion: 为资源受限环境下的高性能医学人工智能提供了实用且前瞻性的解决方案，展示了量子增强技术在医学图像分析中的潜力

Abstract: Intelligent medical image analysis is essential for clinical decision support but is often limited by scarce annotations, constrained computational resources, and suboptimal model generalization. To address these challenges, we propose a lightweight medical image classification framework that integrates self-supervised contrastive learning with quantum-enhanced feature modeling. MobileNetV2 is employed as a compact backbone and pretrained using a SimCLR-style self-supervised paradigm on unlabeled images. A lightweight parameterized quantum circuit (PQC) is embedded as a quantum feature enhancement module, forming a hybrid classical-quantum architecture, which is subsequently fine-tuned on limited labeled data. Experimental results demonstrate that, with only approximately 2-3 million parameters and low computational cost, the proposed method consistently outperforms classical baselines without self-supervised learning or quantum enhancement in terms of Accuracy, AUC, and F1-score. Feature visualization further indicates improved discriminability and representation stability. Overall, this work provides a practical and forward-looking solution for high-performance medical artificial intelligence under resource-constrained settings.

</details>


### [27] [Boundary and Position Information Mining for Aerial Small Object Detection](https://arxiv.org/abs/2601.16617)
*Rongxin Huang,Guangfeng Lin,Wenbo Zhou,Zhirong Li,Wenhuan Wu*

Main category: cs.CV

TL;DR: 提出BPIM框架，通过边界和位置信息挖掘解决无人机图像中小目标检测的尺度不平衡和边缘模糊问题，在多个数据集上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 无人机应用中，由于尺度不平衡和边缘模糊，小目标检测存在准确率低的问题。需要有效利用边界和位置信息来提升检测性能。

Method: 提出BPIM框架，包含PIG模块获取位置信息、BIG模块提取边界信息、CSF模块融合浅层特征、TFF模块结合位置和边界信息、AWF模块融合深层语义特征。采用注意力机制和跨尺度特征融合策略。

Result: 在VisDrone2021、DOTA1.0和WiderPerson数据集上，BPIM相比基线Yolov5-P2表现更好，在计算负载相当的情况下达到SOTA性能。

Conclusion: BPIM框架通过整合边界、位置和尺度信息，有效提升了无人机图像中小目标检测的性能，具有实际应用价值。

Abstract: Unmanned Aerial Vehicle (UAV) applications have become increasingly prevalent in aerial photography and object recognition. However, there are major challenges to accurately capturing small targets in object detection due to the imbalanced scale and the blurred edges. To address these issues, boundary and position information mining (BPIM) framework is proposed for capturing object edge and location cues. The proposed BPIM includes position information guidance (PIG) module for obtaining location information, boundary information guidance (BIG) module for extracting object edge, cross scale fusion (CSF) module for gradually assembling the shallow layer image feature, three feature fusion (TFF) module for progressively combining position and boundary information, and adaptive weight fusion (AWF) module for flexibly merging the deep layer semantic feature. Therefore, BPIM can integrate boundary, position, and scale information in image for small object detection using attention mechanisms and cross-scale feature fusion strategies. Furthermore, BPIM not only improves the discrimination of the contextual feature by adaptive weight fusion with boundary, but also enhances small object perceptions by cross-scale position fusion. On the VisDrone2021, DOTA1.0, and WiderPerson datasets, experimental results show the better performances of BPIM compared to the baseline Yolov5-P2, and obtains the promising performance in the state-of-the-art methods with comparable computation load.

</details>


### [28] [SCHIGAND: A Synthetic Facial Generation Mode Pipeline](https://arxiv.org/abs/2601.16627)
*Ananya Kadali,Sunnie Jehan-Morrison,Orasiki Wellington,Barney Evans,Precious Durojaiye,Richard Guest*

Main category: cs.CV

TL;DR: SCHIGAND是一个结合StyleCLIP、HyperStyle、InterfaceGAN和Diffusion模型的新型合成人脸生成管道，能生成高质量、多样化的面部数据集，适用于生物识别测试。


<details>
  <summary>Details</summary>
Motivation: 由于隐私法规、数据稀缺和伦理问题，获取多样化高质量面部数据集面临挑战。现有生成模型难以平衡真实性、多样性和身份保持。

Method: 提出SCHIGAND管道，整合StyleCLIP、HyperStyle、InterfaceGAN和Diffusion模型，增强身份保持同时生成真实的类内变化和保持类间区分度。

Result: 使用ArcFace评估显示，SCHIGAND在图像质量和多样性之间取得平衡，性能优于现有生成模型，可作为真实数据的补充或替代。

Conclusion: SCHIGAND有潜力为面部生物识别应用提供隐私合规、可扩展的合成数据集解决方案，解决现有生成模型的关键限制。

Abstract: The growing demand for diverse and high-quality facial datasets for training and testing biometric systems is challenged by privacy regulations, data scarcity, and ethical concerns. Synthetic facial images offer a potential solution, yet existing generative models often struggle to balance realism, diversity, and identity preservation. This paper presents SCHIGAND, a novel synthetic face generation pipeline integrating StyleCLIP, HyperStyle, InterfaceGAN, and Diffusion models to produce highly realistic and controllable facial datasets. SCHIGAND enhances identity preservation while generating realistic intra-class variations and maintaining inter-class distinctiveness, making it suitable for biometric testing. The generated datasets were evaluated using ArcFace, a leading facial verification model, to assess their effectiveness in comparison to real-world facial datasets. Experimental results demonstrate that SCHIGAND achieves a balance between image quality and diversity, addressing key limitations of prior generative models. This research highlights the potential of SCHIGAND to supplement and, in some cases, replace real data for facial biometric applications, paving the way for privacy-compliant and scalable solutions in synthetic dataset generation.

</details>


### [29] [Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss](https://arxiv.org/abs/2601.16645)
*Minsu Gong,Nuri Ryu,Jungseul Ok,Sunghyun Cho*

Main category: cs.CV

TL;DR: 提出了一种用于潜在扩散模型图像编辑的结构保持损失(SPL)，通过局部线性模型量化结构差异，无需训练即可在生成过程中保持像素级边缘结构。


<details>
  <summary>Details</summary>
Motivation: 当前基于潜在扩散模型的图像编辑方法在保持像素级边缘结构方面存在挑战，这对于照片级风格迁移和色调调整等任务至关重要。

Method: 提出结构保持损失(SPL)，利用局部线性模型量化输入图像与编辑图像之间的结构差异；无需训练，直接集成到扩散模型的生成过程中；包含后处理步骤减少解码失真、掩码策略实现精确编辑定位、颜色保持损失保护未编辑区域色调。

Result: 实验证实SPL能有效增强结构保真度，在基于潜在扩散模型的图像编辑任务中实现了最先进的性能。

Conclusion: 提出的SPL方法解决了潜在扩散模型图像编辑中的结构保持问题，通过训练自由的方式实现了高质量的像素级结构保真。

Abstract: Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model's generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at https://github.com/gongms00/SPL.

</details>


### [30] [Reliable Brain Tumor Segmentation Based on Spiking Neural Networks with Efficient Training](https://arxiv.org/abs/2601.16652)
*Aurora Pia Ghiardelli,Guangzhi Tang,Tao Sun*

Main category: cs.CV

TL;DR: 提出基于脉冲神经网络的3D脑肿瘤分割框架，通过多视角集成提供体素级不确定性估计，采用FPTT降低计算成本，在BraTS数据集上实现竞争性精度和87%的FLOPs减少。


<details>
  <summary>Details</summary>
Motivation: 开发可靠且能量高效的3D脑肿瘤分割方法，适用于医疗物联网和护理点系统，需要解决传统SNN在语义图像分割中训练计算成本高的问题。

Method: 使用脉冲神经网络进行3D脑肿瘤分割，采用矢状面、冠状面和轴状面多视角SNN模型集成提供体素级不确定性估计，通过前向传播通过时间（FPTT）降低训练计算成本。

Result: 在BraTS 2017和BraTS 2023数据集上展示了竞争性的分割精度、良好校准的不确定性估计，以及87%的FLOPs减少，证明了SNN在可靠低功耗医疗系统中的潜力。

Conclusion: 该框架展示了SNN在3D脑肿瘤分割中的可靠性和能量效率，为医疗物联网和护理点系统提供了有前景的低功耗解决方案。

Abstract: We propose a reliable and energy-efficient framework for 3D brain tumor segmentation using spiking neural networks (SNNs). A multi-view ensemble of sagittal, coronal, and axial SNN models provides voxel-wise uncertainty estimation and enhances segmentation robustness. To address the high computational cost in training SNN models for semantic image segmentation, we employ Forward Propagation Through Time (FPTT), which maintains temporal learning efficiency with significantly reduced computational cost. Experiments on the Multimodal Brain Tumor Segmentation Challenges (BraTS 2017 and BraTS 2023) demonstrate competitive accuracy, well-calibrated uncertainty, and an 87% reduction in FLOPs, underscoring the potential of SNNs for reliable, low-power medical IoT and Point-of-Care systems.

</details>


### [31] [ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction](https://arxiv.org/abs/2601.16672)
*Ming Li,Hui Shan,Kai Zheng,Chentao Shen,Siyu Liu,Yanwei Fu,Zhen Chen,Xiangru Huang*

Main category: cs.CV

TL;DR: ReWeaver：从稀疏多视角RGB图像重建拓扑精确的3D服装和缝纫图案的框架，解决了现有方法无法准确重建服装拓扑和缝纫结构的问题。


<details>
  <summary>Details</summary>
Motivation: 现有服装重建方法通常依赖非结构化表示（如3D高斯泼溅），难以准确重建服装拓扑和缝纫结构，导致重建结果不适合高保真物理模拟。需要解决数字化身、虚拟试穿和机器人操作等应用中的模拟到现实差距问题。

Method: 提出ReWeaver框架，仅需4个输入视角即可预测2D UV空间和3D空间中的接缝、面板及其连接性。构建大规模数据集GCD-TS，包含多视角RGB图像、3D服装几何、纹理人体网格和标注缝纫图案，包含超过100,000个合成样本。

Result: ReWeaver在拓扑精度、几何对齐和接缝-面板一致性方面始终优于现有方法。预测的接缝和面板与多视角图像精确对齐，产生适用于3D感知、高保真物理模拟和机器人操作的结构化2D-3D服装表示。

Conclusion: ReWeaver能够从稀疏多视角图像重建拓扑精确的3D服装和缝纫图案，为数字化身、虚拟试穿和机器人操作等应用提供了高质量的结构化服装表示，有效缩小了模拟到现实的差距。

Abstract: High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency.

</details>


### [32] [Affinity Contrastive Learning for Skeleton-based Human Activity Understanding](https://arxiv.org/abs/2601.16694)
*Hongda Liu,Yunfan Liu,Min Ren,Lin Sui,Yunlong Wang,Zhenan Sun*

Main category: cs.CV

TL;DR: ACLNet提出亲和对比学习网络，通过探索人体动作类别间的聚类关系来提升特征判别力，使用亲和度量、动态温度调度和边界对比策略，在多个骨架数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有骨架动作理解方法多采用对比学习，但未能充分利用类别间的结构相似性，且忽略了异常正样本的影响，需要更有效的特征学习机制。

Method: 提出亲和对比学习网络（ACLNet）：1）使用亲和度量改进相似性测量，形成动作超类提供更丰富的对比信号；2）引入动态温度调度自适应调整不同超类的惩罚强度；3）采用基于边界的对比策略改善类内难正负样本的分离。

Result: 在NTU RGB+D 60、NTU RGB+D 120、Kinetics-Skeleton、PKU-MMD、FineGYM和CASIA-B等数据集上的大量实验表明，该方法在骨架动作识别、步态识别和行人重识别任务上具有优越性。

Conclusion: ACLNet通过探索动作类别间的聚类关系，有效提升了骨架动作理解的性能，为对比学习在人体动作分析中的应用提供了新思路。

Abstract: In skeleton-based human activity understanding, existing methods often adopt the contrastive learning paradigm to construct a discriminative feature space. However, many of these approaches fail to exploit the structural inter-class similarities and overlook the impact of anomalous positive samples. In this study, we introduce ACLNet, an Affinity Contrastive Learning Network that explores the intricate clustering relationships among human activity classes to improve feature discrimination. Specifically, we propose an affinity metric to refine similarity measurements, thereby forming activity superclasses that provide more informative contrastive signals. A dynamic temperature schedule is also introduced to adaptively adjust the penalty strength for various superclasses. In addition, we employ a margin-based contrastive strategy to improve the separation of hard positive and negative samples within classes. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, Kinetics-Skeleton, PKU-MMD, FineGYM, and CASIA-B demonstrate the superiority of our method in skeleton-based action recognition, gait recognition, and person re-identification. The source code is available at https://github.com/firework8/ACLNet.

</details>


### [33] [CER-HV: A CER-Based Human-in-the-Loop Framework for Cleaning Datasets Applied to Arabic-Script HTR](https://arxiv.org/abs/2601.16713)
*Sana Al-azzawi,Elisa Barney,Marcus Liwicki*

Main category: cs.CV

TL;DR: 提出CER-HV框架检测和清理阿拉伯文字手写识别数据集中的标签错误，结合CER噪声检测器和人工验证，显著提升数据质量，并在多个数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯文字手写识别落后于拉丁文字，现有数据集存在标签质量问题（转录、分割、方向、非文本内容等错误），限制了模型性能提升。

Method: CER-HV框架：1）基于CER的噪声检测器，使用精心配置的CRNN模型并采用早停策略避免过拟合噪声样本；2）人机协同（HITL）步骤，人工验证高排名样本。

Result: 在多个数据集上实现SOTA：KHATT（8.45% CER）、PHTI（8.26%）、Ajami（10.66%）、Muharaf（10.11%）。CER-HV在较干净数据集上提升0.3-0.6% CER，在噪声较大数据集上提升1.0-1.8% CER。噪声检测精度达80-90%。

Conclusion: 数据质量是阿拉伯文字手写识别的关键限制因素，CER-HV框架能有效检测和清理标签错误，提升模型性能，且框架具有通用性可应用于其他文本识别数据集。

Abstract: Handwritten text recognition (HTR) for Arabic-script languages still lags behind Latin-script HTR, despite recent advances in model architectures, datasets, and benchmarks. We show that data quality is a significant limiting factor in many published datasets and propose CER-HV (CER-based Ranking with Human Verification) as a framework to detect and clean label errors. CER-HV combines a CER-based noise detector, built on a carefully configured Convolutional Recurrent Neural Network (CRNN) with early stopping to avoid overfitting noisy samples, and a human-in-the-loop (HITL) step that verifies high-ranking samples. The framework reveals that several existing datasets contain previously underreported problems, including transcription, segmentation, orientation, and non-text content errors. These have been identified with up to 90 percent precision in the Muharaf and 80-86 percent in the PHTI datasets.
  We also show that our CRNN achieves state-of-the-art performance across five of the six evaluated datasets, reaching 8.45 percent Character Error Rate (CER) on KHATT (Arabic), 8.26 percent on PHTI (Pashto), 10.66 percent on Ajami, and 10.11 percent on Muharaf (Arabic), all without any data cleaning. We establish a new baseline of 11.3 percent CER on the PHTD (Persian) dataset. Applying CER-HV improves the evaluation CER by 0.3-0.6 percent on the cleaner datasets and 1.0-1.8 percent on the noisier ones. Although our experiments focus on documents written in an Arabic-script language, including Arabic, Persian, Urdu, Ajami, and Pashto, the framework is general and can be applied to other text recognition datasets.

</details>


### [34] [Using Shadows in Circular Synthetic Aperture Sonar Imaging for Target Analysis](https://arxiv.org/abs/2601.16733)
*Yann Le Gall,Nicolas Burlet,Mathieu Simon,Fabien Novella,Samantha Dugelay,Jean-Philippe Malkasse*

Main category: cs.CV

TL;DR: 该论文提出从圆形合成孔径声纳数据中提取阴影信息的方法，用于改进目标分析和三维重建，通过子孔径滤波和固定聚焦阴影增强技术恢复阴影信息。


<details>
  <summary>Details</summary>
Motivation: 圆形合成孔径声纳提供360度海底视图，但传统处理会丢失阴影信息。阴影包含目标形状的互补信息，对目标识别和减少误报至关重要。

Method: 使用子孔径滤波获取沿圆形轨迹不同视角的图像集合，应用固定聚焦阴影增强获得清晰阴影，提出交互界面可视化阴影，采用空间雕刻重建方法从分割阴影推断三维形状。

Result: 结果表明阴影在圆形合成孔径声纳中具有改善目标分析和三维重建的潜力，验证了所提方法的有效性。

Conclusion: 从圆形合成孔径声纳数据中提取阴影信息能够增强目标分析和三维重建能力，为水雷战中的目标识别提供更全面的信息。

Abstract: Circular Synthetic Aperture Sonar (CSAS) provides a 360° azimuth view of the seabed, surpassing the limited aperture and mono-view image of conventional side-scan SAS. This makes CSAS a valuable tool for target recognition in mine warfare where the diversity of point of view is essential for reducing false alarms. CSAS processing typically produces a very high-resolution two-dimensional image. However, the parallax introduced by the circular displacement of the illuminator fill-in the shadow regions, and the shadow cast by an object on the seafloor is lost in favor of azimuth coverage and resolution. Yet the shadows provide complementary information on target shape useful for target recognition. In this paper, we explore a way to retrieve shadow information from CSAS data to improve target analysis and carry 3D reconstruction. Sub-aperture filtering is used to get a collection of images at various points of view along the circular trajectory and fixed focus shadow enhancement (FFSE) is applied to obtain sharp shadows. An interactive interface is also proposed to allow human operators to visualize these shadows along the circular trajectory. A space-carving reconstruction method is applied to infer the 3D shape of the object from the segmented shadows. The results demonstrate the potential of shadows in circular SAS for improving target analysis and 3D reconstruction.

</details>


### [35] [A Step to Decouple Optimization in 3DGS](https://arxiv.org/abs/2601.16736)
*Renjie Ding,Yaonan Wang,Min Liu,Jialin Zhu,Jiazheng Wang,Jiahao Zhao,Wenting Shen,Feixiang He,Xiang Che*

Main category: cs.CV

TL;DR: 该论文分析了3D高斯泼溅（3DGS）优化中的耦合问题，提出了解耦优化方法，并最终设计了AdamW-GS优化器以同时提升优化效率和表示效果。


<details>
  <summary>Details</summary>
Motivation: 3DGS作为实时新视角合成的强大技术，其优化过程采用了深度神经网络中广泛接受的优化方法。然而，考虑到3DGS的物理意义和特定设计，存在两个被忽视的优化细节：更新步骤耦合和梯度耦合，这些复杂耦合问题尚未被充分探索。

Method: 重新审视3DGS优化过程，将其解耦并重组为三个组件：稀疏Adam、重新状态正则化和解耦属性正则化。在3DGS和3DGS-MCMC框架下进行大量实验，基于经验分析重新设计优化器，提出AdamW-GS。

Result: 通过解耦分析提供了对这些组件的更深入理解。提出的AdamW-GS优化器能够同时实现更好的优化效率和表示效果。

Conclusion: 该工作揭示了3DGS优化中的耦合问题，提出了有效的解耦和重新耦合方法，AdamW-GS优化器在优化效率和表示效果方面均表现出优越性。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time novel view synthesis. As an explicit representation optimized through gradient propagation among primitives, optimization widely accepted in deep neural networks (DNNs) is actually adopted in 3DGS, such as synchronous weight updating and Adam with the adaptive gradient. However, considering the physical significance and specific design in 3DGS, there are two overlooked details in the optimization of 3DGS: (i) update step coupling, which induces optimizer state rescaling and costly attribute updates outside the viewpoints, and (ii) gradient coupling in the moment, which may lead to under- or over-effective regularization. Nevertheless, such a complex coupling is under-explored. After revisiting the optimization of 3DGS, we take a step to decouple it and recompose the process into: Sparse Adam, Re-State Regularization and Decoupled Attribute Regularization. Taking a large number of experiments under the 3DGS and 3DGS-MCMC frameworks, our work provides a deeper understanding of these components. Finally, based on the empirical analysis, we re-design the optimization and propose AdamW-GS by re-coupling the beneficial components, under which better optimization efficiency and representation effectiveness are achieved simultaneously.

</details>


### [36] [Automated Road Crack Localization to Guide Highway Maintenance](https://arxiv.org/abs/2601.16737)
*Steffen Knoblauch,Ram Kumar Muthusamy,Pedram Ghamisi,Alexander Zipf*

Main category: cs.CV

TL;DR: 该研究开发了一个利用开源数据（航空影像和OpenStreetMap）微调YOLOv11进行高速公路裂缝检测的框架，并提出了瑞士相对裂缝密度指数来指导全国公路维护。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致温度波动加剧公路路面应力，增加了维护成本，需要更精准高效的维护策略。研究探索开源数据在指导高速公路基础设施维护方面的潜力。

Method: 整合航空影像和OpenStreetMap数据，微调YOLOv11模型进行高速公路裂缝定位。计算瑞士相对裂缝密度指数，并与长期地表温度振幅和交通流量进行相关性分析。

Result: 裂缝分类模型在正类（裂缝）上F1分数为0.84，负类（无裂缝）为0.97。瑞士相对裂缝密度指数与长期地表温度振幅和交通流量的相关性较弱，但在城市中心和交叉口附近观察到显著高值。

Conclusion: 开源数据共享能够推动创新，为公共部门提供更高效的解决方案。提出的裂缝密度指数为公路维护提供了超越传统数据的额外价值，特别是在城市区域。

Abstract: Highway networks are crucial for economic prosperity. Climate change-induced temperature fluctuations are exacerbating stress on road pavements, resulting in elevated maintenance costs. This underscores the need for targeted and efficient maintenance strategies. This study investigates the potential of open-source data to guide highway infrastructure maintenance. The proposed framework integrates airborne imagery and OpenStreetMap (OSM) to fine-tune YOLOv11 for highway crack localization. To demonstrate the framework's real-world applicability, a Swiss Relative Highway Crack Density (RHCD) index was calculated to inform nationwide highway maintenance. The crack classification model achieved an F1-score of $0.84$ for the positive class (crack) and $0.97$ for the negative class (no crack). The Swiss RHCD index exhibited weak correlations with Long-term Land Surface Temperature Amplitudes (LT-LST-A) (Pearson's $r\ = -0.05$) and Traffic Volume (TV) (Pearson's $r\ = 0.17$), underlining the added value of this novel index for guiding maintenance over other data. Significantly high RHCD values were observed near urban centers and intersections, providing contextual validation for the predictions. These findings highlight the value of open-source data sharing to drive innovation, ultimately enabling more efficient solutions in the public sector.

</details>


### [37] [Curated endoscopic retrograde cholangiopancreatography images dataset](https://arxiv.org/abs/2601.16759)
*Alda João Andrade,Mónica Martins,André Ferreira,Tarcísio Araújo,Luís Lopes,Victor Alves*

Main category: cs.CV

TL;DR: 该研究提供了一个大型、经过人工标注的ERCP图像数据集，包含19,018张原始图像和19,317张处理后的图像，其中5,519张已标注，可用于AI辅助的胆胰疾病诊断。


<details>
  <summary>Details</summary>
Motivation: ERCP是胆胰疾病诊断和治疗的关键技术，人工智能有望实现自动化诊断，但公开的ERCP数据集稀缺，限制了相关AI方法的发展和应用。

Method: 收集了来自1,602名患者的ERCP图像，包括19,018张原始图像和19,317张处理后图像。所有图像由两名经验丰富的胃肠病学家手动检查和标注，并由另一名资深专家审核，确保数据质量。

Result: 创建了一个包含5,519张已标注图像的大型ERCP数据集，并通过分类实验验证了数据集的实用性和有效性，为自动ERCP分析提供了基准数据集。

Conclusion: 该研究填补了ERCP公共数据集的空白，为AI辅助的胆胰疾病诊断提供了高质量基准数据集，有望推动自动ERCP分析技术的发展。

Abstract: Endoscopic Retrograde Cholangiopancreatography (ERCP) is a key procedure in the diagnosis and treatment of biliary and pancreatic diseases. Artificial intelligence has been pointed as one solution to automatize diagnosis. However, public ERCP datasets are scarce, which limits the use of such approach. Therefore, this study aims to help fill this gap by providing a large and curated dataset. The collection is composed of 19.018 raw images and 19.317 processed from 1.602 patients. 5.519 images are labeled, which provides a ready to use dataset. All images were manually inspected and annotated by two gastroenterologist with more than 5 years of experience and reviewed by another gastroenterologist with more than 20 years of experience, all with more than 400 ERCP procedures annually. The utility and validity of the dataset is proven by a classification experiment. This collection aims to provide or contribute for a benchmark in automatic ERCP analysis and diagnosis of biliary and pancreatic diseases.

</details>


### [38] [Flow Matching for Probabilistic Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2601.16763)
*Cuong Le,Pavló Melnyk,Bastian Wandt,Mårten Wadenbäck*

Main category: cs.CV

TL;DR: FMPose：基于流匹配生成方法的概率式3D人体姿态估计，通过连续归一化流学习从简单分布到合理3D姿态分布的最优传输，相比扩散方法更快更准确。


<details>
  <summary>Details</summary>
Motivation: 单目相机视角下的3D人体姿态恢复由于深度模糊性是一个高度不适定问题。传统方法常产生错误但过度自信的3D估计，需要引入不确定性测量来处理姿态估计的不确定性。

Method: 提出FMPose，基于流匹配生成方法的概率式3D人体姿态估计。以2D线索为条件，通过连续归一化流学习从简单源分布到合理3D人体姿态分布的最优传输。2D提升条件通过图卷积网络建模，利用人体关节间的可学习连接作为图结构进行特征聚合。

Result: 相比基于扩散的方法，FMPose通过最优传输产生更快更准确的3D姿态生成。在三个常用3D人体姿态估计基准测试（Human3.6M、MPI-INF-3DHP和3DPW）上，FMPose相比当前最先进方法有显著改进。

Conclusion: FMPose通过流匹配生成方法和最优传输，有效解决了3D人体姿态估计中的不确定性问题，相比扩散方法在速度和准确性上都有优势，在多个基准测试上达到了最先进性能。

Abstract: Recovering 3D human poses from a monocular camera view is a highly ill-posed problem due to the depth ambiguity. Earlier studies on 3D human pose lifting from 2D often contain incorrect-yet-overconfident 3D estimations. To mitigate the problem, emerging probabilistic approaches treat the 3D estimations as a distribution, taking into account the uncertainty measurement of the poses. Falling in a similar category, we proposed FMPose, a probabilistic 3D human pose estimation method based on the flow matching generative approach. Conditioned on the 2D cues, the flow matching scheme learns the optimal transport from a simple source distribution to the plausible 3D human pose distribution via continuous normalizing flows. The 2D lifting condition is modeled via graph convolutional networks, leveraging the learnable connections between human body joints as the graph structure for feature aggregation. Compared to diffusion-based methods, the FMPose with optimal transport produces faster and more accurate 3D pose generations. Experimental results show major improvements of our FMPose over current state-of-the-art methods on three common benchmarks for 3D human pose estimation, namely Human3.6M, MPI-INF-3DHP and 3DPW.

</details>


### [39] [AutoRegressive Generation with B-rep Holistic Token Sequence Representation](https://arxiv.org/abs/2601.16771)
*Jiahao Li,Yunpeng Bai,Yongkang Dai,Hao Guo,Hongping Gan,Yilei Shi*

Main category: cs.CV

TL;DR: BrepARG：首个将B-rep几何与拓扑编码为整体token序列的方法，支持基于序列的自回归生成，实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有B-rep表示方法将几何和拓扑特征解耦处理，无法应用transformer等序列生成框架，限制了生成性能

Method: 将B-rep编码为三种token：几何token、位置token和面索引token；分层构建整体token序列：先构建几何块（面和边），再进行几何块排序，最后组装整个B-rep的序列表示；使用带因果掩码的解码器自回归模型学习token序列分布

Result: BrepARG实现了最先进的性能，验证了将B-rep表示为整体token序列的可行性

Conclusion: BrepARG开创了B-rep生成的新方向，证明了序列表示方法在B-rep生成中的有效性

Abstract: Previous representation and generation approaches for the B-rep relied on graph-based representations that disentangle geometric and topological features through decoupled computational pipelines, thereby precluding the application of sequence-based generative frameworks, such as transformer architectures that have demonstrated remarkable performance. In this paper, we propose BrepARG, the first attempt to encode B-rep's geometry and topology into a holistic token sequence representation, enabling sequence-based B-rep generation with an autoregressive architecture. Specifically, BrepARG encodes B-rep into 3 types of tokens: geometry and position tokens representing geometric features, and face index tokens representing topology. Then the holistic token sequence is constructed hierarchically, starting with constructing the geometry blocks (i.e., faces and edges) using the above tokens, followed by geometry block sequencing. Finally, we assemble the holistic sequence representation for the entire B-rep. We also construct a transformer-based autoregressive model that learns the distribution over holistic token sequences via next-token prediction, using a multi-layer decoder-only architecture with causal masking. Experiments demonstrate that BrepARG achieves state-of-the-art (SOTA) performance. BrepARG validates the feasibility of representing B-rep as holistic token sequences, opening new directions for B-rep generation.

</details>


### [40] [CASP: Few-Shot Class-Incremental Learning with CLS Token Attention Steering Prompts](https://arxiv.org/abs/2601.16773)
*Shuai Huang,Xuhan Lin,Yuwu Lu*

Main category: cs.CV

TL;DR: 提出CASP方法，通过CLS令牌注意力引导提示来改进少样本类增量学习，在CLS令牌的查询、键、值投影中引入类共享可训练偏置参数，结合注意力扰动和流形令牌混合策略，在多个数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 少样本类增量学习面临的核心挑战是模型需要快速适应新类别同时缓解灾难性遗忘。现有基于提示的方法在极端少样本增量设置下，模型的迁移和泛化能力变得至关重要，需要利用预训练知识学习可在基础会话中跨未来类别共享的特征表示。

Method: 提出CLS令牌注意力引导提示（CASP），在CLS令牌的查询、键、值投影中引入类共享可训练偏置参数来显式调节自注意力权重。同时设计注意力扰动策略，并在浅层特征空间执行流形令牌混合，合成潜在新类别特征以提升泛化能力。

Result: 在CUB200、CIFAR100和ImageNet-R数据集上的实验表明，CASP在标准和细粒度FSCIL设置中都优于最先进方法，在增量阶段无需微调，同时显著减少参数开销。

Conclusion: CASP方法通过CLS令牌注意力引导和特征增强策略，有效解决了少样本类增量学习中的泛化和遗忘问题，在保持低参数开销的同时实现了优越性能。

Abstract: Few-shot class-incremental learning (FSCIL) presents a core challenge in continual learning, requiring models to rapidly adapt to new classes with very limited samples while mitigating catastrophic forgetting. Recent prompt-based methods, which integrate pretrained backbones with task-specific prompts, have made notable progress. However, under extreme few-shot incremental settings, the model's ability to transfer and generalize becomes critical, and it is thus essential to leverage pretrained knowledge to learn feature representations that can be shared across future categories during the base session. Inspired by the mechanism of the CLS token, which is similar to human attention and progressively filters out task-irrelevant information, we propose the CLS Token Attention Steering Prompts (CASP). This approach introduces class-shared trainable bias parameters into the query, key, and value projections of the CLS token to explicitly modulate the self-attention weights. To further enhance generalization, we also design an attention perturbation strategy and perform Manifold Token Mixup in the shallow feature space, synthesizing potential new class features to improve generalization and reserve the representation capacity for upcoming tasks. Experiments on the CUB200, CIFAR100, and ImageNet-R datasets demonstrate that CASP outperforms state-of-the-art methods in both standard and fine-grained FSCIL settings without requiring fine-tuning during incremental phases and while significantly reducing the parameter overhead.

</details>


### [41] [SLD: Segmentation-Based Landmark Detection for Spinal Ligaments](https://arxiv.org/abs/2601.16782)
*Lara Blomenkamp,Ivanna Kramer,Sabine Bauer,Theresa Schöche*

Main category: cs.CV

TL;DR: 提出一种新颖的脊柱韧带附着点检测方法，通过形状分割和领域规则实现高精度检测，在所有脊柱区域都表现出色


<details>
  <summary>Details</summary>
Motivation: 在生物力学建模中，韧带附着点的准确表示对于模拟椎骨间作用力至关重要。现有自动化检测方法要么局限于特定脊柱区域，要么精度不足，需要更可靠、通用的解决方案。

Method: 首先对3D椎骨进行基于形状的分割，然后应用领域特定规则来识别不同类型的附着点。该方法结合了形状分析和解剖学知识。

Result: 在两个独立的多患者脊柱数据集上验证，平均绝对误差（MAE）为0.7毫米，均方根误差（RMSE）为1.1毫米，优于现有方法，并在所有脊柱区域表现出强泛化能力。

Conclusion: 该方法为脊柱生物力学建模提供了高精度、通用的韧带附着点检测解决方案，显著提升了脊柱模型的可靠性。

Abstract: In biomechanical modeling, the representation of ligament attachments is crucial for a realistic simulation of the forces acting between the vertebrae. These forces are typically modeled as vectors connecting ligament landmarks on adjacent vertebrae, making precise identification of these landmarks a key requirement for constructing reliable spine models. Existing automated detection methods are either limited to specific spinal regions or lack sufficient accuracy. This work presents a novel approach for detecting spinal ligament landmarks, which first performs shape-based segmentation of 3D vertebrae and subsequently applies domain-specific rules to identify different types of attachment points. The proposed method outperforms existing approaches by achieving high accuracy and demonstrating strong generalization across all spinal regions. Validation on two independent spinal datasets from multiple patients yielded a mean absolute error (MAE) of 0.7 mm and a root mean square error (RMSE) of 1.1 mm.

</details>


### [42] [REL-SF4PASS: Panoramic Semantic Segmentation with REL Depth Representation and Spherical Fusion](https://arxiv.org/abs/2601.16788)
*Xuewei Li,Xinghan Bao,Zhimin Chen,Xi Li*

Main category: cs.CV

TL;DR: 提出REL-SF4PASS方法，通过圆柱坐标系的REL深度表示和球形动态多模态融合(SMMF)，改进全景语义分割性能


<details>
  <summary>Details</summary>
Motivation: 现有全景语义分割方法主要关注球面几何的RGB输入或使用原始/HHA格式的深度信息，未能充分利用全景图像的几何特性

Method: 提出REL深度表示（包含校正深度、高程增益垂直倾角和横向方位角）在圆柱坐标系中完整表示3D空间和表面法线方向；设计SMMF融合策略，针对不同全景区域采用不同融合方式，减少ERP投影中圆柱侧面展开的断裂

Result: 在Stanford2D3D Panoramic数据集上，所有3个fold的平均mIoU提升2.35%，面对3D干扰时性能方差减少约70%

Conclusion: REL-SF4PASS通过圆柱坐标系的深度表示和区域自适应的多模态融合，显著提升了全景语义分割的性能和鲁棒性

Abstract: As an important and challenging problem in computer vision, Panoramic Semantic Segmentation (PASS) aims to give complete scene perception based on an ultra-wide angle of view. Most PASS methods often focus on spherical geometry with RGB input or using the depth information in original or HHA format, which does not make full use of panoramic image geometry. To address these shortcomings, we propose REL-SF4PASS with our REL depth representation based on cylindrical coordinate and Spherical-dynamic Multi-Modal Fusion SMMF. REL is made up of Rectified Depth, Elevation-Gained Vertical Inclination Angle, and Lateral Orientation Angle, which fully represents 3D space in cylindrical coordinate style and the surface normal direction. SMMF aims to ensure the diversity of fusion for different panoramic image regions and reduce the breakage of cylinder side surface expansion in ERP projection, which uses different fusion strategies to match the different regions in panoramic images. Experimental results show that REL-SF4PASS considerably improves performance and robustness on popular benchmark, Stanford2D3D Panoramic datasets. It gains 2.35% average mIoU improvement on all 3 folds and reduces the performance variance by approximately 70% when facing 3D disturbance.

</details>


### [43] [Incorporating Eye-Tracking Signals Into Multimodal Deep Visual Models For Predicting User Aesthetic Experience In Residential Interiors](https://arxiv.org/abs/2601.16811)
*Chen-Ying Chien,Po-Chih Kuo*

Main category: cs.CV

TL;DR: 提出双分支CNN-LSTM框架，融合视觉特征和眼动信号预测室内空间美学评价，在客观维度达到72.2%准确率，主观维度66.8%


<details>
  <summary>Details</summary>
Motivation: 室内空间美学体验预测困难，因为感知主观性强且视觉响应复杂。需要结合视觉特征和生理信号来更准确预测美学评价

Method: 收集224个室内设计视频和28名参与者的同步眼动数据，构建双分支CNN-LSTM框架融合视觉特征和眼动信号，预测15个美学维度

Result: 模型在客观维度（如光线）准确率72.2%，主观维度（如放松感）66.8%，优于现有视频基线；眼动训练模型仅用视觉输入也能保持性能；瞳孔反应对客观评估贡献最大

Conclusion: 眼动追踪作为训练时的特权信息具有重要价值，能增强主观评价预测，为室内设计美学评估提供更实用工具

Abstract: Understanding how people perceive and evaluate interior spaces is essential for designing environments that promote well-being. However, predicting aesthetic experiences remains difficult due to the subjective nature of perception and the complexity of visual responses. This study introduces a dual-branch CNN-LSTM framework that fuses visual features with eye-tracking signals to predict aesthetic evaluations of residential interiors. We collected a dataset of 224 interior design videos paired with synchronized gaze data from 28 participants who rated 15 aesthetic dimensions. The proposed model attains 72.2% accuracy on objective dimensions (e.g., light) and 66.8% on subjective dimensions (e.g., relaxation), outperforming state-of-the-art video baselines and showing clear gains on subjective evaluation tasks. Notably, models trained with eye-tracking retain comparable performance when deployed with visual input alone. Ablation experiments further reveal that pupil responses contribute most to objective assessments, while the combination of gaze and visual cues enhances subjective evaluations. These findings highlight the value of incorporating eye-tracking as privileged information during training, enabling more practical tools for aesthetic assessment in interior design.

</details>


### [44] [ColorConceptBench: A Benchmark for Probabilistic Color-Concept Understanding in Text-to-Image Models](https://arxiv.org/abs/2601.16836)
*Chenxi Ruan,Yu Xiao,Yihan Hou,Guosheng Hu,Wei Zeng*

Main category: cs.CV

TL;DR: 本文提出了ColorConceptBench基准，用于评估文本到图像模型对隐含颜色概念的理解能力，发现当前模型在抽象语义敏感性方面存在不足，且这种限制难以通过常规干预解决。


<details>
  <summary>Details</summary>
Motivation: 虽然文本到图像模型已有显著进展，但它们在将颜色与隐含概念关联方面的能力尚未得到充分探索。现有方法主要关注显式颜色名称或代码，缺乏对抽象颜色语义的系统评估。

Method: 提出了ColorConceptBench基准，这是一个基于概率颜色分布的人类标注基准，包含1,281个隐含颜色概念和6,369个人类标注，用于系统评估颜色概念关联能力。

Result: 对七个领先的文本到图像模型的评估显示，当前模型缺乏对抽象语义的敏感性，且这种限制对标准干预（如模型缩放和引导）具有抵抗力。

Conclusion: 实现类人的颜色语义理解不仅需要更大的模型，更需要模型学习和表示隐含意义方式的根本性转变。

Abstract: While text-to-image (T2I) models have advanced considerably, their capability to associate colors with implicit concepts remains underexplored. To address the gap, we introduce ColorConceptBench, a new human-annotated benchmark to systematically evaluate color-concept associations through the lens of probabilistic color distributions. ColorConceptBench moves beyond explicit color names or codes by probing how models translate 1,281 implicit color concepts using a foundation of 6,369 human annotations. Our evaluation of seven leading T2I models reveals that current models lack sensitivity to abstract semantics, and crucially, this limitation appears resistant to standard interventions (e.g., scaling and guidance). This demonstrates that achieving human-like color semantics requires more than larger models, but demands a fundamental shift in how models learn and represent implicit meaning.

</details>


### [45] [No Validation, No Problem: Predicting Model Performance from a Single Gradient](https://arxiv.org/abs/2601.16874)
*Fangzheng Wu,Brian Summa*

Main category: cs.CV

TL;DR: 提出一种无需验证集的检查点选择方法：使用分类头梯度的Frobenius范数作为代理指标，通过单次前向-反向传播即可计算，在多种网络架构上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统检查点选择和早停需要验证集，消耗额外计算和标注资源。本文旨在开发一种无需验证集的轻量级方法，仅通过单次前向-反向传播就能评估模型性能。

Method: 提出使用分类头梯度Frobenius范数||g||_F = ||dL/dW||_F作为性能代理指标。在一个分离特征批次上计算，通过最小化该指标在训练后期窗口中选择检查点。针对不同网络家族（经典CNN、Transformer、现代CNN）提出不同的归一化策略。

Result: 在ImageNet-1k上，该方法与最优检查点的差距仅为4.24%±2.00%（通用设置），经轻微调优后可降至约1.12%。同样适用于COCO检测/分割任务，在扩散模型（CIFAR-10）中也能有效跟踪训练进度，与FID负相关。

Conclusion: 分类头梯度范数是一种高效、无需验证集的检查点选择代理指标，计算开销极低（<0.1%训练时间），可作为验证集选择的替代方案，适用于多种任务和网络架构。

Abstract: We propose a validation-free checkpointing signal from a single forward-backward pass: the Frobenius norm of the classifier-head gradient on one detached-feature batch, ||g||_F = ||dL/dW||_F. Across ImageNet-1k CNNs and Transformers, this proxy is strongly negative with Top-1 and positive with loss. Selecting the checkpoint with the minimum head gradient in a short tail window closes most of the gap to the oracle (4.24% +/- 2.00% with a universal setup, about 1.12% with light per-family tuning). For practical deployment, a head-scale normalization is more stable within classic CNN families (e.g., ResNets), while a feature-scale normalization works well for Transformers and modern CNNs. The same one-batch probe also predicts COCO detection/segmentation mAP. In diffusion (UNet/DDPM on CIFAR-10), it tracks progress and enables near-oracle tail-window selection; it is positively correlated with same-distribution probe MSE and negatively with FID (lower is better), so it can be used as a lightweight, label-free monitor. Validation labels are never used beyond reporting. The probe adds much less than 0.1% of an epoch and works as a drop-in for validation-free checkpoint selection and early stopping.

</details>


### [46] [GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss](https://arxiv.org/abs/2601.16885)
*Yangfan Xu,Lilian Zhang,Xiaofeng He,Pengdong Wu,Wenqi Wu,Jun Mao*

Main category: cs.CV

TL;DR: 提出自监督框架训练VGGT模型，无需真实标签即可增强大规模环境中的定位能力，通过序列几何约束和联合优化损失实现


<details>
  <summary>Details</summary>
Motivation: 现有VGGT模型依赖真实标签训练，难以适应无标签和未见场景，需要自监督方法提升模型在无标签数据上的定位能力

Method: 将传统成对关系扩展到序列几何约束，采样多个源帧并几何投影到不同目标帧以提升时序特征一致性，将物理光度一致性和几何约束公式化为联合优化损失

Result: 模型在数百次迭代内收敛，在大规模定位任务中取得显著改进，局部和全局跨视图注意力层以及相机和深度头都能有效捕捉多视图几何

Conclusion: 提出的自监督框架成功训练VGGT模型，无需真实标签即可提升大规模环境定位能力，为无标签场景的视觉几何理解提供了有效解决方案

Abstract: Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.

</details>


### [47] [Evaluating Large Vision-language Models for Surgical Tool Detection](https://arxiv.org/abs/2601.16895)
*Nakul Poudel,Richard Simon,Cristian A. Linte*

Main category: cs.CV

TL;DR: 评估大型视觉语言模型在手术工具检测任务中的表现，发现Qwen2.5在零样本和微调设置下均优于其他VLM，并与Grounding DINO相比具有更好的零样本泛化能力


<details>
  <summary>Details</summary>
Motivation: 当前大多数手术AI系统是单模态的，无法全面理解手术工作流程，需要能够综合建模手术场景相关组件的通用手术AI系统。大型视觉语言模型在多模态数据处理方面具有潜力，但在手术应用中的系统性研究有限。

Method: 在GraSP机器人手术数据集上评估三种最先进的VLM（Qwen2.5、LLaVA1.5和InternVL3.5），采用零样本和参数高效的LoRA微调两种设置，并与开放集检测基线Grounding DINO进行比较。

Result: Qwen2.5在两种配置下均表现出最佳检测性能；与Grounding DINO相比，Qwen2.5具有更强的零样本泛化能力和相当的微调性能；Qwen2.5在工具识别方面更优，而Grounding DINO在定位方面更强。

Conclusion: 大型视觉语言模型在手术工具检测任务中具有潜力，Qwen2.5表现出色，特别是在零样本泛化方面。未来研究应进一步探索VLM在手术场景中的综合应用。

Abstract: Surgery is a highly complex process, and artificial intelligence has emerged as a transformative force in supporting surgical guidance and decision-making. However, the unimodal nature of most current AI systems limits their ability to achieve a holistic understanding of surgical workflows. This highlights the need for general-purpose surgical AI systems capable of comprehensively modeling the interrelated components of surgical scenes. Recent advances in large vision-language models that integrate multimodal data processing offer strong potential for modeling surgical tasks and providing human-like scene reasoning and understanding. Despite their promise, systematic investigations of VLMs in surgical applications remain limited. In this study, we evaluate the effectiveness of large VLMs for the fundamental surgical vision task of detecting surgical tools. Specifically, we investigate three state-of-the-art VLMs, Qwen2.5, LLaVA1.5, and InternVL3.5, on the GraSP robotic surgery dataset under both zero-shot and parameter-efficient LoRA fine-tuning settings. Our results demonstrate that Qwen2.5 consistently achieves superior detection performance in both configurations among the evaluated VLMs. Furthermore, compared with the open-set detection baseline Grounding DINO, Qwen2.5 exhibits stronger zero-shot generalization and comparable fine-tuned performance. Notably, Qwen2.5 shows superior instrument recognition, while Grounding DINO demonstrates stronger localization.

</details>


### [48] [LoL: Longer than Longer, Scaling Video Generation to Hour](https://arxiv.org/abs/2601.16914)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 提出一种轻量级、无需训练的方法来解决长视频生成中的sink-collapse问题，通过多头RoPE抖动打破注意力同质化，实现实时、流式、无限长度视频生成


<details>
  <summary>Details</summary>
Motivation: 当前自回归长视频生成模型存在错误累积和长期一致性丢失问题，虽然引入了注意力sink帧来缓解性能衰减，但会导致sink-collapse现象：生成内容反复回到sink帧，造成场景重置和循环运动模式

Method: 提出轻量级、无需训练的方法，通过引入多头RoPE抖动来打破多头注意力机制中的同质化，缓解长期崩溃问题

Result: 方法成功缓解了sink-collapse问题，同时保持了生成质量，实现了实时、流式、无限长度视频生成，生成了长达12小时的连续视频

Conclusion: 该方法有效解决了长视频生成中的sink-collapse问题，首次展示了实时、流式、无限长度视频生成能力，在流式视频生成领域取得了最长的公开演示结果

Abstract: Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink frame, resulting in abrupt scene resets and cyclic motion patterns. Our analysis reveals that sink-collapse originates from an inherent conflict between the periodic structure of Rotary Position Embedding (RoPE) and the multi-head attention mechanisms prevalent in current generative models. To address it, we propose a lightweight, training-free approach that effectively suppresses this behavior by introducing multi-head RoPE jitter that breaks inter-head attention homogenization and mitigates long-horizon collapse. Extensive experiments show that our method successfully alleviates sink-collapse while preserving generation quality. To the best of our knowledge, this work achieves the first demonstration of real-time, streaming, and infinite-length video generation with little quality decay. As an illustration of this robustness, we generate continuous videos up to 12 hours in length, which, to our knowledge, is among the longest publicly demonstrated results in streaming video generation.

</details>


### [49] [Reward-Forcing: Autoregressive Video Generation with Reward Feedback](https://arxiv.org/abs/2601.16933)
*Jingran Zhang,Ning Li,Yuanhao Ban,Andrew Bai,Justin Cui*

Main category: cs.CV

TL;DR: 本文提出一种使用奖励信号引导视频生成的方法，替代依赖教师模型的传统自回归方法，在保持高质量的同时简化训练流程。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成的自回归方法严重依赖教师模型，当缺乏强大的自回归教师时性能受限，输出质量通常低于双向模型。需要一种更高效、可扩展的自回归生成方法。

Method: 采用奖励信号引导生成过程，通过奖励信号指导模型，简化训练流程，同时保持高视觉保真度和时间一致性。

Result: 在标准基准测试中，该方法与现有自回归模型性能相当，在某些情况下甚至超过类似规模的双向模型。在VBench上获得84.92分，接近需要大量异构蒸馏的最先进自回归方法（84.31分）。

Conclusion: 使用奖励信号引导的自回归视频生成方法能够避免教师架构的限制，实现高效、可扩展的生成，在保持高质量的同时简化训练流程。

Abstract: While most prior work in video generation relies on bidirectional architectures, recent efforts have sought to adapt these models into autoregressive variants to support near real-time generation. However, such adaptations often depend heavily on teacher models, which can limit performance, particularly in the absence of a strong autoregressive teacher, resulting in output quality that typically lags behind their bidirectional counterparts. In this paper, we explore an alternative approach that uses reward signals to guide the generation process, enabling more efficient and scalable autoregressive generation. By using reward signals to guide the model, our method simplifies training while preserving high visual fidelity and temporal consistency. Through extensive experiments on standard benchmarks, we find that our approach performs comparably to existing autoregressive models and, in some cases, surpasses similarly sized bidirectional models by avoiding constraints imposed by teacher architectures. For example, on VBench, our method achieves a total score of 84.92, closely matching state-of-the-art autoregressive methods that score 84.31 but require significant heterogeneous distillation.

</details>


### [50] [Domain-invariant Mixed-domain Semi-supervised Medical Image Segmentation with Clustered Maximum Mean Discrepancy Alignment](https://arxiv.org/abs/2601.16954)
*Ba-Thinh Lam,Thanh-Huy Nguyen,Hoang-Thien Nguyen,Quang-Khai Bui-Tran,Nguyen Lan Vi Vu,Phat K. Huynh,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: 提出一种域不变混合域半监督分割框架，通过跨域复制粘贴机制增强数据多样性，利用聚类最大均值差异块对齐特征，在仅有少量标注和多个未知域差异下实现鲁棒分割。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中，深度学习依赖大规模专家标注和一致数据分布，但实际中标注稀缺且图像来自多扫描仪/中心，形成混合域设置（未知域标签、严重域差异）。现有半监督或域适应方法通常假设单一域偏移或需要显式域索引，这在现实部署中很少成立。

Method: 提出域不变混合域半监督分割框架：1) 复制粘贴机制(CPM)：通过跨域转移信息区域增强训练集；2) 聚类最大均值差异(CMMD)块：聚类未标注特征并通过MMD目标与标注锚点对齐，鼓励域不变表示；3) 集成在教师-学生框架中。

Result: 在Fundus和M&Ms基准测试中，该方法在仅有少量标注示例和多个未知域差异下，始终优于半监督和域适应方法，实现了鲁棒且精确的分割。

Conclusion: 该方法为混合域半监督医学图像分割提供了一个潜在解决方案，能够有效增强数据多样性并减轻域偏差，在现实世界部署场景中具有应用价值。

Abstract: Deep learning has shown remarkable progress in medical image semantic segmentation, yet its success heavily depends on large-scale expert annotations and consistent data distributions. In practice, annotations are scarce, and images are collected from multiple scanners or centers, leading to mixed-domain settings with unknown domain labels and severe domain gaps. Existing semi-supervised or domain adaptation approaches typically assume either a single domain shift or access to explicit domain indices, which rarely hold in real-world deployment. In this paper, we propose a domain-invariant mixed-domain semi-supervised segmentation framework that jointly enhances data diversity and mitigates domain bias. A Copy-Paste Mechanism (CPM) augments the training set by transferring informative regions across domains, while a Cluster Maximum Mean Discrepancy (CMMD) block clusters unlabeled features and aligns them with labeled anchors via an MMD objective, encouraging domain-invariant representations. Integrated within a teacher-student framework, our method achieves robust and precise segmentation even with very few labeled examples and multiple unknown domain discrepancies. Experiments on Fundus and M&Ms benchmarks demonstrate that our approach consistently surpasses semi-supervised and domain adaptation methods, establishing a potential solution for mixed-domain semi-supervised medical image segmentation.

</details>


### [51] [VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents](https://arxiv.org/abs/2601.16973)
*Zirui Wang,Junyi Zhang,Jiaxin Ge,Long Lian,Letian Fu,Lisa Dunlap,Ken Goldberg,XuDong Wang,Ion Stoica,David M. Chan,Sewon Min,Joseph E. Gonzalez*

Main category: cs.CV

TL;DR: VisGym是一个包含17个环境的评估平台，用于测试和训练视觉语言模型在多步视觉交互中的表现，发现前沿模型在交互设置中表现不佳，并揭示了模型在长上下文利用、视觉表示等方面的具体局限性。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型在多步视觉交互中的表现尚未得到充分研究，特别是在感知、记忆和动作在长时间跨度上的整合方面。需要系统性的评估框架来理解模型在交互环境中的能力与局限。

Method: 开发了VisGym平台，包含17个环境，涵盖符号谜题、真实图像理解、导航和操作任务。平台提供难度、输入表示、规划视野和反馈的灵活控制，并提供了生成结构化演示的多步求解器，支持监督微调。

Result: 所有前沿模型在交互设置中都表现不佳，在简单配置中成功率仅46.6%，在困难配置中仅26.0%。模型难以有效利用长上下文，在无界历史记录下的表现比截断窗口更差。文本符号任务一旦以视觉形式呈现，难度显著增加。

Conclusion: 显式目标观察、文本反馈以及在部分可观测或未知动态设置中的探索性演示能带来一致的性能提升，这为改进多步视觉决策提供了具体的方向。VisGym平台为系统评估和训练视觉语言模型的交互能力提供了重要工具。

Abstract: Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.

</details>


### [52] [SyncLight: Controllable and Consistent Multi-View Relighting](https://arxiv.org/abs/2601.16981)
*David Serrano-Lozano,Anand Bhattad,Luis Herranz,Jean-François Lalonde,Javier Vazquez-Corral*

Main category: cs.CV

TL;DR: SyncLight是首个实现多视角无标定场景下一致参数化重光照的方法，通过单次推理即可在多视图间传播光照编辑，无需相机位姿信息。


<details>
  <summary>Details</summary>
Motivation: 现有生成式单视角重光照方法在多摄像机广播、立体电影和虚拟制作中难以保持严格的光照一致性，需要一种能在多视图间实现一致重光照的技术。

Method: 采用基于潜在桥接匹配的多视图扩散变换器，通过单次推理实现整个图像集的高保真重光照；使用大规模混合数据集（合成环境+真实世界多视图捕获）进行训练。

Result: 仅使用图像对训练，却能零样本泛化到任意数量的视点，有效传播所有视图的光照变化，无需相机位姿信息，实现了实用的多视图捕获系统重光照工作流。

Conclusion: SyncLight首次实现了多视图无标定场景下的一致参数化重光照，为多摄像机广播、立体电影和虚拟制作提供了实用的重光照解决方案。

Abstract: We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene. While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production. SyncLight addresses this by enabling precise control over light intensity and color across a multi-view capture of a scene, conditioned on a single reference edit. Our method leverages a multi-view diffusion transformer trained using a latent bridge matching formulation, achieving high-fidelity relighting of the entire image set in a single inference step. To facilitate training, we introduce a large-scale hybrid dataset comprising diverse synthetic environments -- curated from existing sources and newly designed scenes -- alongside high-fidelity, real-world multi-view captures under calibrated illumination. Surprisingly, though trained only on image pairs, SyncLight generalizes zero-shot to an arbitrary number of viewpoints, effectively propagating lighting changes across all views, without requiring camera pose information. SyncLight enables practical relighting workflows for multi-view capture systems.

</details>


### [53] [AnyView: Synthesizing Any Novel View in Dynamic Scenes](https://arxiv.org/abs/2601.16982)
*Basile Van Hoorick,Dian Chen,Shun Iwase,Pavel Tokmakov,Muhammad Zubair Irshad,Igor Vasiljevic,Swati Gupta,Fangzhou Cheng,Sergey Zakharov,Vitor Campagnolo Guizilini*

Main category: cs.CV

TL;DR: AnyView是一个基于扩散模型的视频生成框架，用于动态视角合成，能够在任意相机位置和轨迹上生成零样本新视频，在极端动态场景中保持时空一致性。


<details>
  <summary>Details</summary>
Motivation: 现代生成视频模型虽然能产生高质量输出，但在高度动态的真实世界环境中难以保持多视角和时空一致性。现有方法通常需要视角之间有显著重叠，限制了在极端动态场景中的应用。

Method: 提出AnyView框架，利用多种数据源（单目2D、多视角静态3D、多视角动态4D数据集）训练通用的时空隐式表示，最小化归纳偏置和几何假设，实现从任意相机轨迹的零样本视频生成。

Result: 在标准基准测试中取得与当前最先进方法竞争的结果；在提出的AnyViewBench极端动态视角合成基准上，大多数基线方法性能大幅下降，而AnyView仍能生成真实、合理且时空一致的视频。

Conclusion: AnyView展示了在极端动态场景中从任意视角生成时空一致视频的能力，为动态视角合成提供了有效的解决方案，特别是在视角重叠有限的挑战性场景中表现优异。

Abstract: Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \textbf{AnyView}, a diffusion-based video generation framework for \emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \textbf{AnyViewBench}, a challenging new benchmark tailored towards \emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \emph{any} viewpoint. Results, data, code, and models can be viewed at: https://tri-ml.github.io/AnyView/

</details>
