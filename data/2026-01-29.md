<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Size Matters: Reconstructing Real-Scale 3D Models from Monocular Images for Food Portion Estimation](https://arxiv.org/abs/2601.20051)
*Gautham Vinod,Bruce Coburn,Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出一种从单目图像恢复真实比例3D重建的方法，用于精确饮食评估，相比现有技术减少近30%的体积估计误差。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病如肥胖和糖尿病需要准确监测食物摄入量，但现有AI饮食评估方法难以从单目图像恢复食物真实比例信息，限制了精准营养领域的应用。

Method: 利用在大规模数据集上训练的模型提取丰富视觉特征，估计重建对象的真实比例，将单视角3D重建转换为具有物理意义的真实比例模型。

Result: 在两个公开数据集上的实验表明，该方法持续优于现有技术，平均绝对体积估计误差减少近30%。

Conclusion: 该方法成功连接了3D计算机视觉与数字健康领域，通过恢复真实比例的3D重建对象，有望增强精准营养领域的应用潜力。

Abstract: The rise of chronic diseases related to diet, such as obesity and diabetes, emphasizes the need for accurate monitoring of food intake. While AI-driven dietary assessment has made strides in recent years, the ill-posed nature of recovering size (portion) information from monocular images for accurate estimation of ``how much did you eat?'' is a pressing challenge. Some 3D reconstruction methods have achieved impressive geometric reconstruction but fail to recover the crucial real-world scale of the reconstructed object, limiting its usage in precision nutrition. In this paper, we bridge the gap between 3D computer vision and digital health by proposing a method that recovers a true-to-scale 3D reconstructed object from a monocular image. Our approach leverages rich visual features extracted from models trained on large-scale datasets to estimate the scale of the reconstructed object. This learned scale enables us to convert single-view 3D reconstructions into true-to-life, physically meaningful models. Extensive experiments and ablation studies on two publicly available datasets show that our method consistently outperforms existing techniques, achieving nearly a 30% reduction in mean absolute volume-estimation error, showcasing its potential to enhance the domain of precision nutrition. Code: https://gitlab.com/viper-purdue/size-matters

</details>


### [2] [DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2601.20064)
*Zhen Yao,Xin Li,Taotao Jing,Shuai Zhang,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: DiSa：一种新颖的显著性感知前景-背景解耦框架，通过显式结合显著性线索解决开放词汇语义分割中的前景偏见和空间定位限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（如CLIP）的开放词汇语义分割方法存在两个关键限制：1）前景偏见，倾向于忽略背景区域；2）有限的空间定位能力，导致物体边界模糊。这些限制源于VLMs在图像-文本对预训练中对显著、物体中心区域的偏见。

Method: 提出DiSa框架，包含两个核心模块：1）显著性感知解耦模块（SDM），通过显式结合显著性线索，以前景-背景解耦的方式分别建模前景和背景集成特征；2）分层细化模块（HRM），利用像素级空间上下文，通过多级更新实现通道级特征细化。

Result: 在六个基准测试上的广泛实验表明，DiSa始终优于最先进的方法。

Conclusion: DiSa通过显著性感知的前景-背景解耦框架有效解决了开放词汇语义分割中的前景偏见和空间定位限制问题，在多个基准测试上取得了优越性能。

Abstract: Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.

</details>


### [3] [Semi-Supervised Masked Autoencoders: Unlocking Vision Transformer Potential with Limited Data](https://arxiv.org/abs/2601.20072)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.CV

TL;DR: SSMAE是一种半监督视觉Transformer训练框架，结合掩码图像重建和分类任务，通过验证驱动的门控机制动态选择伪标签，在标签稀缺场景下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉Transformer在标签数据稀缺但无标签数据丰富场景下的训练挑战，传统方法存在确认偏差问题，需要更有效的半监督学习策略。

Method: 提出SSMAE框架，联合优化掩码图像重建和分类任务；引入验证驱动的门控机制，仅在模型预测可靠且一致时激活伪标签；结合弱增强和强增强视图的一致性验证。

Result: 在CIFAR-10和CIFAR-100上持续优于监督ViT和微调MAE，在10%标签的CIFAR-10上比ViT提升9.24%；证明伪标签引入时机与生成方式同等重要。

Conclusion: SSMAE通过动态伪标签选择和验证驱动门控机制，有效解决了半监督视觉Transformer训练中的确认偏差问题，为数据高效Transformer训练提供了新思路。

Abstract: We address the challenge of training Vision Transformers (ViTs) when labeled data is scarce but unlabeled data is abundant. We propose Semi-Supervised Masked Autoencoder (SSMAE), a framework that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. SSMAE introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions that are consistent across both weakly and strongly augmented views of the same image, reducing confirmation bias. On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Our results demonstrate that when pseudo-labels are introduced is as important as how they are generated for data-efficient transformer training. Codes are available at https://github.com/atik666/ssmae.

</details>


### [4] [Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning](https://arxiv.org/abs/2601.20075)
*Chuan Qin,Constantin Venhoff,Sonia Joseph,Fanyi Xiao,Stefan Scherer*

Main category: cs.CV

TL;DR: 提出稀疏CLIP训练方法，在保持性能的同时提升表征可解释性，挑战了可解释性与性能互斥的传统观念


<details>
  <summary>Details</summary>
Motivation: CLIP作为视觉-语言表示学习的基石，其稠密不透明的潜在表征存在显著可解释性挑战。传统观点认为可解释性与性能存在矛盾，后处理方法如稀疏自编码器会降低下游性能并丧失多模态能力。

Method: 提出简单有效的稀疏CLIP训练方法，将稀疏性直接集成到CLIP训练过程中，而不是采用后处理方式。该方法产生既可解释又高性能的表示。

Result: 相比稀疏自编码器，稀疏CLIP表示保持了强大的下游任务性能，实现了更优的可解释性，并保留了多模态能力。多模态稀疏特征支持直接的语义概念对齐，揭示了跨模态知识的训练动态。

Conclusion: 挑战了可解释性需要牺牲准确性的传统观念，证明可解释性和性能可以协同优化，为未来模型提供了有前景的设计原则。通过概念验证，在稀疏CLIP表示上训练的视觉-语言模型实现了可解释的视觉引导能力。

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP's inherent multimodal capabilities, with most learned features remaining unimodal.
  We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.

</details>


### [5] [NucFuseRank: Dataset Fusion and Performance Ranking for Nuclei Instance Segmentation](https://arxiv.org/abs/2601.20104)
*Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Sepideh Hatamikia,Diana Mechtcheriakova,Amirreza Mahbod*

Main category: cs.CV

TL;DR: 该研究聚焦于H&E染色图像中的细胞核实例分割数据集，而非开发新模型。通过文献综述识别公开数据集，将其标准化为统一格式，使用两种SOTA分割模型评估数据集性能，提出统一测试集(NucFuse-test)用于公平跨数据集评估，以及统一训练集(NucFuse-train)通过融合多数据集提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前细胞核实例分割研究主要关注开发新算法并在有限数据集上评估，缺乏对数据集本身的系统性分析。不同数据集格式不统一，难以进行公平比较和有效利用。本研究旨在填补这一空白，为细胞核实例分割提供数据集层面的基准。

Method: 1. 通过文献综述识别手动标注的公开H&E染色图像数据集；2. 将数据集标准化为统一的输入和标注格式；3. 使用两种SOTA分割模型（基于CNN和CNN-Vision Transformer混合架构）系统评估数据集性能；4. 提出NucFuse-test用于公平跨数据集评估；5. 提出NucFuse-train通过融合多数据集图像提升性能。

Result: 1. 系统评估并排名了多个公开数据集的细胞核实例分割性能；2. 创建了统一的测试集(NucFuse-test)和训练集(NucFuse-train)；3. 进行了全面的分析、外部验证，并将实现代码公开；4. 为H&E染色组织图像的细胞核实例分割模型训练、测试和评估提供了新基准。

Conclusion: 该研究通过关注数据集而非模型开发，为细胞核实例分割领域提供了重要的数据集基准。通过标准化数据集、系统评估性能、创建融合数据集和公开实现，促进了该领域的公平比较和模型性能提升，为下游任务提供了更可靠的基础。

Abstract: Nuclei instance segmentation in hematoxylin and eosin (H&E)-stained images plays an important role in automated histological image analysis, with various applications in downstream tasks. While several machine learning and deep learning approaches have been proposed for nuclei instance segmentation, most research in this field focuses on developing new segmentation algorithms and benchmarking them on a limited number of arbitrarily selected public datasets.
  In this work, rather than focusing on model development, we focused on the datasets used for this task. Based on an extensive literature review, we identified manually annotated, publicly available datasets of H&E-stained images for nuclei instance segmentation and standardized them into a unified input and annotation format. Using two state-of-the-art segmentation models, one based on convolutional neural networks (CNNs) and one based on a hybrid CNN and vision transformer architecture, we systematically evaluated and ranked these datasets based on their nuclei instance segmentation performance. Furthermore, we proposed a unified test set (NucFuse-test) for fair cross-dataset evaluation and a unified training set (NucFuse-train) for improved segmentation performance by merging images from multiple datasets.
  By evaluating and ranking the datasets, performing comprehensive analyses, generating fused datasets, conducting external validation, and making our implementation publicly available, we provided a new benchmark for training, testing, and evaluating nuclei instance segmentation models on H&E-stained histological images.

</details>


### [6] [Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing](https://arxiv.org/abs/2601.20107)
*Zhuchenyang Liu,Ziyu Hu,Yao Zhang,Yu Xiao*

Main category: cs.CV

TL;DR: 提出SAP训练无关剪枝方法，通过识别中间层关键视觉补丁实现90%以上索引向量压缩，同时保持检索性能


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如ColPali）的视觉文档检索存在索引向量过大问题，而训练无关剪枝方法在高压缩场景（>80%）下表现不佳，甚至不如随机选择。先前研究认为视觉令牌重要性是查询依赖的，质疑训练无关剪枝的可行性。

Method: 提出Structural Anchor Pruning (SAP)训练无关剪枝方法，从中间层识别关键视觉补丁；引入Oracle Score Retention (OSR)协议评估层间信息对压缩效率的影响。

Result: 在ViDoRe基准测试中，SAP将索引向量减少超过90%，同时保持稳健的检索保真度；OSR分析显示语义结构锚点补丁存在于中间层，而非传统方法关注的结构信号消散的最终层。

Conclusion: SAP为视觉RAG提供了高度可扩展的解决方案，证明了通过中间层识别结构锚点补丁可以实现高效训练无关压缩，挑战了先前关于视觉令牌重要性查询依赖的结论。

Abstract: Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.

</details>


### [7] [Efficient Token Pruning for LLaDA-V](https://arxiv.org/abs/2601.20168)
*Zhewen Wan,Tianchen Song,Chen Lin,Zhiyong Zhao,Xianpeng Lang*

Main category: cs.CV

TL;DR: 提出针对扩散式多模态模型LLaDA-V的结构化token剪枝策略，通过分析注意力机制发现其跨模态信息主要在中间到深层聚合，因此在首个去噪步骤的中深层剪枝视觉token，减少65%计算成本同时保持95%任务性能。


<details>
  <summary>Details</summary>
Motivation: 扩散式多模态模型如LLaDA-V存在计算开销大的问题，因为其双向注意力机制和迭代去噪范式需要重复处理视觉token。研究发现与自回归解码器不同，LLaDA-V的跨模态信息主要在中间到深层聚合，导致语义对齐延迟。

Method: 提出结构化token剪枝策略，基于注意力分析发现LLaDA-V的跨模态信息主要在中间到深层聚合，因此在首个去噪步骤的中深层选择性剪除视觉token，减少FLOPs同时保留关键语义信息。与FastV的浅层剪枝不同，本方法针对延迟注意力聚合特性进行优化。

Result: 在多个基准测试中，最佳配置减少计算成本达65%，同时保持平均95%的任务性能。这是首个研究扩散式多模态模型中结构化token剪枝的工作。

Conclusion: 该框架为高效LLaDA-V推理提供了经验基础，突出了视觉感知剪枝在扩散式多模态模型中的潜力。通过针对模型特性的结构化剪枝，可以在大幅减少计算开销的同时保持性能。

Abstract: Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.

</details>


### [8] [TeleStyle: Content-Preserving Style Transfer in Images and Videos](https://arxiv.org/abs/2601.20175)
*Shiwen Zhang,Xiaoyan Yang,Bojia Zi,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleStyle是一个基于Qwen-Image-Edit构建的轻量级图像和视频风格迁移模型，通过课程持续学习框架处理干净和噪声数据，实现了内容保持和风格定制的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决扩散变换器(DiTs)在内容保持风格迁移中面临的内容与风格特征纠缠问题，实现既能保持原始内容又能应用新风格的高质量图像和视频风格化。

Method: 基于Qwen-Image-Edit构建，使用高质量特定风格数据集和合成多样化风格三元组，采用课程持续学习框架训练，并引入视频到视频风格化模块增强时间一致性。

Result: 在风格相似性、内容一致性和美学质量三个核心评估指标上达到最先进性能，能够泛化到未见过的风格而不损害内容保真度。

Conclusion: TeleStyle通过创新的课程持续学习框架和高质量数据集，成功解决了内容保持风格迁移的挑战，为图像和视频风格化提供了有效的解决方案。

Abstract: Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle

</details>


### [9] [Automated Marine Biofouling Assessment: Benchmarking Computer Vision and Multimodal LLMs on the Level of Fouling Scale](https://arxiv.org/abs/2601.20196)
*Brayden Hamilton,Tim Cashmore,Peter Driscoll,Trevor Gee,Henry Williams*

Main category: cs.CV

TL;DR: 该研究评估了计算机视觉模型和大型多模态语言模型在船体生物污损严重程度自动分类上的表现，发现两者各有优势，混合方法最有前景。


<details>
  <summary>Details</summary>
Motivation: 船舶船体上的海洋生物污损带来严重的生态、经济和生物安全风险。传统的潜水员检查方法既危险又难以大规模应用，需要开发自动化评估方案。

Method: 研究使用了卷积神经网络、基于Transformer的分割模型和零样本大型多模态语言模型，在专家标注的新西兰初级产业部数据集上评估了生物污损严重程度（LoF等级）的分类性能。

Result: 计算机视觉模型在极端LoF类别上表现出高准确率，但在中间等级上因数据集不平衡和图像构图问题而表现不佳。LLMs通过结构化提示和检索实现了有竞争力的性能，无需训练且输出可解释。

Conclusion: 不同方法具有互补优势，将分割覆盖率与LLM推理相结合的混合方法为可扩展且可解释的生物污损评估提供了有前景的途径。

Abstract: Marine biofouling on vessel hulls poses major ecological, economic, and biosecurity risks. Traditional survey methods rely on diver inspections, which are hazardous and limited in scalability. This work investigates automated classification of biofouling severity on the Level of Fouling (LoF) scale using both custom computer vision models and large multimodal language models (LLMs). Convolutional neural networks, transformer-based segmentation, and zero-shot LLMs were evaluated on an expert-labelled dataset from the New Zealand Ministry for Primary Industries. Computer vision models showed high accuracy at extreme LoF categories but struggled with intermediate levels due to dataset imbalance and image framing. LLMs, guided by structured prompts and retrieval, achieved competitive performance without training and provided interpretable outputs. The results demonstrate complementary strengths across approaches and suggest that hybrid methods integrating segmentation coverage with LLM reasoning offer a promising pathway toward scalable and interpretable biofouling assessment.

</details>


### [10] [DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment](https://arxiv.org/abs/2601.20218)
*Haoyou Deng,Keyu Yan,Chaojie Mao,Xiang Wang,Yu Liu,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: DenseGRPO提出了一种基于密集奖励的偏好对齐框架，通过评估每个去噪步骤的细粒度贡献来解决现有GRPO方法中的稀疏奖励问题，并引入奖励感知的探索空间校准机制。


<details>
  <summary>Details</summary>
Motivation: 现有基于流匹配模型的GRPO方法在文本到图像生成中取得了显著进展，但仍面临稀疏奖励问题：整个去噪轨迹的终端奖励被应用于所有中间步骤，导致全局反馈信号与中间去噪步骤的精确细粒度贡献不匹配。

Method: 1) 提出预测每个去噪步骤的逐步奖励增益作为密集奖励，通过基于ODE的方法在中间干净图像上应用奖励模型；2) 基于估计的密集奖励，提出奖励感知方案，通过自适应调整SDE采样器中特定时间步的随机性注入来校准探索空间。

Result: 在多个标准基准测试上的广泛实验证明了DenseGRPO的有效性，并突显了有效密集奖励在流匹配模型对齐中的关键作用。

Conclusion: DenseGRPO通过引入密集奖励和奖励感知的探索空间校准，解决了现有GRPO方法中的稀疏奖励问题，显著提升了文本到图像生成中的人类偏好对齐效果。

Abstract: Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.

</details>


### [11] [Feature Projection Learning for Better Vision-Language Reasoning](https://arxiv.org/abs/2601.20224)
*Yi Zhang,Weicheng Lin,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: 提出FPL方法，通过特征投影学习将分类问题转化为特征投影问题，结合CLIP原始预测，在少量监督下高效适应下游任务


<details>
  <summary>Details</summary>
Motivation: 现有方法在将VLP模型（如CLIP）适应下游任务时存在性能有限、可学习参数过多或训练时间过长的问题，需要更高效有效的适应方法

Method: 提出特征投影学习(FPL)方法：开发投影模型将类别原型特征投影到查询图像特征空间并重建查询图像特征图，使用负平均平方重建误差作为类别分数，将分类问题转化为特征投影问题

Result: FPL在准确率上显著超越当前最先进方法，表现出优越性能

Conclusion: FPL是一种简单、高效且有效的方法，能够成功解决现有VLP模型适应方法的局限性，在少量监督下实现更好的下游任务适应

Abstract: Vision-Language Pre-Trained models, notably CLIP, that utilize contrastive learning have proven highly adept at extracting generalizable visual features. To inherit the well-learned knowledge of VLP models for downstream tasks, several approaches aim to adapt them efficiently with limited supervision. However, these methods either suffer from limited performance, excessive learnable parameters, or extended training times, all of which hinder their effectiveness in adapting the CLIP model to downstream tasks. In this work, we propose a simple yet efficient and effective method called \textit{\textbf{F}eature \textbf{P}rojection \textbf{L}earning(FPL)} to address these problems. Specifically, we develop a projection model that projects class prototype features into the query image feature space and reconstructs the query image feature map. The negative average squared reconstruction error is used as the class score. In this way, we transform the classification problem into a feature projection problem. The final output of this method is a combination of the prediction from the projection model and the original pre-trained CLIP. Comprehensive empirical evaluations confirm that FPL delivers superior accuracy, surpassing the current state-of-the-art methods by a substantial margin.

</details>


### [12] [Visual Prompt-Agnostic Evolution](https://arxiv.org/abs/2601.20232)
*Junze Wang,Lei Fan,Dezheng Zhang,Weipeng Jing,Donglin Di,Yang Song,Sidong Liu,Cong Cong*

Main category: cs.CV

TL;DR: PAE通过频率域初始化、共享Koopman算子和Lyapunov稳定性正则化，解决了VPT训练不稳定问题，加速收敛并提升精度


<details>
  <summary>Details</summary>
Motivation: 现有视觉提示调优方法存在训练不稳定问题，表现为梯度振荡。浅层提示过早停滞，深层提示高方差振荡，导致跨层不匹配，影响收敛速度和最终性能

Method: 提出Prompt-Agnostic Evolution (PAE)：1) 频率域视角初始化提示，传播主干网络用于识别的频率捷径模式；2) 使用共享Koopman算子实现全局线性变换而非层特定更新；3) 基于Lyapunov稳定性理论引入正则化约束误差放大

Result: 在25个数据集上平均加速收敛1.41倍，精度提升1-3%。PAE与多种VPT变体兼容，无需修改主干网络或推理时变更

Conclusion: PAE通过建模提示动态，解决了VPT训练不稳定问题，实现了更稳定、更高效的视觉提示调优

Abstract: Visual Prompt Tuning (VPT) adapts a frozen Vision Transformer (ViT) to downstream tasks by inserting a small number of learnable prompt tokens into the token sequence at each layer. However, we observe that existing VPT variants often suffer from unstable training dynamics, characterized by gradient oscillations. A layer-wise analysis reveals that shallow-layer prompts tend to stagnate early, while deeper-layer prompts exhibit high-variance oscillations, leading to cross-layer mismatch. These issues slow convergence and degrade final performance. To address these challenges, we propose Prompt-Agnostic Evolution ($\mathtt{PAE}$), which strengthens vision prompt tuning by explicitly modeling prompt dynamics. From a frequency-domain perspective, we initialize prompts in a task-aware direction by uncovering and propagating frequency shortcut patterns that the backbone inherently exploits for recognition. To ensure coherent evolution across layers, we employ a shared Koopman operator that imposes a global linear transformation instead of uncoordinated, layer-specific updates. Finally, inspired by Lyapunov stability theory, we introduce a regularizer that constrains error amplification during evolution. Extensive experiments show that $\mathtt{PAE}$ accelerates convergence with an average $1.41\times$ speedup and improves accuracy by 1--3% on 25 datasets across multiple downstream tasks. Beyond performance, $\mathtt{PAE}$ is prompt-agnostic and lightweight, and it integrates seamlessly with diverse VPT variants without backbone modification or inference-time changes.

</details>


### [13] [BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning](https://arxiv.org/abs/2601.20246)
*Jan Niklas Kolf,Ozan Tezcan,Justin Theiss,Hyung Jun Kim,Wentao Bao,Bhargav Bhushanam,Khushi Gupta,Arun Kejariwal,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: BLenDeR是一种基于扩散模型的采样方法，通过集合论启发的并集和交集操作控制生成样本的类内多样性，提升深度度量学习性能。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型能生成高质量合成数据，用于增强深度度量学习中的真实数据，但现有生成方法在可控地增加类内多样性方面存在局限。

Method: 提出BLenDeR方法，利用集合论启发的并集和交集操作处理去噪残差：并集操作鼓励多个提示中的任何属性，交集操作通过主成分替代提取共同方向，从而可控合成类内多样属性组合。

Result: 在标准DML基准测试中，BLenDeR在多个数据集和骨干网络上持续优于最先进的基线方法，在CUB-200上Recall@1提升3.7%，在Cars-196上提升1.8%。

Conclusion: BLenDeR通过可控的类内多样性增强，有效解决了现有生成方法的局限性，显著提升了深度度量学习的性能。

Abstract: The rise of Deep Generative Models (DGM) has enabled the generation of high-quality synthetic data. When used to augment authentic data in Deep Metric Learning (DML), these synthetic samples enhance intra-class diversity and improve the performance of downstream DML tasks. We introduce BLenDeR, a diffusion sampling method designed to increase intra-class diversity for DML in a controllable way by leveraging set-theory inspired union and intersection operations on denoising residuals. The union operation encourages any attribute present across multiple prompts, while the intersection extracts the common direction through a principal component surrogate. These operations enable controlled synthesis of diverse attribute combinations within each class, addressing key limitations of existing generative approaches. Experiments on standard DML benchmarks demonstrate that BLenDeR consistently outperforms state-of-the-art baselines across multiple datasets and backbones. Specifically, BLenDeR achieves 3.7% increase in Recall@1 on CUB-200 and a 1.8% increase on Cars-196, compared to state-of-the-art baselines under standard experimental settings.

</details>


### [14] [Reversible Efficient Diffusion for Image Fusion](https://arxiv.org/abs/2601.20260)
*Xingxin Xu,Bing Cao,DongDong Li,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出可逆高效扩散模型（RED），一种显式监督训练框架，用于多模态图像融合，结合扩散模型的生成能力同时避免分布估计问题


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现出色，但在图像融合任务中容易丢失细节，这源于马尔可夫过程中的噪声误差累积。同时，将显式监督纳入端到端训练存在计算效率挑战

Method: 提出可逆高效扩散模型（RED），这是一个显式监督训练框架，继承扩散模型的强大生成能力，同时避免分布估计问题

Result: 论文声称RED模型能够解决扩散模型在图像融合中的细节丢失问题，同时提高计算效率

Conclusion: RED框架为多模态图像融合提供了一种有效的解决方案，结合了扩散模型的生成优势，同时克服了传统扩散模型在融合任务中的局限性

Abstract: Multi-modal image fusion aims to consolidate complementary information from diverse source images into a unified representation. The fused image is expected to preserve fine details and maintain high visual fidelity. While diffusion models have demonstrated impressive generative capabilities in image generation, they often suffer from detail loss when applied to image fusion tasks. This issue arises from the accumulation of noise errors inherent in the Markov process, leading to inconsistency and degradation in the fused results. However, incorporating explicit supervision into end-to-end training of diffusion-based image fusion introduces challenges related to computational efficiency. To address these limitations, we propose the Reversible Efficient Diffusion (RED) model - an explicitly supervised training framework that inherits the powerful generative capability of diffusion models while avoiding the distribution estimation.

</details>


### [15] [Hallucination Begins Where Saliency Drops](https://arxiv.org/abs/2601.20279)
*Xiaofeng Zhang,Yuanchao Zhu,Chaochen Gu,Xiaosong Yuan,Qiyan Zhao,Jiawei Cao,Feilong Tang,Sinan Fan,Yaomin Shen,Chen Shen,Hao Tang*

Main category: cs.CV

TL;DR: 提出LVLMs-Saliency框架，通过融合注意力权重和梯度信号来检测和缓解大型视觉语言模型中的幻觉问题，包含SGRS和LocoRE两种推理时机制。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖前向注意力模式，无法可靠区分幻觉和事实输出，忽略了梯度信号对网络传播的影响。需要更可靠的幻觉检测和缓解方法。

Method: 提出LVLMs-Saliency框架：1) 融合注意力权重和输入梯度量化输出token的视觉基础强度；2) 发现幻觉模式：前序token对下一token预测的显著性低时易产生幻觉；3) 提出SGRS（显著性引导拒绝采样）动态过滤候选token；4) 提出LocoRE（局部一致性增强）模块加强当前token与前序token的注意力。

Result: 在多个LVLMs上的实验表明，该方法显著降低幻觉率，同时保持流畅性和任务性能，提供鲁棒且可解释的解决方案。

Conclusion: LVLMs-Saliency框架通过梯度感知分析揭示了幻觉的上下文记忆失效机制，提出的SGRS和LocoRE机制有效缓解幻觉问题，增强了模型可靠性。

Abstract: Recent studies have examined attention dynamics in large vision-language models (LVLMs) to detect hallucinations. However, existing approaches remain limited in reliably distinguishing hallucinated from factually grounded outputs, as they rely solely on forward-pass attention patterns and neglect gradient-based signals that reveal how token influence propagates through the network. To bridge this gap, we introduce LVLMs-Saliency, a gradient-aware diagnostic framework that quantifies the visual grounding strength of each output token by fusing attention weights with their input gradients. Our analysis uncovers a decisive pattern: hallucinations frequently arise when preceding output tokens exhibit low saliency toward the prediction of the next token, signaling a breakdown in contextual memory retention. Leveraging this insight, we propose a dual-mechanism inference-time framework to mitigate hallucinations: (1) Saliency-Guided Rejection Sampling (SGRS), which dynamically filters candidate tokens during autoregressive decoding by rejecting those whose saliency falls below a context-adaptive threshold, thereby preventing coherence-breaking tokens from entering the output sequence; and (2) Local Coherence Reinforcement (LocoRE), a lightweight, plug-and-play module that strengthens attention from the current token to its most recent predecessors, actively counteracting the contextual forgetting behavior identified by LVLMs-Saliency. Extensive experiments across multiple LVLMs demonstrate that our method significantly reduces hallucination rates while preserving fluency and task performance, offering a robust and interpretable solution for enhancing model reliability. Code is available at: https://github.com/zhangbaijin/LVLMs-Saliency

</details>


### [16] [A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency](https://arxiv.org/abs/2601.20284)
*Debopom Sutradhar,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: 提出了一种无需源域数据的多视图增强和潜在空间一致性源自由域适应方法，通过目标域数据直接学习域不变特征，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统域适应方法需要源域数据、对抗训练或复杂伪标签技术，计算成本高。本文旨在开发无需源域数据、更高效的域适应方法。

Method: 使用多视图增强和潜在空间一致性技术，直接从目标域学习域不变特征。引入ConvNeXt编码器，设计结合分类和一致性目标的损失函数，通过最小化多个增强视图特征表示之间的距离来确保特征一致性。

Result: 在Office-31、Office-Home和Office-Caltech数据集上分别达到90.72%、84%和97.12%的平均分类准确率，相比现有方法分别提升+1.23%、+7.26%和+1.77%。

Conclusion: 提出的源自由域适应方法无需源域数据或源-目标对齐，通过多视图增强和潜在空间一致性有效学习域不变特征，在多个基准数据集上显著优于现有方法。

Abstract: Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.

</details>


### [17] [Artifact-Aware Evaluation for High-Quality Video Generation](https://arxiv.org/abs/2601.20297)
*Chen Zhu,Jiashu Zhu,Yanxun Li,Meiqi Wu,Bingze Song,Chubin Chen,Jiahong Wu,Xiangxiang Chu,Yangang Wang*

Main category: cs.CV

TL;DR: 论文提出了一个全面的视频生成质量评估协议，关注外观、运动和相机三个关键方面，定义了10种常见伪影类别，并构建了包含8万视频的大规模数据集GenVID，开发了DVAR框架进行细粒度伪影识别。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成技术的快速发展，评估和审核生成视频变得日益重要。现有方法通常只提供粗略的视频质量分数，缺乏对特定伪影的详细定位和分类。

Method: 提出了一个全面的评估协议，关注影响人类感知的三个关键方面：外观、运动和相机。定义了10种常见伪影类别的分类法。构建了GenVID数据集（8万个由各种先进视频生成模型生成的视频，每个都仔细标注了定义的伪影类别）。开发了DVAR（密集视频伪影识别）框架，用于细粒度的生成伪影识别和分类。

Result: 广泛实验表明，该方法显著提高了伪影检测的准确性，并能够有效过滤低质量内容。

Conclusion: 该研究为视频生成质量评估提供了一个全面的框架，通过细粒度的伪影识别和分类，能够更准确地评估和改善生成视频的质量。

Abstract: With the rapid advancement of video generation techniques, evaluating and auditing generated videos has become increasingly crucial. Existing approaches typically offer coarse video quality scores, lacking detailed localization and categorization of specific artifacts. In this work, we introduce a comprehensive evaluation protocol focusing on three key aspects affecting human perception: Appearance, Motion, and Camera. We define these axes through a taxonomy of 10 prevalent artifact categories reflecting common generative failures observed in video generation. To enable robust artifact detection and categorization, we introduce GenVID, a large-scale dataset of 80k videos generated by various state-of-the-art video generation models, each carefully annotated for the defined artifact categories. Leveraging GenVID, we develop DVAR, a Dense Video Artifact Recognition framework for fine-grained identification and classification of generative artifacts. Extensive experiments show that our approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content.

</details>


### [18] [Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization](https://arxiv.org/abs/2601.20301)
*Jialuo He,Huangxun Chen*

Main category: cs.CV

TL;DR: C-SAM框架通过将锐度感知学习从参数扰动转向掩码扰动，在模型压缩的同时保持鲁棒性，解决了SAM训练模型剪枝后鲁棒性下降的问题。


<details>
  <summary>Details</summary>
Motivation: SAM能提升DNN对输入变化的鲁棒性，但在模型压缩场景下存在局限：剪枝SAM训练模型会破坏鲁棒性，而先剪枝后应用SAM又受限于早期剪枝模式。需要同时优化模型紧凑性和鲁棒性的方法。

Method: 提出C-SAM框架，将锐度感知学习从参数扰动转向掩码扰动。通过在训练中显式扰动剪枝掩码，促进对模型结构更平坦的损失景观，发现同时优化紧凑性和鲁棒性的剪枝模式。

Result: 在CelebA-HQ、Flowers-102和CIFAR-10-C数据集上，使用ResNet-18、GoogLeNet和MobileNet-V2进行实验，C-SAM相比基线方法获得高达42%的认证鲁棒性提升，同时保持与未剪枝模型相当的任务精度。

Conclusion: C-SAM通过掩码扰动实现了模型压缩与鲁棒性的协同优化，为边缘设备部署提供了同时满足紧凑性和鲁棒性要求的有效解决方案。

Abstract: Sharpness-Aware Minimization (SAM) has recently emerged as an effective technique for improving DNN robustness to input variations. However, its interplay with the compactness requirements of on-device DNN deployments remains less explored. Simply pruning a SAM-trained model can undermine robustness, since flatness in the continuous parameter space does not necessarily translate to robustness under the discrete structural changes induced by pruning. Conversely, applying SAM after pruning may be fundamentally constrained by architectural limitations imposed by an early, robustness-agnostic pruning pattern. To address this gap, we propose Compression-aware ShArpness Minimization (C-SAM), a framework that shifts sharpness-aware learning from parameter perturbations to mask perturbations. By explicitly perturbing pruning masks during training, C-SAM promotes a flatter loss landscape with respect to model structure, enabling the discovery of pruning patterns that simultaneously optimize model compactness and robustness to input variations. Extensive experiments on CelebA-HQ, Flowers-102, and CIFAR-10-C across ResNet-18, GoogLeNet, and MobileNet-V2 show that C-SAM consistently achieves higher certified robustness than strong baselines, with improvements of up to 42%, while maintaining task accuracy comparable to the corresponding unpruned models.

</details>


### [19] [Bridging the Applicator Gap with Data-Doping:Dual-Domain Learning for Precise Bladder Segmentation in CT-Guided Brachytherapy](https://arxiv.org/abs/2601.20302)
*Suresh Das,Siladittya Manna,Sayantari Ghosh*

Main category: cs.CV

TL;DR: 该研究提出了一种双域学习策略，通过将少量带有施源器的CT数据（WA）与大量无施源器的CT数据（NA）结合训练，显著提升了膀胱分割在协变量偏移下的性能，仅需10-30%的WA数据即可达到与全WA数据训练相当的效果。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中协变量偏移导致的性能下降是主要挑战。在CT引导的妇科近距离放疗中，带有施源器的CT扫描（WA）稀缺且存在解剖变形和伪影，而无施源器的CT扫描（NA）虽广泛可用但无法有效捕捉WA图像的特征。需要解决如何在有限WA数据下提升分割性能的问题。

Method: 提出双域学习策略，将NA和WA CT数据集成训练。使用精选的混合数据集，在轴向、冠状和矢状面三个平面上进行系统实验，采用多种深度学习架构。通过将少量WA数据（10-30%）掺入以NA为主的训练集中，实现有效的域适应。

Result: 仅使用NA数据无法有效分割WA图像，但掺入10-30%的WA数据后，分割性能显著提升，Dice相似系数最高达0.94，交并比最高达0.92，性能与完全使用WA数据训练的模型相当。

Conclusion: 该研究表明，通过集成解剖相似但分布偏移的数据集，可以有效克服数据稀缺问题，提升深度学习分割在近距离放疗治疗规划中的临床可靠性。双域学习策略为协变量偏移下的医学图像分割提供了实用解决方案。

Abstract: Performance degradation due to covariate shift remains a major challenge for deep learning models in medical image segmentation. An open question is whether samples from a shifted distribution can effectively support learning when combined with limited target domain data. We investigate this problem in the context of bladder segmentation in CT guided gynecological brachytherapy, a critical task for accurate dose optimization and organ at risk sparing. While CT scans without brachytherapy applicators (no applicator: NA) are widely available, scans with applicators inserted (with applicator: WA) are scarce and exhibit substantial anatomical deformation and imaging artifacts, making automated segmentation particularly difficult.
  We propose a dual domain learning strategy that integrates NA and WA CT data to improve robustness and generalizability under covariate shift. Using a curated assorted dataset, we show that NA data alone fail to capture the anatomical and artifact related characteristics of WA images. However, introducing a modest proportion of WA data into a predominantly NA training set leads to significant performance improvements. Through systematic experiments across axial, coronal, and sagittal planes using multiple deep learning architectures, we demonstrate that doping only 10 to 30 percent WA data achieves segmentation performance comparable to models trained exclusively on WA data.
  The proposed approach attains Dice similarity coefficients of up to 0.94 and Intersection over Union scores of up to 0.92, indicating effective domain adaptation and improved clinical reliability. This study highlights the value of integrating anatomically similar but distribution shifted datasets to overcome data scarcity and enhance deep learning based segmentation for brachytherapy treatment planning.

</details>


### [20] [Physically Guided Visual Mass Estimation from a Single RGB Image](https://arxiv.org/abs/2601.20303)
*Sungjae Lee,Junhan Jeong,Yeonjoo Hong,Kwang In Kim*

Main category: cs.CV

TL;DR: 提出一个物理结构化框架，通过单张RGB图像估计物体质量，利用几何体积和材料语义来解决质量估计的歧义性问题。


<details>
  <summary>Details</summary>
Motivation: 从视觉输入估计物体质量具有挑战性，因为质量取决于几何体积和材料密度，这两者都不能直接从RGB外观中观测到。像素级质量预测是不适定的，需要物理有意义的表示来约束解空间。

Method: 从单张RGB图像中，通过单目深度估计恢复物体中心的三维几何以获取体积信息，使用视觉语言模型提取粗略材料语义以指导密度相关推理。这些几何、语义和外观表示通过实例自适应门控机制融合，通过两个独立的回归头预测物理引导的潜在因子（体积相关和密度相关），仅使用质量监督。

Result: 在image2mass和ABO-500数据集上的实验表明，该方法始终优于最先进的方法。

Conclusion: 提出的物理结构化框架通过将视觉线索与物理因素对齐，有效解决了单图像质量估计的歧义性问题，在多个数据集上取得了优越性能。

Abstract: Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.

</details>


### [21] [Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction](https://arxiv.org/abs/2601.20304)
*Genyuan Zhang,Zihao Wang,Zhifan Gao,Lei Xu,Zhen Zhou,Haijun Yu,Jianjia Zhang,Xiujian Liu,Weiwei Zhang,Shaoyu Wang,Huazhu Fu,Fenglin Liu,Weiwen Wu*

Main category: cs.CV

TL;DR: 提出SLDM模型，通过结构约束和语言引导的扩散模型，从低剂量碘对比剂CT生成正常剂量图像，减少对比剂用量同时保持诊断能力。


<details>
  <summary>Details</summary>
Motivation: 碘对比剂在CT检查中能提高敏感性和特异性，但过量使用会导致肾损伤和过敏反应等风险。现有深度学习方法在处理不完全配对图像时难以实现准确增强，主要因为模型识别特定结构的能力有限。

Method: 提出结构约束语言引导扩散模型(SLDM)：1) 有效提取图像结构先验信息约束模型推理，确保增强过程的结构一致性；2) 引入具有空间智能的语义监督策略，整合视觉感知和空间推理功能；3) 应用减影血管增强模块，将对比剂区域对比度调整到适合观察的区间。

Result: 通过视觉比较的定性分析和多个指标的定量结果，证明了该方法在低剂量对比剂CT血管造影重建中的有效性。

Conclusion: SLDM模型能够从低剂量对比剂CT生成高质量的正常剂量图像，在减少对比剂用量的同时保持诊断能力，解决了现有方法在处理不完全配对图像时的局限性。

Abstract: The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.

</details>


### [22] [TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration](https://arxiv.org/abs/2601.20306)
*Yanjie Tu,Qingsen Yan,Axi Niu,Jiacong Tang*

Main category: cs.CV

TL;DR: TPGDiff是一个用于统一图像修复的三重先验引导扩散网络，通过分层整合退化先验、结构先验和语义先验，在严重退化区域实现更好的内容重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖退化先验指导修复，但在严重退化区域的内容重建方面存在困难。虽然最近的工作利用语义信息促进内容生成，但将其整合到扩散模型的浅层会破坏空间结构（如模糊伪影）。

Method: 提出三重先验引导扩散网络（TPGDiff）：1）在整个扩散轨迹中整合退化先验；2）将结构先验引入浅层以捕获细粒度细节；3）将语义先验引入深层以提供高级语义指导；4）开发蒸馏驱动的语义提取器以获取鲁棒的语义先验；5）使用退化提取器学习退化感知先验，实现跨所有时间步的阶段自适应控制。

Result: 在单退化和多退化基准测试上的广泛实验表明，TPGDiff在各种修复场景中实现了优越的性能和泛化能力。

Conclusion: TPGDiff通过分层和互补的先验指导机制，有效解决了统一图像修复中的内容重建问题，特别是在严重退化区域，为多样化退化类型提供了统一的解决方案。

Abstract: All-in-one image restoration aims to address diverse degradation types using a single unified model. Existing methods typically rely on degradation priors to guide restoration, yet often struggle to reconstruct content in severely degraded regions. Although recent works leverage semantic information to facilitate content generation, integrating it into the shallow layers of diffusion models often disrupts spatial structures (\emph{e.g.}, blurring artifacts). To address this issue, we propose a Triple-Prior Guided Diffusion (TPGDiff) network for unified image restoration. TPGDiff incorporates degradation priors throughout the diffusion trajectory, while introducing structural priors into shallow layers and semantic priors into deep layers, enabling hierarchical and complementary prior guidance for image reconstruction. Specifically, we leverage multi-source structural cues as structural priors to capture fine-grained details and guide shallow layers representations. To complement this design, we further develop a distillation-driven semantic extractor that yields robust semantic priors, ensuring reliable high-level guidance at deep layers even under severe degradations. Furthermore, a degradation extractor is employed to learn degradation-aware priors, enabling stage-adaptive control of the diffusion process across all timesteps. Extensive experiments on both single- and multi-degradation benchmarks demonstrate that TPGDiff achieves superior performance and generalization across diverse restoration scenarios. Our project page is: https://leoyjtu.github.io/tpgdiff-project.

</details>


### [23] [OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion](https://arxiv.org/abs/2601.20308)
*Shuoyan Wei,Feng Li,Chen Zhou,Runmin Cong,Yao Zhao,Huihui Bai*

Main category: cs.CV

TL;DR: OSDEnhancer：首个通过高效一步扩散过程实现真实世界时空视频超分辨率的框架，结合线性预插值、时空专家混合和双向变形VAE解码器，在保持时间一致性的同时恢复空间细节。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频超分辨率方面表现出色，但在时空视频超分辨率（STVSR）中潜力尚未充分探索。现有STVSR方法主要基于简化的退化假设，难以应对真实世界中复杂的未知退化情况。重建保真度和时间一致性的高要求使得开发鲁棒的STVSR框架尤为困难。

Method: 提出OSDEnhancer框架：1）使用线性预插值策略初始化基本时空结构；2）训练时空细化与空间增强混合专家（TR-SE MoE），让不同专家路径分别学习时间一致性和空间细节的专门表示；3）引入双向变形变分自编码器（VAE）解码器进行循环时空聚合和传播，增强跨帧重建保真度。

Result: 实验表明该方法在真实世界场景中实现了最先进的性能，同时保持了优越的泛化能力。

Conclusion: OSDEnhancer是首个通过高效一步扩散过程实现真实世界时空视频超分辨率的方法，通过创新的混合专家架构和双向变形VAE解码器，在保持时间一致性的同时有效恢复空间细节，为复杂退化场景下的STVSR提供了鲁棒解决方案。

Abstract: Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.

</details>


### [24] [CPiRi: Channel Permutation-Invariant Relational Interaction for Multivariate Time Series Forecasting](https://arxiv.org/abs/2601.20318)
*Jiyuan Xu,Wenyu Zhang,Xin Jing,Shuai Chen,Shuai Zhang,Jiahao Nie*

Main category: cs.CV

TL;DR: CPiRi是一个通道置换不变的多变量时间序列预测框架，通过解耦时空架构和置换不变正则化，从数据中推断跨通道结构而非记忆固定顺序，实现结构/分布漂移下的稳定预测。


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列预测方法存在两难：通道依赖模型学习跨通道特征但容易过拟合通道顺序，难以适应通道增减或重排；通道独立模型增加灵活性但忽略通道间依赖关系，性能受限。需要一种既能学习跨通道关系又不依赖固定顺序的方法。

Method: CPiRi采用时空解耦架构：冻结的预训练时间编码器提取高质量时间特征，轻量级空间模块学习内容驱动的通道间关系。通过通道洗牌策略在训练中强制实现通道置换不变性，并提供了理论分析证明置换等变性。

Result: 在多个基准测试中取得最先进结果。当通道顺序被打乱时保持稳定，即使只在一半通道上训练也能对未见通道表现出强归纳泛化能力，同时在大规模数据集上保持实际效率。

Conclusion: CPiRi通过通道置换不变框架解决了现有方法的局限性，能够从数据中推断跨通道结构而非记忆固定顺序，在结构/分布漂移场景中无需重新训练即可部署，具有实际应用价值。

Abstract: Current methods for multivariate time series forecasting can be classified into channel-dependent and channel-independent models. Channel-dependent models learn cross-channel features but often overfit the channel ordering, which hampers adaptation when channels are added or reordered. Channel-independent models treat each channel in isolation to increase flexibility, yet this neglects inter-channel dependencies and limits performance. To address these limitations, we propose \textbf{CPiRi}, a \textbf{channel permutation invariant (CPI)} framework that infers cross-channel structure from data rather than memorizing a fixed ordering, enabling deployment in settings with structural and distributional co-drift without retraining. CPiRi couples \textbf{spatio-temporal decoupling architecture} with \textbf{permutation-invariant regularization training strategy}: a frozen pretrained temporal encoder extracts high-quality temporal features, a lightweight spatial module learns content-driven inter-channel relations, while a channel shuffling strategy enforces CPI during training. We further \textbf{ground CPiRi in theory} by analyzing permutation equivariance in multivariate time series forecasting. Experiments on multiple benchmarks show state-of-the-art results. CPiRi remains stable when channel orders are shuffled and exhibits strong \textbf{inductive generalization} to unseen channels even when trained on \textbf{only half} of the channels, while maintaining \textbf{practical efficiency} on large-scale datasets. The source code is released at https://github.com/JasonStraka/CPiRi.

</details>


### [25] [GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction](https://arxiv.org/abs/2601.20331)
*Mai Su,Qihan Yu,Zhongtao Wang,Yilong Li,Chengwei Pan,Yisong Chen,Guoping Wang*

Main category: cs.CV

TL;DR: 本文提出GVGS方法，通过高斯可见性感知的多视角几何一致性约束和渐进式四叉树校准的单目深度约束，解决了3D高斯溅射中表面重建不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射虽然能高效优化和高质量渲染，但准确的表面重建仍然具有挑战性。现有方法通过多视角几何一致性或单目深度先验来改进高斯深度估计，但多视角约束在几何差异大时不可靠，而单目先验存在尺度模糊和局部不一致问题，导致高斯深度监督不准确。

Method: 1. 高斯可见性感知的多视角几何一致性约束：通过聚合共享高斯基元在不同视角下的可见性，实现更准确稳定的几何监督。2. 渐进式四叉树校准的单目深度约束：从粗到细的空间尺度执行块状仿射校准，缓解深度先验的尺度模糊，同时保留细粒度表面细节。

Result: 在DTU和TNT数据集上的大量实验表明，该方法在几何精度上相比先前的高斯基和隐式表面重建方法有持续改进。

Conclusion: 提出的GVGS方法通过结合可见性感知的多视角约束和渐进校准的单目深度约束，有效解决了3D高斯溅射中表面重建的准确性问题，在多个数据集上取得了更好的几何精度。

Abstract: 3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: https://github.com/GVGScode/GVGS.

</details>


### [26] [Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining](https://arxiv.org/abs/2601.20333)
*Ali Zia,Usman Ali,Umer Ramzan,Abdul Rehman,Abdelwahed Khamis,Wei Xiang*

Main category: cs.CV

TL;DR: TopoOT：一种结合拓扑数据分析与最优传输的异常分割框架，通过多尺度拓扑特征和测试时适应实现鲁棒的异常检测


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值的二值化方法在分布偏移下产生脆弱的掩码，而拓扑数据分析能够捕捉跨尺度的结构不变性，更适合异常分割。异常通常表现为全局结构的破坏而非局部波动。

Method: 提出TopoOT框架：1）最优传输链式对齐：在多阈值和多过滤条件下顺序对齐持久图，生成测地稳定性分数；2）稳定性感知伪标签：识别跨尺度一致保留的特征；3）轻量级在线训练头：结合最优传输一致性和对比目标进行测试时适应。

Result: 在标准2D和3D异常检测基准测试中达到最先进性能：2D数据集上平均F1提升高达+24.1%，3D异常分割基准上提升+10.2%。

Conclusion: TopoOT通过将拓扑数据分析与最优传输相结合，实现了对分布偏移的鲁棒适应，为异常分割提供了稳定且可解释的解决方案。

Abstract: Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.

</details>


### [27] [MMSF: Multitask and Multimodal Supervised Framework for WSI Classification and Survival Analysis](https://arxiv.org/abs/2601.20347)
*Chengying She,Chengwei Chen,Xinran Zhang,Ben Wang,Lizhuang Liu,Chengwei Shao,Yun Bian*

Main category: cs.CV

TL;DR: MMSF是一个多任务多模态监督框架，用于整合病理全切片图像和临床数据，通过模态分解与融合提升癌症预后预测性能。


<details>
  <summary>Details</summary>
Motivation: 计算病理学中需要整合多模态证据：千兆像素全切片图像捕捉肿瘤形态，患者级临床描述符提供补充的预后背景。然而，这些异构信号的特征空间具有不同的统计特性和尺度，整合具有挑战性。

Method: MMSF基于线性复杂度的MIL骨干网络，包含：1) 图特征提取模块嵌入组织拓扑结构；2) 临床数据嵌入模块标准化患者属性；3) 特征融合模块对齐模态共享和模态特定表示；4) 基于Mamba的MIL编码器和多任务预测头。

Result: 在CAMELYON16和TCGA-NSCLC数据集上，相比竞争基线获得2.1-6.6%准确率和2.2-6.9% AUC提升；在五个TCGA生存队列上，相比单模态方法获得7.1-9.8% C-index提升，相比多模态替代方法获得5.6-7.1%提升。

Conclusion: MMSF通过显式分解和融合跨模态信息，有效整合病理图像和临床数据，显著提升了癌症预后预测性能，为计算病理学中的多模态整合提供了有效解决方案。

Abstract: Multimodal evidence is critical in computational pathology: gigapixel whole slide images capture tumor morphology, while patient-level clinical descriptors preserve complementary context for prognosis. Integrating such heterogeneous signals remains challenging because feature spaces exhibit distinct statistics and scales. We introduce MMSF, a multitask and multimodal supervised framework built on a linear-complexity MIL backbone that explicitly decomposes and fuses cross-modal information. MMSF comprises a graph feature extraction module embedding tissue topology at the patch level, a clinical data embedding module standardizing patient attributes, a feature fusion module aligning modality-shared and modality-specific representations, and a Mamba-based MIL encoder with multitask prediction heads. Experiments on CAMELYON16 and TCGA-NSCLC demonstrate 2.1--6.6\% accuracy and 2.2--6.9\% AUC improvements over competitive baselines, while evaluations on five TCGA survival cohorts yield 7.1--9.8\% C-index improvements compared with unimodal methods and 5.6--7.1\% over multimodal alternatives.

</details>


### [28] [PalmBridge: A Plug-and-Play Feature Alignment Framework for Open-Set Palmprint Verification](https://arxiv.org/abs/2601.20351)
*Chenke Zhang,Ziyuan Yang,Licheng Yan,Shuyi Li,Andrew Beng Jin Teoh,Bob Zhang,Yi Zhang*

Main category: cs.CV

TL;DR: PalmBridge：基于向量量化的即插即用特征空间对齐框架，用于开放集掌纹验证，通过学习代表性向量来抑制领域偏移引起的干扰变化，提升跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 掌纹识别在生物识别系统中广泛应用，但实际部署中由于异构部署条件导致特征分布偏移，性能下降。现有深度掌纹模型通常假设封闭静态分布，容易过拟合数据集特定纹理而非学习领域不变表示。数据增强方法假设增强样本能近似目标部署分布，但在显著领域不匹配时往往失效。

Method: 提出PalmBridge框架，基于向量量化学习训练特征的紧凑代表性向量集。在注册和验证时，每个特征向量通过最小距离准则映射到最近的代表性向量，然后将映射向量与原始向量混合。代表性向量通过任务监督、特征一致性目标和正交正则化项与骨干网络联合优化，形成稳定结构化的共享嵌入空间。

Result: 在多个掌纹数据集和骨干架构上的实验表明，PalmBridge在数据集内开放集评估中持续降低EER，并提高跨数据集泛化能力，运行时开销可忽略或适度。

Conclusion: PalmBridge通过特征空间对齐有效抑制领域偏移引起的干扰变化，同时保留判别性身份线索，为开放集掌纹验证提供了一种有效的即插即用解决方案。

Abstract: Palmprint recognition is widely used in biometric systems, yet real-world performance often degrades due to feature distribution shifts caused by heterogeneous deployment conditions. Most deep palmprint models assume a closed and stationary distribution, leading to overfitting to dataset-specific textures rather than learning domain-invariant representations. Although data augmentation is commonly used to mitigate this issue, it assumes augmented samples can approximate the target deployment distribution, an assumption that often fails under significant domain mismatch. To address this limitation, we propose PalmBridge, a plug-and-play feature-space alignment framework for open-set palmprint verification based on vector quantization. Rather than relying solely on data-level augmentation, PalmBridge learns a compact set of representative vectors directly from training features. During enrollment and verification, each feature vector is mapped to its nearest representative vector under a minimum-distance criterion, and the mapped vector is then blended with the original vector. This design suppresses nuisance variation induced by domain shifts while retaining discriminative identity cues. The representative vectors are jointly optimized with the backbone network using task supervision, a feature-consistency objective, and an orthogonality regularization term to form a stable and well-structured shared embedding space. Furthermore, we analyze feature-to-representative mappings via assignment consistency and collision rate to assess model's sensitivity to blending weights. Experiments on multiple palmprint datasets and backbone architectures show that PalmBridge consistently reduces EER in intra-dataset open-set evaluation and improves cross-dataset generalization with negligible to modest runtime overhead.

</details>


### [29] [Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models](https://arxiv.org/abs/2601.20354)
*Zengbin Wang,Xuecai Hu,Yong Wang,Feng Xiong,Man Zhang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: SpatialGenEval是一个评估文本到图像模型空间智能的新基准，包含1,230个信息密集的长提示，覆盖25个真实场景和10个空间子领域。研究发现高阶空间推理是主要瓶颈，并通过构建SpatialT2I数据集进行微调，实现了模型性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在处理复杂空间关系（如空间感知、推理和交互）方面存在明显不足，而现有基准测试由于提示设计简短或信息稀疏，未能充分评估这些关键方面。

Method: 提出了SpatialGenEval基准，包含1,230个信息密集的长提示，覆盖25个真实场景，每个提示整合10个空间子领域和相应的10个多项选择题对。同时构建了SpatialT2I数据集，包含15,400个文本-图像对，通过重写提示保持信息密度和图像一致性。

Result: 对21个最先进模型的评估显示，高阶空间推理仍然是主要瓶颈。在Stable Diffusion-XL、Uniworld-V1、OmniGen2等基础模型上进行微调后，分别获得了+4.2%、+5.7%、+4.4%的性能提升，并在空间关系上产生了更真实的效果。

Conclusion: SpatialGenEval基准系统地评估了T2I模型的空间智能，揭示了当前模型的局限性。通过数据中心的范式，即构建信息密集的训练数据，可以有效提升T2I模型的空间智能，为未来研究提供了重要方向。

Abstract: Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.

</details>


### [30] [CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization](https://arxiv.org/abs/2601.20355)
*Yue Liang,Jiatong Du,Ziyi Yang,Yanjun Huang,Hong Chen*

Main category: cs.CV

TL;DR: CURVE是一个因果启发的框架，通过变分不确定性建模和不确定性引导的结构正则化来抑制高方差、环境特定的关系，提升场景图在分布外泛化中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 场景图提供了场景理解的结构化抽象，但它们经常过拟合到虚假相关性，严重阻碍了分布外泛化能力。为了解决这一限制，需要开发能够抑制环境特定关系、提升领域稳定性的方法。

Method: 提出CURVE框架：1）集成变分不确定性建模与不确定性引导的结构正则化；2）应用原型条件去偏技术，将不变交互动态与环境依赖变化解耦；3）促进稀疏且领域稳定的拓扑结构。

Result: 在零样本迁移和低数据模拟到真实适应任务中评估CURVE，验证了其能够学习领域稳定的稀疏拓扑，并提供可靠的不确定性估计来支持分布偏移下的风险预测。

Conclusion: CURVE通过因果启发的方法有效解决了场景图过拟合虚假相关性的问题，提升了分布外泛化能力，为分布偏移下的可靠风险预测提供了支持。

Abstract: Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.

</details>


### [31] [RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching](https://arxiv.org/abs/2601.20364)
*Zhen Liu,Diedong Feng,Hai Jiang,Liaoyuan Zeng,Hao Wang,Chaoyu Feng,Lei Lei,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: RAW-Flow：一种基于流匹配的生成式RGB-to-RAW重建框架，通过确定性潜在空间传输解决细节不一致和颜色偏差问题


<details>
  <summary>Details</summary>
Motivation: 现有学习方法将RGB-to-RAW重建视为直接回归任务，但由于逆ISP问题的病态性和量化RGB图像的信息损失，导致细节不一致和颜色偏差。需要新的视角来解决这些限制。

Method: 1. 将RGB-to-RAW重建重新定义为确定性潜在传输问题；2. 提出RAW-Flow框架，利用流匹配学习潜在空间中的确定性向量场；3. 引入跨尺度上下文引导模块注入分层RGB特征；4. 设计具有特征对齐约束的双域潜在自编码器

Result: 在广泛实验中，RAW-Flow在定量和视觉上都优于现有最先进方法，能够准确重建结构细节和颜色信息

Conclusion: 通过生成式视角和流匹配技术，RAW-Flow有效解决了RGB-to-RAW重建中的细节不一致和颜色偏差问题，为相机ISP逆建模提供了新思路

Abstract: RGB-to-RAW reconstruction, or the reverse modeling of a camera Image Signal Processing (ISP) pipeline, aims to recover high-fidelity RAW data from RGB images. Despite notable progress, existing learning-based methods typically treat this task as a direct regression objective and struggle with detail inconsistency and color deviation, due to the ill-posed nature of inverse ISP and the inherent information loss in quantized RGB images. To address these limitations, we pioneer a generative perspective by reformulating RGB-to-RAW reconstruction as a deterministic latent transport problem and introduce a novel framework named RAW-Flow, which leverages flow matching to learn a deterministic vector field in latent space, to effectively bridge the gap between RGB and RAW representations and enable accurate reconstruction of structural details and color information. To further enhance latent transport, we introduce a cross-scale context guidance module that injects hierarchical RGB features into the flow estimation process. Moreover, we design a dual-domain latent autoencoder with a feature alignment constraint to support the proposed latent transport framework, which jointly encodes RGB and RAW inputs while promoting stable training and high-fidelity reconstruction. Extensive experiments demonstrate that RAW-Flow outperforms state-of-the-art approaches both quantitatively and visually.

</details>


### [32] [Dual-Modality IoT Framework for Integrated Access Control and Environmental Safety Monitoring with Real-Time Cloud Analytics](https://arxiv.org/abs/2601.20366)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib,Nihal Das Ankur,Anish Giri*

Main category: cs.CV

TL;DR: 提出一个整合RFID门禁与多传感器环境安全监测的双模态物联网框架，通过统一云架构实现安全系统协同，成本降低82%，性能优异。


<details>
  <summary>Details</summary>
Motivation: 传统物理安全系统和环境安全监测系统相互独立，导致运营效率低下、应急响应延迟和管理复杂度增加，需要集成解决方案。

Method: 采用双子系统架构：子系统1实现RFID认证和伺服门控，子系统2提供火焰检测、水流测量等安全监测。两者均使用ESP32微控制器进行边缘处理和无线连接，通过统一云架构集成。

Result: 45天实验显示：RFID认证准确率99.2%，平均响应时间0.82秒；火焰检测可靠性98.5%（5米范围）；云数据记录成功率99.8%；总成本5400 BDT（约48美元），比商业方案降低82%。

Conclusion: 该研究建立了安全-安全协同集成的实用框架，证明通过精心架构设计和组件优化可实现专业级性能，同时保持卓越的成本效益和可访问性。

Abstract: The integration of physical security systems with environmental safety monitoring represents a critical advancement in smart infrastructure management. Traditional approaches maintain these systems as independent silos, creating operational inefficiencies, delayed emergency responses, and increased management complexity. This paper presents a comprehensive dual-modality Internet of Things framework that seamlessly integrates RFID-based access control with multi-sensor environmental safety monitoring through a unified cloud architecture. The system comprises two coordinated subsystems: Subsystem 1 implements RFID authentication with servo-actuated gate control and real-time Google Sheets logging, while Subsystem 2 provides comprehensive safety monitoring incorporating flame detection, water flow measurement, LCD status display, and personnel identification. Both subsystems utilize ESP32 microcontrollers for edge processing and wireless connectivity. Experimental evaluation over 45 days demonstrates exceptional performance metrics: 99.2\% RFID authentication accuracy with 0.82-second average response time, 98.5\% flame detection reliability within 5-meter range, and 99.8\% cloud data logging success rate. The system maintains operational integrity during network disruptions through intelligent local caching mechanisms and achieves total implementation cost of 5,400 BDT (approximately \$48), representing an 82\% reduction compared to commercial integrated solutions. This research establishes a practical framework for synergistic security-safety integration, demonstrating that professional-grade performance can be achieved through careful architectural design and component optimization while maintaining exceptional cost-effectiveness and accessibility for diverse application scenarios.

</details>


### [33] [RepSFNet : A Single Fusion Network with Structural Reparameterization for Crowd Counting](https://arxiv.org/abs/2601.20369)
*Mas Nurul Achmadiah,Chi-Chia Sun,Wen-Kai Kuo,Jun-Wei Hsieh*

Main category: cs.CV

TL;DR: RepSFNet：一种轻量级人群计数网络，通过重参数化大核和特征融合模块，在保持高精度的同时显著降低计算成本，适用于实时边缘计算。


<details>
  <summary>Details</summary>
Motivation: 现有人群计数模型在变密度场景中面临尺度变化、遮挡和高计算成本的问题，需要一种既能保持准确性又能实现实时推理的轻量级解决方案。

Method: 使用RepLK-ViT骨干网络进行高效多尺度特征提取；结合ASPP和CAN的特征融合模块实现密度自适应上下文建模；采用Concatenate Fusion模块保持空间分辨率；避免注意力机制和多分支设计以减少参数量；结合MSE和最优传输损失进行训练。

Result: 在ShanghaiTech、NWPU和UCF-QNRF数据集上达到竞争性精度，推理延迟相比最新方法降低高达34%，参数和计算复杂度显著减少。

Conclusion: RepSFNet在准确性和效率之间取得了良好平衡，特别适合实时和低功耗边缘计算应用，为实际部署提供了可行的轻量级解决方案。

Abstract: Crowd counting remains challenging in variable-density scenes due to scale variations, occlusions, and the high computational cost of existing models. To address these issues, we propose RepSFNet (Reparameterized Single Fusion Network), a lightweight architecture designed for accurate and real-time crowd estimation. RepSFNet leverages a RepLK-ViT backbone with large reparameterized kernels for efficient multi-scale feature extraction. It further integrates a Feature Fusion module combining Atrous Spatial Pyramid Pooling (ASPP) and Context-Aware Network (CAN) to achieve robust, density-adaptive context modeling. A Concatenate Fusion module is employed to preserve spatial resolution and generate high-quality density maps. By avoiding attention mechanisms and multi-branch designs, RepSFNet significantly reduces parameters and computational complexity. The training objective combines Mean Squared Error and Optimal Transport loss to improve both count accuracy and spatial distribution alignment. Experiments conducted on ShanghaiTech, NWPU, and UCF-QNRF datasets demonstrate that RepSFNet achieves competitive accuracy while reducing inference latency by up to 34 percent compared to recent state-of-the-art methods, making it suitable for real-time and low-power edge computing applications.

</details>


### [34] [HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation](https://arxiv.org/abs/2601.20383)
*Mengge Liu,Yan Di,Gu Wang,Yun Qu,Dekai Zhu,Yanyan Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: HINT：首个用于多人运动生成的层次化交互建模自回归扩散框架，支持变长文本和可变人数，在InterHuman上FID达到3.100，显著优于之前SOTA的5.154


<details>
  <summary>Details</summary>
Motivation: 现有离线方法生成固定长度、固定人数的运动，无法处理长文本、变长文本和可变人数场景，这促使需要自回归方法逐步预测未来运动

Method: 1）在规范化潜在空间中使用解耦运动表示，分离局部运动语义和人际交互；2）采用滑动窗口策略进行高效在线生成，聚合窗口内局部条件和跨窗口全局条件，捕捉历史轨迹、人际依赖和文本对齐

Result: 在公共基准测试中，HINT与强离线模型性能相当，超越自回归基线。在InterHuman上FID达到3.100，显著优于之前SOTA的5.154

Conclusion: HINT是首个用于多人运动生成的层次化交互建模自回归扩散框架，能有效处理长文本、变长文本和可变人数场景，在细粒度交互建模和长序列一致性方面表现优异

Abstract: Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154.

</details>


### [35] [Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models](https://arxiv.org/abs/2601.20419)
*Yuhao Sun,Chengyi Cai,Jiacheng Zhang,Zesheng Ye,Xingliang Yuan,Feng Liu*

Main category: cs.CV

TL;DR: BiFTA通过视图精炼和描述精炼去除图像块和文本描述中的冗余信息，提升CLIP等视觉语言模型的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将细粒度文本描述与局部图像块对齐来提升CLIP性能，但两者都包含冗余信息，导致对齐效果不佳。

Method: 提出双向精炼方法BiFTA：1) 视图精炼 - 通过高IoU比率去除冗余图像块；2) 描述精炼 - 通过高余弦相似度去除冗余文本描述。

Result: 在6个基准数据集上，BiFTA在ViT-based和ResNet-based CLIP上都取得了优越的零样本性能。

Conclusion: 去除视觉-文本对齐中的冗余信息对提升模型性能至关重要，BiFTA方法有效解决了这一问题。

Abstract: Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.

</details>


### [36] [Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance](https://arxiv.org/abs/2601.20425)
*Chenliang Zhou,Fangcheng Zhong,Weihao Xia,Albert Miao,Canberk Baykal,Cengiz Oztireli*

Main category: cs.CV

TL;DR: 提出Quartet of Diffusions框架，通过四个协调的扩散模型分别学习全局形状潜在表示、对称性、语义部件及其空间组合，实现结构感知的点云生成，保证对称性和部件一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么将形状生成视为整体过程，要么仅支持部件组合，缺乏对对称性和部件结构的显式建模。需要一种能同时保证对称性、部件一致性并提供细粒度控制的生成框架。

Method: 使用四个协调的扩散模型：1)全局形状潜在表示扩散模型；2)对称性扩散模型；3)语义部件扩散模型；4)部件空间组合扩散模型。通过解耦生成过程为可解释组件，实现结构感知生成。

Result: 实验表明该方法达到最先进性能，是首个在生成过程中完全集成并强制执行对称性和部件先验的3D点云生成框架，能保证对称性、部件一致性和高质量多样化输出。

Conclusion: Quartet of Diffusions框架通过显式建模部件组合和对称性，实现了结构感知的点云生成，支持细粒度控制，同时保证全局一致性和高质量输出，为3D形状生成提供了新范式。

Abstract: We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process.

</details>


### [37] [Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding](https://arxiv.org/abs/2601.20430)
*Kun Yin,Yunfei Wu,Bing Liu,Zhongpeng Cai,Xiaotian Li,Huang Chen,Xin Li,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun,Yunsheng Wu,Qianyu Li,Antai Guo,Yanzhen Liao,Yanqiu Qu,Haodong Lin,Chengxu He,Shuangyin Liu*

Main category: cs.CV

TL;DR: Youtu-Parsing是一个高效的多功能文档解析模型，采用ViT视觉编码器和Youtu-LLM-2B语言模型，通过token并行和query并行策略实现5-11倍加速，支持多种文档元素识别，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前文档解析模型在处理复杂文档结构时效率较低，特别是对于表格等结构化内容的识别速度慢。需要开发一个既能处理多种文档元素，又能显著提升解码效率的通用解析模型。

Method: 采用解耦的特征重用框架：1) 动态分辨率ViT视觉编码器提取共享文档特征；2) 提示引导的Youtu-LLM-2B语言模型进行布局分析和区域提示解码；3) 高并行解码策略：token并行（每步生成64个候选token）和query并行（同时预测多个边界框内容）。

Result: 在OmniDocBench和olmOCR-bench基准测试中达到SOTA性能；token并行策略比传统自回归解码快5-11倍；query并行提供额外2倍加速；模型对罕见字符、多语言文本和手写内容具有强鲁棒性。

Conclusion: Youtu-Parsing展示了高效文档解析的可行性，其并行解码策略显著提升了处理速度，同时保持了高质量输出，为大规模文档智能应用提供了实用价值。

Abstract: This paper presents Youtu-Parsing, an efficient and versatile document parsing model designed for high-performance content extraction. The architecture employs a native Vision Transformer (ViT) featuring a dynamic-resolution visual encoder to extract shared document features, coupled with a prompt-guided Youtu-LLM-2B language model for layout analysis and region-prompted decoding. Leveraging this decoupled and feature-reusable framework, we introduce a high-parallelism decoding strategy comprising two core components: token parallelism and query parallelism. The token parallelism strategy concurrently generates up to 64 candidate tokens per inference step, which are subsequently validated through a verification mechanism. This approach yields a 5--11x speedup over traditional autoregressive decoding and is particularly well-suited for highly structured scenarios, such as table recognition. To further exploit the advantages of region-prompted decoding, the query parallelism strategy enables simultaneous content prediction for multiple bounding boxes (up to five), providing an additional 2x acceleration while maintaining output quality equivalent to standard decoding. Youtu-Parsing encompasses a diverse range of document elements, including text, formulas, tables, charts, seals, and hierarchical structures. Furthermore, the model exhibits strong robustness when handling rare characters, multilingual text, and handwritten content. Extensive evaluations demonstrate that Youtu-Parsing achieves state-of-the-art (SOTA) performance on both the OmniDocBench and olmOCR-bench benchmarks. Overall, Youtu-Parsing demonstrates significant experimental value and practical utility for large-scale document intelligence applications.

</details>


### [38] [MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models](https://arxiv.org/abs/2601.20433)
*Wenbo Xu,Wei Lu,Xiangyang Luo,Jiantao Zhou*

Main category: cs.CV

TL;DR: MARE：通过视觉语言模型进行可解释的Deepfake检测，采用多模态对齐和强化学习，结合人类反馈奖励函数和伪造痕迹解耦模块，实现高精度和可靠性的检测与推理。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，Deepfake检测面临新需求。现有方法主要将问题建模为分类或空间定位，缺乏可解释性和可靠性。需要开发能够提供准确检测和可解释推理的方法。

Method: 提出MARE框架：1）设计综合奖励函数，结合人类反馈强化学习（RLHF），激励生成文本-空间对齐的推理内容；2）引入伪造痕迹解耦模块，从高级面部语义中捕获内在伪造痕迹，提升真实性检测能力。

Result: 对MARE生成的推理内容进行全面评估，定量和定性实验结果表明，MARE在准确性和可靠性方面达到了最先进的性能水平。

Conclusion: MARE通过多模态对齐和强化学习，结合人类反馈和伪造痕迹解耦，显著提升了Deepfake检测的准确性和可解释性，为对抗恶意内容传播提供了有效解决方案。

Abstract: Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.

</details>


### [39] [Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection](https://arxiv.org/abs/2601.20461)
*Yanzhu Liu,Xiao Liu,Yuexuan Wang,Mondal Soumik*

Main category: cs.CV

TL;DR: 提出一种通过"污染"真实图像来训练AI生成图像检测器的方法，利用生成器最终组件的共性实现跨生成器的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测器对未见过的图像生成器泛化能力差，但许多现代图像生成器（如扩散模型或自回归模型）共享相似的最终架构组件，这为改进检测器提供了机会

Method: 使用生成器的最终组件"污染"真实图像，训练检测器区分原始真实图像和被污染图像；提出基于生成器最终组件的分类法，将21个常用生成器分类，并基于DINOv3骨干网络进行微调

Result: 仅使用三个代表性类别的各100个样本，检测器在22个未见生成器的测试集上平均准确率达到98.83%，表现出优异的跨生成器泛化能力

Conclusion: 利用生成器最终组件的共性进行训练，可以有效提升AI生成图像检测器的泛化能力，为构建更可靠的在线内容验证系统提供了新思路

Abstract: With the rapid proliferation of powerful image generators, accurate detection of AI-generated images has become essential for maintaining a trustworthy online environment. However, existing deepfake detectors often generalize poorly to images produced by unseen generators. Notably, despite being trained under vastly different paradigms, such as diffusion or autoregressive modeling, many modern image generators share common final architectural components that serve as the last stage for converting intermediate representations into images. Motivated by this insight, we propose to "contaminate" real images using the generator's final component and train a detector to distinguish them from the original real images. We further introduce a taxonomy based on generators' final components and categorize 21 widely used generators accordingly, enabling a comprehensive investigation of our method's generalization capability. Using only 100 samples from each of three representative categories, our detector-fine-tuned on the DINOv3 backbone-achieves an average accuracy of 98.83% across 22 testing sets from unseen generators.

</details>


### [40] [Efficient Autoregressive Video Diffusion with Dummy Head](https://arxiv.org/abs/2601.20499)
*Hang Guo,Zhaoyang Jia,Jiahao Li,Bin Li,Yuanhao Cai,Jiangshan Wang,Yawei Li,Yan Lu*

Main category: cs.CV

TL;DR: 提出Dummy Forcing方法，通过异质内存分配和动态头部编程优化自回归视频扩散模型中的多头自注意力机制，减少历史帧的冗余利用，实现2.0倍加速且质量下降小于0.5%。


<details>
  <summary>Details</summary>
Motivation: 研究发现当前自回归视频扩散模型中的多头自注意力机制对历史帧利用不足，约25%的注意力头几乎只关注当前帧，丢弃它们的KV缓存仅导致轻微性能下降，这表明存在优化空间。

Method: 提出Dummy Forcing方法：1) 异质内存分配减少头部间的上下文冗余；2) 动态头部编程自适应分类头部类型；3) 上下文打包技术实现更激进的缓存压缩。无需额外训练。

Result: 在基线模型上实现最高2.0倍加速，支持24.3 FPS的视频生成，质量下降小于0.5%。

Conclusion: Dummy Forcing通过优化注意力机制中的上下文访问控制，有效提升了自回归视频扩散模型的推理效率，在保持生成质量的同时显著加速视频生成。

Abstract: The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.

</details>


### [41] [Comparative evaluation of training strategies using partially labelled datasets for segmentation of white matter hyperintensities and stroke lesions in FLAIR MRI](https://arxiv.org/abs/2601.20503)
*Jesse Phitidis,Alison Q. Smithard,William N. Whiteley,Joanna M. Wardlaw,Miguel O. Bernabeu,Maria Valdés Hernández*

Main category: cs.CV

TL;DR: 研究探讨了在部分标注数据下训练WMH和ISL联合分割模型的六种策略，发现伪标签方法效果最佳


<details>
  <summary>Details</summary>
Motivation: WMH和ISL是脑小血管病的影像特征，在FLAIR序列中视觉上容易混淆且常同时出现，开发准确分割模型面临挑战

Method: 研究了六种利用部分标注数据训练联合分割模型的策略，结合私有和公开数据集共2052个MRI扫描，其中WMH标注1341个，ISL标注1152个

Result: 多种方法能有效利用部分标注数据提升模型性能，其中伪标签方法效果最好

Conclusion: 伪标签策略是在部分标注数据下训练WMH和ISL联合分割模型的最有效方法

Abstract: White matter hyperintensities (WMH) and ischaemic stroke lesions (ISL) are imaging features associated with cerebral small vessel disease (SVD) that are visible on brain magnetic resonance imaging (MRI) scans. The development and validation of deep learning models to segment and differentiate these features is difficult because they visually confound each other in the fluid-attenuated inversion recovery (FLAIR) sequence and often appear in the same subject. We investigated six strategies for training a combined WMH and ISL segmentation model using partially labelled data. We combined privately held fully and partially labelled datasets with publicly available partially labelled datasets to yield a total of 2052 MRI volumes, with 1341 and 1152 containing ground truth annotations for WMH and ISL respectively. We found that several methods were able to effectively leverage the partially labelled data to improve model performance, with the use of pseudolabels yielding the best result.

</details>


### [42] [Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V](https://arxiv.org/abs/2601.20504)
*Meiqi Wu,Bingze Song,Ruimin Lin,Chen Zhu,Xiaokun Feng,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: 提出LTD（潜在时序差异）作为运动先验来指导损失加权，通过测量潜在空间中的帧间变化，为高差异区域分配更大惩罚，稳定训练并提升动态视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在静态场景表现良好，但在动态视频生成中质量下降，特别是在剧烈动态变化时。现有扩散模型对所有场景使用静态损失，限制了捕捉复杂动态的能力。

Method: 引入Latent Temporal Discrepancy（LTD）作为运动先验来指导损失加权。LTD测量潜在空间中的帧间变化，为高差异区域分配更大惩罚，同时对稳定区域保持常规优化，实现运动感知的训练策略。

Result: 在通用基准VBench和运动聚焦的VMBench上均取得显著提升，分别比强基线高出3.31%和3.58%，在运动质量方面实现显著改进。

Conclusion: LTD作为运动先验能有效指导损失加权，稳定训练过程，使模型更好地重建高频动态，显著提升动态视频生成质量。

Abstract: Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.

</details>


### [43] [Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits](https://arxiv.org/abs/2601.20511)
*Zelong Sun,Jiahui Wu,Ying Ba,Dong Jing,Zhiwu Lu*

Main category: cs.CV

TL;DR: 提出肖像收藏生成任务，构建CHEESE数据集和SCheese框架，通过自适应特征融合和一致性网络解决多属性编辑与细节保持的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体平台激增，用户需要直观方式创建多样化、高质量的肖像收藏。现有方法面临复杂多属性修改（姿态、空间布局、相机视角）和高保真细节保持（身份、服装、配饰）两大挑战。

Method: 1) 构建CHEESE数据集：使用大型视觉语言模型流水线生成24K肖像收藏和573K样本，带有高质量修改文本标注；2) 提出SCheese框架：结合文本引导生成与分层身份细节保持，采用自适应特征融合机制保持身份一致性，使用ConsistencyNet注入细粒度特征保持细节一致性。

Result: CHEESE数据集有效推动了肖像收藏生成任务的发展，SCheese框架在实验中取得了最先进的性能表现。

Conclusion: 本文提出了肖像收藏生成新任务，通过构建大规模数据集和提出创新框架，成功解决了复杂多属性编辑与高保真细节保持的挑战，为社交媒体肖像创作提供了有效解决方案。

Abstract: As social media platforms proliferate, users increasingly demand intuitive ways to create diverse, high-quality portrait collections. In this work, we introduce Portrait Collection Generation (PCG), a novel task that generates coherent portrait collections by editing a reference portrait image through natural language instructions. This task poses two unique challenges to existing methods: (1) complex multi-attribute modifications such as pose, spatial layout, and camera viewpoint; and (2) high-fidelity detail preservation including identity, clothing, and accessories. To address these challenges, we propose CHEESE, the first large-scale PCG dataset containing 24K portrait collections and 573K samples with high-quality modification text annotations, constructed through an Large Vison-Language Model-based pipeline with inversion-based verification. We further propose SCheese, a framework that combines text-guided generation with hierarchical identity and detail preservation. SCheese employs adaptive feature fusion mechanism to maintain identity consistency, and ConsistencyNet to inject fine-grained features for detail consistency. Comprehensive experiments validate the effectiveness of CHEESE in advancing PCG, with SCheese achieving state-of-the-art performance.

</details>


### [44] [Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective](https://arxiv.org/abs/2601.20520)
*Qiyan Zhao,Xiaofeng Zhang,Shuochen Chang,Qianyu Chen,Xiaosong Yuan,Xuhang Chen,Luoqi Liu,Jiajun Zhang,Xu-Yao Zhang,Da-Han Wang*

Main category: cs.CV

TL;DR: 本文提出CoTA方法解决扩散式多模态大语言模型中的重复诅咒问题，通过分析信息流发现重复生成与上下文令牌信息流中断相关，CoTA通过增强上下文令牌注意力并引入惩罚项来缓解重复。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散式多模态大语言模型(dMLLMs)依赖缓存技术加速解码，但这会导致重复文本生成问题（称为"重复诅咒"）。作者希望通过分析信息流来理解这一问题的根本机制。

Method: 通过信息流分析发现三个关键现象，基于此提出CoTA方法：1）增强上下文令牌的注意力以保持信息流模式；2）在解码时引入惩罚项，避免由不确定的上下文令牌驱动的输出。

Result: CoTA在缓解重复生成方面表现出显著效果，并在通用任务上实现了持续的性能提升。实验验证了该方法的有效性。

Conclusion: 重复诅咒源于上下文令牌信息流中断和深层熵不收敛，CoTA通过信息流分析和针对性干预有效解决了这一问题，为dMLLMs的重复生成问题提供了解决方案。

Abstract: Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at https://github.com/ErikZ719/CoTA

</details>


### [45] [AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors](https://arxiv.org/abs/2601.20524)
*Matic Fučka,Vitjan Zavrtanik,Danijel Skočaj*

Main category: cs.CV

TL;DR: AnomalyVFM：一个通用框架，将任何预训练的视觉基础模型转化为强大的零样本异常检测器，通过三阶段合成数据集生成和参数高效适配机制，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于纯视觉基础模型（VFMs）的零样本异常检测方法性能落后于基于视觉语言模型（VLMs）的方法，主要原因是现有辅助异常检测数据集多样性有限，以及VFM适配策略过于浅层。

Method: 提出AnomalyVFM框架：1）三阶段合成数据集生成方案增强数据多样性；2）参数高效适配机制，使用低秩特征适配器和置信度加权的像素损失。

Result: 以RADIO为骨干网络，在9个多样化数据集上平均图像级AUROC达到94.1%，比先前方法显著提升3.3个百分点。

Conclusion: AnomalyVFM通过解决数据多样性和适配深度问题，成功将预训练视觉基础模型转化为高性能零样本异常检测器，显著超越现有方法。

Abstract: Zero-shot anomaly detection aims to detect and localise abnormal regions in the image without access to any in-domain training images. While recent approaches leverage vision-language models (VLMs), such as CLIP, to transfer high-level concept knowledge, methods based on purely vision foundation models (VFMs), like DINOv2, have lagged behind in performance. We argue that this gap stems from two practical issues: (i) limited diversity in existing auxiliary anomaly detection datasets and (ii) overly shallow VFM adaptation strategies. To address both challenges, we propose AnomalyVFM, a general and effective framework that turns any pretrained VFM into a strong zero-shot anomaly detector. Our approach combines a robust three-stage synthetic dataset generation scheme with a parameter-efficient adaptation mechanism, utilising low-rank feature adapters and a confidence-weighted pixel loss. Together, these components enable modern VFMs to substantially outperform current state-of-the-art methods. More specifically, with RADIO as a backbone, AnomalyVFM achieves an average image-level AUROC of 94.1% across 9 diverse datasets, surpassing previous methods by significant 3.3 percentage points. Project Page: https://maticfuc.github.io/anomaly_vfm/

</details>


### [46] [IOTA: Corrective Knowledge-Guided Prompt Learning via Black-White Box Framework](https://arxiv.org/abs/2601.20526)
*Shaokun Wang,Yifan Yu,Yuhang He,Weili Guan,Yihong Gong*

Main category: cs.CV

TL;DR: IOTA提出了一种黑白盒提示学习框架，将数据驱动的黑盒模块与知识驱动的白盒模块结合，通过对比错误预测与正确认知生成纠正知识，指导下游任务适应。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效调优方法将预训练模型视为黑盒，仅依赖数据驱动优化，未能充分利用其内在先验知识，限制了模型在下游任务适应中的潜力。

Method: 提出IOTA框架：1) 白盒模块通过对比错误预测与正确认知生成纠正知识；2) 将知识转化为可解释的人类提示；3) 通过纠正知识引导的提示选择策略指导黑盒模块；4) 联合利用知识和数据驱动学习信号。

Result: 在12个图像分类基准数据集上，在少样本和易到难适应设置下，实验证明了纠正知识的有效性以及该方法优于现有最先进方法。

Conclusion: IOTA通过整合知识驱动和数据驱动的学习信号，实现了有效的下游任务适应，克服了传统参数高效调优方法仅依赖数据驱动的局限性。

Abstract: Recently, adapting pre-trained models to downstream tasks has attracted increasing interest. Previous Parameter-Efficient-Tuning (PET) methods regard the pre-trained model as an opaque Black Box model, relying purely on data-driven optimization and underutilizing their inherent prior knowledge. This oversight limits the models' potential for effective downstream task adaptation. To address these issues, we propose a novel black-whIte bOx prompT leArning framework (IOTA), which integrates a data-driven Black Box module with a knowledge-driven White Box module for downstream task adaptation. Specifically, the White Box module derives corrective knowledge by contrasting the wrong predictions with the right cognition. This knowledge is verbalized into interpretable human prompts and leveraged through a corrective knowledge-guided prompt selection strategy to guide the Black Box module toward more accurate predictions. By jointly leveraging knowledge- and data-driven learning signals, IOTA achieves effective downstream task adaptation. Experimental results on 12 image classification benchmarks under few-shot and easy-to-hard adaptation settings demonstrate the effectiveness of corrective knowledge and the superiority of our method over state-of-the-art methods.

</details>


### [47] [Advancing Open-source World Models](https://arxiv.org/abs/2601.20540)
*Robbyant Team,Zelin Gao,Qiuyu Wang,Yanhong Zeng,Jiapeng Zhu,Ka Leong Cheng,Yixuan Li,Hanlin Wang,Yinghao Xu,Shuailei Ma,Yihang Chen,Jie Liu,Yansong Cheng,Yao Yao,Jiayi Zhu,Yihao Meng,Kecheng Zheng,Qingyan Bai,Jingye Chen,Zehong Shen,Yue Yu,Xing Zhu,Yujun Shen,Hao Ouyang*

Main category: cs.CV

TL;DR: LingBot-World 是一个基于视频生成的开源世界模拟器，具备高保真度、长时记忆和实时交互能力，支持多种环境类型。


<details>
  <summary>Details</summary>
Motivation: 缩小开源与闭源技术之间的差距，为内容创作、游戏和机器人学习等领域提供实用的世界模拟工具。

Method: 基于视频生成技术构建世界模拟器，通过优化算法实现高保真度、长时记忆和低延迟交互。

Result: 开发出具备高保真度、分钟级时间跨度、上下文一致性（长时记忆）和实时交互（<1秒延迟）的世界模拟器，支持多种环境类型。

Conclusion: LingBot-World 作为顶级开源世界模型，将推动社区在内容创作、游戏和机器人学习等领域的应用发展。

Abstract: We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as "long-term memory". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.

</details>


### [48] [DeepSeek-OCR 2: Visual Causal Flow](https://arxiv.org/abs/2601.20552)
*Haoran Wei,Yaofeng Sun,Yukun Li*

Main category: cs.CV

TL;DR: DeepSeek-OCR 2提出了一种新颖的视觉编码器DeepEncoder V2，能够根据图像语义动态重排视觉标记，模拟人类视觉的因果推理处理模式。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型以固定的光栅扫描顺序（左上到右下）处理视觉标记，这与人类视觉感知的灵活、语义驱动的扫描模式相矛盾。人类视觉会根据图像的内在逻辑结构进行因果推理的顺序处理，特别是在复杂布局的图像中。

Method: 设计DeepEncoder V2编码器，赋予其因果推理能力，使其能够在基于LLM的内容解释之前智能地重排视觉标记。探索通过两个级联的1D因果推理结构来实现2D图像理解的新范式。

Result: 提出了一个新颖的架构方法，具有实现真正2D推理的潜力。代码和模型权重已在GitHub上公开。

Conclusion: 这项工作探索了通过级联的1D因果推理结构实现2D图像理解的新范式，为视觉语言模型提供了更接近人类视觉感知的架构方法。

Abstract: We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.

</details>


### [49] [DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression](https://arxiv.org/abs/2601.20564)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: cs.CV

TL;DR: DiffVC-RT是首个实现实时扩散感知神经视频压缩的框架，通过高效架构设计、显隐式一致性建模和异步并行解码，在保持高质量的同时达到206/30fps的实时性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的神经视频压缩面临严重信息丢失、推理延迟过高和时间一致性差等关键挑战，阻碍了实际部署。

Method: 1) 高效信息模型架构：通过策略性模块替换和剪枝降低计算复杂度；2) 显隐式一致性建模：引入零成本在线时间偏移模块和混合隐式约束；3) 异步并行解码流水线：采用混合半精度和批次维度时间偏移设计。

Result: 在HEVC数据集上相比VTM-17.0节省80.1%的LPIPS比特率，在NVIDIA H800 GPU上实现720p视频206fps编码和30fps解码的实时性能。

Conclusion: DiffVC-RT是扩散基视频压缩领域的重要里程碑，首次实现了实时性能，为实际部署铺平了道路。

Abstract: The practical deployment of diffusion-based Neural Video Compression (NVC) faces critical challenges, including severe information loss, prohibitive inference latency, and poor temporal consistency. To bridge this gap, we propose DiffVC-RT, the first framework designed to achieve real-time diffusion-based perceptual NVC. First, we introduce an Efficient and Informative Model Architecture. Through strategic module replacements and pruning, this architecture significantly reduces computational complexity while mitigating structural information loss. Second, to address generative flickering artifacts, we propose Explicit and Implicit Consistency Modeling. We enhance temporal consistency by explicitly incorporating a zero-cost Online Temporal Shift Module within the U-Net, complemented by hybrid implicit consistency constraints. Finally, we present an Asynchronous and Parallel Decoding Pipeline incorporating Mixed Half Precision, which enables asynchronous latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design. Experiments show that DiffVC-RT achieves 80.1% bitrate savings in terms of LPIPS over VTM-17.0 on HEVC dataset with real-time encoding and decoding speeds of 206 / 30 fps for 720p videos on an NVIDIA H800 GPU, marking a significant milestone in diffusion-based video compression.

</details>


### [50] [StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval](https://arxiv.org/abs/2601.20597)
*Shaokun Wang,Weili Guan,Jizhou Han,Jianlong Wu,Yupeng Hu,Liqiang Nie*

Main category: cs.CV

TL;DR: StructAlign提出结构化跨模态对齐方法解决连续文本-视频检索中的特征漂移问题，通过ETF几何先验和跨模态关系保留损失缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 连续文本-视频检索面临灾难性遗忘的严重挑战，主要源于两种特征漂移：模态内特征漂移（每个模态内的持续学习引起）和跨模态非合作特征漂移（导致模态不对齐）。

Method: 1. 引入单纯形等角紧框架作为统一几何先验缓解模态不对齐；2. 设计跨模态ETF对齐损失，将文本和视频特征与类别级ETF原型对齐；3. 设计跨模态关系保留损失，利用互补模态保持跨模态相似性关系。

Result: 在基准数据集上的大量实验表明，该方法在连续检索任务中持续优于最先进的持续学习方法。

Conclusion: 通过联合解决跨模态非合作特征漂移和模态内特征漂移，StructAlign有效缓解了连续文本-视频检索中的灾难性遗忘问题。

Abstract: Continual Text-to-Video Retrieval (CTVR) is a challenging multimodal continual learning setting, where models must incrementally learn new semantic categories while maintaining accurate text-video alignment for previously learned ones, thus making it particularly prone to catastrophic forgetting. A key challenge in CTVR is feature drift, which manifests in two forms: intra-modal feature drift caused by continual learning within each modality, and non-cooperative feature drift across modalities that leads to modality misalignment. To mitigate these issues, we propose StructAlign, a structured cross-modal alignment method for CTVR. First, StructAlign introduces a simplex Equiangular Tight Frame (ETF) geometry as a unified geometric prior to mitigate modality misalignment. Building upon this geometric prior, we design a cross-modal ETF alignment loss that aligns text and video features with category-level ETF prototypes, encouraging the learned representations to form an approximate simplex ETF geometry. In addition, to suppress intra-modal feature drift, we design a Cross-modal Relation Preserving loss, which leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR. Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art continual retrieval approaches.

</details>


### [51] [Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?](https://arxiv.org/abs/2601.20598)
*Lakshman Balasubramanian*

Main category: cs.CV

TL;DR: 本文系统评估了11种ReID模型在9个数据集上的表现，比较了监督学习、自监督学习和语言对齐模型三种训练范式。研究发现监督模型在训练域表现优异但跨域泛化能力差，而语言对齐模型（如SigLIP2）虽未专门训练ReID任务，却展现出惊人的跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 行人重识别在计算机视觉中仍是挑战性问题。本研究旨在评估不同训练范式下ReID模型的跨域鲁棒性，探索基础模型（如SigLIP2）通过更丰富、可迁移的视觉表示提升泛化能力的作用，回答监督模型能否跨域泛化、基础模型在ReID任务中的表现、以及当前模型的弱点等关键问题。

Method: 研究比较了三种训练范式：监督学习、自监督学习和语言对齐模型。通过系统分析11个模型在9个数据集上的表现，评估它们在跨域应用中的鲁棒性。特别关注基础模型（如SigLIP2）在未专门训练ReID任务情况下的表现。

Result: 监督模型在训练域内表现优异，但在跨域数据上表现崩溃；语言对齐模型虽然未专门针对ReID任务进行训练，却展现出令人惊讶的跨域鲁棒性。研究揭示了当前监督模型和基础模型在行人重识别任务中的具体弱点。

Conclusion: 语言对齐模型通过更丰富的视觉表示展现出卓越的跨域泛化能力，为行人重识别研究提供了新方向。监督模型虽然域内表现优秀，但跨域应用受限。基础模型在未专门训练的情况下仍能获得良好ReID性能，表明多模态表示学习的重要性。

Abstract: Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: https://github.com/moiiai-tech/object-reid-benchmark.

</details>


### [52] [CLEAR-Mamba:Towards Accurate, Adaptive and Trustworthy Multi-Sequence Ophthalmic Angiography Classification](https://arxiv.org/abs/2601.20601)
*Zhuonan Wang,Wenjie Yan,Wenqiao Zhang,Xiaohui Song,Jian Ma,Ke Yao,Yibo Yu,Beng Chin Ooi*

Main category: cs.CV

TL;DR: 提出CLEAR-Mamba框架，通过超网络自适应条件层和可靠性感知预测方案，解决眼科血管造影图像分类中的泛化性和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 眼科血管造影（FFA和ICGA）能提供传统眼底摄影无法获取的血流动力学和病灶结构信息，但由于单模态特性、细微病灶模式以及设备间显著差异，现有方法在泛化性和高置信度预测方面仍存在局限。

Method: 提出CLEAR-Mamba框架，基于MedMamba优化架构和训练策略：1）架构上引入HaC（超网络自适应条件层），根据输入特征分布动态生成参数以提升跨域适应性；2）训练上开发RaP（可靠性感知预测方案），基于证据不确定性学习，鼓励模型关注低置信度样本，提升整体稳定性和可靠性；3）构建大规模眼科血管造影数据集用于训练评估。

Result: 实验结果表明CLEAR-Mamba在多种指标上持续优于多个基线模型（包括原始MedMamba），在多疾病分类和可靠性感知预测方面表现出特别优势。

Conclusion: 该研究为特定模态医学图像分类任务提供了平衡泛化性和可靠性的有效解决方案。

Abstract: Medical image classification is a core task in computer-aided diagnosis (CAD), playing a pivotal role in early disease detection, treatment planning, and patient prognosis assessment. In ophthalmic practice, fluorescein fundus angiography (FFA) and indocyanine green angiography (ICGA) provide hemodynamic and lesion-structural information that conventional fundus photography cannot capture. However, due to the single-modality nature, subtle lesion patterns, and significant inter-device variability, existing methods still face limitations in generalization and high-confidence prediction. To address these challenges, we propose CLEAR-Mamba, an enhanced framework built upon MedMamba with optimizations in both architecture and training strategy. Architecturally, we introduce HaC, a hypernetwork-based adaptive conditioning layer that dynamically generates parameters according to input feature distributions, thereby improving cross-domain adaptability. From a training perspective, we develop RaP, a reliability-aware prediction scheme built upon evidential uncertainty learning, which encourages the model to emphasize low-confidence samples and improves overall stability and reliability. We further construct a large-scale ophthalmic angiography dataset covering both FFA and ICGA modalities, comprising multiple retinal disease categories for model training and evaluation. Experimental results demonstrate that CLEAR-Mamba consistently outperforms multiple baseline models, including the original MedMamba, across various metrics-showing particular advantages in multi-disease classification and reliability-aware prediction. This study provides an effective solution that balances generalizability and reliability for modality-specific medical image classification tasks.

</details>


### [53] [GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection](https://arxiv.org/abs/2601.20618)
*Shuguang Zhang,Junhong Lian,Guoxin Yu,Baoxun Xu,Xiang Ao*

Main category: cs.CV

TL;DR: GDCNet使用MLLM生成客观图像描述作为语义锚点，通过计算生成描述与原始文本的语义/情感差异以及视觉-文本保真度来检测多模态讽刺，在MSD2.0基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖跨模态嵌入错位检测不一致性，但当视觉和文本内容松散相关或语义间接时效果不佳；基于LLM生成讽刺线索的方法因生成多样性和主观性引入噪声。

Method: 提出生成差异比较网络(GDCNet)：利用MLLM生成事实基础的图像描述作为稳定语义锚点；计算生成描述与原始文本的语义/情感差异，测量视觉-文本保真度；通过门控模块融合差异特征与视觉/文本表示。

Result: 在MSD基准测试中，GDCNet展现出卓越的准确性和鲁棒性，在MMSD2.0基准上建立了新的最先进水平。

Conclusion: GDCNet通过使用MLLM生成的客观描述作为语义锚点，有效解决了现有方法在松散相关内容和生成噪声方面的问题，在多模态讽刺检测中实现了更准确和鲁棒的检测性能。

Abstract: Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.

</details>


### [54] [OS-Marathon: Benchmarking Computer-Use Agents on Long-Horizon Repetitive Tasks](https://arxiv.org/abs/2601.20650)
*Jing Wu,Daphne Barretto,Yiye Chen,Nicholas Gydé,Yanan Jian,Yuhang He,Vibhav Vineet*

Main category: cs.CV

TL;DR: OS-Marathon：首个针对长时程重复性工作流程的计算机使用代理评估基准，包含242个任务，并提出基于少量示例构建浓缩演示的有效方法


<details>
  <summary>Details</summary>
Motivation: 专业场景中存在大量长时程、重复性的工作流程（如处理报销单据、录入学生成绩），这些任务对人类来说繁琐耗时，但对计算机使用代理（CUAs）来说却非常适合，因为其具有结构化、可重复的子工作流程。当前缺乏相关评估基准是主要瓶颈。

Method: 1）建立OS-Marathon基准，包含242个长时程重复性任务，覆盖2个领域；2）提出一种成本效益高的方法，仅使用少量示例构建浓缩演示，教会代理底层工作流程逻辑，使其能够在更大的未见数据集合上有效执行类似工作流程。

Result: 大量实验证明了这些任务的内在挑战性以及所提方法的有效性。OS-Marathon基准为评估最先进的代理提供了标准化测试平台。

Conclusion: OS-Marathon填补了长时程重复性工作流程评估基准的空白，提出的浓缩演示方法能够有效提升计算机使用代理处理此类任务的能力，为自动化专业工作流程提供了实用解决方案。

Abstract: Long-horizon, repetitive workflows are common in professional settings, such as processing expense reports from receipts and entering student grades from exam papers. These tasks are often tedious for humans since they can extend to extreme lengths proportional to the size of the data to process. However, they are ideal for Computer-Use Agents (CUAs) due to their structured, recurring sub-workflows with logic that can be systematically learned. Identifying the absence of an evaluation benchmark as a primary bottleneck, we establish OS-Marathon, comprising 242 long-horizon, repetitive tasks across 2 domains to evaluate state-of-the-art (SOTA) agents. We then introduce a cost-effective method to construct a condensed demonstration using only few-shot examples to teach agents the underlying workflow logic, enabling them to execute similar workflows effectively on larger, unseen data collections. Extensive experiments demonstrate both the inherent challenges of these tasks and the effectiveness of our proposed method. Project website: https://os-marathon.github.io/.

</details>


### [55] [FD-MAD: Frequency-Domain Residual Analysis for Face Morphing Attack Detection](https://arxiv.org/abs/2601.20656)
*Diogo J. Paulo,Hugo Proença,João C. Neves*

Main category: cs.CV

TL;DR: 提出区域感知的频率域残差建模方法，结合马尔可夫随机场进行全局-局部融合，显著提升单图像人脸融合攻击检测在跨数据集和跨融合场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 人脸融合攻击对电子身份验证和边境控制构成严重威胁，现有单图像融合攻击检测方法在跨数据集场景下表现不佳，需要更鲁棒的检测策略。

Method: 1) 提出残差频率域概念，将信号频率与自然频谱衰减解耦以区分真实和融合数据；2) 使用马尔可夫随机场结合不同面部区域的证据进行全局-局部推理；3) 仅使用频谱特征，在SMDD数据集上训练。

Result: 在FRLL-Morph数据集上平均等错误率为1.85%，在MAD22数据集上排名第二（平均EER 6.12%），在低攻击呈现分类错误率下获得良好的真实呈现分类错误率。

Conclusion: 傅里叶域残差建模结合结构化区域融合为深度S-MAD架构提供了有竞争力的替代方案，在跨数据集和跨融合场景下表现出色。

Abstract: Face morphing attacks present a significant threat to face recognition systems used in electronic identity enrolment and border control, particularly in single-image morphing attack detection (S-MAD) scenarios where no trusted reference is available. In spite of the vast amount of research on this problem, morph detection systems struggle in cross-dataset scenarios. To address this problem, we introduce a region-aware frequency-based morph detection strategy that drastically improves over strong baseline methods in challenging cross-dataset and cross-morph settings using a lightweight approach. Having observed the separability of bona fide and morph samples in the frequency domain of different facial parts, our approach 1) introduces the concept of residual frequency domain, where the frequency of the signal is decoupled from the natural spectral decay to easily discriminate between morph and bona fide data; 2) additionally, we reason in a global and local manner by combining the evidence from different facial regions in a Markov Random Field, which infers a globally consistent decision. The proposed method, trained exclusively on the synthetic morphing attack detection development dataset (SMDD), is evaluated in challenging cross-dataset and cross-morph settings on FRLL-Morph and MAD22 sets. Our approach achieves an average equal error rate (EER) of 1.85\% on FRLL-Morph and ranks second on MAD22 with an average EER of 6.12\%, while also obtaining a good bona fide presentation classification error rate (BPCER) at a low attack presentation classification error rate (APCER) using only spectral features. These findings indicate that Fourier-domain residual modeling with structured regional fusion offers a competitive alternative to deep S-MAD architectures.

</details>


### [56] [ProSkill: Segment-Level Skill Assessment in Procedural Videos](https://arxiv.org/abs/2601.20661)
*Michele Mazzamuto,Daniele Di Mauro,Gianpiero Francesca,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 提出了ProSkill数据集，这是首个用于程序性任务中动作级技能评估的基准数据集，包含绝对技能评估标注和成对评估标注，通过瑞士锦标赛方案和ELO评分系统实现高效标注。


<details>
  <summary>Details</summary>
Motivation: 当前技能评估研究主要集中于体育领域，缺乏复杂程序性活动的大规模数据集，现有研究通常只涉及有限动作数量，且主要关注成对评估或二元标签，无法满足程序性视频中技能评估的需求。

Method: 提出新颖且可扩展的标注协议，采用瑞士锦标赛方案进行高效的成对比较，然后通过基于ELO的评分系统将这些成对评估聚合成一致、连续的全局分数，从而创建绝对技能评估排名。

Result: 创建了ProSkill基准数据集，并用该数据集对主要的技能评估算法进行基准测试，包括基于排名的范式和成对范式，当前最先进算法在该数据集上表现不佳，凸显了该数据集的价值和挑战性。

Conclusion: ProSkill填补了程序性视频技能评估领域的数据集空白，其标注协议和数据集为技能评估研究提供了重要基础，当前算法的不足表明该领域仍有很大研究空间。

Abstract: Skill assessment in procedural videos is crucial for the objective evaluation of human performance in settings such as manufacturing and procedural daily tasks. Current research on skill assessment has predominantly focused on sports and lacks large-scale datasets for complex procedural activities. Existing studies typically involve only a limited number of actions, focus on either pairwise assessments (e.g., A is better than B) or on binary labels (e.g., good execution vs needs improvement). In response to these shortcomings, we introduce ProSkill, the first benchmark dataset for action-level skill assessment in procedural tasks. ProSkill provides absolute skill assessment annotations, along with pairwise ones. This is enabled by a novel and scalable annotation protocol that allows for the creation of an absolute skill assessment ranking starting from pairwise assessments. This protocol leverages a Swiss Tournament scheme for efficient pairwise comparisons, which are then aggregated into consistent, continuous global scores using an ELO-based rating system. We use our dataset to benchmark the main state-of-the-art skill assessment algorithms, including both ranking-based and pairwise paradigms. The suboptimal results achieved by the current state-of-the-art highlight the challenges and thus the value of ProSkill in the context of skill assessment for procedural videos. All data and code are available at https://fpv-iplab.github.io/ProSkill/

</details>


### [57] [bi-modal textual prompt learning for vision-language models in remote sensing](https://arxiv.org/abs/2601.20675)
*Pankhi Kashyap,Mainak Singha,Biplab Banerjee*

Main category: cs.CV

TL;DR: BiMoRS是一个轻量级双模态提示学习框架，专门针对遥感图像任务设计，通过融合图像描述文本和视觉特征来提升跨域泛化性能


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法在自然图像上表现良好，但在遥感图像上存在挑战：多标签场景、类内高变异性、空间分辨率多样性，导致难以识别主导语义线索和泛化到新类别

Method: 使用冻结的图像描述模型（如BLIP-2）从遥感图像提取文本语义摘要，通过BERT分词器进行分词，与CLIP编码器的高级视觉特征融合，轻量级交叉注意力模块基于融合的文本-视觉表示学习可查询提示

Result: 在四个遥感数据集和三个域泛化任务上评估，BiMoRS始终表现出性能提升，平均比强基线高出2%

Conclusion: BiMoRS通过双模态提示学习有效解决了遥感图像的特殊挑战，在不修改CLIP主干的情况下实现了更好的跨域泛化性能

Abstract: Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.

</details>


### [58] [Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework](https://arxiv.org/abs/2601.20689)
*Xinyue Li,Zhichao Zhang,Zhiming Xu,Shubo Xu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: LEAF是一个标签高效的图像质量评估框架，通过从MLLM教师模型蒸馏感知质量先验到轻量级学生回归器，用最小人工监督实现MOS校准。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在IQA任务中表现良好，但适应大规模模型计算成本高且依赖大量MOS标注。作者认为MLLM-based IQA的核心瓶颈不是质量感知能力，而是MOS尺度校准。

Method: 提出LEAF框架：1) 教师MLLM通过点级判断和配对偏好进行密集监督，并估计决策可靠性；2) 学生模型通过联合蒸馏学习教师的质量感知模式；3) 在小规模MOS子集上校准以对齐人类标注。

Result: 在用户生成和AI生成的IQA基准测试中，该方法显著减少了对人工标注的需求，同时保持了强大的MOS对齐相关性，使轻量级IQA在有限标注预算下变得实用。

Conclusion: LEAF通过蒸馏MLLM教师的感知先验到轻量级学生模型，有效解决了MLLM-based IQA中的MOS校准瓶颈，实现了标签高效的图像质量评估。

Abstract: Recent multimodal large language models (MLLMs) have demonstrated strong capabilities in image quality assessment (IQA) tasks. However, adapting such large-scale models is computationally expensive and still relies on substantial Mean Opinion Score (MOS) annotations. We argue that for MLLM-based IQA, the core bottleneck lies not in the quality perception capacity of MLLMs, but in MOS scale calibration. Therefore, we propose LEAF, a Label-Efficient Image Quality Assessment Framework that distills perceptual quality priors from an MLLM teacher into a lightweight student regressor, enabling MOS calibration with minimal human supervision. Specifically, the teacher conducts dense supervision through point-wise judgments and pair-wise preferences, with an estimate of decision reliability. Guided by these signals, the student learns the teacher's quality perception patterns through joint distillation and is calibrated on a small MOS subset to align with human annotations. Experiments on both user-generated and AI-generated IQA benchmarks demonstrate that our method significantly reduces the need for human annotations while maintaining strong MOS-aligned correlations, making lightweight IQA practical under limited annotation budgets.

</details>


### [59] [LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?](https://arxiv.org/abs/2601.20705)
*Zhuang Yu,Lei Shen,Jing Zhao,Shiliang Sun*

Main category: cs.CV

TL;DR: LEMON是一个基于STEM讲座视频的多模态评估基准，专注于长时程推理和跨模态整合，包含2,277个视频片段和4,181个高质量QA对，揭示当前MLLMs在时序推理和教学预测方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉、音频和语言任务上表现显著，但在长格式、知识密集且具有时序结构的教育内容上的性能尚未充分探索。需要建立专门的基准来评估模型在复杂教学场景中的能力。

Method: 构建LEMON基准，包含2,277个STEM讲座视频片段，平均时长196.1秒，涵盖5个学科29门课程。生成4,181个高质量QA对（3,413个多项选择和768个开放性问题），设计6个主要任务和12个子任务，覆盖从感知到推理再到生成的完整认知谱系。

Result: 实验显示各任务间存在显著性能差距，即使是GPT-4o等最先进的MLLMs也在时序推理和教学预测方面表现不佳。基准揭示了当前模型在处理长格式教学内容时的局限性。

Conclusion: LEMON作为一个可扩展且具有挑战性的基准，能够推动多模态感知、推理和生成在长格式教学内容中的发展，帮助识别和解决当前MLLMs在教育应用中的关键瓶颈。

Abstract: Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lecture-based Evaluation benchmark for MultimOdal uNderstanding, focusing on STEM lecture videos that require long-horizon reasoning and cross-modal integration. LEMON comprises 2,277 video segments spanning 5 disciplines and 29 courses, with an average duration of 196.1 seconds, yielding 4,181 high-quality QA pairs, including 3,413 multiple-choice and 768 open-ended questions. Distinct from existing video benchmarks, LEMON features: (1) semantic richness and disciplinary density, (2) tightly coupled video-audio-text modalities, (3) explicit temporal and pedagogical structure, and (4) contextually linked multi-turn questioning. It further encompasses six major tasks and twelve subtasks, covering the full cognitive spectrum from perception to reasoning and then to generation. Comprehensive experiments reveal substantial performance gaps across tasks, highlighting that even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction. We expect LEMON to serve as an extensible and challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional contents.

</details>


### [60] [Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction](https://arxiv.org/abs/2601.20720)
*Matej Halinkovic,Nina Masarykova,Alexey Vinel,Marek Galinski*

Main category: cs.CV

TL;DR: Li-ViP3D++：基于查询的多模态感知与预测框架，通过查询门控可变形融合实现相机与LiDAR在查询空间中的端到端融合，提升自动驾驶系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统模块化感知预测流程存在信息流限制和误差放大问题，现有基于查询的PnP模型在相机与LiDAR融合方面探索不足，常采用启发式对齐和离散选择步骤，导致信息利用不充分和偏差引入。

Method: 提出查询门控可变形融合（QGDF）：(1) 通过掩码注意力跨相机和特征层聚合图像证据；(2) 通过带学习偏移量的全可微分BEV采样提取LiDAR上下文；(3) 应用查询条件门控自适应加权每个智能体的视觉和几何线索。

Result: 在nuScenes数据集上，Li-ViP3D++提升了端到端行为质量和检测性能：EPA达到0.335，mAP达到0.502，显著降低误报率（FP ratio 0.147），且比前代Li-ViP3D更快（139.82 ms vs. 145.91 ms）。

Conclusion: 查询空间中的全可微分相机-LiDAR融合能够在不牺牲部署性的前提下，提高端到端感知预测的鲁棒性，为自动驾驶系统提供更可靠的解决方案。

Abstract: End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.

</details>


### [61] [Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification](https://arxiv.org/abs/2601.20742)
*Xin Jin,Jinming Liu,Yuntao Wei,Junyan Lin,Zhicheng Wang,Jianguo Huang,Xudong Yang,Yanxiao Liu,Wenjun Zeng*

Main category: cs.CV

TL;DR: 该论文探讨了视觉编码与视觉token技术在压缩效率与模型性能之间的本质联系，提出统一框架并展望下一代技术发展。


<details>
  <summary>Details</summary>
Motivation: 传统视觉编码与新兴视觉token技术都追求在最小化计算成本的同时最大化语义信息保真度，两者在压缩与智能任务性能方面存在本质联系，需要统一理解以推动技术进步。

Method: 首先全面综述视觉编码和视觉token技术两大技术家族，从优化角度统一它们，探讨压缩效率与模型性能权衡的本质。基于提出的统一公式，综合双向洞察并预测下一代技术。最后通过实验展示任务导向token在MLLMs、AIGC和具身AI等实际任务中的潜力。

Result: 建立了视觉编码与视觉token技术的统一框架，揭示了压缩效率与模型性能之间的本质联系。实验表明任务导向token在MLLMs、AIGC和具身AI等实际任务中具有巨大潜力，为未来标准化通用token技术提供了理论基础。

Conclusion: 压缩与智能密切相关，视觉编码与视觉token技术本质相通。统一框架为下一代视觉编解码和token技术发展提供了新视角，未来可能像传统编解码标准（如H.264/265）一样，开发出高效统一的通用token技术来支持广泛的智能任务。

Abstract: "Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.

</details>


### [62] [FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models](https://arxiv.org/abs/2601.20791)
*Haonan Zhong,Wei Song,Tingxu Han,Maurice Pagnucco,Jingling Xue,Yang Song*

Main category: cs.CV

TL;DR: FairT2V是一个无需训练的文本到视频生成去偏框架，通过锚点球面测地变换中和提示嵌入，在早期去噪步骤应用去偏以保持时序一致性，显著减少职业相关的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 文本到视频扩散模型虽然进展迅速，但其人口统计偏见（特别是性别偏见）尚未得到充分研究。研究发现预训练文本编码器即使对中性提示也会编码隐含的性别关联，导致生成的视频存在偏见。

Method: 1. 分析T2V模型的偏见来源，发现主要来自预训练文本编码器；2. 提出基于锚点的球面测地变换来中和提示嵌入，同时保持语义；3. 通过动态去噪调度，仅在早期身份形成步骤应用去偏以保持时序一致性；4. 提出结合VideoLLM推理和人工验证的视频级公平性评估协议。

Result: 在Open-Sora模型上的实验表明，FairT2V显著减少了跨职业的人口统计偏见，同时对视频质量影响最小。

Conclusion: FairT2V是一个有效的训练免费去偏框架，能够显著减少文本到视频生成中的性别偏见，同时保持视频质量和时序一致性，为公平AI视频生成提供了实用解决方案。

Abstract: Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.
  Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.

</details>


### [63] [Open-Vocabulary Functional 3D Human-Scene Interaction Generation](https://arxiv.org/abs/2601.20835)
*Jie Liu,Yu Sun,Alpar Cseke,Yao Feng,Nicolas Heron,Michael J. Black,Yan Zhang*

Main category: cs.CV

TL;DR: FunHSI：一个无需训练的、功能驱动框架，从开放词汇任务提示生成功能正确的3D人-场景交互，通过功能感知接触推理和阶段优化确保物理合理性和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对物体功能和相应人-场景接触的显式推理，导致不合理的或功能错误的交互。需要解决在3D场景中生成功能正确的人-场景交互这一开放问题。

Method: 1）功能感知接触推理：识别功能场景元素，重建3D几何，通过接触图建模高级交互；2）利用视觉语言模型合成执行任务的人类图像并估计3D姿态；3）通过阶段优化细化3D身体配置以确保物理合理性和功能正确性。

Result: FunHSI能生成功能正确且物理合理的人-场景交互，支持从"坐在沙发上"到"提高室温"等粗细粒度的功能交互，在多样室内外场景中表现一致优于现有方法。

Conclusion: FunHSI通过显式功能推理和接触建模，解决了生成功能正确3D人-场景交互的挑战，在物理合理性和功能正确性方面优于现有方法，支持开放词汇任务提示。

Abstract: Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as "sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., "increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.

</details>


### [64] [A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion](https://arxiv.org/abs/2601.20847)
*Willams de Lima Costa,Thifany Ketuli Silva de Souza,Jonas Ferreira Silva,Carlos Gabriel Bezerra Pereira,Bruno Reis Vila Nova,Leonardo Silvino Brito,Rafael Raider Leoni,Juliano Silva,Valter Ferreira,Sibele Miguel Soares Neto,Samantha Uehara,Daniel Giacomo,João Marcelo Teixeira,Veronica Teichrieb,Cristiano Coelho de Araújo*

Main category: cs.CV

TL;DR: 提出多模态道路表面分类框架，融合图像和惯性测量，引入ROAD数据集，在多种环境条件下实现稳健性能。


<details>
  <summary>Details</summary>
Motivation: 现有道路表面分类技术由于传感模态有限和数据集环境多样性不足，难以在狭窄操作条件之外泛化。需要解决这些限制以支持环境感知的预测性维护系统。

Method: 引入多模态框架，使用轻量级双向交叉注意力模块融合图像和惯性测量，后接自适应门控层调整域偏移下的模态贡献。创建ROAD数据集，包含真实世界多模态记录、大规模视觉子集和合成子集。

Result: 在PVS基准上比先前最先进方法提升1.4个百分点，在多模态ROAD子集上提升11.6个百分点，在少数类别上保持更高F1分数。在夜间、大雨和混合表面过渡等挑战性视觉条件下表现稳定。

Conclusion: 结合经济型摄像头和IMU传感器与多模态注意力机制，为道路表面理解提供了可扩展、稳健的基础，特别适用于环境多变和成本受限的地区。

Abstract: Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.

</details>


### [65] [FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models](https://arxiv.org/abs/2601.20857)
*Hongyu Zhou,Zisen Shao,Sheng Miao,Pan Wang,Dongfeng Bai,Bingbing Liu,Yiyi Liao*

Main category: cs.CV

TL;DR: FreeFix提出了一种无需微调的2D-3D交替优化方法，利用预训练图像扩散模型提升外推视图渲染质量，通过像素级置信度掩码定位不确定区域进行针对性改进，在保持泛化能力的同时达到与微调方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 神经辐射场和3D高斯泼溅等新视角合成方法依赖密集输入且在外推视图时质量下降。现有基于扩散模型的方法面临泛化性与保真度的权衡：微调扩散模型可提高保真度但可能过拟合，而不微调的方法保持泛化性但保真度较低。

Method: 提出FreeFix方法：1）交替2D-3D优化策略，利用预训练图像扩散模型进行一致性优化而无需昂贵的视频扩散模型；2）引入像素级置信度掩码识别不确定区域进行针对性改进。

Result: 在多个数据集上的实验表明，FreeFix提高了多帧一致性，性能达到或超过基于微调的方法，同时保持了强大的泛化能力。

Conclusion: FreeFix通过创新的2D-3D交替优化和置信度掩码机制，在无需微调的情况下突破了泛化性与保真度的权衡边界，为外推视图渲染提供了有效的解决方案。

Abstract: Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.

</details>
