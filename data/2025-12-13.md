<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence](https://arxiv.org/abs/2512.10863)
*Jingli Lin,Runsen Xu,Shaohao Zhu,Sihan Yang,Peizhou Cao,Yunlong Ran,Miao Hu,Chenming Zhu,Yiman Xie,Yilin Long,Wenbo Hu,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: MMSI-Video-Bench：首个全面评估多模态大语言模型视频空间智能的基准，包含1,106个问题，覆盖感知、规划、预测和跨视频推理四个层次，揭示当前模型与人类存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 空间理解对MLLMs在物理环境中成为通用助手至关重要，但目前缺乏全面评估视频空间智能进展的基准。

Method: 构建包含1,106个问题的基准，基于1,278个视频片段（来自25个数据集和内部视频），采用四层次框架（感知、规划、预测、跨视频推理），所有问题由3DV专家精心设计和评审。

Result: 评估25个开源和专有MLLMs，发现显著的人机差距：多数模型接近随机水平，最佳推理模型落后人类近60%。空间微调模型在基准上泛化效果差，存在几何推理、运动定位、长时预测和跨视频对应等系统性失败。

Conclusion: MMSI-Video-Bench为推进视频空间智能提供了坚实的测试平台，揭示了当前MLLMs在空间理解方面的严重不足，并发现典型帧采样策略、3D空间线索和思维链提示在空间推理任务中效果有限。

Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.

</details>


### [2] [BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models](https://arxiv.org/abs/2512.10932)
*Shengao Wang,Wenqi Wang,Zecheng Wang,Max Whitton,Michael Wakeham,Arjun Chandra,Joey Huang,Pengyue Zhu,Helen Chen,David Li,Jeffrey Li,Shawn Li,Andrew Zagula,Amy Zhao,Andrew Zhu,Sayaka Nakamura,Yuki Yamamoto,Jerry Jun Yokono,Aaron Mueller,Bryan A. Plummer,Kate Saenko,Venkatesh Saligrama,Boqing Gong*

Main category: cs.CV

TL;DR: BabyVLM-V2是一个基于儿童发展轨迹的视觉语言模型框架，通过纵向多模态预训练数据集和认知评估工具包，在小型模型上实现了与GPT-4o竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 儿童早期发展轨迹为视觉基础模型的样本高效预训练提供了自然目标。现有模型缺乏发展心理学基础，需要更符合婴幼儿认知能力的评估方法。

Method: 1) 构建纵向、多方面的婴儿中心化视听语料库预训练集；2) 开发DevCV Toolbox，将NIH Baby Toolbox的视觉相关测量转化为10个多模态任务基准；3) 从零开始预训练紧凑模型。

Result: 紧凑模型在DevCV Toolbox上表现出色，在某些任务上超越了GPT-4o。框架为发展合理的视觉基础模型预训练提供了统一方法。

Conclusion: BabyVLM-V2框架通过发展心理学基础的方法，加速了视觉基础模型的发展合理预训练研究，为样本高效学习提供了新方向。

Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.

</details>


### [3] [Any4D: Unified Feed-Forward Metric 4D Reconstruction](https://arxiv.org/abs/2512.10935)
*Jay Karhade,Nikhil Keetha,Yuchen Zhang,Tanisha Gupta,Akash Sharma,Sebastian Scherer,Deva Ramanan*

Main category: cs.CV

TL;DR: Any4D是一个可扩展的多视角transformer，用于度量尺度、密集前馈4D重建，能够直接生成N帧的逐像素运动和几何预测，支持多种模态输入，在精度和计算效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常专注于2视角密集场景流或稀疏3D点跟踪，而其他4D重建方法主要处理单目RGB视频，缺乏对多模态传感器数据的支持。需要一种更灵活、高效的4D重建框架。

Method: 采用模块化的4D场景表示：将每视角4D预测编码为以局部相机坐标表示的自我中心因子（深度图和相机内参）和以全局世界坐标表示的他者中心因子（相机外参和场景流）。使用可扩展的多视角transformer进行度量尺度的密集前馈4D重建。

Result: 在多种设置下实现卓越性能：精度上误差降低2-3倍，计算效率提高15倍，为多个下游应用开辟了新途径。

Conclusion: Any4D提供了一个灵活、高效的4D重建框架，能够处理多模态输入，在精度和速度上显著优于现有方法，具有广泛的应用前景。

Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.

</details>


### [4] [OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis](https://arxiv.org/abs/2512.10940)
*Xiang Fan,Sharath Girish,Vivek Ramanujan,Chaoyang Wang,Ashkan Mirzaei,Petr Sushko,Aliaksandr Siarohin,Sergey Tulyakov,Ranjay Krishna*

Main category: cs.CV

TL;DR: OmniView是一个统一的4D一致性框架，通过分离空间、时间和视角条件，能够处理多种4D任务，包括新视角合成、文本/图像到视频生成等，在多个基准测试中表现优于任务特定模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法将相机控制注入扩散模型时，通常只专注于特定的4D一致性任务子集（如新视角合成、文本到视频等），导致需要在不同的3D/4D数据子集上进行分散训练。这种碎片化方法限制了模型的通用性和效率。

Method: OmniView采用统一框架，将空间、时间和视角条件分别表示，允许这些输入条件的灵活组合。这种方法使得模型能够处理多种4D任务，包括从静态、动态和多视角输入合成新视角，在时间上前向和后向外推轨迹，以及从文本或图像提示生成具有完整相机控制的视频。

Result: OmniView在多个基准测试中与任务特定模型竞争，显著提升了性能：在LLFF多视角新视角合成数据集上图像质量得分提升33%，在Neural 3D Video动态新视角合成基准上提升60%，在RE-10K静态相机控制上提升20%，在文本条件视频生成中将相机轨迹误差减少了4倍。

Conclusion: OmniView展示了在单一模型中实现强泛化能力的可行性，证明了通用4D视频模型的可行性。该框架为统一的4D内容生成提供了有前景的方向。

Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/

</details>


### [5] [Mull-Tokens: Modality-Agnostic Latent Thinking](https://arxiv.org/abs/2512.10941)
*Arijit Ray,Ahmed Abdelkader,Chengzhi Mao,Bryan A. Plummer,Kate Saenko,Ranjay Krishna,Leonidas Guibas,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: Mull-Tokens是一种模态无关的潜在令牌，允许模型在图像或文本模态中自由思考，以解决多模态推理问题，在空间推理基准上平均提升3%


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在空间、时间、可用性等真实世界推理方面存在局限性，它们依赖专家工具、昂贵的图像生成或手工制作的数据来切换文本和图像思维，需要更简单、可扩展的解决方案

Method: 提出Mull-Tokens（模态无关潜在令牌），预训练这些令牌以在图像或文本模态中保存中间信息。首先使用交错文本-图像轨迹进行监督训练，然后仅使用最终答案进行无监督微调

Result: 在四个具有挑战性的空间推理基准测试中，Mull-Tokens优于仅使用文本推理或交错图像-文本推理的基线方法，平均提升3%，在推理密集的谜题解决任务上提升高达16%

Conclusion: Mull-Tokens为多模态抽象思维提供了简单解决方案，有助于解决文本和视觉推理中的基础挑战，使模型能够自由思考以获得正确答案

Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.

</details>


### [6] [AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation](https://arxiv.org/abs/2512.10943)
*Sharath Girish,Viacheslav Ivanov,Tsai-Shien Chen,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov*

Main category: cs.CV

TL;DR: AlcheMinT是一个统一的主题驱动视频生成框架，通过时间戳条件控制实现多主题的精确时序控制，无需额外交叉注意力模块。


<details>
  <summary>Details</summary>
Motivation: 现有主题驱动视频生成方法缺乏对主题出现和消失的细粒度时序控制，这在组合视频合成、故事板制作和可控动画等应用中至关重要。

Method: 提出新颖的位置编码机制，支持时间间隔编码（对应主题身份），与预训练视频生成模型的位置嵌入无缝集成；引入主题描述性文本标记加强视觉身份与视频描述的绑定；通过标记级联避免额外交叉注意力模块，参数开销极小。

Result: AlcheMinT在视觉质量上与最先进的视频个性化方法相当，同时首次实现了视频内多主题生成的精确时序控制；建立了评估多主题身份保持、视频保真度和时序遵循的基准。

Conclusion: AlcheMinT为主题驱动视频生成提供了精确的时序控制能力，在保持高质量的同时实现了多主题的精确时序管理，为组合视频合成等应用开辟了新可能。

Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT

</details>


### [7] [Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/abs/2512.10949)
*Yiwen Tang,Zoey Guo,Kaixin Zhu,Ray Zhang,Qizhi Chen,Dongzhi Jiang,Junli Liu,Bohan Zeng,Haoming Song,Delin Qu,Tianyi Bai,Dan Xu,Wentao Zhang,Bin Zhao*

Main category: cs.CV

TL;DR: 首次系统研究RL在文本到3D自回归生成中的应用，提出AR3D-R1模型，通过奖励设计、算法优化和层次化训练提升3D生成质量。


<details>
  <summary>Details</summary>
Motivation: RL在2D图像生成中已证明有效，但3D生成由于空间复杂度高、需要全局几何一致性和细粒度局部纹理，对奖励设计和RL算法更加敏感，目前仍缺乏系统研究。

Method: 从四个维度系统研究：1) 奖励设计评估维度与模型选择；2) GRPO变体算法研究，强调token级优化；3) 引入MME-3DR基准测试；4) 提出Hi-GRPO层次化训练范式，优化全局到局部生成。

Result: 开发了AR3D-R1模型，这是首个RL增强的文本到3D模型，能够从粗形状到纹理细化进行专家级生成。研究表明人类偏好对齐和多模态模型对3D属性提供稳健信号。

Conclusion: 本研究为RL驱动的3D生成推理提供了重要见解，证明了RL在复杂3D生成任务中的有效性，并发布了代码和基准测试，推动该领域发展。

Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.

</details>


### [8] [SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model](https://arxiv.org/abs/2512.10957)
*Yukai Shi,Weiyu Li,Zihao Wang,Hongyang Li,Xingyu Chen,Ping Tan,Lei Zhang*

Main category: cs.CV

TL;DR: SceneMaker提出解耦的3D场景生成框架，通过分离去遮挡模型与3D物体生成，并引入统一姿态估计模型，解决严重遮挡和开放集场景下的几何质量和姿态准确性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在严重遮挡和开放集场景下，由于缺乏足够的开放集去遮挡和姿态估计先验知识，难以同时生成高质量几何和准确姿态。

Method: 1) 将去遮挡模型与3D物体生成解耦，利用图像数据集和收集的去遮挡数据集增强开放集遮挡模式处理能力；2) 提出统一姿态估计模型，整合全局和局部机制的自注意力和交叉注意力；3) 构建开放集3D场景数据集扩展姿态估计模型的泛化能力。

Result: 综合实验证明，该解耦框架在室内和开放集场景上均表现出优越性能。

Conclusion: SceneMaker通过解耦设计和统一姿态估计模型，有效解决了严重遮挡和开放集场景下的3D场景生成挑战，代码和数据集已开源。

Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.

</details>
