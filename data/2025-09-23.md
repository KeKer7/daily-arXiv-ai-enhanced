<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 213]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Evaluation of Ensemble Learning Techniques for handwritten OCR Improvement](https://arxiv.org/abs/2509.16221)
*Martin Preiß*

Main category: cs.CV

TL;DR: 本文研究了集成学习与OCR结合在历史病历数字化中的应用，发现集成学习能提高OCR准确率，且训练数据集大小不影响效果。


<details>
  <summary>Details</summary>
Motivation: 为教授研究组的本科项目数字化历史病历手写记录，医疗领域需要高精度OCR，集成学习声称能提高现有方法的准确率。

Method: 采用集成学习方法结合多个机器学习模型与OCR技术，研究其对病历数字化的价值。

Result: 发现集成学习能提高OCR准确率，确定了有效的方法，且训练数据集大小对此无影响。

Conclusion: 集成学习与OCR结合能为病历数字化创造附加价值，是有效的技术方案。

Abstract: For the bachelor project 2021 of Professor Lippert's research group,
handwritten entries of historical patient records needed to be digitized using
Optical Character Recognition (OCR) methods. Since the data will be used in the
future, a high degree of accuracy is naturally required. Especially in the
medical field this has even more importance. Ensemble Learning is a method that
combines several machine learning models and is claimed to be able to achieve
an increased accuracy for existing methods. For this reason, Ensemble Learning
in combination with OCR is investigated in this work in order to create added
value for the digitization of the patient records. It was possible to discover
that ensemble learning can lead to an increased accuracy for OCR, which methods
were able to achieve this and that the size of the training data set did not
play a role here.

</details>


### [2] [Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute](https://arxiv.org/abs/2509.16343)
*Chung-En,Yu,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: VRA是一个无需训练的智能视觉推理框架，通过Think-Critique-Act循环提升视觉语言模型和纯视觉系统的鲁棒性，在视觉推理基准上实现高达40%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 为高风险领域（如遥感和医疗诊断）开发可信赖的智能视觉系统，需要在不进行昂贵重新训练的情况下实现广泛的鲁棒性。

Method: 提出Visual Reasoning Agent (VRA)框架，将现成的视觉语言模型和纯视觉系统封装在Think-Critique-Act循环中，无需训练即可使用。

Result: VRA在具有挑战性的视觉推理基准上实现了高达40%的绝对准确率提升，尽管增加了显著的测试时计算开销。

Conclusion: 未来工作将优化查询路由和早停机制，以减少推理开销，同时在视觉任务中保持可靠性。

Abstract: Developing trustworthy intelligent vision systems for high-stakes domains,
\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness
without costly retraining. We propose \textbf{Visual Reasoning Agent (VRA)}, a
training-free, agentic reasoning framework that wraps off-the-shelf
vision-language models \emph{and} pure vision systems in a
\emph{Think--Critique--Act} loop. While VRA incurs significant additional
test-time computation, it achieves up to 40\% absolute accuracy gains on
challenging visual reasoning benchmarks. Future work will optimize query
routing and early stopping to reduce inference overhead while preserving
reliability in vision tasks.

</details>


### [3] [From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR](https://arxiv.org/abs/2509.16346)
*Juan Castorena,E. Louise Loudermilk,Scott Pokswinski,Rodman Linn*

Main category: cs.CV

TL;DR: ForestGen3D是一个生成式建模框架，能够仅使用航空激光雷达（ALS）输入合成高保真度的3D森林结构，通过条件去噪扩散概率模型重建被遮挡的冠层下细节。


<details>
  <summary>Details</summary>
Motivation: 生态系统中的3D结构对生态过程和干扰响应至关重要，但大规模测量3D植被结构成本高昂且不可行，需要开发能够从稀疏ALS数据重建详细3D结构的方法。

Method: 基于条件去噪扩散概率模型（DDPMs），使用配准的ALS/TLS数据训练，引入基于ALS观测凸包的几何包含先验来确保生态合理性。

Result: 在混合针叶林生态系统中，ForestGen3D在树木、样地和景观尺度上产生高保真重建，在几何相似性和生物物理指标（如树高、胸径、冠幅等）上与TLS参考数据高度匹配。

Conclusion: ForestGen3D成为在仅使用ALS数据环境中进行生态建模、野火模拟和结构燃料表征的可扩展工具，其包含属性可在没有TLS地面真实数据时作为生成质量的实用代理。

Abstract: The 3D structure of living and non-living components in ecosystems plays a
critical role in determining ecological processes and feedbacks from both
natural and human-driven disturbances. Anticipating the effects of wildfire,
drought, disease, or atmospheric deposition depends on accurate
characterization of 3D vegetation structure, yet widespread measurement remains
prohibitively expensive and often infeasible. We introduce ForestGen3D, a novel
generative modeling framework that synthesizes high-fidelity 3D forest
structure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on
conditional denoising diffusion probabilistic models (DDPMs) trained on
co-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate
TLS-like 3D point clouds conditioned on sparse ALS observations, effectively
reconstructing occluded sub-canopy detail at scale. To ensure ecological
plausibility, we introduce a geometric containment prior based on the convex
hull of ALS observations and provide theoretical and empirical guarantees that
generated structures remain spatially consistent. We evaluate ForestGen3D at
tree, plot, and landscape scales using real-world data from mixed conifer
ecosystems, and show that it produces high-fidelity reconstructions that
closely match TLS references in terms of geometric similarity and biophysical
metrics, such as tree height, DBH, crown diameter and crown volume.
Additionally, we demonstrate that the containment property can serve as a
practical proxy for generation quality in settings where TLS ground truth is
unavailable. Our results position ForestGen3D as a scalable tool for ecological
modeling, wildfire simulation, and structural fuel characterization in ALS-only
environments.

</details>


### [4] [Introducing Resizable Region Packing Problem in Image Generation, with a Heuristic Solution](https://arxiv.org/abs/2509.16363)
*Hrishikesh Sharma*

Main category: cs.CV

TL;DR: 本文介绍了一个新的计算问题——可调整锚定区域打包问题（RARP），并将其应用于合成图像数据生成。作者提出了一种贪心启发式算法来解决这个NP难问题，并验证了算法在生成大规模异常检测数据集中的有效性。


<details>
  <summary>Details</summary>
Motivation: 图像数据生成在计算机视觉中是一个比判别问题更难解决的问题。传统方法包括基于图形学和生成模型的方法，但都存在优化问题。本文旨在解决在图像画布上有意义地放置适当大小的相关对象这一核心挑战。

Method: 提出了一种新颖的贪心启发式算法，能够通用地缩放和打包任意数量的任意形状区域到图像画布中。算法通过仔细地迭代打包区域对，同时遵守优化约束。

Result: 通过实现该算法生成了大规模合成异常检测数据集，每个图像样本具有高度变化的打包参数。视觉检查和解决方案正确性验证证明了算法的有效性。

Conclusion: 随着深度学习生成模型的兴起和合成数据生成即将成为主流，新引入的RARP问题预计将在图像科学社区中得到重视，为解决图像数据生成中的优化问题提供了新思路。

Abstract: The problem of image data generation in computer vision has traditionally
been a harder problem to solve, than discriminative problems. Such data
generation entails placing relevant objects of appropriate sizes each, at
meaningful location in a scene canvas. There have been two classes of popular
approaches to such generation: graphics based, and generative models-based.
Optimization problems are known to lurk in the background for both these
classes of approaches. In this paper, we introduce a novel, practically useful
manifestation of the classical Bin Packing problem in the context of generation
of synthetic image data. We conjecture that the newly introduced problem,
Resizable Anchored Region Packing(RARP) Problem, is NP-hard, and provide
detailed arguments about our conjecture. As a first solution, we present a
novel heuristic algorithm that is generic enough and therefore scales and packs
arbitrary number of arbitrary-shaped regions at arbitrary locations, into an
image canvas. The algorithm follows greedy approach to iteratively pack region
pairs in a careful way, while obeying the optimization constraints. The
algorithm is validated by an implementation that was used to generate a
large-scale synthetic anomaly detection dataset, with highly varying degree of
bin packing parameters per image sample i.e. RARP instance. Visual inspection
of such data and checking of the correctness of each solution proves the
effectiveness of our algorithm. With generative modeling being on rise in deep
learning, and synthetic data generation poised to become mainstream, we expect
that the newly introduced problem will be valued in the imaging scientific
community.

</details>


### [5] [Accurate Thyroid Cancer Classification using a Novel Binary Pattern Driven Local Discrete Cosine Transform Descriptor](https://arxiv.org/abs/2509.16382)
*Saurabh Saini,Kapil Ahuja,Marc C. Steinbach,Thomas Wick*

Main category: cs.CV

TL;DR: 本研究开发了一种新的甲状腺癌CAD系统，重点在于特征提取。提出了BPD-LDCT（二进制模式驱动的局部离散余弦变换）描述符，结合LDCT和ILBP来捕获纹理特征，并使用非线性SVM进行分类。在TDID和AUITD数据集上取得了接近100%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 甲状腺超声图像分类具有挑战性，因为甲状腺周围复杂的解剖结构导致组织密度变化和超声波散射，产生噪声和模糊纹理。基于乳腺癌分类的经验，研究者认为需要更好的纹理特征提取方法来提高分类准确性。

Method: 1. 提出BPD-LDCT描述符，结合局部离散余弦变换（LDCT）和改进的局部二进制模式（ILBP）；2. LDCT用于捕获局部纹理特征；3. ILBP用于噪声鲁棒性；4. 使用非线性SVM进行最终分类；5. 在两个公开数据集（TDID和AUITD）上进行两阶段评估。

Result: 第一阶段（良恶性分类）：TDID数据集接近100%，AUITD数据集97%；第二阶段（TI-RADS 4和5亚分类）：TDID数据集接近100%，AUITD数据集99%。

Conclusion: BPD-LDCT描述符在甲状腺癌分类中表现出色，能够有效处理复杂解剖结构带来的挑战，在两个公开数据集上都取得了优异的分类性能，证明了该方法的有效性和鲁棒性。

Abstract: In this study, we develop a new CAD system for accurate thyroid cancer
classification with emphasis on feature extraction. Prior studies have shown
that thyroid texture is important for segregating the thyroid ultrasound images
into different classes. Based upon our experience with breast cancer
classification, we first conjuncture that the Discrete Cosine Transform (DCT)
is the best descriptor for capturing textural features. Thyroid ultrasound
images are particularly challenging as the gland is surrounded by multiple
complex anatomical structures leading to variations in tissue density. Hence,
we second conjuncture the importance of localization and propose that the Local
DCT (LDCT) descriptor captures the textural features best in this context.
Another disadvantage of complex anatomy around the thyroid gland is scattering
of ultrasound waves resulting in noisy and unclear textures. Hence, we third
conjuncture that one image descriptor is not enough to fully capture the
textural features and propose the integration of another popular texture
capturing descriptor (Improved Local Binary Pattern, ILBP) with LDCT. ILBP is
known to be noise resilient as well. We term our novel descriptor as Binary
Pattern Driven Local Discrete Cosine Transform (BPD-LDCT). Final classification
is carried out using a non-linear SVM. The proposed CAD system is evaluated on
the only two publicly available thyroid cancer datasets, namely TDID and AUITD.
The evaluation is conducted in two stages. In Stage I, thyroid nodules are
categorized as benign or malignant. In Stage II, the malignant cases are
further sub-classified into TI-RADS (4) and TI-RADS (5). For Stage I
classification, our proposed model demonstrates exceptional performance of
nearly 100% on TDID and 97% on AUITD. In Stage II classification, the proposed
model again attains excellent classification of close to 100% on TDID and 99%
on AUITD.

</details>


### [6] [StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes](https://arxiv.org/abs/2509.16415)
*Zhengri Wu,Yiran Wang,Yu Wen,Zeyu Zhang,Biao Wu,Hao Tang*

Main category: cs.CV

TL;DR: StereoAdapter是一个参数高效的自监督框架，用于水下立体深度估计，通过LoRA适配的单目基础编码器和循环立体细化模块，在模拟和真实世界基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决水下立体深度估计中的两个关键挑战：参数高效地适应大型视觉基础编码器到水下领域，以及紧密融合全局一致但尺度模糊的单目先验与局部度量但光度脆弱的立体对应关系。

Method: 提出StereoAdapter框架，包含LoRA适配的单目基础编码器和循环立体细化模块，采用动态LoRA适配进行高效秩选择，并在合成UW-StereoDepth-40K数据集上进行预训练以增强鲁棒性。

Result: 在TartanAir和SQUID基准测试中分别提升6.11%和5.12%，BlueROV2机器人真实世界部署进一步证明了方法的鲁棒性。

Conclusion: StereoAdapter通过参数高效的自监督学习有效解决了水下立体深度估计的挑战，在性能和鲁棒性方面均优于现有最先进方法。

Abstract: Underwater stereo depth estimation provides accurate 3D geometry for robotics
tasks such as navigation, inspection, and mapping, offering metric depth from
low-cost passive cameras while avoiding the scale ambiguity of monocular
methods. However, existing approaches face two critical challenges: (i)
parameter-efficiently adapting large vision foundation encoders to the
underwater domain without extensive labeled data, and (ii) tightly fusing
globally coherent but scale-ambiguous monocular priors with locally metric yet
photometrically fragile stereo correspondences. To address these challenges, we
propose StereoAdapter, a parameter-efficient self-supervised framework that
integrates a LoRA-adapted monocular foundation encoder with a recurrent stereo
refinement module. We further introduce dynamic LoRA adaptation for efficient
rank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to
enhance robustness under diverse underwater conditions. Comprehensive
evaluations on both simulated and real-world benchmarks show improvements of
6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,
while real-world deployment with the BlueROV2 robot further demonstrates the
consistent robustness of our approach. Code:
https://github.com/AIGeeksGroup/StereoAdapter. Website:
https://aigeeksgroup.github.io/StereoAdapter.

</details>


### [7] [AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead](https://arxiv.org/abs/2509.16421)
*Aiden Chang,Celso De Melo,Stephanie M. Lukin*

Main category: cs.CV

TL;DR: Aha是一个自回归高亮检测框架，用于实时视频流理解，通过自然语言任务描述预测每帧视频的相关性，在标准基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法假设可以访问完整视频，不适用于在线或流式场景。需要支持实时决策的逐步推理能力。

Method: 使用多模态视觉语言模型和轻量级解耦头，结合动态SinkCache机制实现恒定内存使用，无需访问未来视频帧。

Result: 在TVSum和Mr.Hisum基准测试中分别提升5.9%和8.3%的mAP，超越之前的所有离线方法。

Conclusion: Aha展示了作为实时推理模块在机器人应用中的潜力，支持下游规划和长期理解任务。

Abstract: Real-time understanding of continuous video streams is essential for
intelligent agents operating in high-stakes environments, including autonomous
vehicles, surveillance drones, and disaster response robots. Yet, most existing
video understanding and highlight detection methods assume access to the entire
video during inference, making them unsuitable for online or streaming
scenarios. In particular, current models optimize for offline summarization,
failing to support step-by-step reasoning needed for real-time decision-making.
We introduce Aha, an autoregressive highlight detection framework that predicts
the relevance of each video frame against a task described in natural language.
Without accessing future video frames, Aha utilizes a multimodal
vision-language model and lightweight, decoupled heads trained on a large,
curated dataset of human-centric video labels. To enable scalability, we
introduce the Dynamic SinkCache mechanism that achieves constant memory usage
across infinite-length streams without degrading performance on standard
benchmarks. This encourages the hidden representation to capture high-level
task objectives, enabling effective frame-level rankings for informativeness,
relevance, and uncertainty with respect to the natural language task. Aha
achieves state-of-the-art (SOTA) performance on highlight detection benchmarks,
surpassing even prior offline, full-context approaches and video-language
models by +5.9% on TVSum and +8.3% on Mr.Hisum in mAP (mean Average Precision).
We explore Aha's potential for real-world robotics applications given a
task-oriented natural language input and a continuous, robot-centric video.
Both experiments demonstrate Aha's potential effectiveness as a real-time
reasoning module for downstream planning and long-horizon understanding.

</details>


### [8] [3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction](https://arxiv.org/abs/2509.16423)
*Maria Taktasheva,Lily Goli,Alessandro Fiorini,Zhen,Li,Daniel Rebain,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: 提出一种混合2D/3D高斯表示方法，联合优化约束平面高斯和自由形式高斯，解决平坦无纹理表面重建问题


<details>
  <summary>Details</summary>
Motivation: 当前辐射场方法在平坦无纹理表面重建效果不佳，产生不均匀和半透明重建结果，而表面重建方法又牺牲了视觉质量

Method: 端到端方法动态检测和优化平面区域，联合优化约束平面（2D）高斯和自由形式（3D）高斯

Result: 在ScanNet++和ScanNetv2上实现最先进的深度估计，在网格提取方面表现出色，不过度拟合特定相机模型

Conclusion: 该方法能有效生成高质量的室内场景重建，在视觉保真度和几何精度方面都有显著提升

Abstract: Recent advances in radiance fields and novel view synthesis enable creation
of realistic digital twins from photographs. However, current methods struggle
with flat, texture-less surfaces, creating uneven and semi-transparent
reconstructions, due to an ill-conditioned photometric reconstruction
objective. Surface reconstruction methods solve this issue but sacrifice visual
quality. We propose a novel hybrid 2D/3D representation that jointly optimizes
constrained planar (2D) Gaussians for modeling flat surfaces and freeform (3D)
Gaussians for the rest of the scene. Our end-to-end approach dynamically
detects and refines planar regions, improving both visual fidelity and
geometric accuracy. It achieves state-of-the-art depth estimation on ScanNet++
and ScanNetv2, and excels at mesh extraction without overfitting to a specific
camera model, showing its effectiveness in producing high-quality
reconstruction of indoor scenes.

</details>


### [9] [TractoTransformer: Diffusion MRI Streamline Tractography using CNN and Transformer Networks](https://arxiv.org/abs/2509.16429)
*Itzik Waizman,Yakov Gusakov,Itay Benou,Tammy Riklin Raviv*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的新型白质纤维束追踪方法，结合CNN提取局部特征，提高神经通路映射的精度和完整性


<details>
  <summary>Details</summary>
Motivation: 白质纤维束追踪面临交叉、合并和扇形配置等挑战，传统方法难以处理噪声和模糊测量，需要更有效的序列建模方法

Method: 使用Transformer建模白质流线的序列特性，结合CNN提取局部微结构特征，整合轨迹上下文和当前扩散MRI测量来预测纤维方向

Result: 在Tractometer工具包评估中取得与最先进方法竞争的性能，在TractoInferno数据集上展示出对真实数据的强泛化能力

Conclusion: 提出的Transformer-CNN混合方法相比传统纤维束追踪模型能更精确和完整地映射神经通路

Abstract: White matter tractography is an advanced neuroimaging technique that
reconstructs the 3D white matter pathways of the brain from diffusion MRI data.
It can be framed as a pathfinding problem aiming to infer neural fiber
trajectories from noisy and ambiguous measurements, facing challenges such as
crossing, merging, and fanning white-matter configurations. In this paper, we
propose a novel tractography method that leverages Transformers to model the
sequential nature of white matter streamlines, enabling the prediction of fiber
directions by integrating both the trajectory context and current diffusion MRI
measurements. To incorporate spatial information, we utilize CNNs that extract
microstructural features from local neighborhoods around each voxel. By
combining these complementary sources of information, our approach improves the
precision and completeness of neural pathway mapping compared to traditional
tractography models. We evaluate our method with the Tractometer toolkit,
achieving competitive performance against state-of-the-art approaches, and
present qualitative results on the TractoInferno dataset, demonstrating strong
generalization to real-world data.

</details>


### [10] [Improved mmFormer for Liver Fibrosis Staging via Missing-Modality Compensation](https://arxiv.org/abs/2509.16436)
*Zhejia Zhang,Junjie Wang,Le Zhang*

Main category: cs.CV

TL;DR: 提出基于mmFormer的多模态MRI分类模型，通过自适应模块处理任意缺失模态组合，在CARE 2025挑战赛的肝纤维化分期任务中取得良好性能


<details>
  <summary>Details</summary>
Motivation: 解决临床MRI中因设备差异或患者配合问题导致的模态缺失问题，提升模型在真实场景下的鲁棒性

Method: 保留mmFormer的混合模态特定编码器和模态相关编码器，集成缺失模态补偿模块（零填充、模态可用性掩码、可学习统计参数的Delta函数），采用交叉验证集成策略

Result: 在肝纤维化分期任务中，肝硬化检测准确率66.67%（AUC 71.73%），显著纤维化检测准确率74.17%（AUC 68.48%）

Conclusion: 所提方法能有效处理MRI模态缺失问题，在真实临床场景中具有实用价值

Abstract: In real-world clinical settings, magnetic resonance imaging (MRI) frequently
suffers from missing modalities due to equipment variability or patient
cooperation issues, which can significantly affect model performance. To
address this issue, we propose a multimodal MRI classification model based on
the mmFormer architecture with an adaptive module for handling arbitrary
combinations of missing modalities. Specifically, this model retains the hybrid
modality-specific encoders and the modality-correlated encoder from mmFormer to
extract consistent lesion features across available modalities. In addition, we
integrate a missing-modality compensation module which leverages zero-padding,
modality availability masks, and a Delta Function with learnable statistical
parameters to dynamically synthesize proxy features for recovering missing
information. To further improve prediction performance, we adopt a
cross-validation ensemble strategy by training multiple models on different
folds and applying soft voting during inference. This method is evaluated on
the test set of Comprehensive Analysis & Computing of REal-world medical images
(CARE) 2025 challenge, targeting the Liver Fibrosis Staging (LiFS) task based
on non-contrast dynamic MRI scans including T1-weighted imaging (T1WI),
T2-weighted imaging (T2WI), and diffusion-weighted imaging (DWI). For Cirrhosis
Detection and Substantial Fibrosis Detection on in-distribution vendors, our
model obtains accuracies of 66.67%, and 74.17%, and corresponding area under
the curve (AUC) scores of 71.73% and 68.48%, respectively.

</details>


### [11] [AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks](https://arxiv.org/abs/2509.16438)
*Mohamed Eltahir,Osamah Sarraj,Abdulrahman Alfrihidi,Taha Alshatiri,Mohammed Khurd,Mohammed Bremoo,Tanveer Hussain*

Main category: cs.CV

TL;DR: AutoArabic是一个三阶段框架，利用大型语言模型将非阿拉伯语视频检索基准翻译成现代标准阿拉伯语，显著减少人工修订工作量，并开发了阿拉伯语视频检索基准DiDeMo-AR。


<details>
  <summary>Details</summary>
Motivation: 当前视频到文本和文本到视频检索主要由英语基准主导，阿拉伯语缺乏本地化评估指标和基准数据集。

Method: 采用三阶段框架：1）使用LLM进行翻译；2）集成错误检测模块（准确率97%）；3）生成阿拉伯语基准DiDeMo-AR，包含40,144条流畅阿拉伯语描述。

Result: 在阿拉伯语和英语基准上训练CLIP风格模型，发现性能差距约为3个百分点（Recall@1），表明阿拉伯语本地化保持了基准难度。不同后编辑预算下性能单调提升，原始LLM输出仍可使用。

Conclusion: AutoArabic框架有效解决了阿拉伯语视频检索基准的缺乏问题，错误检测模块准确率高，框架可复现到其他语言，代码已开源。

Abstract: Video-to-text and text-to-video retrieval are dominated by English benchmarks
(e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet
Arabic remains underserved, lacking localized evaluation metrics. We introduce
a three-stage framework, AutoArabic, utilizing state-of-the-art large language
models (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic,
reducing the manual revision required by nearly fourfold. The framework
incorporates an error detection module that automatically flags potential
translation errors with 97% accuracy. Applying the framework to DiDeMo, a video
retrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent
Arabic descriptions. An analysis of the translation errors is provided and
organized into an insightful taxonomy to guide future Arabic localization
efforts. We train a CLIP-style baseline with identical hyperparameters on the
Arabic and English variants of the benchmark, finding a moderate performance
gap (about 3 percentage points at Recall@1), indicating that Arabic
localization preserves benchmark difficulty. We evaluate three post-editing
budgets (zero/ flagged-only/ full) and find that performance improves
monotonically with more post-editing, while the raw LLM output (zero-budget)
remains usable. To ensure reproducibility to other languages, we made the code
available at https://github.com/Tahaalshatiri/AutoArabic.

</details>


### [12] [KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models](https://arxiv.org/abs/2509.16452)
*Son Hai Nguyen,Diwei Wang,Jinhyeok Jang,Hyewon Seo*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉语言模型的知识增强提示学习方法，用于室内日常动作识别，在ETRI-Activity3D数据集上达到超过95%的准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发能够在复杂真实环境中安全可靠运行的自主机器人需要准确的基于视觉的动作识别能力。

Method: 采用提示学习框架，将每个动作的类别级文本描述作为可学习提示嵌入到预训练的视觉语言模型中，设计并评估了多种文本描述结构和编码策略。

Result: 在ETRI-Activity3D数据集上的实验表明，仅使用RGB视频输入即可达到超过95%的准确率，优于最先进的方法。

Conclusion: 知识增强提示方法能够以最少的监督实现鲁棒的动作识别，证明了该方法的有效性。

Abstract: Accurate vision-based action recognition is crucial for developing autonomous
robots that can operate safely and reliably in complex, real-world
environments. In this work, we advance video-based recognition of indoor daily
actions for robotic perception by leveraging vision-language models (VLMs)
enriched with domain-specific knowledge. We adapt a prompt-learning framework
in which class-level textual descriptions of each action are embedded as
learnable prompts into a frozen pre-trained VLM backbone. Several strategies
for structuring and encoding these textual descriptions are designed and
evaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our
method, using only RGB video inputs at test time, achieves over 95\% accuracy
and outperforms state-of-the-art approaches. These results highlight the
effectiveness of knowledge-augmented prompts in enabling robust action
recognition with minimal supervision.

</details>


### [13] [Explainable Gait Abnormality Detection Using Dual-Dataset CNN-LSTM Models](https://arxiv.org/abs/2509.16472)
*Parth Agarwal,Sangaa Chatterjee,Md Faisal Kabir,Suman Saha*

Main category: cs.CV

TL;DR: 提出双分支CNN-LSTM框架，结合1D关节特征和3D轮廓特征，通过SHAP和Grad-CAM提供可解释性，在步态分析中达到98.6%准确率


<details>
  <summary>Details</summary>
Motivation: 当前步态诊断模型缺乏可解释性且依赖单一数据集，需要开发跨数据集的可解释步态分析方法

Method: 双分支CNN-LSTM框架：1D分支处理GAVD的关节特征，3D分支处理OU-MVLP的轮廓特征，使用SHAP进行时间归因和Grad-CAM进行空间定位

Result: 在保留测试集上达到98.6%的准确率，具有强召回率和F1分数

Conclusion: 该方法在临床和生物识别领域推进了可解释的步态分析

Abstract: Gait is a key indicator in diagnosing movement disorders, but most models
lack interpretability and rely on single datasets. We propose a dual-branch
CNN-LSTM framework a 1D branch on joint-based features from GAVD and a 3D
branch on silhouettes from OU-MVLP. Interpretability is provided by SHAP
(temporal attributions) and Grad-CAM (spatial localization).On held-out sets,
the system achieves 98.6% accuracy with strong recall and F1. This approach
advances explainable gait analysis across both clinical and biometric domains.

</details>


### [14] [Cross-Corpus and Cross-domain Handwriting Assessment of NeuroDegenerative Diseases via Time-Series-to-Image Conversion](https://arxiv.org/abs/2509.16474)
*Gabrielle Chavez,Laureano Moro-Velazquez,Ankur Butala,Najim Dehak,Thomas Thebaud*

Main category: cs.CV

TL;DR: 提出一个结合时间序列和图像的联合分类器框架，用于通过笔迹分析检测神经系统疾病如帕金森病和阿尔茨海默病，在多个数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于特征或计算机视觉的方法在处理不同形式笔迹数据（时间序列和图像）时泛化能力不足，需要一种能够统一处理多种笔迹信号的方法来改善神经系统疾病的运动缺陷检测。

Method: 基于ImageNet-1k预训练的ResNet50构建联合分类器，同时利用笔迹的时间序列和图像数据，进行二元分类实验。

Result: 在现有数据集上达到最先进性能，特别是在NLS数据集的画钟和螺旋任务中表现显著提升，交叉数据集和多数据集实验F1分数高达98%，显示出良好的泛化能力。

Conclusion: 该模型能够有效泛化到不同形式的笔迹信号，有望增强神经系统疾病中运动缺陷的检测能力。

Abstract: Handwriting is significantly affected by neurological disorders (ND) such as
Parkinson's disease (PD) and Alzheimer's disease (AD). Prior works have
analyzed handwriting tasks using feature-based approaches or computer-vision
techniques, but these methods have struggled to generalize across multiple
datasets, particularly between temporal features represented as time-series and
images. We propose a framework that leverages both time-series and images of
handwriting through a joint classifier, based on a ResNet50 pretrained on
ImageNet-1k. Binary classification experiments demonstrate state-of-the-art
performances on existing time-series and image datasets, with significant
improvement on specific drawing and writing tasks from the NeuroLogical Signals
(NLS) dataset. In particular, the proposed model demonstrates improved
performance on Draw Clock and Spiral tasks. Additionally, cross-dataset and
multi-dataset experiments were consistently able to achieve high F1 scores, up
to 98 for PD detection, highlighting the potential of the proposed model to
generalize over different forms of handwriting signals, and enhance the
detection of motor deficits in ND.

</details>


### [15] [Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs](https://arxiv.org/abs/2509.16476)
*Qinyu Chen,Jiawen Qi*

Main category: cs.CV

TL;DR: GazeVLM是一个无需训练的框架，利用人类眼动注视作为自然监督信号来优化视觉语言模型的推理效率，通过提取注视驱动的感兴趣区域来减少冗余视觉token，同时保持任务相关细节。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型存在视觉token冗余问题，导致推理效率低下，阻碍在边缘设备上的实时应用。现有方法需要架构修改或中间激活访问，增加了计算和内存开销，并且存在提示与感兴趣区域不对齐的问题。

Method: 提出GazeVLM框架，通过提取人类眼动注视驱动的感兴趣区域，可选地结合低分辨率全局视图，模拟中央凹-周边感知机制来减少冗余视觉token。

Result: 在VOILA-COCO基准测试中，GazeVLM将视觉token减少高达93.1%，总token减少59.6%，FLOPs减少50%，同时相对于全分辨率基线保持更好的答案质量。

Conclusion: 将模型计算与人类眼动注视对齐，为消费设备上的高效VLM推理提供了一种简单、即插即用的解决方案。

Abstract: Vision-Language Models (VLMs) deliver impressive performance in understanding
visual content with language instructions. However, redundancy in vision tokens
results in the degenerated inference efficiency of VLMs, which hinders
real-time use on edge consumer devices such as AR/VR devices. Existing
efficiency methods commonly prune visual tokens using learned saliency, sparse
attention schedules, or controller policies, but they often require
architectural modification or access to intermediate activations. These
pipelines add inference-time modules that increase compute and memory and often
lead to an accuracy trade-off. Moreover, they also suffer from misalignment
between the prompts and the region of interest in the images. Without human
guidance, the model may focus on the wrong regions and miss small,
high-frequency details when prompts or scenes change. In this paper, we propose
GazeVLM, a training-free framework that uses the human eye gaze as a natural
supervisory signal to allocate computation where it matters. By extracting
gaze-driven regions of interest (ROIs) and optionally combining them with a
low-resolution global view, GazeVLM mimics fovea-periphery perception to cut
redundant visual tokens while preserving task-relevant details. We evaluate the
visual question answering tasks on Qwen2.5-VL-3B/7B on the VOILA-COCO benchmark
with human gaze. Quality of the answer is assessed by GPT-4o pairwise judging
and a weighted score over coverage, accuracy, details, and fluency. Efficiency
is measured by token counts and FLOPs. GazeVLM reduces visual tokens by up to
93.1%, total tokens by up to 59.6%, and FLOPs by 50%, while keeping better
answer quality relative to full-resolution baselines. Our results show that
aligning model computation with human gaze offers a simple, plug-and-play path
toward efficient VLM inference on consumer devices.

</details>


### [16] [Thermal Imaging-based Real-time Fall Detection using Motion Flow and Attention-enhanced Convolutional Recurrent Architecture](https://arxiv.org/abs/2509.16479)
*Christopher Silver,Thangarajah Akilan*

Main category: cs.CV

TL;DR: 该研究提出了一种基于双向卷积长短期记忆网络（BiConvLSTM）的热成像跌倒检测方法，通过集成多种注意力机制，在TSF数据集上达到99.7%的ROC-AUC性能，为老年人跌倒检测提供了非侵入式、隐私保护的解决方案。


<details>
  <summary>Details</summary>
Motivation: 老年人跌倒是一个重大的公共卫生问题，现有解决方案在可靠性、用户依从性和实用性方面存在挑战。利益相关者更倾向于无需用户交互、非穿戴式、被动式、隐私保护且实时的跌倒检测系统。

Method: 采用双向卷积长短期记忆网络（BiConvLSTM）模型，增强空间、时间、特征、自注意力和通用注意力机制。通过系统实验探索注意力机制、循环模块和运动流的集成，识别出最优架构。

Result: BiConvLSTM在TSF数据集上达到99.7%的ROC-AUC性能，并在新出现的多样化隐私保护基准TF-66上表现出稳健结果。

Conclusion: 研究结果突出了所提出模型的泛化能力和实用性，为热成像跌倒检测设定了新标准，为可部署的高性能解决方案铺平了道路。

Abstract: Falls among seniors are a major public health issue. Existing solutions using
wearable sensors, ambient sensors, and RGB-based vision systems face challenges
in reliability, user compliance, and practicality. Studies indicate that
stakeholders, such as older adults and eldercare facilities, prefer
non-wearable, passive, privacy-preserving, and real-time fall detection systems
that require no user interaction. This study proposes an advanced thermal fall
detection method using a Bidirectional Convolutional Long Short-Term Memory
(BiConvLSTM) model, enhanced with spatial, temporal, feature, self, and general
attention mechanisms. Through systematic experimentation across hundreds of
model variations exploring the integration of attention mechanisms, recurrent
modules, and motion flow, we identified top-performing architectures. Among
them, BiConvLSTM achieved state-of-the-art performance with a ROC-AUC of
$99.7\%$ on the TSF dataset and demonstrated robust results on TF-66, a newly
emerged, diverse, and privacy-preserving benchmark. These results highlight the
generalizability and practicality of the proposed model, setting new standards
for thermal fall detection and paving the way toward deployable,
high-performance solutions.

</details>


### [17] [Octree Latent Diffusion for Semantic 3D Scene Generation and Completion](https://arxiv.org/abs/2509.16483)
*Xujia Zhang,Brendan Crowe,Christoffer Heckman*

Main category: cs.CV

TL;DR: 本文提出了Octree Latent Semantic Diffusion框架，统一处理3D语义场景的补全、扩展和生成任务，支持室内外场景，通过双八叉图潜在表示和两阶段扩散过程实现高效生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法将3D语义场景的补全、扩展和生成问题解耦处理，且通常针对特定领域（如室内/室外）需要不同模型。为了统一这些技术并提供跨域兼容性，需要开发一个通用框架。

Method: 使用双八叉图潜在表示作为高效的分层稀疏占用结构。方法分为两阶段：(i)结构扩散预测二分信号构建粗粒度占用八叉树，(ii)潜在语义扩散生成语义嵌入，通过图VAE解码为体素级语义标签。利用推理时潜在修复/外绘进行场景补全/扩展。

Result: 展示了高质量的结构、连贯的语义和从单次LiDAR扫描的鲁棒补全能力，以及对分布外LiDAR数据的零样本泛化能力。

Conclusion: 在双八叉图潜在空间中进行基于生成的补全是实际且可扩展的替代方案，适用于真实世界机器人感知任务。

Abstract: The completion, extension, and generation of 3D semantic scenes are an
interrelated set of capabilities that are useful for robotic navigation and
exploration. Existing approaches seek to decouple these problems and solve them
oneoff. Additionally, these approaches are often domain-specific, requiring
separate models for different data distributions, e.g. indoor vs. outdoor
scenes. To unify these techniques and provide cross-domain compatibility, we
develop a single framework that can perform scene completion, extension, and
generation in both indoor and outdoor scenes, which we term Octree Latent
Semantic Diffusion. Our approach operates directly on an efficient dual octree
graph latent representation: a hierarchical, sparse, and memory-efficient
occupancy structure. This technique disentangles synthesis into two stages: (i)
structure diffusion, which predicts binary split signals to construct a coarse
occupancy octree, and (ii) latent semantic diffusion, which generates semantic
embeddings decoded by a graph VAE into voxellevel semantic labels. To perform
semantic scene completion or extension, our model leverages inference-time
latent inpainting, or outpainting respectively. These inference-time methods
use partial LiDAR scans or maps to condition generation, without the need for
retraining or finetuning. We demonstrate highquality structure, coherent
semantics, and robust completion from single LiDAR scans, as well as zero-shot
generalization to out-of-distribution LiDAR data. These results indicate that
completion-through-generation in a dual octree graph latent space is a
practical and scalable alternative to regression-based pipelines for real-world
robotic perception tasks.

</details>


### [18] [RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation](https://arxiv.org/abs/2509.16500)
*Tianyi Yan,Wencheng Han,Xia Zhou,Xueyang Zhang,Kun Zhan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出RLGF方法，通过强化学习结合几何反馈来改进视频扩散模型，解决合成数据中的几何失真问题，显著提升自动驾驶感知任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的视频生成模型虽然视觉逼真，但存在细微的几何失真，限制了其在自动驾驶下游感知任务中的实用性。使用合成数据与真实数据在3D目标检测性能上存在显著差距。

Method: 引入强化学习与几何反馈（RLGF），通过专门的潜在空间自动驾驶感知模型提供奖励来优化视频扩散模型。核心组件包括高效的潜在空间窗口优化技术和分层几何奖励系统，用于点-线-平面对齐和场景占用一致性。

Result: 在nuScenes数据集上的DiVE模型应用RLGF后，几何误差显著减少（VP误差降低21%，深度误差降低57%），3D目标检测mAP提升12.7%，缩小了与真实数据性能的差距。

Conclusion: RLGF提供了一种即插即用的解决方案，能够生成几何正确且可靠的自动驾驶开发用合成视频。

Abstract: Synthetic data is crucial for advancing autonomous driving (AD) systems, yet
current state-of-the-art video generation models, despite their visual realism,
suffer from subtle geometric distortions that limit their utility for
downstream perception tasks. We identify and quantify this critical issue,
demonstrating a significant performance gap in 3D object detection when using
synthetic versus real data. To address this, we introduce Reinforcement
Learning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusion
models by incorporating rewards from specialized latent-space AD perception
models. Its core components include an efficient Latent-Space Windowing
Optimization technique for targeted feedback during diffusion, and a
Hierarchical Geometric Reward (HGR) system providing multi-level rewards for
point-line-plane alignment, and scene occupancy coherence. To quantify these
distortions, we propose GeoScores. Applied to models like DiVE on nuScenes,
RLGF substantially reduces geometric errors (e.g., VP error by 21\%, Depth
error by 57\%) and dramatically improves 3D object detection mAP by 12.7\%,
narrowing the gap to real-data performance. RLGF offers a plug-and-play
solution for generating geometrically sound and reliable synthetic videos for
AD development.

</details>


### [19] [CommonForms: A Large, Diverse Dataset for Form Field Detection](https://arxiv.org/abs/2509.16506)
*Joe Barrow*

Main category: cs.CV

TL;DR: 本文介绍了CommonForms数据集和FFDNet模型，用于表单字段检测任务，将问题建模为对象检测问题


<details>
  <summary>Details</summary>
Motivation: 需要解决表单字段自动检测的问题，现有商业解决方案功能有限且缺乏开源数据集和模型

Method: 通过过滤Common Crawl中的PDF文档构建数据集，提出FFDNet-Small和FFDNet-Large两种模型架构

Result: FFDNet模型在测试集上达到很高的平均精度，训练成本低于500美元，优于商业PDF阅读器

Conclusion: 这是首个大规模表单字段检测数据集和开源模型，为相关研究提供了重要资源

Abstract: This paper introduces CommonForms, a web-scale dataset for form field
detection. It casts the problem of form field detection as object detection:
given an image of a page, predict the location and type (Text Input, Choice
Button, Signature) of form fields. The dataset is constructed by filtering
Common Crawl to find PDFs that have fillable elements. Starting with 8 million
documents, the filtering process is used to arrive at a final dataset of
roughly 55k documents that have over 450k pages. Analysis shows that the
dataset contains a diverse mixture of languages and domains; one third of the
pages are non-English, and among the 14 classified domains, no domain makes up
more than 25% of the dataset.
  In addition, this paper presents a family of form field detectors,
FFDNet-Small and FFDNet-Large, which attain a very high average precision on
the CommonForms test set. Each model cost less than $500 to train. Ablation
results show that high-resolution inputs are crucial for high-quality form
field detection, and that the cleaning process improves data efficiency over
using all PDFs that have fillable fields in Common Crawl. A qualitative
analysis shows that they outperform a popular, commercially available PDF
reader that can prepare forms. Unlike the most popular commercially available
solutions, FFDNet can predict checkboxes in addition to text and signature
fields. This is, to our knowledge, the first large scale dataset released for
form field detection, as well as the first open source models. The dataset,
models, and code will be released at https://github.com/jbarrow/commonforms

</details>


### [20] [OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution](https://arxiv.org/abs/2509.16507)
*Hanting Li,Huaao Tang,Jianhong Han,Tianxiong Zhou,Jiulong Cui,Haizhen Xie,Yan Chen,Jie Hu*

Main category: cs.CV

TL;DR: 本文提出OS-DiffVSR，一种单步扩散模型用于真实世界视频超分辨率，在保持高质量的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的VSR方法在视频质量和推理效率之间存在权衡，多步扩散过程导致推理速度慢，难以满足实际应用需求。

Method: 设计了相邻帧对抗训练范式来提升合成视频质量，并采用多帧融合机制来保持帧间时间一致性并减少视频闪烁。

Result: 在多个流行VSR基准测试上的实验表明，OS-DiffVSR甚至能够比需要数十个采样步骤的现有扩散基VSR方法获得更好的质量。

Conclusion: OS-DiffVSR成功解决了扩散基VSR方法中质量与效率的权衡问题，实现了高质量的单步视频超分辨率。

Abstract: Recently, latent diffusion models has demonstrated promising performance in
real-world video super-resolution (VSR) task, which can reconstruct
high-quality videos from distorted low-resolution input through multiple
diffusion steps. Compared to image super-resolution (ISR), VSR methods needs to
process each frame in a video, which poses challenges to its inference
efficiency. However, video quality and inference efficiency have always been a
trade-off for the diffusion-based VSR methods. In this work, we propose
One-Step Diffusion model for real-world Video Super-Resolution, namely
OS-DiffVSR. Specifically, we devise a novel adjacent frame adversarial training
paradigm, which can significantly improve the quality of synthetic videos.
Besides, we devise a multi-frame fusion mechanism to maintain inter-frame
temporal consistency and reduce the flicker in video. Extensive experiments on
several popular VSR benchmarks demonstrate that OS-DiffVSR can even achieve
better quality than existing diffusion-based VSR methods that require dozens of
sampling steps.

</details>


### [21] [SlowFast-SCI: Slow-Fast Deep Unfolding Learning for Spectral Compressive Imaging](https://arxiv.org/abs/2509.16509)
*Haijin Zeng,Xuan Lu,Yurong Zhang,Yongyong Chen,Jingyong Su,Jie Liu*

Main category: cs.CV

TL;DR: SlowFast-SCI是一个双速度框架，将慢速累积学习和快速在线适应相结合，用于光谱压缩成像的深度展开网络，显著减少参数和计算量，并在分布外数据上实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有深度展开方法仅模仿慢速学习过程，依赖大量预训练，缺乏快速适应新光学配置的能力，导致在分布外相机上表现不佳，且计算量大、推理速度慢。

Method: 提出双阶段框架：慢速学习阶段预训练或重用基于先验的主干网络，并通过成像指导蒸馏到紧凑的快速展开模型；快速学习阶段在每个块中嵌入轻量级适应模块，通过双域损失进行自监督测试时训练，无需重新训练主干网络。

Result: 实现了70%以上的参数和FLOPs减少，在分布外数据上PSNR提升高达5.79 dB，保持跨域适应性，适应速度提升4倍。

Conclusion: SlowFast-SCI是首个测试时适应驱动的深度展开框架，将离线鲁棒性与在线校准相结合，为自适应性、可部署成像系统开辟了新途径。

Abstract: Humans learn in two complementary ways: a slow, cumulative process that
builds broad, general knowledge, and a fast, on-the-fly process that captures
specific experiences. Existing deep-unfolding methods for spectral compressive
imaging (SCI) mirror only the slow component-relying on heavy pre-training with
many unfolding stages-yet they lack the rapid adaptation needed to handle new
optical configurations. As a result, they falter on out-of-distribution
cameras, especially in bespoke spectral setups unseen during training. This
depth also incurs heavy computation and slow inference. To bridge this gap, we
introduce SlowFast-SCI, a dual-speed framework seamlessly integrated into any
deep unfolding network beyond SCI systems. During slow learning, we pre-train
or reuse a priors-based backbone and distill it via imaging guidance into a
compact fast-unfolding model. In the fast learning stage, lightweight
adaptation modules are embedded within each block and trained self-supervised
at test time via a dual-domain loss-without retraining the backbone. To the
best of our knowledge, SlowFast-SCI is the first test-time adaptation-driven
deep unfolding framework for efficient, self-adaptive spectral reconstruction.
Its dual-stage design unites offline robustness with on-the-fly per-sample
calibration-yielding over 70% reduction in parameters and FLOPs, up to 5.79 dB
PSNR improvement on out-of-distribution data, preserved cross-domain
adaptability, and a 4x faster adaptation speed. In addition, its modularity
integrates with any deep-unfolding network, paving the way for self-adaptive,
field-deployable imaging and expanded computational imaging modalities. Code
and models are available at https://github.com/XuanLu11/SlowFast-SCI.

</details>


### [22] [Seeing Culture: A Benchmark for Visual Reasoning and Grounding](https://arxiv.org/abs/2509.16517)
*Burak Satar,Zhixin Ma,Patrick A. Irawan,Wilfried A. Mulyawan,Jing Jiang,Ee-Peng Lim,Chong-Wah Ngo*

Main category: cs.CV

TL;DR: 本文提出了Seeing Culture Benchmark (SCB)，这是一个专注于文化推理的多模态基准测试，要求视觉语言模型在文化丰富的图像上进行两阶段推理：多选视觉问答和相关文化文物的分割。


<details>
  <summary>Details</summary>
Motivation: 现有的文化数据集在提供文化推理方面存在不足，且许多文化代表性不足。SCB旨在解决这些问题，特别关注经常被忽视的东南亚文化。

Method: SCB采用两阶段评估方法：第一阶段通过多选视觉问答选择正确的视觉选项；第二阶段对相关文化文物进行分割作为推理证据。数据集包含1,065张图像，涵盖7个东南亚国家的138种文化文物。

Result: 对各种视觉语言模型的评估揭示了跨模态文化推理的复杂性，并突显了在文化细微场景中视觉推理与空间定位之间的差距。

Conclusion: SCB作为一个关键基准，有助于识别当前模型的不足，为文化推理领域的未来发展提供指导。

Abstract: Multimodal vision-language models (VLMs) have made substantial progress in
various tasks that require a combined understanding of visual and textual
content, particularly in cultural understanding tasks, with the emergence of
new cultural datasets. However, these datasets frequently fall short of
providing cultural reasoning while underrepresenting many cultures. In this
paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural
reasoning with a novel approach that requires VLMs to reason on culturally rich
images in two stages: i) selecting the correct visual option with
multiple-choice visual question answering (VQA), and ii) segmenting the
relevant cultural artifact as evidence of reasoning. Visual options in the
first stage are systematically organized into three types: those originating
from the same country, those from different countries, or a mixed group.
Notably, all options are derived from a singular category for each type.
Progression to the second stage occurs only after a correct visual option is
chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural
artifacts across five categories from seven Southeast Asia countries, whose
diverse cultures are often overlooked, accompanied by 3,178 questions, of which
1,093 are unique and meticulously curated by human annotators. Our evaluation
of various VLMs reveals the complexities involved in cross-modal cultural
reasoning and highlights the disparity between visual reasoning and spatial
grounding in culturally nuanced scenarios. The SCB serves as a crucial
benchmark for identifying these shortcomings, thereby guiding future
developments in the field of cultural reasoning.
https://github.com/buraksatar/SeeingCulture

</details>


### [23] [FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers](https://arxiv.org/abs/2509.16518)
*Sankeerth Durvasula,Kavya Sreedhar,Zain Moustafa,Suraj Kothawade,Ashish Gondimalla,Suvinay Subramanian,Narges Shahidi,Nandita Vijaykumar*

Main category: cs.CV

TL;DR: 本文提出FG-Attn，一种用于长上下文扩散变换器的细粒度稀疏注意力机制，通过异步聚集加载操作实现比块稀疏注意力更精细的计算跳过，在视频生成任务上实现1.41-1.65倍加速。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器生成真实视频需要大量计算，注意力层是主要瓶颈。现有块稀疏注意力方法跳过整个MxM块的计算，未能充分利用注意力图中的稀疏性。

Method: 提出FG-Attn稀疏注意力机制，在Mx1切片粒度上跳过计算，开发异步聚集加载操作来高效加载稀疏相关的键值向量。

Result: 在单H100 GPU上，对5秒480p视频实现平均1.55倍（最高1.65倍）加速，对5秒720p视频实现平均1.41倍（最高1.49倍）加速。

Conclusion: 细粒度稀疏注意力比块稀疏注意力更有效地利用注意力稀疏性，显著提升视频扩散模型的推理速度。

Abstract: Generating realistic videos with diffusion transformers demands significant
computation, with attention layers the central bottleneck; even producing a
short clip requires running a transformer over a very long sequence of
embeddings, e.g., more than 30K embeddings for a 5-second video, incurring
significant latency. Prior work aims to mitigate this bottleneck by exploiting
sparsity in the attention layers to reduce computation. However, these works
typically rely on block-sparse attention, which skips score computation only
when all entries in a block of attention scores (corresponding to M queries and
M keys, with M = 64 typically) are zero. This coarse-granular skipping of
attention scores does not fully exploit sparsity in the attention map and
leaves room for improvement. In this work, we propose FG-Attn, a sparse
attention mechanism for long-context diffusion transformers that leverages
sparsity at a fine granularity. Unlike block-sparse attention, which skips
entire MxM blocks, our approach skips computations at the granularity of Mx1
slices of the attention map. Each slice is produced by query-key dot products
between a block of query vectors and a single key. To implement our proposed
sparse attention mechanism, we develop a new efficient bulk-load operation
called asynchronous-gather load. This load operation gathers a sparse set of
relevant key-value vectors from memory and arranges them into packed tiles in
the GPU's shared memory. Only a sparse set of keys relevant to those queries
are loaded into shared memory when computing attention for a block of queries,
in contrast to loading full blocks of key tokens in block-sparse attention. Our
fine-grained sparse attention, applied to video diffusion models, achieves an
average 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average
1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.

</details>


### [24] [PM25Vision: A Large-Scale Benchmark Dataset for Visual Estimation of Air Quality](https://arxiv.org/abs/2509.16519)
*Yang Han*

Main category: cs.CV

TL;DR: PM25Vision (PM25V) 是迄今为止最大最全面的数据集，用于从街景图像估计空气质量（特别是PM2.5浓度），包含11,114张图像与PM2.5读数匹配，空间精度达5公里。


<details>
  <summary>Details</summary>
Motivation: 现有数据集规模有限且空间精度不足（多为城市级别），需要更大规模、更高精度的数据集来支持从街景图像准确估计PM2.5浓度的研究。

Method: 构建了数据收集、同步和清洗流程，收集了11年期间3,261个AQI监测站的11,114张街景图像与时间戳和地理位置匹配的PM2.5读数，并使用CNN和Transformer架构进行基线模型性能评估。

Result: 创建了规模显著超过先前基准的数据集，空间精度达到5公里，远超许多数据集的城市场级精度，并提供了基线模型性能。

Conclusion: PM25V数据集是公开可用的，为从街景图像估计PM2.5浓度提供了大规模、高精度的基准数据集。

Abstract: We introduce PM25Vision (PM25V), the largest and most comprehensive dataset
to date for estimating air quality - specifically PM2.5 concentrations - from
street-level images. The dataset contains over 11,114 images matched with
timestamped and geolocated PM2.5 readings across 3,261 AQI monitoring stations
and 11 years, significantly exceeding the scale of previous benchmarks. The
spatial accuracy of this dataset has reached 5 kilometers, far exceeding the
city-level accuracy of many datasets. We describe the data collection,
synchronization, and cleaning pipelines, and provide baseline model
performances using CNN and transformer architectures. Our dataset is publicly
available.

</details>


### [25] [Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity](https://arxiv.org/abs/2509.16527)
*Guangze Zheng,Shijie Lin,Haobo Zuo,Si Si,Ming-Shan Wang,Changhong Fu,Jia Pan*

Main category: cs.CV

TL;DR: 本文提出了一种基于格子玻尔兹曼模型（LBM）的视觉跟踪方法，通过分解视觉表示为动态像素格子，并利用碰撞-流过程解决像素运动状态，实现实时在线视觉跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有视觉跟踪方法在处理真实世界像素动态性方面存在局限性，需要一种能够在线实时适应复杂视觉跟踪任务的有效方法。

Method: LBM通过多层预测-更新网络获取目标像素的高维分布，预测阶段在目标像素的空间邻域内进行格子碰撞，在时间视觉上下文中进行格子流；更新阶段利用在线视觉表示修正像素分布。

Result: 在TAP-Vid和RoboTap等真实世界点跟踪基准测试中验证了LBM的效率，在TAO、BFT和OVT-B等大规模开放世界物体跟踪基准测试中进一步证明了其实用性。

Conclusion: LBM展示了在在线实时方式下的实际适用性，能够高效适应真实世界视觉跟踪任务，相比现有方法具有显著优势。

Abstract: This work proposes the Lattice Boltzmann Model (LBM) to learn real-world
pixel dynamicity for visual tracking. LBM decomposes visual representations
into dynamic pixel lattices and solves pixel motion states through
collision-streaming processes. Specifically, the high-dimensional distribution
of the target pixels is acquired through a multilayer predict-update network to
estimate the pixel positions and visibility. The predict stage formulates
lattice collisions among the spatial neighborhood of target pixels and develops
lattice streaming within the temporal visual context. The update stage
rectifies the pixel distributions with online visual representations. Compared
with existing methods, LBM demonstrates practical applicability in an online
and real-time manner, which can efficiently adapt to real-world visual tracking
tasks. Comprehensive evaluations of real-world point tracking benchmarks such
as TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of
large-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B
further demonstrates LBM's real-world practicality.

</details>


### [26] [Advancing Reference-free Evaluation of Video Captions with Factual Analysis](https://arxiv.org/abs/2509.16538)
*Shubhashis Roy Dipta,Tz-Ying Wu,Subarna Tripathi*

Main category: cs.CV

TL;DR: 本文提出了一种无需参考基准的无参考视频字幕评估框架VC-Inspector，专注于事实准确性评估，解决了传统依赖人工标注字幕的评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕评估方法依赖人工标注的参考字幕，这在多样化视频领域中成本高昂且不切实际。传统基于参考的评估协议无法有效评估真实世界视频的字幕质量。

Method: 利用大语言模型生成不同质量的伪字幕作为监督数据，训练多模态模型Qwen2.5-VL作为评估器，实现参考无关的事实准确性评估。

Result: 在VATEX-Eval数据集上，VC-Inspector与人类判断的一致性优于现有方法，并在图像字幕数据集Flickr8K-Expert和Flickr8K-CF上表现出良好的泛化能力。

Conclusion: VC-Inspector提供了一个可扩展且可泛化的解决方案，为多样化视频领域中更有效和客观的评估方法铺平了道路。

Abstract: Video captions offer concise snapshots of actors, objects, and actions within
a video, serving as valuable assets for applications such as question answering
and event localization. However, acquiring human annotations for video captions
is costly or even impractical, especially when dealing with diverse video
domains. Existing models trained on supervised datasets face challenges in
evaluating performance across different domains due to the reliance on
reference-based evaluation protocols, which necessitate ground truth captions.
This assumption is unrealistic for evaluating videos in the wild. To address
these limitations, we propose a reference-free evaluation framework that does
not require ground truth captions, focusing on factual grounding to ensure
accurate assessment of caption quality. We introduce VC-Inspector, a novel
caption quality evaluator that is both reference-free and factually grounded.
Utilizing large language models, we generate pseudo captions of varying quality
based on supervised data, which are subsequently used to train a multimodal
model (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior
alignment with human judgments on the VATEX-Eval dataset, outperforming
existing methods. The performance also generalizes to image caption datasets,
Flickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos.
Overall, VC-Inspector offers a scalable and generalizable solution for
evaluating the factual accuracy of video captions, paving the way for more
effective and objective assessment methodologies in diverse video domains.

</details>


### [27] [Efficient Rectified Flow for Image Fusion](https://arxiv.org/abs/2509.16549)
*Zirui Wang,Jiayi Zhang,Tianwei Guan,Yuhan Zhou,Xingyuan Li,Minjing Dong,Jinyuan Liu*

Main category: cs.CV

TL;DR: RFfusion是一种基于Rectified Flow的高效一步扩散模型，用于图像融合任务。该方法通过直线化采样路径实现一步采样，无需额外训练，同时提出针对图像融合的变分自编码器架构和两阶段训练策略，显著提升推理效率并保持高质量融合效果。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在图像融合领域取得显著进展，但存在计算复杂、推理时间冗余的问题，限制了实际应用。需要开发更高效的扩散模型方法来解决这些局限性。

Method: 1. 将Rectified Flow引入图像融合任务，直线化扩散模型采样路径实现一步采样；2. 设计针对图像融合的变分自编码器架构，在潜在空间嵌入融合操作降低计算复杂度；3. 提出两阶段训练策略，解决传统VAE重建目标与图像融合需求之间的差异。

Result: 大量实验表明，该方法在推理速度和融合质量方面均优于其他最先进方法，实现了高效高质量的图像融合。

Conclusion: RFfusion通过Rectified Flow技术和专门设计的VAE架构，成功解决了扩散模型在图像融合中的效率问题，为高效图像融合提供了有效解决方案。

Abstract: Image fusion is a fundamental and important task in computer vision, aiming
to combine complementary information from different modalities to fuse images.
In recent years, diffusion models have made significant developments in the
field of image fusion. However, diffusion models often require complex
computations and redundant inference time, which reduces the applicability of
these methods. To address this issue, we propose RFfusion, an efficient
one-step diffusion model for image fusion based on Rectified Flow. We
incorporate Rectified Flow into the image fusion task to straighten the
sampling path in the diffusion model, achieving one-step sampling without the
need for additional training, while still maintaining high-quality fusion
results. Furthermore, we propose a task-specific variational autoencoder (VAE)
architecture tailored for image fusion, where the fusion operation is embedded
within the latent space to further reduce computational complexity. To address
the inherent discrepancy between conventional reconstruction-oriented VAE
objectives and the requirements of image fusion, we introduce a two-stage
training strategy. This approach facilitates the effective learning and
integration of complementary information from multi-modal source images,
thereby enabling the model to retain fine-grained structural details while
significantly enhancing inference efficiency. Extensive experiments demonstrate
that our method outperforms other state-of-the-art methods in terms of both
inference speed and fusion quality. Code is available at
https://github.com/zirui0625/RFfusion.

</details>


### [28] [ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting](https://arxiv.org/abs/2509.16552)
*Xiaoyang Yan,Muleilan Pei,Shaojie Shen*

Main category: cs.CV

TL;DR: 提出了一种新颖的时空高斯泼溅（ST-GS）框架，通过双模式注意力机制和几何感知时序融合方案，增强基于高斯方法的3D占据预测中的空间交互和时序一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D语义高斯方法在视觉中心自动驾驶中建模占据时，存在多视图空间交互不足和多帧时序一致性有限的问题。

Method: 开发了指导信息空间聚合策略（双模式注意力机制）来增强高斯表示的空间交互，并引入了几何感知时序融合方案来利用历史上下文改善场景完成的时序连续性。

Result: 在大规模nuScenes占据预测基准上的广泛实验表明，该方法不仅达到了最先进的性能，而且相比现有基于高斯的方法提供了显著更好的时序一致性。

Conclusion: ST-GS框架有效解决了高斯方法中的空间和时序建模限制，为视觉中心自动驾驶提供了更全面的场景理解能力。

Abstract: 3D occupancy prediction is critical for comprehensive scene understanding in
vision-centric autonomous driving. Recent advances have explored utilizing 3D
semantic Gaussians to model occupancy while reducing computational overhead,
but they remain constrained by insufficient multi-view spatial interaction and
limited multi-frame temporal consistency. To overcome these issues, in this
paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework
to enhance both spatial and temporal modeling in existing Gaussian-based
pipelines. Specifically, we develop a guidance-informed spatial aggregation
strategy within a dual-mode attention mechanism to strengthen spatial
interaction in Gaussian representations. Furthermore, we introduce a
geometry-aware temporal fusion scheme that effectively leverages historical
context to improve temporal continuity in scene completion. Extensive
experiments on the large-scale nuScenes occupancy prediction benchmark showcase
that our proposed approach not only achieves state-of-the-art performance but
also delivers markedly better temporal consistency compared to existing
Gaussian-based methods.

</details>


### [29] [Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose](https://arxiv.org/abs/2509.16557)
*Muhammad Hamza,Danish Hamid,Muhammad Tahir Akram*

Main category: cs.CV

TL;DR: I2S是一个多阶段框架，通过3D手部姿态分析实现无干扰的用户身份识别，在AR环境中达到97.52%的F1分数，模型轻量且推理快速。


<details>
  <summary>Details</summary>
Motivation: 在飞机驾驶舱、航空航天维护和手术等高风险环境中，需要可靠的人机交互识别和用户身份验证技术来支持AR个性化辅助系统。

Method: I2S采用多阶段框架：首先提取3D手部姿态的手工特征（空间、频率、运动学、方向特征和新型IHSE描述符），然后依次进行物体类别识别、人机交互识别，最终实现用户身份识别。

Result: 在ARCTIC和H2O数据集上的评估显示，最优配置达到97.52%的平均F1分数，模型大小小于4MB，推理时间仅0.1秒。

Conclusion: I2S在保持轻量级模型和快速推理的同时，实现了最先进的性能，非常适合安全关键的AR系统实时设备认证。

Abstract: Human-Object Interaction Recognition (HOIR) and user identification play a
crucial role in advancing augmented reality (AR)-based personalized assistive
technologies. These systems are increasingly being deployed in high-stakes,
human-centric environments such as aircraft cockpits, aerospace maintenance,
and surgical procedures. This research introduces I2S (Interact2Sign), a multi
stage framework designed for unobtrusive user identification through human
object interaction recognition, leveraging 3D hand pose analysis in egocentric
videos. I2S utilizes handcrafted features extracted from 3D hand poses and per
forms sequential feature augmentation: first identifying the object class,
followed by HOI recognition, and ultimately, user identification. A
comprehensive feature extraction and description process was carried out for 3D
hand poses, organizing the extracted features into semantically meaningful
categories: Spatial, Frequency, Kinematic, Orientation, and a novel descriptor
introduced in this work, the Inter-Hand Spatial Envelope (IHSE). Extensive
ablation studies were conducted to determine the most effective combination of
features. The optimal configuration achieved an impressive average F1-score of
97.52% for user identification, evaluated on a bimanual object manipulation
dataset derived from the ARCTIC and H2O datasets. I2S demonstrates
state-of-the-art performance while maintaining a lightweight model size of
under 4 MB and a fast inference time of 0.1 seconds. These characteristics make
the proposed framework highly suitable for real-time, on-device authentication
in security-critical, AR-based systems.

</details>


### [30] [Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization](https://arxiv.org/abs/2509.16560)
*Ji Soo Lee,Byungoh Ko,Jaewon Cho,Howoong Lee,Jaewoon Byun,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: CaRe-DPO是一个检索框架，通过检索相关性分数直接优化字幕生成，解决了多模态大语言模型生成的字幕在细粒度检索中区分度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型生成的字幕在视觉相似视频之间缺乏区分度，且传统字幕评估指标（如BLEU）不适用于需要区分候选者的检索任务。

Method: 提出双组直接偏好优化（DG-DPO）学习策略，通过建模不同视频和字幕对组之间的偏好来监督字幕生成；同时开发了基于角色嵌入的MLLM检索模型，更好地区分不同功能角色的文本输入。

Result: 实验表明CaRe-DPO通过有效利用辅助知识生成细粒度字幕，显著提升了检索性能。

Conclusion: CaRe-DPO框架成功解决了细粒度文本-视频检索中字幕生成的区分度问题，为检索任务提供了更有效的字幕生成方法。

Abstract: In text-video retrieval, auxiliary captions are often used to enhance video
understanding, bridging the gap between the modalities. While recent advances
in multi-modal large language models (MLLMs) have enabled strong zero-shot
caption generation, we observe that such captions tend to be generic and
indistinguishable across visually similar videos, limiting their utility for
fine-grained retrieval. Moreover, conventional captioning approaches are
typically evaluated using language generation metrics, such as BLEU, which are
not typically tailored for retrieval tasks that require making discriminative
distinctions between candidates. To address this, we propose
$\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes caption
generation using retrieval relevance scores. At its core is Dual-Group Direct
Preference Optimization (DG-DPO), a novel learning strategy that supervises
captioning by modeling preferences across groups of distinct video and caption
pairs. In addition, we present an MLLM-based retrieval model that incorporates
role-embeddings to better distinguish between textual inputs with different
functional roles, such as an auxiliary caption and a text query. Through
extensive experiments, we demonstrate that CaRe-DPO significantly enhances
retrieval performance by effectively leveraging auxiliary knowledge to generate
fine-grained captions for retrieval. Code is available at
https://github.com/mlvlab/CaReDPO.

</details>


### [31] [V-CECE: Visual Counterfactual Explanations via Conceptual Edits](https://arxiv.org/abs/2509.16567)
*Nikolaos Spanos,Maria Lymperaiou,Giorgos Filandrianos,Konstantinos Thomas,Athanasios Voulodimos,Giorgos Stamou*

Main category: cs.CV

TL;DR: 提出了一种无需训练的即插即用黑盒反事实生成框架，利用预训练图像编辑扩散模型生成人类级别的反事实解释，揭示了人类推理与神经网络行为之间的解释差距。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒反事实生成框架忽视语义内容且严重依赖训练，需要一种能够产生人类可理解解释的零训练方法。

Method: 使用预训练图像编辑扩散模型，基于理论保证的最优编辑建议逐步编辑，无需访问分类器内部结构。

Result: 通过CNN、ViT和LVLM分类器的实验验证，展示了人类推理与模型行为之间的解释差距，并通过人类评估证实。

Conclusion: 该框架提供可解释的反事实生成过程，能够产生人类级别的解释，有助于理解神经网络决策过程。

Abstract: Recent black-box counterfactual generation frameworks fail to take into
account the semantic content of the proposed edits, while relying heavily on
training to guide the generation process. We propose a novel, plug-and-play
black-box counterfactual generation framework, which suggests step-by-step
edits based on theoretical guarantees of optimal edits to produce human-level
counterfactual explanations with zero training. Our framework utilizes a
pre-trained image editing diffusion model, and operates without access to the
internals of the classifier, leading to an explainable counterfactual
generation process. Throughout our experimentation, we showcase the explanatory
gap between human reasoning and neural model behavior by utilizing both
Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision
Language Model (LVLM) classifiers, substantiated through a comprehensive human
evaluation.

</details>


### [32] [A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis](https://arxiv.org/abs/2509.16582)
*Antonio Scardace,Lemuel Puglisi,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: DeepSSIM是一种新颖的自监督度量方法，用于量化生成模型中的记忆化现象，在医学影像合成数据生成中检测训练数据泄露，相比现有方法F1分数平均提升52.03%。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型在医学影像中具有重要应用价值，但存在记忆敏感训练数据的风险，可能导致患者信息泄露。目前缺乏可扩展的方法来检测生成模型中的记忆化问题。

Method: 提出DeepSSIM自监督度量方法，通过将图像投影到学习嵌入空间，使嵌入间的余弦相似度与图像空间的真实SSIM分数匹配。训练时使用结构保持增强技术来捕获领域特定的解剖特征。

Result: 在脑部MRI合成数据的案例研究中，DeepSSIM相比最先进的记忆化度量方法，F1分数平均提升52.03%，表现出优越性能。

Conclusion: DeepSSIM为检测生成模型中的训练数据泄露提供了一种有效的解决方案，在医学影像领域具有重要应用价值。

Abstract: Deep generative models have emerged as a transformative tool in medical
imaging, offering substantial potential for synthetic data generation. However,
recent empirical studies highlight a critical vulnerability: these models can
memorize sensitive training data, posing significant risks of unauthorized
patient information disclosure. Detecting memorization in generative models
remains particularly challenging, necessitating scalable methods capable of
identifying training data leakage across large sets of generated samples. In
this work, we propose DeepSSIM, a novel self-supervised metric for quantifying
memorization in generative models. DeepSSIM is trained to: i) project images
into a learned embedding space and ii) force the cosine similarity between
embeddings to match the ground-truth SSIM (Structural Similarity Index) scores
computed in the image space. To capture domain-specific anatomical features,
training incorporates structure-preserving augmentations, allowing DeepSSIM to
estimate similarity reliably without requiring precise spatial alignment. We
evaluate DeepSSIM in a case study involving synthetic brain MRI data generated
by a Latent Diffusion Model (LDM) trained under memorization-prone conditions,
using 2,195 MRI scans from two publicly available datasets (IXI and CoRR).
Compared to state-of-the-art memorization metrics, DeepSSIM achieves superior
performance, improving F1 scores by an average of +52.03% over the best
existing method. Code and data of our approach are publicly available at the
following link: https://github.com/brAIn-science/DeepSSIM.

</details>


### [33] [SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving](https://arxiv.org/abs/2509.16588)
*Haiming Zhang,Yiyao Zhu,Wending Zhou,Xu Yan,Yingjie Cai,Bingbing Liu,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: SQS是一种专为稀疏感知模型设计的查询驱动预训练方法，通过3D高斯表示和自监督溅射学习细粒度上下文特征，在自动驾驶的占用预测和3D目标检测任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏感知模型采用查询驱动范式，避免了显式的密集BEV或体素构建，实现了高效计算和快速推理。但现有方法在预训练方面仍有改进空间，需要专门针对SPMs设计更有效的预训练策略。

Method: SQS引入了一个插件模块，在预训练期间从稀疏查询预测3D高斯表示，利用自监督溅射通过重建多视角图像和深度图来学习细粒度上下文特征。在微调阶段，预训练的高斯查询通过查询交互机制与任务特定查询显式连接。

Result: 在自动驾驶基准测试上的广泛实验表明，SQS在多个基于查询的3D感知任务中带来了显著的性能提升，特别是在占用预测（+1.3 mIoU）和3D目标检测（+1.0 NDS）方面，显著优于现有最先进的预训练方法。

Conclusion: SQS为稀疏感知模型提供了一种有效的预训练框架，通过查询驱动的溅射预训练策略，成功提升了3D感知任务的性能，证明了该方法在自动驾驶场景中的实用价值。

Abstract: Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes
explicit dense BEV or volumetric construction, enabling highly efficient
computation and accelerated inference. In this paper, we introduce SQS, a novel
query-based splatting pre-training specifically designed to advance SPMs in
autonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian
representations from sparse queries during pre-training, leveraging
self-supervised splatting to learn fine-grained contextual features through the
reconstruction of multi-view images and depth maps. During fine-tuning, the
pre-trained Gaussian queries are seamlessly integrated into downstream networks
via query interaction mechanisms that explicitly connect pre-trained queries
with task-specific queries, effectively accommodating the diverse requirements
of occupancy prediction and 3D object detection. Extensive experiments on
autonomous driving benchmarks demonstrate that SQS delivers considerable
performance gains across multiple query-based 3D perception tasks, notably in
occupancy prediction and 3D object detection, outperforming prior
state-of-the-art pre-training approaches by a significant margin (i.e., +1.3
mIoU on occupancy prediction and +1.0 NDS on 3D detection).

</details>


### [34] [FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection](https://arxiv.org/abs/2509.16602)
*Minji Heo,Simon S. Woo*

Main category: cs.CV

TL;DR: 本文提出了FakeChain基准测试，用于评估深度伪造检测模型在混合多步伪造场景下的性能，发现检测器主要依赖最后一步的伪造痕迹而非累积痕迹，导致泛化能力受限。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测研究主要关注单步伪造，而现实中存在通过多种方法（如Face-Swapping、GAN、Diffusion）顺序组合创建的混合多步伪造，这对检测模型构成了新的技术挑战。

Method: 构建了包含1步、2步和3步伪造的大规模基准测试FakeChain，使用五种最先进的生成器合成伪造数据，分析检测性能和频谱特性在不同步骤、生成器组合和质量设置下的变化。

Result: 检测性能高度依赖于最终伪造类型，当与训练分布不同时，F1分数最多下降58.83%，表明检测器主要依赖最后阶段的伪造痕迹而非累积痕迹。

Conclusion: 检测模型需要明确考虑伪造历史和序列，FakeChain等基准测试对于反映现实世界中日益复杂的合成场景具有重要意义。

Abstract: Multi-step or hybrid deepfakes, created by sequentially applying different
deepfake creation methods such as Face-Swapping, GAN-based generation, and
Diffusion methods, can pose an emerging and unforseen technical challenge for
detection models trained on single-step forgeries. While prior studies have
mainly focused on detecting isolated single manipulation, little is known about
the detection model behavior under such compositional, hybrid, and complex
manipulation pipelines. In this work, we introduce \textbf{FakeChain}, a
large-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using
five state-of-the-art representative generators. Using this approach, we
analyze detection performance and spectral properties across hybrid
manipulation at different step, along with varying generator combinations and
quality settings. Surprisingly, our findings reveal that detection performance
highly depends on the final manipulation type, with F1-score dropping by up to
\textbf{58.83\%} when it differs from training distribution. This clearly
demonstrates that detectors rely on last-stage artifacts rather than cumulative
manipulation traces, limiting generalization. Such findings highlight the need
for detection models to explicitly consider manipulation history and sequences.
Our results highlight the importance of benchmarks such as FakeChain,
reflecting growing synthesis complexity and diversity in real-world scenarios.
Our sample code is available
here\footnote{https://github.com/minjihh/FakeChain}.

</details>


### [35] [Describe-to-Score: Text-Guided Efficient Image Complexity Assessment](https://arxiv.org/abs/2509.16609)
*Shipeng Liu,Zhonglin Zhang,Dengfeng Chen,Liang Zhao*

Main category: cs.CV

TL;DR: 提出D2S框架，通过视觉-文本融合方法评估图像复杂度，结合视觉和语义特征提高准确性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有图像复杂度评估方法主要依赖视觉特征，忽略了高层次语义信息，限制了准确性和泛化能力

Method: D2S框架使用预训练视觉语言模型生成图像描述，提出特征对齐和熵分布对齐机制，融合多模态信息进行复杂度评估

Result: 在IC9600数据集上优于现有方法，在无参考图像质量评估基准上保持竞争力

Conclusion: 多模态融合在复杂度相关任务中具有有效性和高效性，D2S在训练时使用多模态信息但推理时仅需视觉分支，避免了多模态计算开销

Abstract: Accurately assessing image complexity (IC) is critical for computer vision,
yet most existing methods rely solely on visual features and often neglect
high-level semantic information, limiting their accuracy and generalization. We
introduce vision-text fusion for IC modeling. This approach integrates visual
and textual semantic features, increasing representational diversity. It also
reduces the complexity of the hypothesis space, which enhances both accuracy
and generalization in complexity assessment. We propose the D2S
(Describe-to-Score) framework, which generates image captions with a
pre-trained vision-language model. We propose the feature alignment and entropy
distribution alignment mechanisms, D2S guides semantic information to inform
complexity assessment while bridging the gap between vision and text
modalities. D2S utilizes multi-modal information during training but requires
only the vision branch during inference, thereby avoiding multi-modal
computational overhead and enabling efficient assessment. Experimental results
demonstrate that D2S outperforms existing methods on the IC9600 dataset and
maintains competitiveness on no-reference image quality assessment (NR-IQA)
benchmark, validating the effectiveness and efficiency of multi-modal fusion in
complexity-related tasks. Code is available at:
https://github.com/xauat-liushipeng/D2S

</details>


### [36] [Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model](https://arxiv.org/abs/2509.16617)
*David Kreismann*

Main category: cs.CV

TL;DR: 该研究微调地理空间基础模型，用于预测未来气候情景下的城市地表温度，并探索其对土地覆盖变化的响应。模型在像素级降尺度误差低于1.74°C，外推能力达3.62°C。


<details>
  <summary>Details</summary>
Motivation: 随着城市化和气候变化加剧，城市热岛效应日益严重。传统机器学习模型和有限数据基础设施往往预测不准确，特别是在服务不足地区。地理空间基础模型具有强泛化能力和最小微调需求，为传统方法受限的情况提供了替代方案。

Method: 微调地理空间基础模型来预测未来气候情景下的城市地表温度，并使用模拟植被策略探索模型对土地覆盖变化的响应。

Result: 微调后的模型实现了像素级降尺度误差低于1.74°C，并与地面真实模式保持一致，展示了高达3.62°C的外推能力。

Conclusion: 地理空间基础模型在城市温度预测方面表现出色，特别是在传统方法受限的区域，为城市热岛效应缓解规划提供了可靠工具。

Abstract: As urbanization and climate change progress, urban heat island effects are
becoming more frequent and severe. To formulate effective mitigation plans,
cities require detailed air temperature data. However, predictive analytics
methods based on conventional machine learning models and limited data
infrastructure often provide inaccurate predictions, especially in underserved
areas. In this context, geospatial foundation models trained on unstructured
global data demonstrate strong generalization and require minimal fine-tuning,
offering an alternative for predictions where traditional approaches are
limited. This study fine-tunes a geospatial foundation model to predict urban
land surface temperatures under future climate scenarios and explores its
response to land cover changes using simulated vegetation strategies. The
fine-tuned model achieved pixel-wise downscaling errors below 1.74 {\deg}C and
aligned with ground truth patterns, demonstrating an extrapolation capacity up
to 3.62 {\deg}C.

</details>


### [37] [Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery](https://arxiv.org/abs/2509.16618)
*Pengfei Hao,Hongqiu Wang,Shuaibo Li,Zhaohu Xing,Guang Yang,Kaishun Wu,Lei Zhu*

Main category: cs.CV

TL;DR: 本文提出Surgical-MambaLLM方法，首次将Mamba2与LLM结合用于外科手术视觉问答定位任务，通过CBMI模块实现多模态融合，并设计SIP扫描模式增强空间感知能力，在EndoVis数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以建立文本与视觉细节之间的复杂依赖关系，且对外科手术场景的空间信息感知能力不足。LLMs的发展为解决外科手术视觉问答定位任务提供了新机遇。

Method: 提出Surgical-MambaLLM方法：1）CBMI模块利用Mamba2进行跨模态融合；2）SIP扫描模式针对手术场景几何特性设计，增强空间理解能力。

Result: 在EndoVis17-VQLA和EndoVis18-VQLA数据集上的实验表明，该方法显著优于现有最优方法，提升了Surgical-VQLA任务的性能。

Conclusion: Surgical-MambaLLM通过结合Mamba2和LLM，有效解决了外科手术视觉问答定位中的跨模态依赖和空间感知问题，为手术场景理解提供了新思路。

Abstract: In recent years, Visual Question Localized-Answering in robotic surgery
(Surgical-VQLA) has gained significant attention for its potential to assist
medical students and junior doctors in understanding surgical scenes. Recently,
the rapid development of Large Language Models (LLMs) has provided more
promising solutions for this task. However, current methods struggle to
establish complex dependencies between text and visual details, and have
difficulty perceiving the spatial information of surgical scenes. To address
these challenges, we propose a novel method, Surgical-MambaLLM, which is the
first to combine Mamba2 with LLM in the surgical domain, that leverages
Mamba2's ability to effectively capture cross-modal dependencies and perceive
spatial information in surgical scenes, thereby enhancing the LLMs'
understanding of surgical images. Specifically, we propose the Cross-modal
Bidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effective
multimodal fusion, with its cross-modal integration capabilities. Additionally,
tailored to the geometric characteristics of surgical scenes, we design the
Surgical Instrument Perception (SIP) scanning mode for Mamba2 to scan the
surgical images, enhancing the model's spatial understanding of the surgical
scene. Extensive experiments demonstrate that our Surgical-MambaLLM model
outperforms the state-of-the-art methods on the EndoVis17-VQLA and
EndoVis18-VQLA datasets, significantly improving the performance of the
Surgical-VQLA task.

</details>


### [38] [CGTGait: Collaborative Graph and Transformer for Gait Emotion Recognition](https://arxiv.org/abs/2509.16623)
*Junjie Zhou,Haijun Xiong,Junhao Lu,Ziyu Lin,Bin Feng*

Main category: cs.CV

TL;DR: 提出CGTGait框架，结合图卷积和Transformer进行步态情感识别，通过双向跨流融合模块整合姿态和运动特征，在降低82.2%计算复杂度的同时达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有步态情感识别方法主要关注空间和局部时间信息，无法捕捉长距离时间依赖关系

Method: CGTGait框架包含多个CGT块，每个块使用图卷积捕获帧级空间拓扑，Transformer建模全局时间依赖，并引入双向跨流融合模块整合姿态和运动特征

Result: 在Emotion-Gait和ELMD数据集上达到SOTA或竞争性性能，测试时计算复杂度降低82.2%（仅需0.34G FLOPs）

Conclusion: CGTGait能有效提取判别性时空特征，在保持高性能的同时显著降低计算成本

Abstract: Skeleton-based gait emotion recognition has received significant attention
due to its wide-ranging applications. However, existing methods primarily focus
on extracting spatial and local temporal motion information, failing to capture
long-range temporal representations. In this paper, we propose
\textbf{CGTGait}, a novel framework that collaboratively integrates graph
convolution and transformers to extract discriminative spatiotemporal features
for gait emotion recognition. Specifically, CGTGait consists of multiple CGT
blocks, where each block employs graph convolution to capture frame-level
spatial topology and the transformer to model global temporal dependencies.
Additionally, we introduce a Bidirectional Cross-Stream Fusion (BCSF) module to
effectively aggregate posture and motion spatiotemporal features, facilitating
the exchange of complementary information between the two streams. We evaluate
our method on two widely used datasets, Emotion-Gait and ELMD, demonstrating
that our CGTGait achieves state-of-the-art or at least competitive performance
while reducing computational complexity by approximately \textbf{82.2\%} (only
requiring 0.34G FLOPs) during testing. Code is available at
\small{https://github.com/githubzjj1/CGTGait.}

</details>


### [39] [Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning](https://arxiv.org/abs/2509.16628)
*Janak Kapuriya,Anwar Shaikh,Arnav Goel,Medha Hira,Apoorv Singh,Jay Saraf,Sanjana,Vaibhav Nauriyal,Avinash Anand,Zhengkui Wang,Rajiv Ratn Shah*

Main category: cs.CV

TL;DR: VCASFT是一种新的学习范式，通过利用图像描述作为零样本提示来增强小型视觉语言模型在科学视觉问答任务上的性能。该方法在ScienceQA基准测试中表现出色，并针对低资源语言开发了HiSciVQA数据集和基于LLM的新评估方案。


<details>
  <summary>Details</summary>
Motivation: 提升小型视觉语言模型在科学视觉问答任务上的性能，特别是针对低资源语言的需求，解决现有数据集和评估方法的局限性。

Method: VCASFT学习范式，使用图像描述作为零样本提示，结合问答对进行指令调优。开发了HiSciVQA数据集（2,245个高质量印地语多模态问答对）和基于LLM的新评估方案。

Result: 在ScienceQA基准测试中表现出显著的性能提升，证明了该方法在各种教育场景中的适应性和有效性。新评估方案提供了比传统n-gram匹配更深入的模型效果洞察。

Conclusion: VCASFT是一种有效的学习范式，能够显著提升小型VLMs在科学VQA任务上的性能，特别是对低资源语言有重要价值。所有代码和数据集将开源供研究社区使用。

Abstract: In this study, we introduce Vision-Caption aware Supervised FineTuning
(VCASFT), a novel learning paradigm designed to enhance the performance of
smaller Vision Language Models(VLMs) on scientific visual question
answering(VQA) tasks. VCASFT leverages image captions as zero-shot prompts
alongside question-answer pairs and instruction-tunes models to yield
significant performance improvements. To comprehensively evaluate VCASFT, we
benchmark it on ScienceQA, which consists of questions across diverse
languages, subjects, and fields, demonstrating its adaptability and
effectiveness in a variety of educational contexts. Additionally, to further
demonstrate the effectiveness of this technique on lowresource languages, we
developed HiSciVQA, a dataset comprising 2,245 high-quality, hand-annotated
Hindi multimodal Q&A pairs. This dataset addresses the critical need for
low-resource language Q&A datasets and serves as a foundation for testing
VCASFT. Additionally, we introduce a novel LLM-based evaluation scheme to
evaluate VLMs on HiSciVQA which offers deeper insights into model effectiveness
surpassing traditional n-gram matching accuracy metrics. We are committed to
advancing the field by open-sourcing all code files and the HiSciVQA dataset
for the research community.

</details>


### [40] [Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation](https://arxiv.org/abs/2509.16630)
*Yue Ma,Zexuan Yan,Hongyu Liu,Hongfa Wang,Heng Pan,Yingqing He,Junkun Yuan,Ailing Zeng,Chengfei Cai,Heung-Yeung Shum,Zhifeng Li,Wei Liu,Linfeng Zhang,Qifeng Chen*

Main category: cs.CV

TL;DR: Follow-Your-Emoji-Faster是一个基于扩散模型的高效肖像动画框架，通过面部关键点驱动，解决了身份保持、表情准确迁移和长期时间一致性的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决肖像动画中身份保持、表情准确迁移和长期时间一致性的核心挑战，同时确保生成效率。

Method: 增强Stable Diffusion，引入表情感知关键点作为显式运动信号和细粒度面部损失；采用渐进生成策略和泰勒插值缓存实现2.6倍无损加速。

Result: 在EmojiBench++基准测试中表现出优越的动画质量和可控性，支持真实人脸、卡通、雕塑和动物等多种肖像类型。

Conclusion: 该方法高效生成高质量动画结果，用户友好且易于访问，代码和数据集将公开。

Abstract: We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework
for freestyle portrait animation driven by facial landmarks. The main
challenges in this task are preserving the identity of the reference portrait,
accurately transferring target expressions, and maintaining long-term temporal
consistency while ensuring generation efficiency. To address identity
preservation and accurate expression retargeting, we enhance Stable Diffusion
with two key components: a expression-aware landmarks as explicit motion
signals, which improve motion alignment, support exaggerated expressions, and
reduce identity leakage; and a fine-grained facial loss that leverages both
expression and facial masks to better capture subtle expressions and faithfully
preserve the reference appearance. With these components, our model supports
controllable and expressive animation across diverse portrait types, including
real faces, cartoons, sculptures, and animals. However, diffusion-based
frameworks typically struggle to efficiently generate long-term stable
animation results, which remains a core challenge in this task. To address
this, we propose a progressive generation strategy for stable long-term
animation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless
acceleration. These two strategies ensure that our method produces high-quality
results efficiently, making it user-friendly and accessible. Finally, we
introduce EmojiBench++, a more comprehensive benchmark comprising diverse
portraits, driving videos, and landmark sequences. Extensive evaluations on
EmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior
performance in both animation quality and controllability. The code, training
dataset and benchmark will be found in https://follow-your-emoji.github.io/.

</details>


### [41] [DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration](https://arxiv.org/abs/2509.16632)
*Weiran Chen,Guiqian Zhu,Ying Li,Yi Ji,Chunping Liu*

Main category: cs.CV

TL;DR: DA-Font是一个少样本字体生成框架，通过双注意力混合模块解决现有方法存在的笔画错误、伪影和模糊等问题，在保持字符形状准确性和风格纹理方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 少样本字体生成可以显著降低人工字体设计的成本，但现有方法由于字体风格的多样性和复杂性，生成结果常常存在可见缺陷，如笔画错误、伪影和模糊。

Method: 提出DA-Font框架，集成双注意力混合模块（DAHM），包含组件注意力块和关系注意力块，分别利用内容图像的组件信息指导风格迁移过程，并通过与原始和风格化组件表示交互来细化空间关系。还设计了角点一致性损失和弹性网格特征损失来改善几何对齐。

Result: 大量实验表明，DA-Font在多种字体风格和字符上优于现有最先进方法，在增强结构完整性和局部保真度方面表现出有效性。

Conclusion: DA-Font通过双注意力机制和新的损失函数设计，有效解决了少样本字体生成中的质量问题，为字体设计提供了高效的自动化解决方案。

Abstract: Few-shot font generation aims to create new fonts with a limited number of
glyph references. It can be used to significantly reduce the labor cost of
manual font design. However, due to the variety and complexity of font styles,
the results generated by existing methods often suffer from visible defects,
such as stroke errors, artifacts and blurriness. To address these issues, we
propose DA-Font, a novel framework which integrates a Dual-Attention Hybrid
Module (DAHM). Specifically, we introduce two synergistic attention blocks: the
component attention block that leverages component information from content
images to guide the style transfer process, and the relation attention block
that further refines spatial relationships through interacting the content
feature with both original and stylized component-wise representations. These
two blocks collaborate to preserve accurate character shapes and stylistic
textures. Moreover, we also design a corner consistency loss and an elastic
mesh feature loss to better improve geometric alignment. Extensive experiments
show that our DA-Font outperforms the state-of-the-art methods across diverse
font styles and characters, demonstrating its effectiveness in enhancing
structural integrity and local fidelity. The source code can be found at
\href{https://github.com/wrchen2001/DA-Font}{\textit{https://github.com/wrchen2001/DA-Font}}.

</details>


### [42] [When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs](https://arxiv.org/abs/2509.16633)
*Abhirama Subramanyam Penamakuri,Navlika Singh,Piyush Arora,Anand Mishra*

Main category: cs.CV

TL;DR: 提出了MPA框架，通过无标签图像和大型视觉语言模型的知识转移来提升小型视觉语言模型的性能，减少性能差距同时保持计算效率


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型性能优秀但计算成本高，小型模型效率高但性能差距大，需要一种方法在保持效率的同时提升小模型性能

Method: MPA框架采用基于对等性的策略，精确识别大小模型之间的知识差异，仅针对这些差异进行优化训练，不依赖标注数据

Result: 在TextVQA、ST-VQ、ChartQA和OKVQA四个VQA基准测试中，MPA一致提升了小型模型的性能，缩小了性能差距

Conclusion: MPA框架有效提升了小型视觉语言模型的性能，在保持计算效率的同时显著缩小了与大型模型的性能差距

Abstract: Large Vision-Language Models (L-VLMs) have demonstrated remarkable
performance in various vision and language tasks, including visual question
answering (VQA). However, their high computational cost makes them impractical
for resource-constrained settings and inference-heavy applications. In
contrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer
from a significant performance gap compared to their larger counterparts. In
this work, we introduce the Model Parity Aligner (MPA), a novel framework
designed to systematically improve S-VLMs by leveraging unlabeled images and
effective knowledge transfer from L-VLMs. Instead of traditional knowledge
distillation methods that rely on labeled training data, MPA employs a
strategic parity-based approach that precisely identifies the knowledge
disparities between S-VLMs and L-VLMs, and optimizes training by targeting only
these disparities. We conduct extensive experiments on four diverse VQA
benchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires
specialized reasoning capabilities such as text recognition, chart
interpretation, and commonsense and factual understanding. Our results
demonstrate that MPA consistently enhances the performance of S-VLMs on all
benchmarks, reducing the performance gap while maintaining computational
efficiency. We make our code publicly available.

</details>


### [43] [Towards Anytime Retrieval: A Benchmark for Anytime Person Re-Identification](https://arxiv.org/abs/2509.16635)
*Xulin Li,Yan Lu,Bin Liu,Jiaze Li,Qinhong Yang,Tao Gong,Qi Chu,Mang Ye,Nenghai Yu*

Main category: cs.CV

TL;DR: 本文提出了Anytime Person Re-identification (AT-ReID)任务，旨在解决现有ReID方法在时间跨度大、场景多变情况下的局限性，并构建了首个大规模数据集AT-USTC和统一模型Uni-AT。


<details>
  <summary>Details</summary>
Motivation: 现有ReID任务和数据集受限于特定时间和场景，无法满足实际应用中全天候、长时跨度的行人检索需求。

Method: 收集了AT-USTC数据集（403k图像，21个月跨度），提出Uni-AT模型包含多场景ReID框架、属性专家混合模块和分层动态加权策略。

Result: 实验表明该模型在所有场景下都取得了满意结果并表现出优秀的泛化能力。

Conclusion: AT-ReID任务具有重要研究价值，提出的数据集和模型为解决多场景行人检索问题提供了有效方案。

Abstract: In real applications, person re-identification (ReID) is expected to retrieve
the target person at any time, including both daytime and nighttime, ranging
from short-term to long-term. However, existing ReID tasks and datasets can not
meet this requirement, as they are constrained by available time and only
provide training and evaluation for specific scenarios. Therefore, we
investigate a new task called Anytime Person Re-identification (AT-ReID), which
aims to achieve effective retrieval in multiple scenarios based on variations
in time. To address the AT-ReID problem, we collect the first large-scale
dataset, AT-USTC, which contains 403k images of individuals wearing multiple
clothes captured by RGB and IR cameras. Our data collection spans 21 months,
and 270 volunteers were photographed on average 29.1 times across different
dates or scenes, 4-15 times more than current datasets, providing conditions
for follow-up investigations in AT-ReID. Further, to tackle the new challenge
of multi-scenario retrieval, we propose a unified model named Uni-AT, which
comprises a multi-scenario ReID (MS-ReID) framework for scenario-specific
features learning, a Mixture-of-Attribute-Experts (MoAE) module to alleviate
inter-scenario interference, and a Hierarchical Dynamic Weighting (HDW)
strategy to ensure balanced training across all scenarios. Extensive
experiments show that our model leads to satisfactory results and exhibits
excellent generalization to all scenarios.

</details>


### [44] [Unlocking Hidden Potential in Point Cloud Networks with Attention-Guided Grouping-Feature Coordination](https://arxiv.org/abs/2509.16639)
*Shangzhuo Xie,Qianqian Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级的Grouping-Feature协调模块(GF-Core)，通过同时调控分组层和特征提取层来实现更精细的特征聚合，并引入了专门针对点云输入的自监督预训练策略，显著提升了点云分析任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注新颖的结构设计，但传统的基于点的架构（通过顺序采样、分组和特征提取层处理原始点）显示出未充分利用的潜力。作者发现通过策略性的模块集成而非结构修改可以解锁显著的性能提升。

Method: 提出了Grouping-Feature协调模块(GF-Core)，这是一个轻量级可分离组件，同时调控分组层和特征提取层以支持更细致的特征聚合。此外，还引入了专门为点云输入设计的自监督预训练策略。

Result: 在ModelNet40数据集上，该方法将基线网络准确率提升至94.0%，与先进框架性能相当但保持架构简单性。在ScanObjectNN数据集的三个变体上，分别获得了2.96%、6.34%和6.32%的改进。

Conclusion: 通过模块集成策略而非结构修改，可以有效提升点云分析性能，证明了传统点云架构的潜力可以通过更智能的模块协调来充分释放。

Abstract: Point cloud analysis has evolved with diverse network architectures, while
existing works predominantly focus on introducing novel structural designs.
However, conventional point-based architectures - processing raw points through
sequential sampling, grouping, and feature extraction layers - demonstrate
underutilized potential. We notice that substantial performance gains can be
unlocked through strategic module integration rather than structural
modifications. In this paper, we propose the Grouping-Feature Coordination
Module (GF-Core), a lightweight separable component that simultaneously
regulates both grouping layer and feature extraction layer to enable more
nuanced feature aggregation. Besides, we introduce a self-supervised
pretraining strategy specifically tailored for point-based inputs to enhance
model robustness in complex point cloud analysis scenarios. On ModelNet40
dataset, our method elevates baseline networks to 94.0% accuracy, matching
advanced frameworks' performance while preserving architectural simplicity. On
three variants of the ScanObjectNN dataset, we obtain improvements of 2.96%,
6.34%, and 6.32% respectively.

</details>


### [45] [ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents](https://arxiv.org/abs/2509.16645)
*Yichen Wang,Hangtao Zhang,Hewen Pan,Ziqi Zhou,Xianlong Wang,Peijin Guo,Lulu Xue,Shengshan Hu,Minghui Li,Leo Yu Zhang*

Main category: cs.CV

TL;DR: 提出了ADVEDM框架，一种针对视觉语言模型的细粒度对抗攻击方法，通过仅修改关键对象的感知来影响VLM在具身决策任务中的输出，同时保持其他区域的语义完整性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击要么需要完全了解受害VLM（不切实际），要么因破坏过多语义信息导致与任务上下文不一致，产生无效输出。需要一种更有效的攻击方法来影响物理世界中的智能体行为。

Method: 设计了ADVEDM框架及其两个变体：ADVEDM-R（从图像中移除特定对象语义）和ADVEDM-A（向图像中添加新对象语义），实现细粒度的感知控制。

Result: 在通用场景和具身决策任务中的实验结果表明，该方法具有细粒度控制能力和优秀的攻击性能。

Conclusion: ADVEDM框架能够有效影响VLM输出有效但错误的决策，对物理世界中的智能体构成更实质性的安全威胁。

Abstract: Vision-Language Models (VLMs), with their strong reasoning and planning
capabilities, are widely used in embodied decision-making (EDM) tasks in
embodied agents, such as autonomous driving and robotic manipulation. Recent
research has increasingly explored adversarial attacks on VLMs to reveal their
vulnerabilities. However, these attacks either rely on overly strong
assumptions, requiring full knowledge of the victim VLM, which is impractical
for attacking VLM-based agents, or exhibit limited effectiveness. The latter
stems from disrupting most semantic information in the image, which leads to a
misalignment between the perception and the task context defined by system
prompts. This inconsistency interrupts the VLM's reasoning process, resulting
in invalid outputs that fail to affect interactions in the physical world. To
this end, we propose a fine-grained adversarial attack framework, ADVEDM, which
modifies the VLM's perception of only a few key objects while preserving the
semantics of the remaining regions. This attack effectively reduces conflicts
with the task context, making VLMs output valid but incorrect decisions and
affecting the actions of agents, thus posing a more substantial safety threat
in the physical world. We design two variants of based on this framework,
ADVEDM-R and ADVEDM-A, which respectively remove the semantics of a specific
object from the image and add the semantics of a new object into the image. The
experimental results in both general scenarios and EDM tasks demonstrate
fine-grained control and excellent attack performance.

</details>


### [46] [Are VLMs Ready for Lane Topology Awareness in Autonomous Driving?](https://arxiv.org/abs/2509.16654)
*Xin Chen,Jia He,Maozheng Li,Dongliang Xu,Tianyu Wang,Yixiao Chen,Zhixin Lin,Yue Yao*

Main category: cs.CV

TL;DR: 该论文系统评估了视觉语言模型在道路拓扑理解方面的能力，发现当前模型在空间推理方面存在瓶颈，特别是开源模型表现较差，而模型能力与模型大小、推理标记长度和示例数量呈正相关。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多模态推理方面取得了显著进展，但在自动驾驶领域的应用仍然有限。特别是理解道路拓扑这一安全导航的关键能力尚未得到充分研究，现有模型在该任务上的表现不尽如人意。

Method: 将多视角图像投影到统一的鸟瞰图坐标系中融合成BEV车道，基于这些BEV车道设计了四个拓扑相关的诊断性VQA任务，通过广泛评估分析不同模型的表现。

Result: 前沿闭源模型（如GPT-4o）在某些任务上准确率较高，但在时间性问题上的表现仍不理想（如GPT-4o在二分类问题上仅达到67.8%）。开源模型即使达到300亿参数规模也表现不佳。

Conclusion: 空间推理仍然是当前视觉语言模型的基本瓶颈，模型能力与模型规模、推理标记长度和示例数量呈正相关，这为未来研究指明了方向。

Abstract: Vision-Language Models (VLMs) have recently shown remarkable progress in
multimodal reasoning, yet their applications in autonomous driving remain
limited. In particular, the ability to understand road topology, a key
requirement for safe navigation, has received relatively little attention.
While some recent works have begun to explore VLMs in driving contexts, their
performance on topology reasoning is far from satisfactory. In this work, we
systematically evaluate VLMs' capabilities in road topology understanding.
Specifically, multi-view images are projected into unified ground-plane
coordinate system and fused into bird's-eye-view (BEV) lanes. Based on these
BEV lanes, we formulate four topology-related diagnostic VQA tasks, which
together capture essential components of spatial topology reasoning. Through
extensive evaluation, we find that while frontier closed-source models (e.g.,
GPT-4o) achieve relatively high accuracy in some tasks, they still fail in some
temporal questions that humans can answer (e.g., GPT-4o achieve only 67.8% in
vector, a two-class classification problem). Furthermore, we find open-source
VLMs, even at 30B scale, struggle significantly. These results indicate that
spatial reasoning remains a fundamental bottleneck for current VLMs. We also
find that the model's capability is positively correlated with model size,
length of reasoning tokens and shots provided as examples, showing direction
for future research.

</details>


### [47] [MedCutMix: A Data-Centric Approach to Improve Radiology Vision-Language Pre-training with Disease Awareness](https://arxiv.org/abs/2509.16673)
*Sinuo Wang,Yutong Xie,Yuyuan Liu,Qi Wu*

Main category: cs.CV

TL;DR: 提出了MedCutMix方法，一种多模态疾病中心的数据增强技术，用于解决医学视觉语言预训练中数据稀缺和隐私问题，通过在医学报告中执行诊断句子CutMix，并建立诊断句子与医学图像的跨模态注意力来指导图像模态的注意力流形混合。


<details>
  <summary>Details</summary>
Motivation: 医学视觉语言预训练依赖图像-文本数据集，但面临隐私担忧和配对标注成本高的问题。现有数据增强方法难以捕捉医学数据的细微复杂变化，多样性有限。

Method: MedCutMix在医学报告内执行诊断句子CutMix，建立诊断句子与医学图像的跨注意力机制，指导图像模态的注意力流形混合。

Result: 在四个下游放射学诊断数据集上超越了先前方法，显示出在放射学视觉语言预训练中提升性能和泛化能力的有效性。

Conclusion: MedCutMix是一种有效的多模态疾病中心数据增强方法，能够显著提升医学视觉语言预训练的性能和泛化能力。

Abstract: Vision-Language Pre-training (VLP) is drawing increasing interest for its
ability to minimize manual annotation requirements while enhancing semantic
understanding in downstream tasks. However, its reliance on image-text datasets
poses challenges due to privacy concerns and the high cost of obtaining paired
annotations. Data augmentation emerges as a viable strategy to address this
issue, yet existing methods often fall short of capturing the subtle and
complex variations in medical data due to limited diversity. To this end, we
propose MedCutMix, a novel multi-modal disease-centric data augmentation
method. MedCutMix performs diagnostic sentence CutMix within medical reports
and establishes the cross-attention between the diagnostic sentence and medical
image to guide attentive manifold mix within the imaging modality. Our approach
surpasses previous methods across four downstream radiology diagnosis datasets,
highlighting its effectiveness in enhancing performance and generalizability in
radiology VLP.

</details>


### [48] [FitPro: A Zero-Shot Framework for Interactive Text-based Pedestrian Retrieval in Open World](https://arxiv.org/abs/2509.16674)
*Zengli Luo,Canlong Zhang,Xiaochun Lu,Zhixin Li*

Main category: cs.CV

TL;DR: FitPro是一个面向开放世界的交互式零样本文本行人检索框架，通过特征对比解码、增量语义挖掘和查询感知分层检索三个创新组件，解决了现有方法在模型泛化性和语义理解方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有文本行人检索方法在受限场景下取得进展，但在开放世界交互检索中面临模型泛化能力有限和语义理解不足的挑战。

Method: FitPro包含三个核心组件：1）特征对比解码（FCD）- 通过提示引导的对比解码生成高质量结构化行人描述；2）增量语义挖掘（ISM）- 从多视角观察构建整体行人表示；3）查询感知分层检索（QHR）- 根据查询类型动态优化检索流程。

Result: 在五个公共数据集和两种评估协议上的大量实验表明，FitPro在交互检索中显著克服了现有方法的泛化限制和语义建模约束。

Conclusion: FitPro为实际部署铺平了道路，代码和数据将在GitHub上发布。

Abstract: Text-based Pedestrian Retrieval (TPR) aims to retrieve specific target
pedestrians in visual scenes according to natural language descriptions.
Although existing methods have achieved progress under constrained settings,
interactive retrieval in the open-world scenario still suffers from limited
model generalization and insufficient semantic understanding. To address these
challenges, we propose FitPro, an open-world interactive zero-shot TPR
framework with enhanced semantic comprehension and cross-scene adaptability.
FitPro has three innovative components: Feature Contrastive Decoding (FCD),
Incremental Semantic Mining (ISM), and Query-aware Hierarchical Retrieval
(QHR). The FCD integrates prompt-guided contrastive decoding to generate
high-quality structured pedestrian descriptions from denoised images,
effectively alleviating semantic drift in zero-shot scenarios. The ISM
constructs holistic pedestrian representations from multi-view observations to
achieve global semantic modeling in multi-turn interactions,thereby improving
robustness against viewpoint shifts and fine-grained variations in
descriptions. The QHR dynamically optimizes the retrieval pipeline according to
query types, enabling efficient adaptation to multi-modal and multi-view
inputs. Extensive experiments on five public datasets and two evaluation
protocols demonstrate that FitPro significantly overcomes the generalization
limitations and semantic modeling constraints of existing methods in
interactive retrieval, paving the way for practical deployment. The code and
data will be released at https://github.com/
lilo4096/FitPro-Interactive-Person-Retrieval.

</details>


### [49] [Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation Towards Embodied Intelligence](https://arxiv.org/abs/2509.16677)
*Wenxin Li,Kunyu Peng,Di Wen,Ruiping Liu,Mengfei Duan,Kai Luo,Kailun Yang*

Main category: cs.CV

TL;DR: 该论文首次研究了基于动作的视频对象分割在标签噪声下的问题，提出了两种噪声类型（文本提示噪声和掩码标注噪声），建立了首个基准数据集ActiSeg-NL，并评估了六种标签噪声学习策略。


<details>
  <summary>Details</summary>
Motivation: 基于动作的视频对象分割依赖于大规模注释和提示，但这些注释成本高、不一致且容易受到多模态噪声（如不精确的掩码和指代模糊）的影响，这一挑战尚未被探索。

Method: 引入两种标签噪声类型，建立ActiSeg-NL基准数据集，评估六种标签噪声学习策略，并提出并行掩码头机制（PMHM）来处理掩码标注噪声。

Result: 不同学习策略表现出不同的鲁棒性特征，受前景-背景权衡的制约。定性评估揭示了边界泄漏、定位错误等特征性失败模式。

Conclusion: 建立了首个基于动作的视频对象分割在标签噪声下的基准，提供了全面的噪声类型与失败模式分析，提出的PMHM机制能有效处理掩码标注噪声。

Abstract: Embodied intelligence relies on accurately segmenting objects actively
involved in interactions. Action-based video object segmentation addresses this
by linking segmentation with action semantics, but it depends on large-scale
annotations and prompts that are costly, inconsistent, and prone to multimodal
noise such as imprecise masks and referential ambiguity. To date, this
challenge remains unexplored. In this work, we take the first step by studying
action-based video object segmentation under label noise, focusing on two
sources: textual prompt noise (category flips and within-category noun
substitutions) and mask annotation noise (perturbed object boundaries to mimic
imprecise supervision). Our contributions are threefold. First, we introduce
two types of label noises for the action-based video object segmentation task.
Second, we build up the first action-based video object segmentation under a
label noise benchmark ActiSeg-NL and adapt six label-noise learning strategies
to this setting, and establish protocols for evaluating them under textual,
boundary, and mixed noise. Third, we provide a comprehensive analysis linking
noise types to failure modes and robustness gains, and we introduce a Parallel
Mask Head Mechanism (PMHM) to address mask annotation noise. Qualitative
evaluations further reveal characteristic failure modes, including boundary
leakage and mislocalization under boundary perturbations, as well as occasional
identity substitutions under textual flips. Our comparative analysis reveals
that different learning strategies exhibit distinct robustness profiles,
governed by a foreground-background trade-off where some achieve balanced
performance while others prioritize foreground accuracy at the cost of
background precision. The established benchmark and source code will be made
publicly available at https://github.com/mylwx/ActiSeg-NL.

</details>


### [50] [IPF-RDA: An Information-Preserving Framework for Robust Data Augmentation](https://arxiv.org/abs/2509.16678)
*Suorong Yang,Hongchao Yang,Suhan Guo,Furao Shen,Jian Zhao*

Main category: cs.CV

TL;DR: IPF-RDA是一个新颖的信息保留框架，旨在增强数据增强的鲁棒性，通过识别对数据增强操作最敏感的关键点并自适应保留重要信息，从而提升深度模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 数据增强虽然能提升模型泛化能力，但会引入分布偏移和噪声，限制深度网络的潜力并降低性能。

Method: 提出两个核心组件：(1)类判别信息估计算法，识别对数据增强操作最脆弱的点及其重要性分数；(2)信息保留方案，自适应保留增强样本中的关键信息并确保数据多样性。将数据增强方法分为三类并相应集成到框架中。

Result: 在CIFAR-10、CIFAR-100、Tiny-ImageNet、CUHK03、Market1501、Oxford Flower和MNIST等多个数据集上的广泛实验表明，IPF-RDA能持续改进多种最先进数据增强方法的性能。

Conclusion: IPF-RDA虽然简单，但能有效增强数据增强方法的鲁棒性并充分发挥其潜力，具有良好的性能和可扩展性。

Abstract: Data augmentation is widely utilized as an effective technique to enhance the
generalization performance of deep models. However, data augmentation may
inevitably introduce distribution shifts and noises, which significantly
constrain the potential and deteriorate the performance of deep networks. To
this end, we propose a novel information-preserving framework, namely IPF-RDA,
to enhance the robustness of data augmentations in this paper. IPF-RDA combines
the proposal of (i) a new class-discriminative information estimation algorithm
that identifies the points most vulnerable to data augmentation operations and
corresponding importance scores; And (ii) a new information-preserving scheme
that preserves the critical information in the augmented samples and ensures
the diversity of augmented data adaptively. We divide data augmentation methods
into three categories according to the operation types and integrate these
approaches into our framework accordingly. After being integrated into our
framework, the robustness of data augmentation methods can be enhanced and
their full potential can be unleashed. Extensive experiments demonstrate that
although being simple, IPF-RDA consistently improves the performance of
numerous commonly used state-of-the-art data augmentation methods with popular
deep models on a variety of datasets, including CIFAR-10, CIFAR-100,
Tiny-ImageNet, CUHK03, Market1501, Oxford Flower, and MNIST, where its
performance and scalability are stressed. The implementation is available at
https://github.com/Jackbrocp/IPF-RDA.

</details>


### [51] [ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering](https://arxiv.org/abs/2509.16680)
*Xingjian Diao,Weiyi Wu,Keyi Kong,Peijun Qing,Xinwen Xu,Ming Cheng,Soroush Vosoughi,Jiang Gui*

Main category: cs.CV

TL;DR: ProtoVQA是一个基于原型的可解释视觉问答框架，通过学习问题感知原型来连接答案和判别性图像区域，提供准确答案和人类可理解的解释。


<details>
  <summary>Details</summary>
Motivation: 随着VQA在医疗影像和自动驾驶等安全关键领域的应用增加，模型不仅需要提供准确答案，还需要提供人类易于理解和验证的解释。原型建模在视觉推理任务中显示出可解释性潜力，但在VQA领域尚未充分探索。

Method: ProtoVQA框架包含三个核心组件：(i)学习问题感知原型作为推理锚点；(ii)应用空间约束匹配确保证据的连贯性和语义相关性；(iii)通过共享原型骨干同时支持答案生成和定位任务。

Result: 在Visual7W数据集上的实验表明，ProtoVQA能够产生忠实、细粒度的解释，同时保持有竞争力的准确率。提出的VLAS指标有效评估了解释质量。

Conclusion: ProtoVQA推进了透明可信VQA系统的发展，为安全关键应用提供了可解释的视觉问答解决方案。

Abstract: Visual Question Answering (VQA) is increasingly used in diverse applications
ranging from general visual reasoning to safety-critical domains such as
medical imaging and autonomous systems, where models must provide not only
accurate answers but also explanations that humans can easily understand and
verify. Prototype-based modeling has shown promise for interpretability by
grounding predictions in semantically meaningful regions for purely visual
reasoning tasks, yet remains underexplored in the context of VQA. We present
ProtoVQA, a unified prototypical framework that (i) learns question-aware
prototypes that serve as reasoning anchors, connecting answers to
discriminative image regions, (ii) applies spatially constrained matching to
ensure that the selected evidence is coherent and semantically relevant, and
(iii) supports both answering and grounding tasks through a shared prototype
backbone. To assess explanation quality, we propose the Visual-Linguistic
Alignment Score (VLAS), which measures how well the model's attended regions
align with ground-truth evidence. Experiments on Visual7W show that ProtoVQA
yields faithful, fine-grained explanations while maintaining competitive
accuracy, advancing the development of transparent and trustworthy VQA systems.

</details>


### [52] [Active View Selection for Scene-level Multi-view Crowd Counting and Localization with Limited Labels](https://arxiv.org/abs/2509.16684)
*Qi Zhang,Bin Li,Antoni B. Chan,Hui Huang*

Main category: cs.CV

TL;DR: 本文研究多视角人群计数和定位中的视角选择问题，提出独立视角选择(IVS)和主动视角选择(AVS)方法，旨在实现跨场景能力和有限标签需求下的场景级最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注输入视角的准确预测，忽略了选择最佳相机视角来全面感知场景中所有人群的问题。现有视角选择方法需要大量标注视角和图像，缺乏跨场景能力，限制了应用场景。

Method: 提出IVS方法考虑视角和场景几何信息进行独立视角选择；基于IVS提出AVS方法，联合优化视角选择、标注和下游任务，在视角选择过程中同时考虑几何信息和下游任务模型预测。

Result: 在多视角计数和定位任务上的实验表明，主动视角选择方法(AVS)具有跨场景和有限标签需求的优势，优于现有方法且应用场景更广。

Conclusion: 本文提出的主动视角选择方法解决了多视角人群分析中的视角选择问题，实现了更好的场景级性能，具有重要的实际应用价值。

Abstract: Multi-view crowd counting and localization fuse the input multi-views for
estimating the crowd number or locations on the ground. Existing methods mainly
focus on accurately predicting on the crowd shown in the input views, which
neglects the problem of choosing the `best' camera views to perceive all crowds
well in the scene. Besides, existing view selection methods require massive
labeled views and images, and lack the ability for cross-scene settings,
reducing their application scenarios. Thus, in this paper, we study the view
selection issue for better scene-level multi-view crowd counting and
localization results with cross-scene ability and limited label demand, instead
of input-view-level results. We first propose an independent view selection
method (IVS) that considers view and scene geometries in the view selection
strategy and conducts the view selection, labeling, and downstream tasks
independently. Based on IVS, we also put forward an active view selection
method (AVS) that jointly optimizes the view selection, labeling, and
downstream tasks. In AVS, we actively select the labeled views and consider
both the view/scene geometries and the predictions of the downstream task
models in the view selection process. Experiments on multi-view counting and
localization tasks demonstrate the cross-scene and the limited label demand
advantages of the proposed active view selection method (AVS), outperforming
existing methods and with wider application scenarios.

</details>


### [53] [Towards a Transparent and Interpretable AI Model for Medical Image Classifications](https://arxiv.org/abs/2509.16685)
*Binbin Wen,Yihang Wu,Tareef Daqqaq,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 本文研究了可解释人工智能（XAI）方法在医疗领域的应用，通过医疗数据集模拟展示XAI如何提高AI决策的透明度，并讨论了该领域面临的挑战。


<details>
  <summary>Details</summary>
Motivation: AI在医疗领域的应用日益重要，但复杂AI模型的不透明性限制了其临床实用性，因此需要研究XAI方法来提高AI决策的透明度和可解释性。

Method: 使用多种医疗数据集进行模拟实验，阐明XAI模型的内部工作机制，展示XAI如何有效解释AI预测结果。

Result: 基于数据集的模拟实验表明，XAI能够有效解释AI预测，改善医疗专业人员的决策过程。

Conclusion: 需要持续开发和探索XAI技术，特别是在多样化医疗数据集方面的应用，以促进其在医疗领域的采用和有效性。

Abstract: The integration of artificial intelligence (AI) into medicine is remarkable,
offering advanced diagnostic and therapeutic possibilities. However, the
inherent opacity of complex AI models presents significant challenges to their
clinical practicality. This paper focuses primarily on investigating the
application of explainable artificial intelligence (XAI) methods, with the aim
of making AI decisions transparent and interpretable. Our research focuses on
implementing simulations using various medical datasets to elucidate the
internal workings of the XAI model. These dataset-driven simulations
demonstrate how XAI effectively interprets AI predictions, thus improving the
decision-making process for healthcare professionals. In addition to a survey
of the main XAI methods and simulations, ongoing challenges in the XAI field
are discussed. The study highlights the need for the continuous development and
exploration of XAI, particularly from the perspective of diverse medical
datasets, to promote its adoption and effectiveness in the healthcare domain.

</details>


### [54] [Spectral Compressive Imaging via Chromaticity-Intensity Decomposition](https://arxiv.org/abs/2509.16690)
*Xiaodong Wang,Zijun He,Ping Wang,Lishun Wang,Yanan Hu,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种色度-强度分解框架CIDNet，用于解决编码孔径快照光谱成像(CASSI)中的HSI重建问题，通过分离光照不变的色度信息和空间平滑的强度信息，提高了光谱和色度保真度。


<details>
  <summary>Details</summary>
Motivation: CASSI捕获的测量值纠缠了空间和光谱信息，导致HSI重建成为严重不适定逆问题。此外，捕获的辐射度依赖于场景光照，难以恢复对光照条件不变的固有光谱反射率。

Method: 提出色度-强度分解框架，将HSI分解为空间平滑的强度图和光谱变化的色度立方体。开发CIDNet网络，集成混合空间-光谱Transformer重建精细色度，以及退化感知的空间自适应噪声估计模块。

Result: 在合成和真实CASSI数据集上的广泛实验表明，该方法在光谱和色度保真度方面均实现了优越性能。

Conclusion: 色度-强度分解框架有效解决了CASSI中的HSI重建挑战，能够恢复光照不变的反射率特征，代码和模型将公开提供。

Abstract: In coded aperture snapshot spectral imaging (CASSI), the captured measurement
entangles spatial and spectral information, posing a severely ill-posed inverse
problem for hyperspectral images (HSIs) reconstruction. Moreover, the captured
radiance inherently depends on scene illumination, making it difficult to
recover the intrinsic spectral reflectance that remains invariant to lighting
conditions. To address these challenges, we propose a chromaticity-intensity
decomposition framework, which disentangles an HSI into a spatially smooth
intensity map and a spectrally variant chromaticity cube. The chromaticity
encodes lighting-invariant reflectance, enriched with high-frequency spatial
details and local spectral sparsity. Building on this decomposition, we develop
CIDNet, a Chromaticity-Intensity Decomposition unfolding network within a
dual-camera CASSI system. CIDNet integrates a hybrid spatial-spectral
Transformer tailored to reconstruct fine-grained and sparse spectral
chromaticity and a degradation-aware, spatially-adaptive noise estimation
module that captures anisotropic noise across iterative stages. Extensive
experiments on both synthetic and real-world CASSI datasets demonstrate that
our method achieves superior performance in both spectral and chromaticity
fidelity. Code and models will be publicly available.

</details>


### [55] [InstanceAssemble: Layout-Aware Image Generation via Instance Assembling Attention](https://arxiv.org/abs/2509.16691)
*Qiang Xiang,Shuang Sun,Binglei Li,Dejia Song,Huaxia Li,Nemo Chen,Xu Tang,Yao Hu,Junping Zhang*

Main category: cs.CV

TL;DR: 本文提出了InstanceAssemble方法，通过实例组装注意力机制实现基于布局条件的图像生成，支持边界框位置控制和多模态内容控制，并在新基准Denselayout上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前布局到图像生成方法性能仍不理想，需要更精确的位置控制和内容控制能力。

Method: 采用实例组装注意力机制，通过轻量级LoRA模块适配现有DiT-based T2I模型，支持边界框和多模态内容控制。

Result: 在包含5k图像、90k实例的Denselayout基准上达到最先进性能，并提出了可解释的评估指标LGS。

Conclusion: InstanceAssemble方法在复杂布局条件下表现出色，且与多样式LoRA模块兼容性强。

Abstract: Diffusion models have demonstrated remarkable capabilities in generating
high-quality images. Recent advancements in Layout-to-Image (L2I) generation
have leveraged positional conditions and textual descriptions to facilitate
precise and controllable image synthesis. Despite overall progress, current L2I
methods still exhibit suboptimal performance. Therefore, we propose
InstanceAssemble, a novel architecture that incorporates layout conditions via
instance-assembling attention, enabling position control with bounding boxes
(bbox) and multimodal content control including texts and additional visual
content. Our method achieves flexible adaption to existing DiT-based T2I models
through light-weighted LoRA modules. Additionally, we propose a Layout-to-Image
benchmark, Denselayout, a comprehensive benchmark for layout-to-image
generation, containing 5k images with 90k instances in total. We further
introduce Layout Grounding Score (LGS), an interpretable evaluation metric to
more precisely assess the accuracy of L2I generation. Experiments demonstrate
that our InstanceAssemble method achieves state-of-the-art performance under
complex layout conditions, while exhibiting strong compatibility with diverse
style LoRA modules.

</details>


### [56] [Animalbooth: multimodal feature enhancement for animal subject personalization](https://arxiv.org/abs/2509.16702)
*Chen Liu,Haitao Wu,Kafeng Wang,Xiaowang Zhang*

Main category: cs.CV

TL;DR: AnimalBooth是一个用于个性化动物图像生成的框架，通过Animal Net和自适应注意力模块增强身份保持，并引入频率控制特征集成模块来指导扩散过程，从全局结构到细节纹理逐步生成。


<details>
  <summary>Details</summary>
Motivation: 个性化动物图像生成面临外观线索丰富和形态变异大的挑战，现有方法存在跨域特征对齐错误导致身份漂移的问题。

Method: 提出AnimalBooth框架，包含Animal Net和自适应注意力模块来缓解跨域对齐错误，以及频率控制特征集成模块在潜在空间应用离散余弦变换滤波来指导扩散过程。

Result: 在多个基准测试中，AnimalBooth始终优于强基线方法，在身份保真度和感知质量方面都有显著提升。

Conclusion: AnimalBooth通过创新的身份保持机制和频率控制方法，有效解决了动物图像生成中的身份漂移问题，并构建了AnimalBench数据集推动该领域研究。

Abstract: Personalized animal image generation is challenging due to rich appearance
cues and large morphological variability. Existing approaches often exhibit
feature misalignment across domains, which leads to identity drift. We present
AnimalBooth, a framework that strengthens identity preservation with an Animal
Net and an adaptive attention module, mitigating cross domain alignment errors.
We further introduce a frequency controlled feature integration module that
applies Discrete Cosine Transform filtering in the latent space to guide the
diffusion process, enabling a coarse to fine progression from global structure
to detailed texture. To advance research in this area, we curate AnimalBench, a
high resolution dataset for animal personalization. Extensive experiments show
that AnimalBooth consistently outperforms strong baselines on multiple
benchmarks and improves both identity fidelity and perceptual quality.

</details>


### [57] [When Confidence Fails: Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation](https://arxiv.org/abs/2509.16704)
*Pan Liu,Jinshi Liu*

Main category: cs.CV

TL;DR: 本文提出置信度可分学习（CSL）方法，解决半监督语义分割中伪标签选择的挑战，通过凸优化建立样本特定决策边界，并在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用固定置信度阈值选择伪标签，但无法处理网络过度自信倾向，导致正确和错误预测在高置信度区域重叠，同时丢弃低置信度预测会破坏空间语义连续性。

Method: CSL将伪标签选择建模为置信度分布特征空间中的凸优化问题，建立样本特定决策边界，并引入可靠像素的随机掩码来学习低可靠性区域的上下文关系。

Result: 在Pascal、Cityscapes和COCO基准测试上的广泛实验表明，CSL相比最先进方法表现更优。

Conclusion: CSL有效解决了伪标签选择中的关键问题，通过优化决策边界和上下文学习机制提升了半监督语义分割的性能。

Abstract: While significant advances exist in pseudo-label generation for
semi-supervised semantic segmentation, pseudo-label selection remains
understudied. Existing methods typically use fixed confidence thresholds to
retain high-confidence predictions as pseudo-labels. However, these methods
cannot cope with network overconfidence tendency, where correct and incorrect
predictions overlap significantly in high-confidence regions, making separation
challenging and amplifying model cognitive bias. Meanwhile, the direct
discarding of low-confidence predictions disrupts spatial-semantic continuity,
causing critical context loss. We propose Confidence Separable Learning (CSL)
to address these limitations. CSL formulates pseudo-label selection as a convex
optimization problem within the confidence distribution feature space,
establishing sample-specific decision boundaries to distinguish reliable from
unreliable predictions. Additionally, CSL introduces random masking of reliable
pixels to guide the network in learning contextual relationships from
low-reliability regions, thereby mitigating the adverse effects of discarding
uncertain predictions. Extensive experimental results on the Pascal,
Cityscapes, and COCO benchmarks show that CSL performs favorably against
state-of-the-art methods. Code and model weights are available at
https://github.com/PanLiuCSU/CSL.

</details>


### [58] [Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding](https://arxiv.org/abs/2509.16721)
*Haoyuan Li,Rui Liu,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: Text-Scene是一个自动将3D场景解析为文本描述以进行场景理解的框架，通过几何分析和多模态大语言模型生成准确、详细的场景描述，并提出了InPlan3D基准来评估3D任务规划能力。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在3D场景理解方面的挑战，包括3D环境涉及更丰富的概念（空间关系、功能、物理、布局等）以及缺乏大规模3D视觉语言数据集的问题。

Method: 开发Text-Scene框架，自动识别3D场景中的物体属性和空间关系，然后生成整个场景的连贯摘要，结合几何分析和多模态大语言模型，无需人工干预。

Result: 实验结果表明，文本解析能够忠实表示3D场景并有益于下游任务。提出了包含636个室内场景中3174个长期规划任务的InPlan3D基准。

Conclusion: 该方法通过语言使3D场景内容易于理解，强调清晰性和可访问性，代码和数据集将发布。

Abstract: Enabling agents to understand and interact with complex 3D scenes is a
fundamental challenge for embodied artificial intelligence systems. While
Multimodal Large Language Models (MLLMs) have achieved significant progress in
2D image understanding, extending such capabilities to 3D scenes remains
difficult: 1) 3D environment involves richer concepts such as spatial
relationships, affordances, physics, layout, and so on, 2) the absence of
large-scale 3D vision-language datasets has posed a significant obstacle. In
this paper, we introduce Text-Scene, a framework that automatically parses 3D
scenes into textual descriptions for scene understanding. Given a 3D scene, our
model identifies object attributes and spatial relationships, and then
generates a coherent summary of the whole scene, bridging the gap between 3D
observation and language without requiring human-in-the-loop intervention. By
leveraging both geometric analysis and MLLMs, Text-Scene produces descriptions
that are accurate, detailed, and human-interpretable, capturing object-level
details and global-level context. Experimental results on benchmarks
demonstrate that our textual parses can faithfully represent 3D scenes and
benefit downstream tasks. To evaluate the reasoning capability of MLLMs, we
present InPlan3D, a comprehensive benchmark for 3D task planning, consisting of
3174 long-term planning tasks across 636 indoor scenes. We emphasize clarity
and accessibility in our approach, aiming to make 3D scene content
understandable through language. Code and datasets will be released.

</details>


### [59] [Pain in 3D: Generating Controllable Synthetic Faces for Automated Pain Assessment](https://arxiv.org/abs/2509.16727)
*Xin Lei Lin,Soroush Mehraban,Abhishek Moturu,Babak Taati*

Main category: cs.CV

TL;DR: 本文提出了3DPain合成数据集和ViTPain框架，用于解决自动疼痛评估中的数据不平衡和生成模型控制精度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 自动疼痛评估对于非沟通性患者（如痴呆症患者）至关重要，但现有数据集存在严重的人口统计和标签不平衡问题，且当前生成模型无法精确控制面部动作单元和临床验证的疼痛水平。

Method: 采用三阶段框架：生成多样化3D网格，使用扩散模型进行纹理处理，应用AU驱动的面部绑定技术合成多视角面部图像。同时提出ViTPain框架，通过热图训练的教师模型指导RGB图像训练的学生模型。

Result: 创建了包含82,500个样本的大规模合成数据集，涵盖25,000个疼痛表情热图和2,500个合成身份，在年龄、性别和种族方面保持平衡。ViTPain框架提高了准确性、可解释性和临床可靠性。

Conclusion: 3DPain和ViTPain共同为通用自动疼痛评估建立了可控、多样化和临床基础的方法基础。

Abstract: Automated pain assessment from facial expressions is crucial for
non-communicative patients, such as those with dementia. Progress has been
limited by two challenges: (i) existing datasets exhibit severe demographic and
label imbalance due to ethical constraints, and (ii) current generative models
cannot precisely control facial action units (AUs), facial structure, or
clinically validated pain levels.
  We present 3DPain, a large-scale synthetic dataset specifically designed for
automated pain assessment, featuring unprecedented annotation richness and
demographic diversity. Our three-stage framework generates diverse 3D meshes,
textures them with diffusion models, and applies AU-driven face rigging to
synthesize multi-view faces with paired neutral and pain images, AU
configurations, PSPI scores, and the first dataset-level annotations of
pain-region heatmaps. The dataset comprises 82,500 samples across 25,000 pain
expression heatmaps and 2,500 synthetic identities balanced by age, gender, and
ethnicity.
  We further introduce ViTPain, a Vision Transformer based cross-modal
distillation framework in which a heatmap-trained teacher guides a student
trained on RGB images, enhancing accuracy, interpretability, and clinical
reliability. Together, 3DPain and ViTPain establish a controllable, diverse,
and clinically grounded foundation for generalizable automated pain assessment.

</details>


### [60] [Min: Mixture of Noise for Pre-Trained Model-Based Class-Incremental Learning](https://arxiv.org/abs/2509.16738)
*Kai Jiang,Zhengyan Shi,Dell Zhang,Hongyuan Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为Mixture of Noise (Min)的方法，通过在类增量学习中学习有益噪声来缓解预训练模型参数漂移问题，提升模型在持续学习中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的类增量学习方法在微调预训练模型时会导致参数漂移，损害模型的泛化能力。作者认为参数漂移可以被视为一种噪声，但噪声并非总是有害的，适当的噪声可以抑制低相关性特征，为未来任务留出空间。

Method: Min方法基于信息理论指导，从新任务的高维特征中学习任务特定噪声，动态调整不同任务噪声的混合权重，并将有益噪声嵌入中间特征以掩盖无效模式的响应。

Result: 在六个基准数据集上的广泛实验表明，Min在大多数增量设置中达到了最先进的性能，特别是在50步增量设置中表现尤为突出。

Conclusion: 研究表明有益噪声在持续学习中具有显著潜力，能够有效缓解预训练模型在适应新任务时的泛化能力退化问题。

Abstract: Class Incremental Learning (CIL) aims to continuously learn new categories
while retaining the knowledge of old ones. Pre-trained models (PTMs) show
promising capabilities in CIL. However, existing approaches that apply
lightweight fine-tuning to backbones still induce parameter drift, thereby
compromising the generalization capability of pre-trained models. Parameter
drift can be conceptualized as a form of noise that obscures critical patterns
learned for previous tasks. However, recent researches have shown that noise is
not always harmful. For example, the large number of visual patterns learned
from pre-training can be easily abused by a single task, and introducing
appropriate noise can suppress some low-correlation features, thus leaving a
margin for future tasks. To this end, we propose learning beneficial noise for
CIL guided by information theory and propose Mixture of Noise (Min), aiming to
mitigate the degradation of backbone generalization from adapting new tasks.
Specifically, task-specific noise is learned from high-dimension features of
new tasks. Then, a set of weights is adjusted dynamically for optimal mixture
of different task noise. Finally, Min embeds the beneficial noise into the
intermediate features to mask the response of inefficient patterns. Extensive
experiments on six benchmark datasets demonstrate that Min achieves
state-of-the-art performance in most incremental settings, with particularly
outstanding results in 50-steps incremental settings. This shows the
significant potential for beneficial noise in continual learning.

</details>


### [61] [CAMBench-QR : A Structure-Aware Benchmark for Post-Hoc Explanations with QR Understanding](https://arxiv.org/abs/2509.16745)
*Ritabrata Chakraborty,Avijit Dasgupta,Sandeep Chaurasia*

Main category: cs.CV

TL;DR: CAMBench-QR是一个结构感知的基准测试，利用QR码的规范几何结构来评估CAM方法是否在必要子结构上放置显著性，同时避免背景干扰。


<details>
  <summary>Details</summary>
Motivation: 当前视觉解释方法往往看似合理但缺乏结构忠实性，需要一种能够测试方法是否真正关注图像结构特征的评估标准。

Method: 通过合成QR码/非QR码数据，使用精确掩码和受控失真，开发结构感知指标（查找器/时序线质量比、背景泄漏、覆盖AUC、结构距离等），并评估代表性高效CAM方法在零样本和最后块微调两种实用场景下的表现。

Result: 建立了可复现的基准测试框架，提供了训练方案和评估指标，能够有效测试视觉解释方法的结构感知能力。

Conclusion: CAMBench-QR可以作为视觉解释是否真正具有结构感知能力的试金石，为视觉解释方法提供简单可复现的评估标准。

Abstract: Visual explanations are often plausible but not structurally faithful. We
introduce CAMBench-QR, a structure-aware benchmark that leverages the canonical
geometry of QR codes (finder patterns, timing lines, module grid) to test
whether CAM methods place saliency on requisite substructures while avoiding
background. CAMBench-QR synthesizes QR/non-QR data with exact masks and
controlled distortions, and reports structure-aware metrics (Finder/Timing Mass
Ratios, Background Leakage, coverage AUCs, Distance-to-Structure) alongside
causal occlusion, insertion/deletion faithfulness, robustness, and latency. We
benchmark representative, efficient CAMs (LayerCAM, EigenGrad-CAM, XGrad-CAM)
under two practical regimes of zero-shot and last-block fine-tuning. The
benchmark, metrics, and training recipes provide a simple, reproducible
yardstick for structure-aware evaluation of visual explanations. Hence we
propose that CAMBENCH-QR can be used as a litmus test of whether visual
explanations are truly structure-aware.

</details>


### [62] [HyPlaneHead: Rethinking Tri-plane-like Representations in Full-Head Image Synthesis](https://arxiv.org/abs/2509.16748)
*Heyuan Li,Kenkun Liu,Lingteng Qiu,Qi Zuo,Keru Zheng,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出了一种新型混合平面（hy-plane）表示方法，结合平面和球形平面的优点，解决了传统三平面表示中的特征纠缠、特征映射不均匀和特征穿透问题，在头部图像合成任务中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统三平面表示存在特征纠缠导致镜像伪影，球形三平面虽然缓解了特征纠缠但存在特征映射不均匀问题，且两种方法都存在特征穿透问题，限制了其性能潜力。

Method: 提出混合平面表示，结合平面和球形平面的优势；引入近等面积扭曲策略替代传统的theta-phi扭曲，最大化特征图利用率；生成器合成单通道统一特征图而非多通道分离特征图，消除特征穿透。

Result: HyPlaneHead方法在头部图像合成任务中实现了最先进的性能。

Conclusion: 通过系统分析三平面表示的问题并提出创新解决方案，hy-plane表示能够充分发挥三平面方法的潜力，为3D感知GANs提供了更有效的表示方法。

Abstract: Tri-plane-like representations have been widely adopted in 3D-aware GANs for
head image synthesis and other 3D object/scene modeling tasks due to their
efficiency. However, querying features via Cartesian coordinate projection
often leads to feature entanglement, which results in mirroring artifacts. A
recent work, SphereHead, attempted to address this issue by introducing
spherical tri-planes based on a spherical coordinate system. While it
successfully mitigates feature entanglement, SphereHead suffers from uneven
mapping between the square feature maps and the spherical planes, leading to
inefficient feature map utilization during rendering and difficulties in
generating fine image details. Moreover, both tri-plane and spherical tri-plane
representations share a subtle yet persistent issue: feature penetration across
convolutional channels can cause interference between planes, particularly when
one plane dominates the others. These challenges collectively prevent
tri-plane-based methods from reaching their full potential. In this paper, we
systematically analyze these problems for the first time and propose innovative
solutions to address them. Specifically, we introduce a novel hybrid-plane
(hy-plane for short) representation that combines the strengths of both planar
and spherical planes while avoiding their respective drawbacks. We further
enhance the spherical plane by replacing the conventional theta-phi warping
with a novel near-equal-area warping strategy, which maximizes the effective
utilization of the square feature map. In addition, our generator synthesizes a
single-channel unified feature map instead of multiple feature maps in separate
channels, thereby effectively eliminating feature penetration. With a series of
technical improvements, our hy-plane representation enables our method,
HyPlaneHead, to achieve state-of-the-art performance in full-head image
synthesis.

</details>


### [63] [DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation Conditioned on Natural Images](https://arxiv.org/abs/2509.16767)
*Ozgur Kara,Harris Nisar,James M. Rehg*

Main category: cs.CV

TL;DR: DiffEye是一个基于扩散模型的训练框架，用于生成连续且多样化的眼动轨迹，解决了现有方法无法捕捉人类视觉注意力多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有眼动预测模型通常使用离散的注视点序列，丢弃了原始轨迹中的丰富信息，且无法捕捉不同观察者之间的变异性，通常预测固定长度的单一扫描路径。

Method: 提出DiffEye扩散模型，使用视觉刺激作为条件，引入对应位置嵌入(CPE)组件将空间注视信息与视觉输入的基于补丁的语义特征对齐，利用原始眼动轨迹而非扫描路径进行训练。

Result: DiffEye能够生成高质量、真实的眼动模式，在扫描路径生成方面达到最先进性能，并首次实现连续眼动轨迹的生成。

Conclusion: DiffEye是首个在自然图像上使用扩散模型并充分利用原始眼动数据丰富性的方法，生成的轨迹能更准确地反映人类视觉注意力的分布。

Abstract: Numerous models have been developed for scanpath and saliency prediction,
which are typically trained on scanpaths, which model eye movement as a
sequence of discrete fixation points connected by saccades, while the rich
information contained in the raw trajectories is often discarded. Moreover,
most existing approaches fail to capture the variability observed among human
subjects viewing the same image. They generally predict a single scanpath of
fixed, pre-defined length, which conflicts with the inherent diversity and
stochastic nature of real-world visual attention. To address these challenges,
we propose DiffEye, a diffusion-based training framework designed to model
continuous and diverse eye movement trajectories during free viewing of natural
images. Our method builds on a diffusion model conditioned on visual stimuli
and introduces a novel component, namely Corresponding Positional Embedding
(CPE), which aligns spatial gaze information with the patch-based semantic
features of the visual input. By leveraging raw eye-tracking trajectories
rather than relying on scanpaths, DiffEye captures the inherent variability in
human gaze behavior and generates high-quality, realistic eye movement
patterns, despite being trained on a comparatively small dataset. The generated
trajectories can also be converted into scanpaths and saliency maps, resulting
in outputs that more accurately reflect the distribution of human visual
attention. DiffEye is the first method to tackle this task on natural images
using a diffusion model while fully leveraging the richness of raw eye-tracking
data. Our extensive evaluation shows that DiffEye not only achieves
state-of-the-art performance in scanpath generation but also enables, for the
first time, the generation of continuous eye movement trajectories. Project
webpage: https://diff-eye.github.io/

</details>


### [64] [MMPart: Harnessing Multi-Modal Large Language Models for Part-Aware 3D Generation](https://arxiv.org/abs/2509.16768)
*Omid Bonakdar,Nasser Mozayani*

Main category: cs.CV

TL;DR: MMPart是一个从单张图像生成部件感知3D模型的创新框架，通过VLM生成提示词，分阶段生成隔离图像和多视角图像，最终重建为3D模型


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法将目标对象表示为封闭网格，缺乏结构信息，限制了编辑、动画和语义理解。现有部件感知方法存在用户无法控制对象分离方式和遮挡部分想象的问题

Method: 1. 使用VLM基于输入图像和用户描述生成提示词；2. 生成模型根据初始图像和提示词生成每个对象的隔离图像；3. 多视角生成阶段生成一致的多视角图像；4. 重建模型将多视角图像转换为3D模型

Result: 提出了MMPart框架，能够从单张图像生成具有部件结构的3D模型，解决了现有方法在对象分离控制和遮挡部分想象方面的局限性

Conclusion: MMPart框架为生成部件感知3D模型提供了创新解决方案，通过多阶段处理实现了对对象分离和遮挡部分想象的有效控制

Abstract: Generative 3D modeling has advanced rapidly, driven by applications in VR/AR,
metaverse, and robotics. However, most methods represent the target object as a
closed mesh devoid of any structural information, limiting editing, animation,
and semantic understanding. Part-aware 3D generation addresses this problem by
decomposing objects into meaningful components, but existing pipelines face
challenges: in existing methods, the user has no control over which objects are
separated and how model imagine the occluded parts in isolation phase. In this
paper, we introduce MMPart, an innovative framework for generating part-aware
3D models from a single image. We first use a VLM to generate a set of prompts
based on the input image and user descriptions. In the next step, a generative
model generates isolated images of each object based on the initial image and
the previous step's prompts as supervisor (which control the pose and guide
model how imagine previously occluded areas). Each of those images then enters
the multi-view generation stage, where a number of consistent images from
different views are generated. Finally, a reconstruction model converts each of
these multi-view images into a 3D model.

</details>


### [65] [Artificial Satellite Trails Detection Using U-Net Deep Neural Network and Line Segment Detector Algorithm](https://arxiv.org/abs/2509.16771)
*Xiaohan Chen,Hongrui Gu,Cunshi Wang,Haiyang Mu,Jie Zheng,Junju Du,Jing Ren,Zhou Fan,Jing Li*

Main category: cs.CV

TL;DR: 提出了一种结合U-Net深度学习网络和LSD算法的卫星轨迹检测模型，用于解决天文观测中卫星轨迹干扰问题。


<details>
  <summary>Details</summary>
Motivation: 随着人造卫星数量的快速增长，天文成像受到越来越严重的干扰。卫星反射阳光会在测光图像中产生条纹状伪影，这些卫星轨迹会引入虚假源并导致显著的光度测量误差。

Method: 使用U-Net深度神经网络进行图像分割，结合Line Segment Detector (LSD)算法，在375张基于Mini-SiTian阵列数据生成的模拟卫星轨迹图像上进行训练。

Result: 对于信噪比(SNR)大于3的轨迹，检测率超过99%。在Mini-SiTian阵列真实观测数据上，模型召回率达到79.57%，精确率达到74.56%。

Conclusion: 该模型能够有效检测天文图像中的卫星轨迹，为减少卫星干扰对天文观测的影响提供了可行的技术方案。

Abstract: With the rapid increase in the number of artificial satellites, astronomical
imaging is experiencing growing interference. When these satellites reflect
sunlight, they produce streak-like artifacts in photometry images. Such
satellite trails can introduce false sources and cause significant photometric
errors. As a result, accurately identifying the positions of satellite trails
in observational data has become essential. In this work, we propose a
satellite trail detection model that combines the U-Net deep neural network for
image segmentation with the Line Segment Detector (LSD) algorithm. The model is
trained on 375 simulated images of satellite trails, generated using data from
the Mini-SiTian Array. Experimental results show that for trails with a
signal-to-noise ratio (SNR) greater than 3, the detection rate exceeds 99.
Additionally, when applied to real observational data from the Mini-SiTian
Array, the model achieves a recall of 79.57 and a precision of 74.56.

</details>


### [66] [Benchmarking and Mitigating MCQA Selection Bias of Large Vision-Language Models](https://arxiv.org/abs/2509.16805)
*Md. Atabuzzaman,Ali Asgarov,Chris Thomas*

Main category: cs.CV

TL;DR: 本文研究了大型视觉语言模型在多项选择题中的选择偏见问题，提出了推理时logit级别的去偏见方法，无需重新训练即可有效减轻偏见并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然LVLMs在视觉问答任务中表现优异，但多项选择题中的选择偏见问题（如模型偏好特定选项标记或位置）尚未得到充分研究。

Method: 提出推理时logit级别的去偏见方法，通过通用和上下文提示估计集成偏见向量，并应用置信度自适应校正来修正模型输出。

Result: 实验表明LVLMs存在一致的选择偏见，且随任务难度增加而加剧；所提出的去偏见方法显著减少了偏见，并在挑战性设置中提高了准确率。

Conclusion: 这项工作揭示了LVLMs在MCQA中的局限性，并提出了一种实用的方法来提高其在细粒度视觉推理中的鲁棒性。

Abstract: Large Vision-Language Models (LVLMs) have achieved strong performance on
vision-language tasks, particularly Visual Question Answering (VQA). While
prior work has explored unimodal biases in VQA, the problem of selection bias
in Multiple-Choice Question Answering (MCQA), where models may favor specific
option tokens (e.g., "A") or positions, remains underexplored. In this paper,
we investigate both the presence and nature of selection bias in LVLMs through
fine-grained MCQA benchmarks spanning easy, medium, and hard difficulty levels,
defined by the semantic similarity of the options. We further propose an
inference-time logit-level debiasing method that estimates an ensemble bias
vector from general and contextual prompts and applies confidence-adaptive
corrections to the model's output. Our method mitigates bias without retraining
and is compatible with frozen LVLMs. Extensive experiments across several
state-of-the-art models reveal consistent selection biases that intensify with
task difficulty, and show that our mitigation approach significantly reduces
bias while improving accuracy in challenging settings. This work offers new
insights into the limitations of LVLMs in MCQA and presents a practical
approach to improve their robustness in fine-grained visual reasoning. Datasets
and code are available at:
https://github.com/Atabuzzaman/Selection-Bias-of-LVLMs

</details>


### [67] [MedGS: Gaussian Splatting for Multi-Modal 3D Medical Imaging](https://arxiv.org/abs/2509.16806)
*Kacper Marzol,Ignacy Kolton,Weronika Smolak-Dyżewska,Joanna Kaleta,Marcin Mazur,Przemysław Spurek*

Main category: cs.CV

TL;DR: MedGS是一个半监督神经隐式表面重建框架，使用高斯泼溅插值机制处理多模态3D医学影像数据，实现高效的帧间插值和高质量表面重建。


<details>
  <summary>Details</summary>
Motivation: 传统方法在医学影像表面重建和帧间插值中面临图像噪声和帧间信息不完整的限制，需要更鲁棒的解决方案。

Method: 将医学影像数据表示为3D空间中连续的2D帧，使用基于高斯分布的建模方法，结合高斯泼溅插值机制进行半监督学习。

Result: MedGS比传统神经隐式方法训练更高效，具有更好的噪声鲁棒性，支持灵活编辑，并能精确建模复杂解剖结构且减少伪影。

Conclusion: MedGS框架非常适合医学影像中的可扩展和实际应用，为多模态3D医学成像提供了有效的表面重建解决方案。

Abstract: Multi-modal three-dimensional (3D) medical imaging data, derived from
ultrasound, magnetic resonance imaging (MRI), and potentially computed
tomography (CT), provide a widely adopted approach for non-invasive anatomical
visualization. Accurate modeling, registration, and visualization in this
setting depend on surface reconstruction and frame-to-frame interpolation.
Traditional methods often face limitations due to image noise and incomplete
information between frames. To address these challenges, we present MedGS, a
semi-supervised neural implicit surface reconstruction framework that employs a
Gaussian Splatting (GS)-based interpolation mechanism. In this framework,
medical imaging data are represented as consecutive two-dimensional (2D) frames
embedded in 3D space and modeled using Gaussian-based distributions. This
representation enables robust frame interpolation and high-fidelity surface
reconstruction across imaging modalities. As a result, MedGS offers more
efficient training than traditional neural implicit methods. Its explicit
GS-based representation enhances noise robustness, allows flexible editing, and
supports precise modeling of complex anatomical structures with fewer
artifacts. These features make MedGS highly suitable for scalable and practical
applications in medical imaging.

</details>


### [68] [Looking in the mirror: A faithful counterfactual explanation method for interpreting deep image classification models](https://arxiv.org/abs/2509.16822)
*Townim Faisal Chowdhury,Vu Minh Hieu Phan,Kewen Liao,Nanyu Dong,Minh-Son To,Anton Hengel,Johan Verjans,Zhibin Liao*

Main category: cs.CV

TL;DR: 提出了一种名为Mirror-CFE的新方法，直接在分类器的特征空间中生成反事实解释，将决策边界视为"镜子"来反射特征表示，无需依赖额外的图像编码器和生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有的反事实解释方法通常依赖额外的图像编码器和生成模型来创建合理的图像，但忽略了分类器自身的特征空间和决策边界，无法解释分类器学习的内在特征空间和决策边界。

Method: Mirror-CFE方法直接在分类器特征空间中操作，将决策边界作为镜子反射特征表示，学习从特征空间到图像空间的映射函数，同时保持距离关系，实现源图像与其反事实之间的平滑过渡。

Result: 在四个图像数据集上的实验表明，Mirror-CFE在保持输入相似性的同时，在有效性方面优于最先进的解释方法。

Conclusion: Mirror-CFE通过生成逐步过渡的可视化，揭示了特征如何随着分类置信度变化而演化，为分类器的决策过程提供了可解释的可视化。

Abstract: Counterfactual explanations (CFE) for deep image classifiers aim to reveal
how minimal input changes lead to different model decisions, providing critical
insights for model interpretation and improvement. However, existing CFE
methods often rely on additional image encoders and generative models to create
plausible images, neglecting the classifier's own feature space and decision
boundaries. As such, they do not explain the intrinsic feature space and
decision boundaries learned by the classifier. To address this limitation, we
propose Mirror-CFE, a novel method that generates faithful counterfactual
explanations by operating directly in the classifier's feature space, treating
decision boundaries as mirrors that ``reflect'' feature representations in the
mirror. Mirror-CFE learns a mapping function from feature space to image space
while preserving distance relationships, enabling smooth transitions between
source images and their counterfactuals. Through extensive experiments on four
image datasets, we demonstrate that Mirror-CFE achieves superior performance in
validity while maintaining input resemblance compared to state-of-the-art
explanation methods. Finally, mirror-CFE provides interpretable visualization
of the classifier's decision process by generating step-wise transitions that
reveal how features evolve as classification confidence changes.

</details>


### [69] [L2M-Reg: Building-level Uncertainty-aware Registration of Outdoor LiDAR Point Clouds and Semantic 3D City Models](https://arxiv.org/abs/2509.16832)
*Ziyang Xu,Benedikt Schwab,Yihui Yang,Thomas H. Kolbe,Christoph Holst*

Main category: cs.CV

TL;DR: L2M-Reg是一种基于平面的精细配准方法，专门解决LiDAR点云与语义3D城市模型在LoD2级别下的配准问题，通过考虑模型不确定性来提高配准精度。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云与语义3D城市模型的精确配准是城市数字孪生的基础，但在LoD2级别的单个建筑层面实现精确配准仍具挑战性，主要由于语义3D城市模型存在泛化不确定性。

Method: L2M-Reg包含三个关键步骤：建立可靠的平面对应关系、构建伪平面约束的Gauss-Helmert模型、自适应估计垂直平移。

Result: 在三个真实世界数据集上的实验表明，L2M-Reg比现有的基于ICP和基于平面的方法更准确且计算效率更高。

Conclusion: L2M-Reg为存在模型不确定性的LiDAR到模型配准问题提供了一种新颖的建筑级解决方案。

Abstract: Accurate registration between LiDAR (Light Detection and Ranging) point
clouds and semantic 3D city models is a fundamental topic in urban digital
twinning and a prerequisite for downstream tasks, such as digital construction,
change detection and model refinement. However, achieving accurate
LiDAR-to-Model registration at individual building level remains challenging,
particularly due to the generalization uncertainty in semantic 3D city models
at the Level of Detail 2 (LoD2). This paper addresses this gap by proposing
L2M-Reg, a plane-based fine registration method that explicitly accounts for
model uncertainty. L2M-Reg consists of three key steps: establishing reliable
plane correspondence, building a pseudo-plane-constrained Gauss-Helmert model,
and adaptively estimating vertical translation. Experiments on three real-world
datasets demonstrate that L2M-Reg is both more accurate and computationally
efficient than existing ICP-based and plane-based methods. Overall, L2M-Reg
provides a novel building-level solution regarding LiDAR-to-Model registration
when model uncertainty is present.

</details>


### [70] [ISCS: Parameter-Guided Channel Ordering and Grouping for Learned Image Compression](https://arxiv.org/abs/2509.16853)
*Jinhao Wang,Cihan Ruan,Nam Ling,Wei Wang,Wei Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于预训练VAE的LIC模型中识别和组织重要通道的通用方法，通过参数统计特性来估计通道重要性，发现了不变显著通道空间(ISCS)结构，并基于此设计了确定性通道排序和分组策略，实现了切片并行解码，提高了编码效率。


<details>
  <summary>Details</summary>
Motivation: 现有LIC研究中发现只有少量潜在通道对重建至关重要，但现有方法依赖昂贵的数据集特定消融测试且孤立分析通道，忽略了通道间的相互依赖关系。

Method: 利用权重方差、偏置大小和成对相关性等内在参数统计特性来估计通道重要性，识别出ISCS结构（显著核心通道和显著辅助通道），并基于此设计确定性通道排序和分组策略。

Result: 在多个LIC架构上的实验表明，该方法有效降低了比特率和计算量，同时保持了重建质量。

Conclusion: 该方法为现有学习压缩框架提供了一种实用且模块化的增强方案，提高了编码和计算效率。

Abstract: Prior studies in learned image compression (LIC) consistently show that only
a small subset of latent channels is critical for reconstruction, while many
others carry limited information. Exploiting this imbalance could improve both
coding and computational efficiency, yet existing approaches often rely on
costly, dataset-specific ablation tests and typically analyze channels in
isolation, ignoring their interdependencies.
  We propose a generalizable, dataset-agnostic method to identify and organize
important channels in pretrained VAE-based LIC models. Instead of brute-force
empirical evaluations, our approach leverages intrinsic parameter
statistics-weight variances, bias magnitudes, and pairwise correlations-to
estimate channel importance. This analysis reveals a consistent organizational
structure, termed the Invariant Salient Channel Space (ISCS), where
Salient-Core channels capture dominant structures and Salient-Auxiliary
channels provide complementary details. Building on ISCS, we introduce a
deterministic channel ordering and grouping strategy that enables
slice-parallel decoding, reduces redundancy, and improves bitrate efficiency.
  Experiments across multiple LIC architectures demonstrate that our method
effectively reduces bitrate and computation while maintaining reconstruction
quality, providing a practical and modular enhancement to existing learned
compression frameworks.

</details>


### [71] [ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM](https://arxiv.org/abs/2509.16863)
*Amanuel T. Dufera,Yuan-Li Cai*

Main category: cs.CV

TL;DR: ConfidentSplat是一个基于3D高斯泼溅的RGB-only SLAM系统，通过置信度加权融合机制结合多视角几何和单目深度先验，解决现有方法因不可靠深度估计导致的几何不准确问题。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-only 3DGS SLAM方法由于深度估计不可靠导致几何精度不足，需要一种能够自适应融合多源深度信息的方法来提高重建质量。

Method: 采用置信度加权融合机制，动态整合多视角几何深度和Omnidata ViT单目深度先验，基于多视角几何一致性生成可靠性估计，指导可变形3D高斯泼溅地图的优化。

Result: 在TUM-RGBD、ScanNet等标准基准测试和移动数据集上，相比基线方法在重建精度（L1深度误差）和新视角合成质量（PSNR、SSIM、LPIPS）方面均有显著提升。

Conclusion: 置信度感知的传感器融合方法能够有效推进密集视觉SLAM技术的先进水平，特别是在挑战性条件下表现优异。

Abstract: We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM
system for robust, highfidelity RGB-only reconstruction. Addressing geometric
inaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable
depth estimation, ConfidentSplat incorporates a core innovation: a
confidence-weighted fusion mechanism. This mechanism adaptively integrates
depth cues from multiview geometry with learned monocular priors (Omnidata
ViT), dynamically weighting their contributions based on explicit reliability
estimates-derived predominantly from multi-view geometric consistency-to
generate high-fidelity proxy depth for map supervision. The resulting proxy
depth guides the optimization of a deformable 3DGS map, which efficiently
adapts online to maintain global consistency following pose updates from a
DROID-SLAM-inspired frontend and backend optimizations (loop closure, global
bundle adjustment). Extensive validation on standard benchmarks (TUM-RGBD,
ScanNet) and diverse custom mobile datasets demonstrates significant
improvements in reconstruction accuracy (L1 depth error) and novel view
synthesis fidelity (PSNR, SSIM, LPIPS) over baselines, particularly in
challenging conditions. ConfidentSplat underscores the efficacy of principled,
confidence-aware sensor fusion for advancing state-of-the-art dense visual
SLAM.

</details>


### [72] [$\mathtt{M^3VIR}$: A Large-Scale Multi-Modality Multi-View Synthesized Benchmark Dataset for Image Restoration and Content Creation](https://arxiv.org/abs/2509.16873)
*Yuanzhi Li,Lebin Zhou,Nam Ling,Zhenghao Chen,Wei Wang,Wei Jiang*

Main category: cs.CV

TL;DR: 该论文提出了M³VIR数据集，这是一个专为游戏和娱乐行业设计的大规模多模态多视图数据集，用于解决现有数据集在捕捉游戏内容独特特征方面的局限性，并为可控视频生成提供基准。


<details>
  <summary>Details</summary>
Motivation: 现有数据集通常局限于特定领域或依赖人工降质，无法准确捕捉游戏内容的独特特征，且缺乏可控视频生成的基准。

Method: 使用Unreal Engine 5渲染80个场景的8个类别，提供高保真度的LR-HR配对和多视图帧，包括M³VIR_MR用于超分辨率、新视图合成及组合任务，以及M³VIR_MS用于可控视频生成研究。

Result: 建立了多个最先进的超分辨率和新视图合成方法的性能基准，并为可控视频生成领域提供了首个多风格、对象级真实数据集。

Conclusion: 通过发布M³VIR数据集，旨在促进下一代云游戏和娱乐中AI驱动的恢复、压缩和可控内容生成的研究。

Abstract: The gaming and entertainment industry is rapidly evolving, driven by
immersive experiences and the integration of generative AI (GAI) technologies.
Training such models effectively requires large-scale datasets that capture the
diversity and context of gaming environments. However, existing datasets are
often limited to specific domains or rely on artificial degradations, which do
not accurately capture the unique characteristics of gaming content. Moreover,
benchmarks for controllable video generation remain absent.
  To address these limitations, we introduce $\mathtt{M^3VIR}$, a large-scale,
multi-modal, multi-view dataset specifically designed to overcome the
shortcomings of current resources. Unlike existing datasets, $\mathtt{M^3VIR}$
provides diverse, high-fidelity gaming content rendered with Unreal Engine 5,
offering authentic ground-truth LR-HR paired and multi-view frames across 80
scenes in 8 categories. It includes $\mathtt{M^3VIR\_MR}$ for super-resolution
(SR), novel view synthesis (NVS), and combined NVS+SR tasks, and
$\mathtt{M^3VIR\_{MS}}$, the first multi-style, object-level ground-truth set
enabling research on controlled video generation. Additionally, we benchmark
several state-of-the-art SR and NVS methods to establish performance baselines.
While no existing approaches directly handle controlled video generation,
$\mathtt{M^3VIR}$ provides a benchmark for advancing this area. By releasing
the dataset, we aim to facilitate research in AI-powered restoration,
compression, and controllable content generation for next-generation cloud
gaming and entertainment.

</details>


### [73] [SAM-DCE: Addressing Token Uniformity and Semantic Over-Smoothing in Medical Segmentation](https://arxiv.org/abs/2509.16886)
*Yingzhen Hu,Yiheng Zhong,Ruobing Li,Yingxue Su,Jiabao An,Feilong Tang,Jionglong Su,Imran Razzak*

Main category: cs.CV

TL;DR: SAM-DCE是一种改进的医学图像分割方法，通过平衡局部判别性和全局语义，解决SAM模型在医学图像中遇到的领域偏移、解剖变异性和语义过平滑问题。


<details>
  <summary>Details</summary>
Motivation: SAM模型在自然图像上表现出强大的零样本分割能力，但在医学图像中由于领域偏移、解剖变异性和对用户提示的依赖而遇到困难。现有的无提示适应方法虽然减少了专家干预，但仍存在鲁棒性和适应性有限的问题。

Method: 提出SAM-DCE方法，通过平衡局部判别性和全局语义，缓解标记均匀性问题，增强类间可分性，并通过细粒度一致的表示丰富掩码解码过程。

Result: 在多个医学基准测试上的广泛实验验证了该方法的有效性。

Conclusion: SAM-DCE能够有效提升SAM模型在医学图像分割中的性能，解决了语义过平滑和标记均匀性问题。

Abstract: The Segment Anything Model (SAM) demonstrates impressive zero-shot
segmentation ability on natural images but encounters difficulties in medical
imaging due to domain shifts, anatomical variability, and its reliance on
user-provided prompts. Recent prompt-free adaptations alleviate the need for
expert intervention, yet still suffer from limited robustness and adaptability,
often overlooking the issues of semantic over-smoothing and token uniformity.
We propose SAM-DCE, which balances local discrimination and global semantics
while mitigating token uniformity, enhancing inter-class separability, and
enriching mask decoding with fine-grained, consistent representations.
Extensive experiments on diverse medical benchmarks validate its effectiveness.

</details>


### [74] [Rethinking Evaluation of Infrared Small Target Detection](https://arxiv.org/abs/2509.16888)
*Youwei Pang,Xiaoqi Zhao,Lihe Zhang,Huchuan Lu,Georges El Fakhri,Xiaofeng Liu,Shijian Lu*

Main category: cs.CV

TL;DR: 本文针对红外小目标检测领域现有评估方法的局限性，提出了混合级度量、系统误差分析和跨数据集评估的新框架。


<details>
  <summary>Details</summary>
Motivation: 当前红外小目标检测的评估方法存在三个主要问题：1）使用碎片化的像素级和目标级指标，无法全面评估模型能力；2）过度关注整体性能分数，忽视关键误差分析；3）采用数据集特定的训练测试范式，阻碍了对模型鲁棒性和泛化能力的理解。

Method: 提出混合级度量方法，结合像素级和目标级性能；开发系统误差分析方法；强调跨数据集评估的重要性。

Result: 建立了一个更全面、合理的分层分析框架，并发布了开源工具包以促进标准化基准测试。

Conclusion: 该研究旨在推动开发更有效、更鲁棒的红外小目标检测模型，为领域提供更科学的评估标准。

Abstract: As an essential vision task, infrared small target detection (IRSTD) has seen
significant advancements through deep learning. However, critical limitations
in current evaluation protocols impede further progress. First, existing
methods rely on fragmented pixel- and target-level specific metrics, which
fails to provide a comprehensive view of model capabilities. Second, an
excessive emphasis on overall performance scores obscures crucial error
analysis, which is vital for identifying failure modes and improving real-world
system performance. Third, the field predominantly adopts dataset-specific
training-testing paradigms, hindering the understanding of model robustness and
generalization across diverse infrared scenarios. This paper addresses these
issues by introducing a hybrid-level metric incorporating pixel- and
target-level performance, proposing a systematic error analysis method, and
emphasizing the importance of cross-dataset evaluation. These aim to offer a
more thorough and rational hierarchical analysis framework, ultimately
fostering the development of more effective and robust IRSTD models. An
open-source toolkit has be released to facilitate standardized benchmarking.

</details>


### [75] [Learning from Gene Names, Expression Values and Images: Contrastive Masked Text-Image Pretraining for Spatial Transcriptomics Representation Learning](https://arxiv.org/abs/2509.16892)
*Jiahe Qian,Yaoyu Fang,Ziqiao Weng,Xinkun Wang,Lee A. Cooper,Bo Zhou*

Main category: cs.CV

TL;DR: CoMTIP是一个用于空间转录组学的对比掩码文本-图像预训练框架，通过联合学习图像、基因名称和表达值来获得更好的跨模态表示，在多种下游任务上超越现有方法并实现零样本基因表达预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅使用基因名称或表达值，缺乏基因语义信息和数值关联，且仅依赖图像-文本对齐监督，忽略了重要的视觉上下文信息。

Method: 提出CoMTIP框架：视觉分支使用掩码特征建模重建遮挡图像块；文本分支使用可扩展的基因-文本编码器并行处理所有基因句子，采用配对感知对抗训练保持基因-值关联；在共享的InfoNCE优化空间中对齐图像和文本表示。

Result: 在公共空间转录组学数据集上的实验表明，CoMTIP在多种下游任务上超越先前方法，并实现了现有方法不具备的零样本基因表达预测能力。

Conclusion: CoMTIP通过联合学习图像、基因名称和表达值，同时捕获细粒度视觉上下文，为空间转录组学提供了更强大的跨模态预训练框架。

Abstract: Spatial transcriptomics aims to connect high-resolution histology images with
spatially resolved gene expression. To achieve better performance on downstream
tasks such as gene expression prediction, large-scale pre-training is required
to obtain generalisable representations that can bridge histology and
transcriptomics across tissues, protocols, and laboratories. Existing
cross-modal pre-training approaches for spatial transcriptomics rely on either
gene names or expression values in isolation, which strips the gene branch of
essential semantics and breaks the association between each gene and its
quantitative magnitude. In addition, by restricting supervision to image-text
alignment, these methods ignore intrinsic visual cues that are critical for
learning robust image features. We present CoMTIP, the first Contrastive Masked
Text-Image Pretraining framework that jointly learns from images, gene names,
and expression values while capturing fine-grained visual context for spatial
transcriptomics. The vision branch uses Masked Feature Modeling to reconstruct
occluded patches and learn context-aware image embeddings. The text branch
applies a scalable Gene-Text Encoder that processes all gene sentences in
parallel, enriches each gene and its numerical value with dedicated embeddings,
and employs Pair-aware Adversarial Training (PAAT) to preserve correct
gene-value associations. Image and text representations are aligned in a shared
InfoNCE-optimised space. Experiments on public spatial transcriptomics datasets
show that CoMTIP not only surpasses previous methods on diverse downstream
tasks but also achieves zero-shot gene expression prediction, a capability that
existing approaches do not provide.

</details>


### [76] [PRISM: Precision-Recall Informed Data-Free Knowledge Distillation via Generative Diffusion](https://arxiv.org/abs/2509.16897)
*Xuewan He,Jielei Wang,Zihan Cheng,Yuchen Su,Shiyue Huang,Guoming Lu*

Main category: cs.CV

TL;DR: PRISM是一种基于精度-召回率的数据自由知识蒸馏方法，通过能量引导分布对齐和多样化提示工程解决大规模图像合成中的模式崩溃问题


<details>
  <summary>Details</summary>
Motivation: 现有数据自由知识蒸馏方法在小规模图像上表现良好，但在大规模图像合成时会出现模式崩溃，导致知识转移受限。直接使用现成扩散模型生成数据集面临精度-召回率挑战

Method: 提出PRISM方法：1）能量引导分布对齐避免生成分布外样本；2）多样化提示工程增强对真实分布流形的覆盖

Result: 在多个大规模图像数据集上的实验证明了PRISM的优越性，使用PRISM训练的模型展现出强大的领域泛化能力

Conclusion: PRISM通过解决精度-召回率挑战，有效提升了数据自由知识蒸馏在大规模图像上的性能

Abstract: Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to
a student without access to the real in-distribution (ID) data. While existing
methods perform well on small-scale images, they suffer from mode collapse when
synthesizing large-scale images, resulting in limited knowledge transfer.
Recently, leveraging advanced generative models to synthesize photorealistic
images has emerged as a promising alternative. Nevertheless, directly using
off-the-shelf diffusion to generate datasets faces the precision-recall
challenges: 1) ensuring synthetic data aligns with the real distribution, and
2) ensuring coverage of the real ID manifold. In response, we propose PRISM, a
precision-recall informed synthesis method. Specifically, we introduce
Energy-guided Distribution Alignment to avoid the generation of
out-of-distribution samples, and design the Diversified Prompt Engineering to
enhance coverage of the real ID manifold. Extensive experiments on various
large-scale image datasets demonstrate the superiority of PRISM. Moreover, we
demonstrate that models trained with PRISM exhibit strong domain
generalization.

</details>


### [77] [ME-Mamba: Multi-Expert Mamba with Efficient Knowledge Capture and Fusion for Multimodal Survival Analysis](https://arxiv.org/abs/2509.16900)
*Chengsheng Zhang,Linhao Qu,Xiaoyu Liu,Zhijian Song*

Main category: cs.CV

TL;DR: 提出了ME-Mamba系统，通过多专家架构整合病理图像和基因组数据，实现高效的癌症生存分析，在TCGA数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 病理图像通常只有幻灯片级标签，阻碍了从千兆像素WSI中学习判别性表征。多模态生存分析整合病理图像和基因组数据成为有前景的方法

Method: 设计三个专家模块：病理专家和基因组专家分别处理单模态数据，采用Mamba架构；协同专家通过最优传输学习token级局部对应关系，使用最大均值差异进行全局跨模态融合

Result: 在TCGA的五个数据集上进行了广泛实验，证明了方法的先进性能

Conclusion: 通过病理专家、基因组专家和协同专家的协作，该方法实现了稳定准确的生存分析，且计算复杂度相对较低

Abstract: Survival analysis using whole-slide images (WSIs) is crucial in cancer
research. Despite significant successes, pathology images typically only
provide slide-level labels, which hinders the learning of discriminative
representations from gigapixel WSIs. With the rapid advancement of
high-throughput sequencing technologies, multimodal survival analysis
integrating pathology images and genomics data has emerged as a promising
approach. We propose a Multi-Expert Mamba (ME-Mamba) system that captures
discriminative pathological and genomic features while enabling efficient
integration of both modalities. This approach achieves complementary
information fusion without losing critical information from individual
modalities, thereby facilitating accurate cancer survival analysis.
Specifically, we first introduce a Pathology Expert and a Genomics Expert to
process unimodal data separately. Both experts are designed with Mamba
architectures that incorporate conventional scanning and attention-based
scanning mechanisms, allowing them to extract discriminative features from long
instance sequences containing substantial redundant or irrelevant information.
Second, we design a Synergistic Expert responsible for modality fusion. It
explicitly learns token-level local correspondences between the two modalities
via Optimal Transport, and implicitly enhances distribution consistency through
a global cross-modal fusion loss based on Maximum Mean Discrepancy. The fused
feature representations are then passed to a mamba backbone for further
integration. Through the collaboration of the Pathology Expert, Genomics
Expert, and Synergistic Expert, our method achieves stable and accurate
survival analysis with relatively low computational complexity. Extensive
experimental results on five datasets in The Cancer Genome Atlas (TCGA)
demonstrate our state-of-the-art performance.

</details>


### [78] [SLAM-Former: Putting SLAM into One Transformer](https://arxiv.org/abs/2509.16909)
*Yijun Yuan,Zhuoguang Chen,Kenan Li,Weibang Wang,Hang Zhao*

Main category: cs.CV

TL;DR: SLAM-Former是一个基于transformer的神经SLAM方法，将完整的SLAM功能集成到单一transformer中，包含前端实时处理和后台全局优化，在密集SLAM任务中达到或超越最先进方法。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM系统通常需要复杂的模块化设计，作者希望开发一个统一的神经方法，将SLAM的前端和后台功能集成到单一transformer架构中，简化系统设计并提升性能。

Method: 采用transformer架构，包含前端和后台两个部分：前端实时处理单目图像序列进行增量建图和跟踪，后台进行全局优化确保几何一致性，两者交替执行相互促进。

Result: 综合实验结果表明，SLAM-Former在密集SLAM任务中实现了优于或与最先进方法相竞争的性能表现。

Conclusion: SLAM-Former成功展示了将完整SLAM功能集成到单一transformer中的可行性，为神经SLAM系统提供了新的设计思路，在保持实时性的同时实现了高质量的建图和跟踪效果。

Abstract: We present SLAM-Former, a novel neural approach that integrates full SLAM
capabilities into a single transformer. Similar to traditional SLAM systems,
SLAM-Former comprises both a frontend and a backend that operate in tandem. The
frontend processes sequential monocular images in real-time for incremental
mapping and tracking, while the backend performs global refinement to ensure a
geometrically consistent result. This alternating execution allows the frontend
and backend to mutually promote one another, enhancing overall system
performance. Comprehensive experimental results demonstrate that SLAM-Former
achieves superior or highly competitive performance compared to
state-of-the-art dense SLAM methods.

</details>


### [79] [Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification](https://arxiv.org/abs/2509.16935)
*Lavish Ramchandani,Gunjan Deotale,Dev Kumar Das*

Main category: cs.CV

TL;DR: 本文研究使用大型视觉基础模型（Virchow、Virchow2和UNI）结合LoRA参数高效微调方法，用于非典型有丝分裂（AMFs）分类，在MIDOG 2025挑战中取得88.37%的平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 非典型有丝分裂（AMFs）的检测对肿瘤侵袭性和预后评估至关重要，但由于形态学特征细微、类别不平衡和病理学家间观察差异等因素，其检测仍面临重大挑战。

Method: 采用大型视觉基础模型（Virchow、Virchow2、UNI）结合低秩适应（LoRA）进行参数高效微调，通过不同LoRA秩和随机/分组数据分割进行广泛实验，分析模型在不同条件下的鲁棒性。

Result: 最佳方法（Virchow模型+LoRA秩8+三折交叉验证集成）在初步测试集上达到88.37%的平衡准确率，在挑战排行榜中并列第9名。

Conclusion: 结果表明基础模型结合高效适应策略在非典型有丝分裂分类中具有潜力，但需要在特异性和领域泛化方面进一步改进。

Abstract: Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated
with tumor aggressiveness and poor prognosis. Their detection remains a
significant challenge due to subtle morphological cues, class imbalance, and
inter-observer variability among pathologists. The MIDOG 2025 challenge
introduced a dedicated track for atypical mitosis classification, enabling
systematic evaluation of deep learning methods. In this study, we investigated
the use of large vision foundation models, including Virchow, Virchow2, and
UNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We
conducted extensive experiments with different LoRA ranks, as well as random
and group-based data splits, to analyze robustness under varied conditions. Our
best approach, Virchow with LoRA rank 8 and ensemble of three-fold
cross-validation, achieved a balanced accuracy of 88.37% on the preliminary
test set, ranking joint 9th in the challenge leaderboard. These results
highlight the promise of foundation models with efficient adaptation strategies
for the classification of atypical mitosis, while underscoring the need for
improvements in specificity and domain generalization.

</details>


### [80] [Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2509.16942)
*Bin Wang,Fei Deng,Zeyu Chen,Zhicheng Yu,Yiguang Liu*

Main category: cs.CV

TL;DR: ProSFDA是一个原型引导的无源域自适应框架，用于遥感图像语义分割，通过原型加权伪标签和原型对比策略解决伪标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 解决无源域自适应中由于目标域缺乏真实标签导致的伪标签噪声问题，这种噪声阻碍了有效缓解域偏移。

Method: 采用原型加权伪标签进行可靠的自训练，并引入原型对比策略促进同类特征的聚合，从而在没有真实监督的情况下学习判别性目标域表示。

Result: 大量实验表明，该方法显著优于现有方法。

Conclusion: ProSFDA框架通过原型引导机制有效解决了SFDA中的伪标签噪声问题，提升了遥感图像语义分割的域自适应性能。

Abstract: Source-Free Domain Adaptation (SFDA) enables domain adaptation for semantic
segmentation of Remote Sensing Images (RSIs) using only a well-trained source
model and unlabeled target domain data. However, the lack of ground-truth
labels in the target domain often leads to the generation of noisy
pseudo-labels. Such noise impedes the effective mitigation of domain shift
(DS). To address this challenge, we propose ProSFDA, a prototype-guided SFDA
framework. It employs prototype-weighted pseudo-labels to facilitate reliable
self-training (ST) under pseudo-labels noise. We, in addition, introduce a
prototype-contrast strategy that encourages the aggregation of features
belonging to the same class, enabling the model to learn discriminative target
domain representations without relying on ground-truth supervision. Extensive
experiments show that our approach substantially outperforms existing methods.

</details>


### [81] [Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception](https://arxiv.org/abs/2509.16944)
*Yuheng Shi,Xiaohuan Pei,Minjing Dong,Chang Xu*

Main category: cs.CV

TL;DR: 提出了一种自蒸馏区域建议网络（SD-RPN），通过将MLLM中间层的噪声注意力图转化为高质量伪RoI标签，训练轻量级RPN来实现高效、无需标注的细粒度视觉感知。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型需要高分辨率视觉信息进行细粒度感知，但处理全分辨率图像计算成本高。现有RoI方法面临两难选择：基于训练的方法依赖大规模标注数据，而无需训练的方法计算效率低且准确性差。

Method: 构建SD-RPN管道，将MLLM中间层的噪声注意力图去噪并解决歧义，转化为高质量伪RoI标签，用于训练轻量级RPN。RPN通过单次前向传播预测RoI，解耦RoI识别与自回归生成。

Result: 在LLaVA-1.5架构上集成SD-RPN，仅使用少量（如10K）问答对训练，在TextVQA、DocVQA和V-Star等未见基准上实现超过10%的绝对准确率提升。

Conclusion: SD-RPN为增强MLLM的细粒度感知提供了实用且可扩展的解决方案，无需昂贵的监督或全模型微调。

Abstract: Multimodal Large Language Models (MLLMs) require high-resolution visual
information to perform fine-grained perception, yet processing entire
high-resolution images is computationally prohibitive. While recent methods
leverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they
typically present a difficult trade-off: training-based approaches depend on
large-scale annotated datasets, while training-free methods that utilize the
model's internal attention are computationally inefficient and less accurate,
requiring either multi-pass prefill stages or reliance on the slow
auto-regressive decoding process. In this paper, we propose an efficient,
annotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves
this trade-off. The SD-RPN is built around a pipeline that transforms the noisy
attention maps from the MLLM's middle layers into high-quality pseudo-RoI
labels by explicitly denoising the signal and resolving ambiguity. We use these
labels to train a lightweight Region Proposal Network (RPN) that learns a more
precise localization. This RPN is also highly efficient, predicting the RoI in
a single forward pass using features from the MLLM's middle layers, decoupling
RoI identification from the auto-regressive generation and avoiding costly
multi-pass operations.To validate our approach, we integrate the framework into
the LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K)
question-answer pairs, our method demonstrates exceptional data efficiency and
generalization, achieving over a 10% absolute accuracy improvement on unseen
benchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a
practical and scalable solution for enhancing the fine-grained perception of
MLLMs without requiring costly supervision or full model fine-tuning. Code is
available at https://github.com/YuHengsss/SD-RPN.

</details>


### [82] [Leveraging RGB Images for Pre-Training of Event-Based Hand Pose Estimation](https://arxiv.org/abs/2509.16949)
*Ruicong Liu,Takehiko Ohkawa,Tze Ho Elden Tse,Mingfang Zhang,Angela Yao,Yoichi Sato*

Main category: cs.CV

TL;DR: RPEP是首个基于事件的3D手部姿态估计预训练方法，利用标记的RGB图像和未配对的未标记事件数据，通过分解手部运动为逐步动作和运动反转约束来生成更真实的事件数据。


<details>
  <summary>Details</summary>
Motivation: 事件数据具有高时间分辨率和低延迟的优势，但在手部姿态估计中的应用受限于标记训练数据的稀缺。现有伪事件生成技术假设物体静止，难以处理动态移动的手部。

Method: RPEP通过构建伪事件-RGB对，将手部运动分解为较小的逐步动作来捕捉关节的时间变化，并引入运动反转约束来正则化事件生成。

Result: 预训练模型在真实事件数据上显著优于最先进方法，在EvRealHands上实现高达24%的改进，且只需少量标记样本进行微调即可获得强性能。

Conclusion: RPEP方法有效解决了事件数据标记稀缺的问题，为基于事件的手部姿态估计提供了实用的预训练解决方案。

Abstract: This paper presents RPEP, the first pre-training method for event-based 3D
hand pose estimation using labeled RGB images and unpaired, unlabeled event
data. Event data offer significant benefits such as high temporal resolution
and low latency, but their application to hand pose estimation is still limited
by the scarcity of labeled training data. To address this, we repurpose real
RGB datasets to train event-based estimators. This is done by constructing
pseudo-event-RGB pairs, where event data is generated and aligned with the
ground-truth poses of RGB images. Unfortunately, existing pseudo-event
generation techniques assume stationary objects, thus struggling to handle
non-stationary, dynamically moving hands. To overcome this, RPEP introduces a
novel generation strategy that decomposes hand movements into smaller,
step-by-step motions. This decomposition allows our method to capture temporal
changes in articulation, constructing more realistic event data for a moving
hand. Additionally, RPEP imposes a motion reversal constraint, regularizing
event generation using reversed motion. Extensive experiments show that our
pre-trained model significantly outperforms state-of-the-art methods on real
event data, achieving up to 24% improvement on EvRealHands. Moreover, it
delivers strong performance with minimal labeled samples for fine-tuning,
making it well-suited for practical deployment.

</details>


### [83] [VidCLearn: A Continual Learning Approach for Text-to-Video Generation](https://arxiv.org/abs/2509.16956)
*Luca Zanchetta,Lorenzo Papa,Luca Maiano,Irene Amerini*

Main category: cs.CV

TL;DR: VidCLearn是一个用于扩散式文本到视频生成的持续学习框架，通过师生架构和生成重放来解决现有模型难以融入新数据的问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成模型依赖静态知识，难以在不重新训练的情况下融入新数据，限制了模型的适应性和扩展性。

Method: 采用师生架构，学生模型通过新文本-视频对增量更新，教师模型通过生成重放保留已学知识；引入时间一致性损失增强运动平滑性，视频检索模块在推理时提供结构指导。

Result: 实验结果表明VidCLearn在视觉质量、语义对齐和时间连贯性方面优于基线方法。

Conclusion: VidCLearn框架在保持满意生成性能的同时，比现有模型计算效率更高，为文本到视频生成的持续学习提供了有效解决方案。

Abstract: Text-to-video generation is an emerging field in generative AI, enabling the
creation of realistic, semantically accurate videos from text prompts. While
current models achieve impressive visual quality and alignment with input text,
they typically rely on static knowledge, making it difficult to incorporate new
data without retraining from scratch. To address this limitation, we propose
VidCLearn, a continual learning framework for diffusion-based text-to-video
generation. VidCLearn features a student-teacher architecture where the student
model is incrementally updated with new text-video pairs, and the teacher model
helps preserve previously learned knowledge through generative replay.
Additionally, we introduce a novel temporal consistency loss to enhance motion
smoothness and a video retrieval module to provide structural guidance at
inference. Our architecture is also designed to be more computationally
efficient than existing models while retaining satisfactory generation
performance. Experimental results show VidCLearn's superiority over baseline
methods in terms of visual quality, semantic alignment, and temporal coherence.

</details>


### [84] [MO R-CNN: Multispectral Oriented R-CNN for Object Detection in Remote Sensing Image](https://arxiv.org/abs/2509.16957)
*Leiyu Wang,Biao Jin,Feng Huang,Liqiong Chen,Zhengyong Wang,Xiaohai He,Honggang Chen*

Main category: cs.CV

TL;DR: 提出MO R-CNN轻量级框架，用于多光谱遥感图像中的定向目标检测，通过异构特征提取网络、单模态监督和基于条件的多模态标签融合来解决模态间差异问题。


<details>
  <summary>Details</summary>
Motivation: 多光谱图像中的定向目标检测面临模态内和模态间差异的挑战，现有方法虽然提高了检测精度但计算复杂度和内存消耗过高，限制了实际性能。

Method: 提出MO R-CNN框架，包含异构特征提取网络（HFEN）自适应对齐和增强多模态特征，单模态监督（SMS）约束多尺度特征学习，以及基于条件的多模态标签融合（CMLF）提供更鲁棒的监督信号。

Result: 在DroneVehicle、VEDAI和OGSOD数据集上的实验证明了该方法的优越性，实现了轻量级高性能的多光谱定向目标检测。

Conclusion: MO R-CNN通过创新的特征提取、监督学习和标签融合策略，有效解决了多光谱遥感图像中定向目标检测的挑战，在保持轻量化的同时提升了检测性能。

Abstract: Oriented object detection for multi-spectral imagery faces significant
challenges due to differences both within and between modalities. Although
existing methods have improved detection accuracy through complex network
architectures, their high computational complexity and memory consumption
severely restrict their performance. Motivated by the success of large kernel
convolutions in remote sensing, we propose MO R-CNN, a lightweight framework
for multi-spectral oriented detection featuring heterogeneous feature
extraction network (HFEN), single modality supervision (SMS), and
condition-based multimodal label fusion (CMLF). HFEN leverages inter-modal
differences to adaptively align, merge, and enhance multi-modal features. SMS
constrains multi-scale features and enables the model to learn from multiple
modalities. CMLF fuses multimodal labels based on specific rules, providing the
model with a more robust and consistent supervisory signal. Experiments on the
DroneVehicle, VEDAI and OGSOD datasets prove the superiority of our method. The
source code is available at:https://github.com/Iwill-github/MORCNN.

</details>


### [85] [Penalizing Boundary Activation for Object Completeness in Diffusion Models](https://arxiv.org/abs/2509.16968)
*Haoyang Xu,Tianhao Zhao,Sibei Yang,Yutian Li*

Main category: cs.CV

TL;DR: 本文分析了扩散模型中物体不完整显示的问题，发现RandomCrop数据增强是主要原因，并提出了一种无需重新训练的方法来改善物体完整性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成中表现出色，但普遍存在物体显示不完整的问题，这限制了模型在下游应用中的性能。

Method: 提出了一种训练免费的解决方案，在早期去噪步骤中对图像边界的激活值进行惩罚，该方法可轻松应用于预训练的Stable Diffusion模型，只需最小修改和可忽略的计算开销。

Result: 大量实验证明该方法有效，在物体完整性和图像质量方面都有显著提升。

Conclusion: 通过分析RandomCrop对物体连续性的破坏，提出的边界激活惩罚方法成功解决了物体不完整问题，为扩散模型的改进提供了实用方案。

Abstract: Diffusion models have emerged as a powerful technique for text-to-image (T2I)
generation, creating high-quality, diverse images across various domains.
However, a common limitation in these models is the incomplete display of
objects, where fragments or missing parts undermine the model's performance in
downstream applications. In this study, we conduct an in-depth analysis of the
incompleteness issue and reveal that the primary factor behind incomplete
object generation is the usage of RandomCrop during model training. This widely
used data augmentation method, though enhances model generalization ability,
disrupts object continuity during training. To address this, we propose a
training-free solution that penalizes activation values at image boundaries
during the early denoising steps. Our method is easily applicable to
pre-trained Stable Diffusion models with minimal modifications and negligible
computational overhead. Extensive experiments demonstrate the effectiveness of
our method, showing substantial improvements in object integrity and image
quality.

</details>


### [86] [LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing Object Detection](https://arxiv.org/abs/2509.16970)
*Wei Liao,Chunyan Xu,Chenxu Wang,Zhen Cui*

Main category: cs.CV

TL;DR: 本文提出了一种基于LLM语义引导的稀疏标注遥感目标检测框架，通过大语言模型的语义推理能力生成高质量伪标签，解决了传统密集伪标签方法的选择模糊性和置信度估计不一致问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏标注在遥感目标检测中面临密集目标分布和类别不平衡的挑战，现有密集伪标签方法存在选择模糊性和置信度估计不一致的局限性。

Method: 提出LLM辅助语义引导框架，集成LLM生成的语义先验，设计类感知密集伪标签分配机制和自适应硬负样本重加权模块，为未标注和稀疏标注数据自适应分配伪标签。

Result: 在DOTA和HRSC2016数据集上的实验表明，该方法优于现有的单阶段检测器框架，显著提升了稀疏标注下的检测性能。

Conclusion: LLM辅助的语义引导框架有效解决了稀疏标注遥感目标检测的挑战，为伪标签生成提供了新的思路。

Abstract: Sparse annotation in remote sensing object detection poses significant
challenges due to dense object distributions and category imbalances. Although
existing Dense Pseudo-Label methods have demonstrated substantial potential in
pseudo-labeling tasks, they remain constrained by selection ambiguities and
inconsistencies in confidence estimation.In this paper, we introduce an
LLM-assisted semantic guidance framework tailored for sparsely annotated remote
sensing object detection, exploiting the advanced semantic reasoning
capabilities of large language models (LLMs) to distill high-confidence
pseudo-labels.By integrating LLM-generated semantic priors, we propose a
Class-Aware Dense Pseudo-Label Assignment mechanism that adaptively assigns
pseudo-labels for both unlabeled and sparsely labeled data, ensuring robust
supervision across varying data distributions. Additionally, we develop an
Adaptive Hard-Negative Reweighting Module to stabilize the supervised learning
branch by mitigating the influence of confounding background information.
Extensive experiments on DOTA and HRSC2016 demonstrate that the proposed method
outperforms existing single-stage detector-based frameworks, significantly
improving detection performance under sparse annotations.

</details>


### [87] [The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA](https://arxiv.org/abs/2509.16972)
*Quanzhu Niu,Dengxian Gong,Shihao Chen,Tao Zhang,Yikang Zhou,Haobo Yuan,Lu Qi,Xiangtai Li,Shunping Ji*

Main category: cs.CV

TL;DR: SaSaSa2VA通过在Sa2VA基础上改进稀疏帧采样和单一[SEG]token问题，在RVOS任务中取得SOTA效果


<details>
  <summary>Details</summary>
Motivation: 解决RVOS任务中稀疏帧采样和依赖单一[SEG]token限制分割性能的问题

Method: 提出Segmentation Augmented and Selective Averaged Sa2VA，通过高效的分割增强和测试时集成技术改进grounded MLLMs

Result: 在LSVOS挑战赛RVOS赛道获得J&F 67.45，排名第一，比第二名高出2.80分

Conclusion: 分割增强和测试时集成技术能显著提升grounded MLLMs在RVOS任务中的性能

Abstract: Referring video object segmentation (RVOS) requires segmenting and tracking
objects in videos conditioned on natural-language expressions, demanding
fine-grained understanding of both appearance and motion. Building on Sa2VA,
which couples a Multi-modal Large Language Model (MLLM) with the video
segmentation model SAM2, we identify two key bottlenecks that limit
segmentation performance: sparse frame sampling and reliance on a single [SEG]
token for an entire video. We propose Segmentation Augmented and Selective
Averaged Sa2VA SaSaSa2VA to address these issues. On the 7th LSVOS Challenge
(RVOS track), SaSaSa2VA achieves a $J\&F$ of 67.45, ranking first and
surpassing the runner-up by 2.80 points. This result and ablation studies
demonstrate that efficient segmentation augmentation and test-time ensembling
substantially enhance grounded MLLMs for RVOS. The code is released in Sa2VA
repository: https://github.com/magic-research/Sa2VA.

</details>


### [88] [Optimal Transport for Handwritten Text Recognition in a Low-Resource Regime](https://arxiv.org/abs/2509.16977)
*Petros Georgoulas Wraight,Giorgos Sfikas,Ioannis Kordonis,Petros Maragos,George Retsinas*

Main category: cs.CV

TL;DR: 提出了一种基于最优传输的迭代视觉语义对齐框架，用于解决低资源手写文本识别问题，能够在标注数据稀缺的情况下利用词汇特征知识提升识别准确率


<details>
  <summary>Details</summary>
Motivation: 传统HTR方法需要大量标注数据，不适用于历史档案等低资源领域。本文旨在开发一种能够在标注数据稀缺情况下利用词汇特征知识的框架

Method: 采用迭代自举方法，通过最优传输将未标注图像中的视觉特征与语义词汇表示对齐，从少量标注样本开始，迭代匹配词图像与文本标签，生成高置信度伪标签，并在增长的数据集上重新训练识别器

Result: 数值实验表明，该迭代视觉语义对齐方案在低资源HTR基准测试中显著提高了识别准确率

Conclusion: 该框架为低资源HTR任务提供了一种有效的解决方案，能够在标注数据有限的情况下实现良好的识别性能

Abstract: Handwritten Text Recognition (HTR) is a task of central importance in the
field of document image understanding. State-of-the-art methods for HTR require
the use of extensive annotated sets for training, making them impractical for
low-resource domains like historical archives or limited-size modern
collections. This paper introduces a novel framework that, unlike the standard
HTR model paradigm, can leverage mild prior knowledge of lexical
characteristics; this is ideal for scenarios where labeled data are scarce. We
propose an iterative bootstrapping approach that aligns visual features
extracted from unlabeled images with semantic word representations using
Optimal Transport (OT). Starting with a minimal set of labeled examples, the
framework iteratively matches word images to text labels, generates
pseudo-labels for high-confidence alignments, and retrains the recognizer on
the growing dataset. Numerical experiments demonstrate that our iterative
visual-semantic alignment scheme significantly improves recognition accuracy on
low-resource HTR benchmarks.

</details>


### [89] [VCE: Safe Autoregressive Image Generation via Visual Contrast Exploitation](https://arxiv.org/abs/2509.16986)
*Feng Han,Chao Gong,Zhipeng Wei,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出了Visual Contrast Exploitation (VCE)框架，用于保护自回归图像生成模型免受不安全概念（如NSFW内容、侵权风格）的影响，通过对比图像对构建和DPO训练实现精确概念擦除。


<details>
  <summary>Details</summary>
Motivation: 自回归图像生成模型（如GPT-4o、LlamaGen）能够生成逼真图像，但也可能产生NSFW内容和侵权风格，现有概念擦除方法主要针对扩散模型，不适用于自回归模型。

Method: VCE框架包含：(1)创新的对比图像对构建范式，精确解耦不安全概念与内容语义；(2)基于DPO的训练方法，增强模型识别和利用视觉对比特征的能力。

Result: 在三个挑战性任务（艺术家风格擦除、显式内容擦除、对象移除）上的实验表明，该方法能有效保护模型，在擦除不安全概念的同时保持无关安全概念的完整性，达到最先进水平。

Conclusion: VCE是首个针对自回归图像生成模型的概念擦除方法，填补了该领域的研究空白，代码和模型已开源。

Abstract: Recently, autoregressive image generation models have wowed audiences with
their remarkable capability in creating surprisingly realistic images. Models
such as GPT-4o and LlamaGen can not only produce images that faithfully mimic
renowned artistic styles like Ghibli, Van Gogh, or Picasso, but also
potentially generate Not-Safe-For-Work (NSFW) content, raising significant
concerns regarding copyright infringement and ethical use. Despite these
concerns, methods to safeguard autoregressive text-to-image models remain
underexplored. Previous concept erasure methods, primarily designed for
diffusion models that operate in denoising latent space, are not directly
applicable to autoregressive models that generate images token by token. To
address this critical gap, we propose Visual Contrast Exploitation (VCE), a
novel framework comprising: (1) an innovative contrastive image pair
construction paradigm that precisely decouples unsafe concepts from their
associated content semantics, and (2) a sophisticated DPO-based training
approach that enhances the model's ability to identify and leverage visual
contrastive features from image pairs, enabling precise concept erasure. Our
comprehensive experiments across three challenging tasks-artist style erasure,
explicit content erasure, and object removal-demonstrate that our method
effectively secures the model, achieving state-of-the-art results while erasing
unsafe concepts and maintaining the integrity of unrelated safe concepts. The
code and models are available at https://github.com/Maplebb/VCE.

</details>


### [90] [A Cross-Hierarchical Multi-Feature Fusion Network Based on Multiscale Encoder-Decoder for Hyperspectral Change Detection](https://arxiv.org/abs/2509.16988)
*Mingshuai Sheng,Bhatti Uzair Aslam,Junfeng Zhang,Siling Feng,Yonis Gulzar*

Main category: cs.CV

TL;DR: 本文提出了一种基于多尺度编码器-解码器架构的跨层次多特征融合网络（CHMFFN），用于高光谱变化检测，通过多尺度特征提取、双核通道-空间注意力模块和自适应特征融合机制，有效提升了变化检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有高光谱变化检测方法在多尺度特征利用不足和差异特征融合效率低的问题，提高环境监测和灾害评估等应用的准确性。

Method: 采用多尺度编码器-解码器架构，前端使用多尺度特征提取子网络，包含双核通道-空间注意力模块提取光谱-空间-时间特征；编码器通过残差块和不同感受野的卷积核捕获多尺度特征；解码器通过跳跃连接恢复空间分辨率；光谱-时间变化特征学习模块学习跨时间变化特征；自适应融合模块动态平衡层次差异特征。

Result: 在四个公开高光谱数据集上的实验表明，CHMFFN优于现有最先进方法，验证了其有效性。

Conclusion: 所提出的CHMFFN网络通过有效的多尺度特征提取和自适应融合机制，在高光谱变化检测任务中取得了优越的性能。

Abstract: Hyperspectral change detection (HCD) aims to accurately identify land-cover
changes in hyperspectral images of the same area acquired at different times,
with key applications in environmental monitoring and disaster assessment. To
address limitations of existing methods, such as insufficient use of multiscale
features and low efficiency in differential feature fusion, this paper proposes
a cross-hierarchical multi-feature fusion network (CHMFFN) based on a
multiscale encoder-decoder architecture. The front-end adopts a multiscale
feature extraction subnetwork, built on an encoder-decoder backbone with
residual connections and a dual-core channel-spatial attention (DCCSA) module
to extract spectral-spatial-temporal features (SSTF). The encoder captures
multiscale features from shallow details to deep semantics via residual blocks
and convolutional kernels with varying receptive fields. The decoder restores
spatial resolution and suppresses noise information through skip connections
integrating encoder features. Additionally, a spectral-temporal change feature
learning (STCFL) module learns cross-temporal change features at different
levels, strengthening inter-temporal difference capture. An adaptive fusion of
advanced features (AFAF) module dynamically balances hierarchical differential
features via adaptive weights, enhancing representation of complex changes.
Experiments on four public hyperspectral datasets show CHMFFN outperforms
state-of-the-art methods, verifying its effectiveness.

</details>


### [91] [DocIQ: A Benchmark Dataset and Feature Fusion Network for Document Image Quality Assessment](https://arxiv.org/abs/2509.17012)
*Zhichao Ma,Fan Huang,Lu Zhao,Fengjun Guo,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了DIQA-5000主观文档图像质量评估数据集，并开发了一个专门的无参考DIQA模型，利用文档布局特征和多级特征融合来评估文档图像质量。


<details>
  <summary>Details</summary>
Motivation: 文档图像质量评估在OCR、文档修复等应用中很重要，但缺乏专门的数据集和模型。现有通用IQA模型在文档图像上表现不佳。

Method: 构建DIQA-5000数据集（5000张图像，15人标注），提出无参考DIQA模型，使用文档布局特征降低计算成本，设计特征融合模块整合多级特征，为每个质量维度使用独立的质量头预测分数分布。

Result: 实验结果表明，该方法在DIQA-5000和另一个OCR精度相关的文档图像数据集上优于当前最先进的通用IQA模型。

Conclusion: 提出的专门DIQA模型能有效评估文档图像质量，在多个维度上优于通用方法，为文档图像处理应用提供了更好的质量评估工具。

Abstract: Document image quality assessment (DIQA) is an important component for
various applications, including optical character recognition (OCR), document
restoration, and the evaluation of document image processing systems. In this
paper, we introduce a subjective DIQA dataset DIQA-5000. The DIQA-5000 dataset
comprises 5,000 document images, generated by applying multiple document
enhancement techniques to 500 real-world images with diverse distortions. Each
enhanced image was rated by 15 subjects across three rating dimensions: overall
quality, sharpness, and color fidelity. Furthermore, we propose a specialized
no-reference DIQA model that exploits document layout features to maintain
quality perception at reduced resolutions to lower computational cost.
Recognizing that image quality is influenced by both low-level and high-level
visual features, we designed a feature fusion module to extract and integrate
multi-level features from document images. To generate multi-dimensional
scores, our model employs independent quality heads for each dimension to
predict score distributions, allowing it to learn distinct aspects of document
image quality. Experimental results demonstrate that our method outperforms
current state-of-the-art general-purpose IQA models on both DIQA-5000 and an
additional document image dataset focused on OCR accuracy.

</details>


### [92] [When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration](https://arxiv.org/abs/2509.17024)
*Wenxuan Fang,Jili Fan,Chao Wang,Xiantao Hu,Jiangwei Weng,Ying Tai,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: LCDiff是一个新颖的恶劣天气图像恢复框架，通过亮度-色度分解网络和亮度引导扩散模型，在YCbCr色彩空间中分别处理退化相关的亮度和退化不变的色度，无需显式退化提示即可实现高质量图像恢复。


<details>
  <summary>Details</summary>
Motivation: 传统任务特定方法难以泛化到未见或复杂退化类型，而基于提示学习的方法过度依赖视觉语言模型的退化估计能力，导致恢复结果不一致。

Method: 提出LCDiff框架，包含亮度-色度分解网络（LCDN）和亮度引导扩散模型（LGDM）。LCDN在YCbCr色彩空间分别处理亮度和色度，LGDM利用退化相关亮度信息作为引导条件，并采用动态时间步损失优化去噪网络。

Result: 大量实验表明，该方法超越了现有最先进方法，在恶劣天气图像恢复任务中设立了新的基准。

Conclusion: LCDiff通过亮度-色度分解和亮度引导扩散模型，有效缓解天气引起的退化并保持色彩保真度，为恶劣天气图像恢复提供了有效的解决方案。

Abstract: Adverse Weather Image Restoration (AWIR) is a highly challenging task due to
the unpredictable and dynamic nature of weather-related degradations.
Traditional task-specific methods often fail to generalize to unseen or complex
degradation types, while recent prompt-learning approaches depend heavily on
the degradation estimation capabilities of vision-language models, resulting in
inconsistent restorations. In this paper, we propose \textbf{LCDiff}, a novel
framework comprising two key components: \textit{Lumina-Chroma Decomposition
Network} (LCDN) and \textit{Lumina-Guided Diffusion Model} (LGDM). LCDN
processes degraded images in the YCbCr color space, separately handling
degradation-related luminance and degradation-invariant chrominance components.
This decomposition effectively mitigates weather-induced degradation while
preserving color fidelity. To further enhance restoration quality, LGDM
leverages degradation-related luminance information as a guiding condition,
eliminating the need for explicit degradation prompts. Additionally, LGDM
incorporates a \textit{Dynamic Time Step Loss} to optimize the denoising
network, ensuring a balanced recovery of both low- and high-frequency features
in the image. Finally, we present DriveWeather, a comprehensive all-weather
driving dataset designed to enable robust evaluation. Extensive experiments
demonstrate that our approach surpasses state-of-the-art methods, setting a new
benchmark in AWIR. The dataset and code are available at:
https://github.com/fiwy0527/LCDiff.

</details>


### [93] [Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic Views](https://arxiv.org/abs/2509.17027)
*Zhenya Yang*

Main category: cs.CV

TL;DR: 提出基于高斯泼溅的框架，从内窥镜数据直接重建交互式手术场景，解决传统方法重建效率低、细节差的问题，并通过虚拟相机正则化和物理模拟实现实时变形。


<details>
  <summary>Details</summary>
Motivation: 传统手术模拟环境构建方法繁琐耗时、难以扩展，导致细节不足和模拟不真实。需要一种高效、高质量的数据驱动方法来重建真实的手术场景。

Method: 使用高斯泼溅表示法重建手术场景，引入虚拟相机正则化方法缓解视角受限导致的过拟合问题，结合深度正则化优化几何精度，并采用基于稀疏控制节点的材料点方法实现快速物理变形模拟。

Result: 在代表性手术数据上的实验表明，该方法能在几分钟内重建手术场景，并实时生成物理上合理的变形效果，支持用户自定义交互。

Conclusion: 该方法成功解决了手术场景重建的效率和质量问题，为医疗培训提供了更真实、交互性更强的模拟环境。

Abstract: Surgical simulation is essential for medical training, enabling practitioners
to develop crucial skills in a risk-free environment while improving patient
safety and surgical outcomes. However, conventional methods for building
simulation environments are cumbersome, time-consuming, and difficult to scale,
often resulting in poor details and unrealistic simulations. In this paper, we
propose a Gaussian Splatting-based framework to directly reconstruct
interactive surgical scenes from endoscopic data while ensuring efficiency,
rendering quality, and realism. A key challenge in this data-driven simulation
paradigm is the restricted movement of endoscopic cameras, which limits
viewpoint diversity. As a result, the Gaussian Splatting representation
overfits specific perspectives, leading to reduced geometric accuracy. To
address this issue, we introduce a novel virtual camera-based regularization
method that adaptively samples virtual viewpoints around the scene and
incorporates them into the optimization process to mitigate overfitting. An
effective depth-based regularization is applied to both real and virtual views
to further refine the scene geometry. To enable fast deformation simulation, we
propose a sparse control node-based Material Point Method, which integrates
physical properties into the reconstructed scene while significantly reducing
computational costs. Experimental results on representative surgical data
demonstrate that our method can efficiently reconstruct and simulate surgical
scenes from sparse endoscopic views. Notably, our method takes only a few
minutes to reconstruct the surgical scene and is able to produce physically
plausible deformations in real-time with user-defined interactions.

</details>


### [94] [From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning](https://arxiv.org/abs/2509.17040)
*Hang Du,Jiayang Zhang,Guoshun Nan,Wendi Deng,Zhenyan Chen,Chenyang Zhang,Wang Xiao,Shan Huang,Yuqi Pan,Tao Qi,Sicong Leng*

Main category: cs.CV

TL;DR: 该论文提出了一个名为MIR的新基准，用于评估多模态大语言模型在多图像交错推理任务上的能力，并提出了分阶段课程学习策略来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前的多图像基准测试忽略了交错文本上下文和图像与文本之间的特定关系，限制了模型对复杂场景的理解和跨模态相关性的捕捉能力。

Method: 提出了MIR基准，要求模型在多个图像和交错文本上下文中进行联合推理，并采用分阶段课程学习策略，从简单到复杂逐步训练模型。

Result: 大量实验表明，该方法显著提升了多模态大语言模型在MIR基准和其他现有基准上的推理性能。

Conclusion: MIR基准将促进多图像交错推理研究的进一步发展，提升多模态大语言模型处理复杂跨模态任务的能力。

Abstract: Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language
Models (MLLMs) ability to jointly comprehend and reason across multiple images
and their associated textual contexts, introducing unique challenges beyond
single-image or non-interleaved multi-image tasks. While current multi-image
benchmarks overlook interleaved textual contexts and neglect distinct
relationships between individual images and their associated texts, enabling
models to reason over multi-image interleaved data may significantly enhance
their comprehension of complex scenes and better capture cross-modal
correlations. To bridge this gap, we introduce a novel benchmark MIR, requiring
joint reasoning over multiple images accompanied by interleaved textual
contexts to accurately associate image regions with corresponding texts and
logically connect information across images. To enhance MLLMs ability to
comprehend multi-image interleaved data, we introduce reasoning steps for each
instance within the benchmark and propose a stage-wise curriculum learning
strategy. This strategy follows an "easy to hard" approach, progressively
guiding models from simple to complex scenarios, thereby enhancing their
ability to handle challenging tasks. Extensive experiments benchmarking
multiple MLLMs demonstrate that our method significantly enhances models
reasoning performance on MIR and other established benchmarks. We believe that
MIR will encourage further research into multi-image interleaved reasoning,
facilitating advancements in MLLMs capability to handle complex inter-modal
tasks.Our code and dataset are available at
https://github.com/Shelly-coder239/MIRBench.

</details>


### [95] [Towards Generalized Synapse Detection Across Invertebrate Species](https://arxiv.org/abs/2509.17041)
*Samia Mohinta,Daniel Franco-Barranco,Shi Yan Lee,Albert Cardona*

Main category: cs.CV

TL;DR: 本文提出了SimpSyn，一种轻量级的单阶段神经网络，用于电子显微镜图像中的突触检测，在多个数据集上超越了现有最先进方法，证明了简单模型在大规模连接组学中的实用性。


<details>
  <summary>Details</summary>
Motivation: 解决电子显微镜图像中突触自动检测的挑战，包括稀疏标注、形态变异性和跨数据集域偏移问题，为大规模神经环路研究提供可靠工具。

Method: 提出SimpSyn模型：基于单阶段残差U-Net，预测突触前后位点的双通道球形掩码；使用四个不同物种的EM数据集构建基准；采用局部峰值检测和距离过滤等简单后处理策略。

Result: SimpSyn在所有测试数据集的突触位点检测F1分数上均优于Synful方法；在组合数据集上训练时达到竞争性性能；简单后处理策略即可获得强性能。

Conclusion: 轻量级模型与任务结构对齐时，可以为大规模连接组学管道中的突触检测提供实用且可扩展的解决方案，无需复杂的测试时启发式方法。

Abstract: Behavioural differences across organisms, whether healthy or pathological,
are closely tied to the structure of their neural circuits. Yet, the fine-scale
synaptic changes that give rise to these variations remain poorly understood,
in part due to persistent challenges in detecting synapses reliably and at
scale. Volume electron microscopy (EM) offers the resolution required to
capture synaptic architecture, but automated detection remains difficult due to
sparse annotations, morphological variability, and cross-dataset domain shifts.
To address this, we make three key contributions. First, we curate a diverse EM
benchmark spanning four datasets across two invertebrate species: adult and
larval Drosophila melanogaster, and Megaphragma viggianii (micro-WASP). Second,
we propose SimpSyn, a single-stage Residual U-Net trained to predict
dual-channel spherical masks around pre- and post-synaptic sites, designed to
prioritize training and inference speeds and annotation efficiency over
architectural complexity. Third, we benchmark SimpSyn against Buhmann et al.'s
Synful [1], a state-of-the-art multi-task model that jointly infers synaptic
pairs. Despite its simplicity, SimpSyn consistently outperforms Synful in
F1-score across all volumes for synaptic site detection. While generalization
across datasets remains limited, SimpSyn achieves competitive performance when
trained on the combined cohort. Finally, ablations reveal that simple
post-processing strategies - such as local peak detection and distance-based
filtering - yield strong performance without complex test-time heuristics.
Taken together, our results suggest that lightweight models, when aligned with
task structure, offer a practical and scalable solution for synapse detection
in large-scale connectomic pipelines.

</details>


### [96] [AgriDoctor: A Multimodal Intelligent Assistant for Agriculture](https://arxiv.org/abs/2509.17044)
*Mingqing Zhang,Zhuoning Xu,Peijie Wang,Rongji Li,Liang Wang,Qiang Liu,Jian Xu,Xuyao Zhang,Shu Wu,Liang Wang*

Main category: cs.CV

TL;DR: AgriDoctor是一个模块化多模态框架，用于作物病害诊断和农业知识交互，通过结合视觉和语言模型解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有作物病害诊断方法主要依赖单模态模型，无法整合领域知识和支持语言交互，而现有大语言模型在农业领域表现有限。

Method: 提出AgriDoctor框架，包含路由器、分类器、检测器、知识检索器和LLMs五个核心组件，并构建AgriMM基准数据集进行训练评估。

Result: 实验表明AgriDoctor在精细农业任务上显著优于现有最先进的多模态语言模型。

Conclusion: AgriDoctor为智能可持续农业应用建立了新的范式，展示了基于代理的多模态推理在农业领域的潜力。

Abstract: Accurate crop disease diagnosis is essential for sustainable agriculture and
global food security. Existing methods, which primarily rely on unimodal models
such as image-based classifiers and object detectors, are limited in their
ability to incorporate domain-specific agricultural knowledge and lack support
for interactive, language-based understanding. Recent advances in large
language models (LLMs) and large vision-language models (LVLMs) have opened new
avenues for multimodal reasoning. However, their performance in agricultural
contexts remains limited due to the absence of specialized datasets and
insufficient domain adaptation. In this work, we propose AgriDoctor, a modular
and extensible multimodal framework designed for intelligent crop disease
diagnosis and agricultural knowledge interaction. As a pioneering effort to
introduce agent-based multimodal reasoning into the agricultural domain,
AgriDoctor offers a novel paradigm for building interactive and domain-adaptive
crop health solutions. It integrates five core components: a router,
classifier, detector, knowledge retriever and LLMs. To facilitate effective
training and evaluation, we construct AgriMM, a comprehensive benchmark
comprising 400000 annotated disease images, 831 expert-curated knowledge
entries, and 300000 bilingual prompts for intent-driven tool selection.
Extensive experiments demonstrate that AgriDoctor, trained on AgriMM,
significantly outperforms state-of-the-art LVLMs on fine-grained agricultural
tasks, establishing a new paradigm for intelligent and sustainable farming
applications.

</details>


### [97] [Learning Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query Optimization](https://arxiv.org/abs/2509.17049)
*Peng Wang,Yong Li,Lin Zhao,Xiu-Shen Wei*

Main category: cs.CV

TL;DR: 提出了一种基于可学习查询的属性感知哈希码学习方法，通过定制查询集捕获细粒度属性信息，并引入辅助分支建模高阶属性交互，提升低比特哈希码的检索准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决细粒度图像检索中哈希码与具体视觉属性对应的问题，增强哈希码的可解释性和相关性，特别是针对低比特哈希码的复杂优化挑战。

Method: 使用可学习查询集进行属性感知哈希码学习，部署辅助分支建模高阶属性交互以优化低比特哈希码的复杂优化问题。

Result: 在基准数据集上的实验表明，该方法生成的属性感知哈希码在检索准确性和鲁棒性方面优于现有技术，特别是在低比特哈希码场景下表现突出。

Conclusion: 该方法在细粒度图像哈希任务中具有显著潜力，能够生成更具解释性和相关性的哈希码，提升检索性能。

Abstract: Fine-grained hashing has become a powerful solution for rapid and efficient
image retrieval, particularly in scenarios requiring high discrimination
between visually similar categories. To enable each hash bit to correspond to
specific visual attributes, we propoe a novel method that harnesses learnable
queries for attribute-aware hash codes learning. This method deploys a tailored
set of queries to capture and represent nuanced attribute-level information
within the hashing process, thereby enhancing both the interpretability and
relevance of each hash bit. Building on this query-based optimization
framework, we incorporate an auxiliary branch to help alleviate the challenges
of complex landscape optimization often encountered with low-bit hash codes.
This auxiliary branch models high-order attribute interactions, reinforcing the
robustness and specificity of the generated hash codes. Experimental results on
benchmark datasets demonstrate that our method generates attribute-aware hash
codes and consistently outperforms state-of-the-art techniques in retrieval
accuracy and robustness, especially for low-bit hash codes, underscoring its
potential in fine-grained image hashing tasks.

</details>


### [98] [Geodesic Prototype Matching via Diffusion Maps for Interpretable Fine-Grained Recognition](https://arxiv.org/abs/2509.17050)
*Junhao Jia,Yunyou Liu,Yifei Sun,Huangwei Chen,Feiwei Qin,Changmiao Wang,Yong Peng*

Main category: cs.CV

TL;DR: 本文提出GeoProto框架，通过扩散空间和Nyström插值在深度特征的内在几何中锚定相似性，解决原型可解释细粒度识别中欧氏距离失效的问题。


<details>
  <summary>Details</summary>
Motivation: 深度视觉特征普遍存在非线性流形结构，欧氏距离难以捕捉真实相似性，这在需要捕捉细微语义差异的原型可解释细粒度识别中尤为严重。

Method: 将每个类的潜在流形结构蒸馏到扩散空间，引入可微分的Nyström插值使几何结构对未见样本和可学习原型都可用，使用紧凑的每类地标集并定期更新以保证效率。

Result: 在CUB-200-2011和Stanford Cars数据集上的实验表明，GeoProto框架产生的原型聚焦于语义对齐的部分，显著优于欧氏原型网络。

Conclusion: GeoProto通过利用深度特征的内在几何结构，有效提升了原型可解释细粒度识别的性能，证明了在非线性流形中锚定相似性的重要性。

Abstract: Nonlinear manifolds are widespread in deep visual features, where Euclidean
distances often fail to capture true similarity. This limitation becomes
particularly severe in prototype-based interpretable fine-grained recognition,
where subtle semantic distinctions are essential. To address this challenge, we
propose a novel paradigm for prototype-based recognition that anchors
similarity within the intrinsic geometry of deep features. Specifically, we
distill the latent manifold structure of each class into a diffusion space and
introduce a differentiable Nystr\"om interpolation, making the geometry
accessible to both unseen samples and learnable prototypes. To ensure
efficiency, we employ compact per-class landmark sets with periodic updates.
This design keeps the embedding aligned with the evolving backbone, enabling
fast and scalable inference. Extensive experiments on the CUB-200-2011 and
Stanford Cars datasets show that our GeoProto framework produces prototypes
focusing on semantically aligned parts, significantly outperforming Euclidean
prototype networks.

</details>


### [99] [CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a Few-shot Manner](https://arxiv.org/abs/2509.17065)
*Yao Du,Jiarong Guo,Xiaomeng Li*

Main category: cs.CV

TL;DR: CardiacCLIP是一个基于视频的框架，通过注意力机制和多分辨率输入增强LVEF预测，在少样本设置下显著提高了超声心动图分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LVEF估计方法依赖大规模标注视频数据集，成本高且临床适应性有限。现有视觉语言模型如EchoCLIP无法捕捉关键的时间动态和局部心脏结构。

Method: 提出MFL（多帧学习）注意力机制选择性融合信息帧，以及EchoZoom多尺度特征提取策略细化心脏结构空间表示。

Result: 在EchoNet-Dynamic数据集上，1-shot设置下MAE降低了2.07，显著提高了诊断准确性。

Conclusion: CardiacCLIP作为CLIP模型在少样本超声心动图视频分析中的新适应方法，有效解决了现有方法的局限性。

Abstract: Echocardiography is a vital non-invasive modality for cardiac assessment,
with left ventricular ejection fraction (LVEF) serving as a key indicator of
heart function. Existing LVEF estimation methods depend on large-scale
annotated video datasets, which are costly and limit adaptability across
various clinical settings. Recent vision-language models for echocardiography,
such as EchoCLIP, apply image-to-text pretraining but fail to capture crucial
temporal dynamics and localized cardiac structures essential for accurate
diagnosis. To address these challenges, we propose CardiacCLIP, a video-based
framework that enhances LVEF prediction through attention-based frame
aggregation and multi-resolution input scaling. Specifically, we introduce MFL
(Multi Frame Learning), a novel attention-based mechanism for selectively
fusing informative frames, and EchoZoom, a multi-scale feature extraction
strategy that refines spatial representations of cardiac structures. As a novel
adaptation of CLIP models for few-shot echocardiogram video analysis, our
approach significantly improves diagnostic accuracy, reducing MAE by 2.07 on
the EchoNet-Dynamic dataset under 1-shot setting. The code is available at
https://github.com/xmed-lab/CardiacCLIP.

</details>


### [100] [Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models](https://arxiv.org/abs/2509.17074)
*Qian Zhang,Lin Zhang,Xing Fang,Mingxin Zhang,Zhiyuan Wei,Ran Song,Wei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于信息约束的文本引导视觉可供性学习框架，通过最大化可供性区域特征与文本提示之间的互信息，以及对象级视觉特征与类别文本特征之间的互信息，实现特征层面的文本-图像对齐，在单样本可供性学习中达到新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用视觉-语言基础模型进行可供性学习时，忽视了保持视觉图像与语言描述之间特征对齐的重要性，导致文本引导识别可供性区域的效果不理想。

Method: 设计可供性互信息约束来同时学习合适的文本提示和任务导向的视觉特征；提出对象级信息约束来捕获高质量的对象表示，为识别可供性区域提供更可靠的语义先验。

Result: 在AGD20K数据集上的实验结果表明，该方法优于现有方法，在单样本可供性学习中达到了新的最优性能。

Conclusion: 通过信息约束实现文本-图像特征对齐是提升文本引导可供性学习效果的有效途径，所提出的框架为视觉可供性学习提供了新的解决方案。

Abstract: Visual affordance learning is crucial for robots to understand and interact
effectively with the physical world. Recent advances in this field attempt to
leverage pre-trained knowledge of vision-language foundation models to learn
affordance properties with limited training data, providing a novel paradigm
for visual affordance learning. However, these methods overlook the
significance of maintaining feature alignment between visual images and
language descriptions for identifying affordance areas with textual guidance,
and thus may lead to suboptimal results. In this paper, we present an
informative framework for text-guided affordance learning, which involves
information-based constraints to achieve text-image alignment at feature level.
Specifically, we design an affordance mutual information constraint that helps
learn appropriate textual prompts and task-oriented visual features
simultaneously by maximizing the mutual information between the features of the
affordance areas in the input images and the corresponding textual prompts. In
addition, we propose an object-level information constraint that maximizes the
mutual information between the visual features of a given object and the text
features of the category it belongs to. This enables the model to capture
high-quality representations for the object, providing more reliable semantic
priors for identifying affordance regions. Experimental results on the AGD20K
dataset show that the proposed method outperforms existing approaches and
achieves the new state-of-the-art in one-shot affordance learning.

</details>


### [101] [Enhanced Detection of Tiny Objects in Aerial Images](https://arxiv.org/abs/2509.17078)
*Kihyun Kim,Michalis Lazarou,Tania Stathaki*

Main category: cs.CV

TL;DR: 本文针对YOLOv8在检测小物体时性能不足的问题，特别是在航拍图像中检测微小物体时，提出了三种增强策略：输入图像分辨率调整、数据增强和注意力机制，并设计了MoonNet管道来提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 一阶段检测器如YOLOv8虽然训练速度快，但在检测小物体时性能较差，尤其是在航拍图像中由于目标分辨率低和背景杂乱，这一问题更加严重。

Method: 提出了三种增强策略：调整输入图像分辨率、数据增强和注意力机制。设计了MoonNet管道，将SE Block和CBAM注意力模块集成到YOLOv8的主干网络中，并增加了通道数。

Result: 图像尺寸放大和适当的数据增强可以提升性能。MoonNet主干网络相比原始YOLOv8获得了更高的检测精度，并在与YOLC模型集成时在微小物体基准测试中达到了最先进的性能。

Conclusion: MoonNet证明了其适应性和潜力，通过简单的增强策略和注意力机制的集成，有效提升了YOLOv8在微小物体检测上的性能。

Abstract: While one-stage detectors like YOLOv8 offer fast training speed, they often
under-perform on detecting small objects as a trade-off. This becomes even more
critical when detecting tiny objects in aerial imagery due to low-resolution
targets and cluttered backgrounds. To address this, we introduce three
enhancement strategies -- input image resolution adjustment, data augmentation,
and attention mechanisms -- that can be easily implemented on YOLOv8. We
demonstrate that image size enlargement and the proper use of augmentation can
lead to enhancement. Additionally, we designed a Mixture of Orthogonal
Neural-modules Network (MoonNet) pipeline which consists of attention-augmented
CNNs. Two well-known attention modules, the Squeeze-and-Excitation Block (SE
Block) and the Convolutional Block Attention Module (CBAM), were integrated
into the backbone of YOLOv8 with an increased number of channels, and the
MoonNet backbone obtained improved detection accuracy compared to the original
YOLOv8. MoonNet further proved its adaptability and potential by achieving
state-of-the-art performance on a tiny-object benchmark when integrated with
the YOLC model. Our codes are available at: https://github.com/Kihyun11/MoonNet

</details>


### [102] [A Dual-Modulation Framework for RGB-T Crowd Counting via Spatially Modulated Attention and Adaptive Fusion](https://arxiv.org/abs/2509.17079)
*Yuhong Feng,Hongtao Chen,Qi Zhang,Jie Chen,Zhaoxi He,Mingzhe Liu,Jianghai Liao*

Main category: cs.CV

TL;DR: 提出了一种双调制框架，通过空间调制注意力（SMA）和自适应融合调制（AFM）来解决RGB-热成像人群计数中的注意力分散和模态融合问题，显著提升了人群定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的方法在RGB-热成像人群计数中虽然能捕捉全局上下文，但缺乏空间归纳偏置导致注意力分散到无关背景区域，影响人群定位精度，且不同模态间的有效融合仍具挑战。

Method: 提出双调制框架：1）空间调制注意力（SMA）使用可学习的空间衰减掩码惩罚远距离token间的注意力，防止注意力扩散到背景；2）自适应融合调制（AFM）通过动态门控机制优先选择最可靠的模态进行自适应跨模态融合。

Result: 在RGB-热成像人群计数数据集上的大量实验表明，该方法相比之前的工作具有优越性能。

Conclusion: 所提出的双调制框架有效解决了RGB-热成像人群计数中的注意力分散和模态融合问题，提升了人群定位精度和计数性能。

Abstract: Accurate RGB-Thermal (RGB-T) crowd counting is crucial for public safety in
challenging conditions. While recent Transformer-based methods excel at
capturing global context, their inherent lack of spatial inductive bias causes
attention to spread to irrelevant background regions, compromising crowd
localization precision. Furthermore, effectively bridging the gap between these
distinct modalities remains a major hurdle. To tackle this, we propose the Dual
Modulation Framework, comprising two modules: Spatially Modulated Attention
(SMA), which improves crowd localization by using a learnable Spatial Decay
Mask to penalize attention between distant tokens and prevent focus from
spreading to the background; and Adaptive Fusion Modulation (AFM), which
implements a dynamic gating mechanism to prioritize the most reliable modality
for adaptive cross-modal fusion. Extensive experiments on RGB-T crowd counting
datasets demonstrate the superior performance of our method compared to
previous works. Code available at
https://github.com/Cht2924/RGBT-Crowd-Counting.

</details>


### [103] [HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis](https://arxiv.org/abs/2509.17083)
*Zipeng Wang,Dan Xu*

Main category: cs.CV

TL;DR: HyRF提出了一种结合显式高斯和神经场的混合场景表示方法，通过分解场景为紧凑的高斯集合和网格神经场，显著减少内存占用并保持实时渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)虽然能实现实时高质量新视角合成，但存在显著内存开销问题。现有神经场压缩方法难以捕捉高斯属性的高频空间变化，导致细节重建质量下降。

Method: HyRF将场景分解为：(1)存储关键高频参数的紧凑显式高斯集合；(2)预测剩余属性的网格神经场。采用解耦神经场架构分别建模几何和颜色，并提出混合渲染方案结合高斯泼溅和神经场预测背景。

Result: 实验表明HyRF在保持实时性能的同时，相比3DGS将模型大小减少20倍以上，并达到最先进的渲染质量。

Conclusion: HyRF成功结合了显式高斯和神经场的优势，在显著压缩模型的同时保持了高质量的实时渲染能力，为3D场景表示提供了有效的解决方案。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative
to NeRF-based approaches, enabling real-time, high-quality novel view synthesis
through explicit, optimizable 3D Gaussians. However, 3DGS suffers from
significant memory overhead due to its reliance on per-Gaussian parameters to
model view-dependent effects and anisotropic shapes. While recent works propose
compressing 3DGS with neural fields, these methods struggle to capture
high-frequency spatial variations in Gaussian properties, leading to degraded
reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a
novel scene representation that combines the strengths of explicit Gaussians
and neural fields. HyRF decomposes the scene into (1) a compact set of explicit
Gaussians storing only critical high-frequency parameters and (2) grid-based
neural fields that predict remaining properties. To enhance representational
capacity, we introduce a decoupled neural field architecture, separately
modeling geometry (scale, opacity, rotation) and view-dependent color.
Additionally, we propose a hybrid rendering scheme that composites Gaussian
splatting with a neural field-predicted background, addressing limitations in
distant scene representation. Experiments demonstrate that HyRF achieves
state-of-the-art rendering quality while reducing model size by over 20 times
compared to 3DGS and maintaining real-time performance. Our project page is
available at https://wzpscott.github.io/hyrf/.

</details>


### [104] [MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors](https://arxiv.org/abs/2509.17084)
*Binhua Huang,Nan Wang,Arjun Parakash,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCLIP-Lite是一个高效的双流视频动作识别框架，通过结合冻结的CLIP图像编码器和轻量级运动向量网络，仅训练小型MLP头，在UCF101数据集上达到89.2%的Top-1准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视频动作识别模型计算成本高且依赖大量视频预训练，而CLIP模型具有强大的零样本能力但缺乏时间信息，运动向量能高效提供时间信息。需要结合两者的优势。

Method: 提出双流后期融合框架：一个流使用冻结的CLIP图像编码器提取静态特征，另一个流使用轻量级网络处理原始运动向量提取动态特征，仅训练一个小的MLP融合头。

Result: 在UCF101数据集上达到89.2%的Top-1准确率，显著优于零样本基线(65.0%)和仅使用运动向量的基线(66.5%)。

Conclusion: 该工作为视频理解提供了一个高效的新基准，有效弥合了大型静态模型与低成本动态运动线索之间的差距。

Abstract: Video action recognition is a fundamental task in computer vision, but
state-of-the-art models are often computationally expensive and rely on
extensive video pre-training. In parallel, large-scale vision-language models
like Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shot
capabilities on static images, while motion vectors (MV) provide highly
efficient temporal information directly from compressed video streams. To
synergize the strengths of these paradigms, we propose MoCLIP-Lite, a simple
yet powerful two-stream late fusion framework for efficient video recognition.
Our approach combines features from a frozen CLIP image encoder with features
from a lightweight, supervised network trained on raw MV. During fusion, both
backbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head is
trained, ensuring extreme efficiency. Through comprehensive experiments on the
UCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy,
significantly outperforming strong zero-shot (65.0%) and MV-only (66.5%)
baselines. Our work provides a new, highly efficient baseline for video
understanding that effectively bridges the gap between large static models and
dynamic, low-cost motion cues. Our code and models are available at
https://github.com/microa/MoCLIP-Lite.

</details>


### [105] [SFN-YOLO: Towards Free-Range Poultry Detection via Scale-aware Fusion Networks](https://arxiv.org/abs/2509.17086)
*Jie Chen,Yuhong Feng,Tao Dai,Mingzhe Liu,Hongtao Chen,Zhaoxi He,Jiancong Bai*

Main category: cs.CV

TL;DR: SFN-YOLO是一种创新的家禽检测方法，通过尺度感知融合技术结合局部细节和全局上下文，在复杂自由放养环境中实现高效检测。该方法在M-SCOPE数据集上达到80.7%的mAP，参数仅7.2M，比基准模型减少35.1%。


<details>
  <summary>Details</summary>
Motivation: 自由放养环境中的家禽检测面临多尺度目标、遮挡和复杂动态背景等挑战，现有检测方法在这些场景下效果有限。

Method: 提出SFN-YOLO方法，采用尺度感知融合技术，结合局部特征和全局上下文信息。同时开发了专门针对自由放养条件的M-SCOPE数据集。

Result: 模型在mAP指标上达到80.7%，参数数量仅7.2M，比基准模型减少35.1%，同时保持了良好的跨域泛化能力。

Conclusion: SFN-YOLO具有高效实时的检测能力，支持自动化智能家禽养殖应用，代码和数据集已开源。

Abstract: Detecting and localizing poultry is essential for advancing smart poultry
farming. Despite the progress of detection-centric methods, challenges persist
in free-range settings due to multiscale targets, obstructions, and complex or
dynamic backgrounds. To tackle these challenges, we introduce an innovative
poultry detection approach named SFN-YOLO that utilizes scale-aware fusion.
This approach combines detailed local features with broader global context to
improve detection in intricate environments. Furthermore, we have developed a
new expansive dataset (M-SCOPE) tailored for varied free-range conditions.
Comprehensive experiments demonstrate our model achieves an mAP of 80.7% with
just 7.2M parameters, which is 35.1% fewer than the benchmark, while retaining
strong generalization capability across different domains. The efficient and
real-time detection capabilities of SFN-YOLO support automated smart poultry
farming. The code and dataset can be accessed at
https://github.com/chenjessiee/SFN-YOLO.

</details>


### [106] [AlignedGen: Aligning Style Across Generated Images](https://arxiv.org/abs/2509.17088)
*Jiexuan Zhang,Yiheng Du,Qian Wang,Weiqi Li,Yu Gu,Jian Zhang*

Main category: cs.CV

TL;DR: AlignedGen是一个无需训练的新框架，通过Shifted Position Embedding和Advanced Attention Sharing技术解决DiT模型中风格一致性问题，支持外部图像作为风格参考。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在相同风格提示下难以保持图像间的风格一致性，阻碍了在创意工作流中的实际应用。现有方法局限于U-Net架构，存在质量低、伪影等问题，且不兼容更优的DiT模型。

Method: 1. 提出Shifted Position Embedding解决位置嵌入冲突问题；2. 开发Advanced Attention Sharing技术套件；3. 设计高效的QKV特征提取算法支持外部风格参考。

Result: 实验验证该方法能有效增强生成图像间的风格一致性，同时保持精确的文图对齐。

Conclusion: AlignedGen框架成功解决了DiT模型中的风格一致性问题，为扩散模型在创意应用中的实际部署提供了有效解决方案。

Abstract: Despite their generative power, diffusion models struggle to maintain style
consistency across images conditioned on the same style prompt, hindering their
practical deployment in creative workflows. While several training-free methods
attempt to solve this, they are constrained to the U-Net architecture, which
not only leads to low-quality results and artifacts like object repetition but
also renders them incompatible with superior Diffusion Transformer (DiT). To
address these issues, we introduce AlignedGen, a novel training-free framework
that enhances style consistency across images generated by DiT models. Our work
first reveals a critical insight: naive attention sharing fails in DiT due to
conflicting positional signals from improper position embeddings. We introduce
Shifted Position Embedding (ShiftPE), an effective solution that resolves this
conflict by allocating a non-overlapping set of positional indices to each
image. Building on this foundation, we develop Advanced Attention Sharing
(AAS), a suite of three techniques meticulously designed to fully unleash the
potential of attention sharing within the DiT. Furthermore, to broaden the
applicability of our method, we present an efficient query, key, and value
feature extraction algorithm, enabling our method to seamlessly incorporate
external images as style references. Extensive experimental results validate
that our method effectively enhances style consistency across generated images
while maintaining precise text-to-image alignment.

</details>


### [107] [Uncertainty-Supervised Interpretable and Robust Evidential Segmentation](https://arxiv.org/abs/2509.17098)
*Yuzhu Li,An Sui,Fuping Wu,Xiahai Zhuang*

Main category: cs.CV

TL;DR: 提出一种自监督的不确定性估计方法，通过三个关于不确定性与图像梯度关系的原则设计监督损失，提高医学图像分割中不确定性的可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割中的不确定性估计方法缺乏有效监督，导致预测的可解释性和鲁棒性不足。

Method: 基于不确定性与边界梯度、噪声关系的三个原则，设计两种不确定性监督损失，实现自监督的不确定性学习。

Result: 在OOD场景下取得优于现有方法的结果，同时保持竞争力的分割性能，显著提升不确定性估计的可解释性和鲁棒性。

Conclusion: 该方法通过自监督方式有效指导不确定性学习，为医学图像分割提供了更可靠的不确定性估计工具。

Abstract: Uncertainty estimation has been widely studied in medical image segmentation
as a tool to provide reliability, particularly in deep learning approaches.
However, previous methods generally lack effective supervision in uncertainty
estimation, leading to low interpretability and robustness of the predictions.
In this work, we propose a self-supervised approach to guide the learning of
uncertainty. Specifically, we introduce three principles about the
relationships between the uncertainty and the image gradients around boundaries
and noise. Based on these principles, two uncertainty supervision losses are
designed. These losses enhance the alignment between model predictions and
human interpretation. Accordingly, we introduce novel quantitative metrics for
evaluating the interpretability and robustness of uncertainty. Experimental
results demonstrate that compared to state-of-the-art approaches, the proposed
method can achieve competitive segmentation performance and superior results in
out-of-distribution (OOD) scenarios while significantly improving the
interpretability and robustness of uncertainty estimation. Code is available
via https://github.com/suiannaius/SURE.

</details>


### [108] [The SAGES Critical View of Safety Challenge: A Global Benchmark for AI-Assisted Surgical Quality Assessment](https://arxiv.org/abs/2509.17100)
*Deepak Alapatt,Jennifer Eckhoff,Zhiliang Lyu,Yutong Ban,Jean-Paul Mazellier,Sarah Choksi,Kunyi Yang,2024 CVS Challenge Consortium,Quanzheng Li,Filippo Filicori,Xiang Li,Pietro Mascagni,Daniel A. Hashimoto,Guy Rosman,Ozanan Meireles,Nicolas Padoy*

Main category: cs.CV

TL;DR: SAGES CVS Challenge是首个由外科学会组织的AI竞赛，旨在通过腹腔镜胆囊切除术中的关键安全视图评估手术质量，解决了AI在手术质量评估中部署的关键障碍。


<details>
  <summary>Details</summary>
Motivation: 通过AI技术民主化手术专业知识获取，应用于培训、指导和认证，解决手术质量评估中性能、不确定性和临床变异性的挑战。

Method: 开发了EndoGlacier框架管理大规模异质手术视频和多标注者工作流，组织全球54个机构的国际合作，使用1000个专家标注视频进行AI模型评估。

Result: 13个国际团队参与，实现了17%的相对性能提升，80%以上的校准误差减少，以及17%的相对鲁棒性改进。

Conclusion: 该挑战赛为未来开发稳健、可临床部署的手术质量评估AI提供了方法论指导，推动了该领域的研究进展。

Abstract: Advances in artificial intelligence (AI) for surgical quality assessment
promise to democratize access to expertise, with applications in training,
guidance, and accreditation. This study presents the SAGES Critical View of
Safety (CVS) Challenge, the first AI competition organized by a surgical
society, using the CVS in laparoscopic cholecystectomy, a universally
recommended yet inconsistently performed safety step, as an exemplar of
surgical quality assessment. A global collaboration across 54 institutions in
24 countries engaged hundreds of clinicians and engineers to curate 1,000
videos annotated by 20 surgical experts according to a consensus-validated
protocol. The challenge addressed key barriers to real-world deployment in
surgery, including achieving high performance, capturing uncertainty in
subjective assessment, and ensuring robustness to clinical variability. To
enable this scale of effort, we developed EndoGlacier, a framework for managing
large, heterogeneous surgical video and multi-annotator workflows. Thirteen
international teams participated, achieving up to a 17\% relative gain in
assessment performance, over 80\% reduction in calibration error, and a 17\%
relative improvement in robustness over the state-of-the-art. Analysis of
results highlighted methodological trends linked to model performance,
providing guidance for future research toward robust, clinically deployable AI
for surgical quality assessment.

</details>


### [109] [CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception](https://arxiv.org/abs/2509.17107)
*Lingzhao Kong,Jiacheng Lin,Siyu Li,Kai Luo,Zhiyong Li,Kailun Yang*

Main category: cs.CV

TL;DR: CoBEVMoE是一个新颖的协作感知框架，在BEV空间中使用动态专家混合架构，通过动态生成专家来建模异构特征，提升多智能体协作感知性能。


<details>
  <summary>Details</summary>
Motivation: 现有中间融合方法主要关注对齐相似特征，但忽略了智能体之间的感知多样性。由于视角和空间位置差异，智能体往往获得异构观测数据。

Method: 提出CoBEVMoE框架，在BEV空间中采用动态专家混合架构，每个专家根据特定智能体的输入特征动态生成，能够提取独特可靠线索并关注共享语义。还引入动态专家度量损失来增强专家间多样性。

Result: 在OPV2V和DAIR-V2X-C数据集上的实验表明，CoBEVMoE达到最先进性能：相机BEV分割IoU提升+1.5%，激光雷达3D检测AP@50提升+3.0%。

Conclusion: 验证了基于专家的异构特征建模在多智能体协作感知中的有效性，代码将公开提供。

Abstract: Collaborative perception aims to extend sensing coverage and improve
perception accuracy by sharing information among multiple agents. However, due
to differences in viewpoints and spatial positions, agents often acquire
heterogeneous observations. Existing intermediate fusion methods primarily
focus on aligning similar features, often overlooking the perceptual diversity
among agents. To address this limitation, we propose CoBEVMoE, a novel
collaborative perception framework that operates in the Bird's Eye View (BEV)
space and incorporates a Dynamic Mixture-of-Experts (DMoE) architecture. In
DMoE, each expert is dynamically generated based on the input features of a
specific agent, enabling it to extract distinctive and reliable cues while
attending to shared semantics. This design allows the fusion process to
explicitly model both feature similarity and heterogeneity across agents.
Furthermore, we introduce a Dynamic Expert Metric Loss (DEML) to enhance
inter-expert diversity and improve the discriminability of the fused
representation. Extensive experiments on the OPV2V and DAIR-V2X-C datasets
demonstrate that CoBEVMoE achieves state-of-the-art performance. Specifically,
it improves the IoU for Camera-based BEV segmentation by +1.5% on OPV2V and the
AP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C, verifying the
effectiveness of expert-based heterogeneous feature modeling in multi-agent
collaborative perception. The source code will be made publicly available at
https://github.com/godk0509/CoBEVMoE.

</details>


### [110] [Stencil: Subject-Driven Generation with Context Guidance](https://arxiv.org/abs/2509.17120)
*Gordon Chen,Ziqi Huang,Cheston Tan,Ziwei Liu*

Main category: cs.CV

TL;DR: Stencil是一个新颖的文本到图像生成框架，通过联合使用两个扩散模型来解决主题一致性生成中的质量与效率权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型在跨生成和上下文中难以保持主题一致性，现有微调方法存在质量与效率的固有权衡：微调大模型质量高但计算昂贵，微调轻量级模型效率高但图像保真度差，且在小样本微调时会损害预训练模型的先验知识。

Method: Stencil框架在推理过程中联合使用两个扩散模型：高效微调一个轻量级模型来处理主题图像，同时使用一个大型冻结预训练模型在推理过程中提供上下文指导，以最小的开销注入丰富的先验知识来增强生成效果。

Result: Stencil能够在不到一分钟内生成高质量、新颖的主题渲染，实现了最先进的性能，为主题驱动生成设立了新的基准。

Conclusion: Stencil通过双模型协同工作的方法，成功解决了主题一致性生成中的质量-效率权衡问题，为文本到图像生成领域提供了高效且高质量的解决方案。

Abstract: Recent text-to-image diffusion models can generate striking visuals from text
prompts, but they often fail to maintain subject consistency across generations
and contexts. One major limitation of current fine-tuning approaches is the
inherent trade-off between quality and efficiency. Fine-tuning large models
improves fidelity but is computationally expensive, while fine-tuning
lightweight models improves efficiency but compromises image fidelity.
Moreover, fine-tuning pre-trained models on a small set of images of the
subject can damage the existing priors, resulting in suboptimal results. To
this end, we present Stencil, a novel framework that jointly employs two
diffusion models during inference. Stencil efficiently fine-tunes a lightweight
model on images of the subject, while a large frozen pre-trained model provides
contextual guidance during inference, injecting rich priors to enhance
generation with minimal overhead. Stencil excels at generating high-fidelity,
novel renditions of the subject in less than a minute, delivering
state-of-the-art performance and setting a new benchmark in subject-driven
generation.

</details>


### [111] [SAEC: Scene-Aware Enhanced Edge-Cloud Collaborative Industrial Vision Inspection with Multimodal LLM](https://arxiv.org/abs/2509.17136)
*Yuhao Tian,Zheming Yang*

Main category: cs.CV

TL;DR: SAEC是一个面向工业视觉检测的场景感知边缘-云协同框架，通过MLLM实现高效缺陷检测，在保持高精度的同时显著降低计算成本和能耗


<details>
  <summary>Details</summary>
Motivation: 解决工业视觉检测中MLLM计算成本过高与轻量边缘模型复杂场景检测能力不足的矛盾，实现精度与效率的平衡

Method: 包含三个协同组件：高效MLLM微调用于复杂缺陷检测、轻量级多尺度场景复杂度估计、自适应边缘-云调度器，根据场景复杂度动态分配计算资源

Result: 在MVTec AD和KSDD2数据集上分别达到85.11%和82.72%准确率，比Qwen提升22.1%和20.8%，比LLaVA提升33.3%和31.6%，运行时间减少22.4%，每正确决策能耗降低40%-74%

Conclusion: SAEC框架成功解决了工业视觉检测中的精度-效率权衡问题，为资源受限环境下的高质量缺陷检测提供了可行方案

Abstract: Industrial vision inspection requires high accuracy under stringent resource
constraints, yet existing approaches face a fundamental trade-off. Multimodal
LLMs (MLLMs) deliver strong reasoning capabilities but incur prohibitive
computational costs, while lightweight edge models often fail on complex cases.
In this paper, we present SAEC, a scene-aware enhanced edge-cloud collaborative
industrial vision inspection framework with MLLM. The framework is composed of
three synergistic components: (1) Efficient MLLM Fine-Tuning for Complex Defect
Inspection, (2) Lightweight Multiscale Scene-Complexity Estimation, and (3)
Adaptive Edge-Cloud Scheduler. Together, these modules enable robust defect
detection by tailoring multimodal reasoning to scene complexity and dynamically
balancing computation between edge and cloud resources. Experimental results on
MVTec AD and KSDD2 datasets demonstrate that SAEC attains 85.11% and 82.72%
accuracy, surpassing Qwen by 22.1% and 20.8%, and LLaVA by 33.3% and 31.6%. It
also reduces runtime by up to 22.4% and cuts energy per correct decision by
40%-74%. The code is available at https://github.com/YuHao-Tian/SAEC.

</details>


### [112] [SynergyNet: Fusing Generative Priors and State-Space Models for Facial Beauty Prediction](https://arxiv.org/abs/2509.17172)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 本文提出MD-Net，一种双流架构，结合预训练扩散模型的U-Net编码器和Vision Mamba，通过交叉注意力机制整合局部美学细节和全局面部结构，在人脸美颜预测任务上达到新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN和ViT的模型存在架构偏差：CNN擅长局部特征提取但难以处理长距离依赖，ViT能建模全局关系但计算成本高。需要一种能同时处理局部美学细节和全局面部和谐性的方法。

Method: MD-Net采用双流架构：第一流使用预训练潜在扩散模型的冻结U-Net编码器提取细粒度美学特征；第二流使用Vision Mamba高效捕获全局面部结构。通过交叉注意力机制整合两种互补表示。

Result: 在SCUT-FBP5500基准测试中，MD-Net达到Pearson相关系数0.9235，创造了新的最先进水平。

Conclusion: MD-Net证明了融合生成式和序列建模范式的混合架构在复杂视觉评估任务中的巨大潜力，成功解决了局部与全局特征提取的权衡问题。

Abstract: The automated prediction of facial beauty is a benchmark task in affective
computing that requires a sophisticated understanding of both local aesthetic
details (e.g., skin texture) and global facial harmony (e.g., symmetry,
proportions). Existing models, based on either Convolutional Neural Networks
(CNNs) or Vision Transformers (ViTs), exhibit inherent architectural biases
that limit their performance; CNNs excel at local feature extraction but
struggle with long-range dependencies, while ViTs model global relationships at
a significant computational cost. This paper introduces the
\textbf{Mamba-Diffusion Network (MD-Net)}, a novel dual-stream architecture
that resolves this trade-off by delegating specialized roles to
state-of-the-art models. The first stream leverages a frozen U-Net encoder from
a pre-trained latent diffusion model, providing a powerful generative prior for
fine-grained aesthetic qualities. The second stream employs a Vision Mamba
(Vim), a modern state-space model, to efficiently capture global facial
structure with linear-time complexity. By synergistically integrating these
complementary representations through a cross-attention mechanism, MD-Net
creates a holistic and nuanced feature space for prediction. Evaluated on the
SCUT-FBP5500 benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson
Correlation of \textbf{0.9235} and demonstrating the significant potential of
hybrid architectures that fuse generative and sequential modeling paradigms for
complex visual assessment tasks.

</details>


### [113] [Ambiguous Medical Image Segmentation Using Diffusion Schrödinger Bridge](https://arxiv.org/abs/2509.17187)
*Lalith Bharadwaj Baru,Kamalaker Dadi,Tapabrata Chakraborti,Raju S. Bapi*

Main category: cs.CV

TL;DR: SSB是首个将Schrödinger Bridge应用于模糊医学图像分割的方法，通过建模图像-掩码联合动态来提升分割性能，在多个数据集上达到SOTA效果


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临病变边界模糊和掩码变异性大的挑战，需要能够处理模糊边界并保持结构完整性的方法

Method: 提出Segmentation Schrödinger Bridge (SSB)方法，建模图像-掩码联合动态，使用新颖的损失函数保持多样性，并提出Diversity Divergence Index (DDI)量化评估者间变异性

Result: SSB在LIDC-IDRI、COCA和RACER数据集上实现了最先进的性能，能够准确分割模糊边界并保持结构完整性

Conclusion: SSB为模糊医学图像分割提供了一种有效的解决方案，能够处理边界不清晰的情况并量化分割的多样性

Abstract: Accurate segmentation of medical images is challenging due to unclear lesion
boundaries and mask variability. We introduce \emph{Segmentation Sch\"{o}dinger
Bridge (SSB)}, the first application of Sch\"{o}dinger Bridge for ambiguous
medical image segmentation, modelling joint image-mask dynamics to enhance
performance. SSB preserves structural integrity, delineates unclear boundaries
without additional guidance, and maintains diversity using a novel loss
function. We further propose the \emph{Diversity Divergence Index} ($D_{DDI}$)
to quantify inter-rater variability, capturing both diversity and consensus.
SSB achieves state-of-the-art performance on LIDC-IDRI, COCA, and RACER
(in-house) datasets.

</details>


### [114] [Echo-Path: Pathology-Conditioned Echo Video Generation](https://arxiv.org/abs/2509.17190)
*Kabir Hamzah Muhammad,Marawan Elbatel,Yi Qin,Xiaomeng Li*

Main category: cs.CV

TL;DR: 提出Echo-Path生成框架，通过病理条件机制合成特定心脏病变的超声心动图视频，用于解决罕见病理数据稀缺问题，提升自动诊断模型的性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，超声心动图是重要诊断工具，但某些病理数据稀缺限制了自动诊断模型的发展。

Method: 在先进回声视频生成器中引入病理条件机制，学习并控制疾病特异性结构和运动模式，生成针对房间隔缺损和肺动脉高压的超声视频。

Result: 合成视频视觉保真度高，分布距离低；生成的回声显示合理的病理标志；使用合成数据训练的分类器在真实数据上泛化良好，将ASD和PAH诊断准确率分别提升7%和8%。

Conclusion: Echo-Path能有效生成高质量病理特异性超声视频，解决数据稀缺问题，显著提升下游诊断性能，具有重要临床应用价值。

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality
globally, and echocardiography is critical for diagnosis of both common and
congenital cardiac conditions. However, echocardiographic data for certain
pathologies are scarce, hindering the development of robust automated diagnosis
models. In this work, we propose Echo-Path, a novel generative framework to
produce echocardiogram videos conditioned on specific cardiac pathologies.
Echo-Path can synthesize realistic ultrasound video sequences that exhibit
targeted abnormalities, focusing here on atrial septal defect (ASD) and
pulmonary arterial hypertension (PAH). Our approach introduces a
pathology-conditioning mechanism into a state-of-the-art echo video generator,
allowing the model to learn and control disease-specific structural and motion
patterns in the heart. Quantitative evaluation demonstrates that the synthetic
videos achieve low distribution distances, indicating high visual fidelity.
Clinically, the generated echoes exhibit plausible pathology markers.
Furthermore, classifiers trained on our synthetic data generalize well to real
data and, when used to augment real training sets, it improves downstream
diagnosis of ASD and PAH by 7\% and 8\% respectively. Code, weights and dataset
are available here https://github.com/Marshall-mk/EchoPathv1

</details>


### [115] [VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery](https://arxiv.org/abs/2509.17191)
*Jinchao Ge,Tengfei Cheng,Biao Wu,Zeyu Zhang,Shiya Huang,Judith Bishop,Gillian Shepherd,Meng Fang,Ling Chen,Yang Zhao*

Main category: cs.CV

TL;DR: VaseVL是一个针对古希腊陶器分析的MLLM系统，通过SFT-then-RL方法将评估转化为监督，结合问题类型分类和针对性奖励优化，在风格分类和历史归属任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在文化遗产分析领域存在局限性：通用模型缺乏领域专业知识，而SFT方法容易过拟合表面模式，导致认证和历史归属推理脆弱。需要为古希腊陶器分析开发具有专家级推理能力的稳健模型。

Method: 构建问题类型分类法，通过探测SFT模型定位类型特定的性能差距，使用类型条件化、面向组合性的奖励针对这些差距进行优化。同时发布了包含31,773张图像的VaseVQA基准数据集。

Result: 在风格分类和历史归属任务上达到最先进结果，相比仅使用SFT的基线模型，在组合鲁棒性方面有显著提升。

Conclusion: 验证了诊断引导、分类法条件化的奖励工程方法的有效性，为未来研究提供了可复用的资源。代码和数据集将在GitHub上公开。

Abstract: Analyzing cultural-heritage artifacts remains challenging for MLLMs: general
models lack domain expertise, and SFT often overfits superficial patterns,
yielding brittle reasoning for authentication and historical attribution. This
raises the question of how to equip MLLMs with robust, expert-level reasoning
for ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns
evaluation into supervision: we construct a taxonomy of question types, probe
the SFT model to localize type-specific performance gaps, and optimize with
type-conditioned, compositionality-oriented rewards targeting those gaps. We
also release VaseVQA, a comprehensive benchmark of 31,773 images designed to
probe deep understanding. Experiments show state-of-the-art results on style
classification and historical attribution with marked gains in compositional
robustness over SFT-only baselines, validating diagnosis-guided,
taxonomy-conditioned reward engineering and providing a reusable resource for
future research. Code and dataset will be available at
https://github.com/AIGeeksGroup/VaseVQA.

</details>


### [116] [Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation](https://arxiv.org/abs/2509.17206)
*Gunner Stone,Sushmita Sarker,Alireza Tavakkoli*

Main category: cs.CV

TL;DR: 提出了一种基于扩散的框架，将逐点语义条件嵌入到生成过程中，联合合成几何和语义信息，生成结构连贯且分割感知的3D点云。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法主要捕获几何信息，语义信息通常通过外部分割或聚类后处理施加，而不是集成到生成过程本身。

Method: 扩散框架将每个点与其语义标签对应的条件变量关联，指导扩散动力学，实现几何和语义的联合合成。

Result: 通过比较引导和非引导扩散过程，证明了条件变量对扩散动力学和生成质量的显著影响。实验验证了方法的有效性。

Conclusion: 该方法能够生成针对特定部件和特征的详细准确3D点云，在几何和语义联合生成方面表现出色。

Abstract: Generating realistic 3D point clouds is a fundamental problem in computer
vision with applications in remote sensing, robotics, and digital object
modeling. Existing generative approaches primarily capture geometry, and when
semantics are considered, they are typically imposed post hoc through external
segmentation or clustering rather than integrated into the generative process
itself. We propose a diffusion-based framework that embeds per-point semantic
conditioning directly within generation. Each point is associated with a
conditional variable corresponding to its semantic label, which guides the
diffusion dynamics and enables the joint synthesis of geometry and semantics.
This design produces point clouds that are both structurally coherent and
segmentation-aware, with object parts explicitly represented during synthesis.
Through a comparative analysis of guided and unguided diffusion processes, we
demonstrate the significant impact of conditional variables on diffusion
dynamics and generation quality. Extensive experiments validate the efficacy of
our approach, producing detailed and accurate 3D point clouds tailored to
specific parts and features.

</details>


### [117] [Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds](https://arxiv.org/abs/2509.17207)
*Gunner Stone,Youngsook Choi,Alireza Tavakkoli,Ankita Shukla*

Main category: cs.CV

TL;DR: Point-RTD是一种新颖的3D点云预训练策略，通过替换令牌去噪的破坏-重建框架提升token鲁棒性，相比传统掩码重建方法在性能和效率上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于掩码的重建任务通过隐藏数据片段进行预测，但这种方法在3D点云任务中可能不够有效。Point-RTD旨在通过破坏-重建框架更有效地学习结构先验知识。

Method: Point-RTD采用判别器-生成器架构，通过破坏点云token然后进行去噪重建，而非传统的掩码预测方法。这种破坏-重建框架能够更好地提升token的鲁棒性。

Result: 在ShapeNet数据集上，Point-RTD相比PointMAE重建误差降低93%以上，测试集上的Chamfer距离降低14倍以上。方法收敛更快，在ShapeNet、ModelNet10和ModelNet40基准测试中分类准确率更高。

Conclusion: Point-RTD在3D点云预训练任务中明显优于基线Point-MAE框架，在重建精度、收敛速度和分类性能方面都取得了显著提升。

Abstract: Pre-training strategies play a critical role in advancing the performance of
transformer-based models for 3D point cloud tasks. In this paper, we introduce
Point-RTD (Replaced Token Denoising), a novel pretraining strategy designed to
improve token robustness through a corruption-reconstruction framework. Unlike
traditional mask-based reconstruction tasks that hide data segments for later
prediction, Point-RTD corrupts point cloud tokens and leverages a
discriminator-generator architecture for denoising. This shift enables more
effective learning of structural priors and significantly enhances model
performance and efficiency. On the ShapeNet dataset, Point-RTD reduces
reconstruction error by over 93% compared to PointMAE, and achieves more than
14x lower Chamfer Distance on the test set. Our method also converges faster
and yields higher classification accuracy on ShapeNet, ModelNet10, and
ModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework
in every case.

</details>


### [118] [MirrorSAM2: Segment Mirror in Videos with Depth Perception](https://arxiv.org/abs/2509.17220)
*Mingchen Xu,Yukun Lai,Ze Ji,Jing Wu*

Main category: cs.CV

TL;DR: MirrorSAM2是首个将Segment Anything Model 2（SAM2）适配到RGB-D视频镜面分割任务的框架，通过四个定制模块解决镜面检测中的关键挑战，在VMD和DVMD基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决镜面检测中的反射模糊和纹理混淆等关键挑战，将SAM2的能力扩展到无需提示的自动视频镜面分割场景。

Method: 提出四个定制模块：深度扭曲模块（RGB和深度对齐）、深度引导多尺度点提示生成器（自动提示生成）、频率细节注意力融合模块（增强结构边界）、镜面掩码解码器（可学习镜面令牌进行精细分割）。

Result: 在VMD和DVMD基准测试中实现最先进性能，即使在小型镜面、弱边界和强反射等挑战性条件下也表现优异。

Conclusion: MirrorSAM2成功将SAM2扩展到无提示设置的自动视频镜面分割任务，是首个实现这一目标的工作，证明了RGB和深度信息互补性的有效利用。

Abstract: This paper presents MirrorSAM2, the first framework that adapts Segment
Anything Model 2 (SAM2) to the task of RGB-D video mirror segmentation.
MirrorSAM2 addresses key challenges in mirror detection, such as reflection
ambiguity and texture confusion, by introducing four tailored modules: a Depth
Warping Module for RGB and depth alignment, a Depth-guided Multi-Scale Point
Prompt Generator for automatic prompt generation, a Frequency Detail Attention
Fusion Module to enhance structural boundaries, and a Mirror Mask Decoder with
a learnable mirror token for refined segmentation. By fully leveraging the
complementarity between RGB and depth, MirrorSAM2 extends SAM2's capabilities
to the prompt-free setting. To our knowledge, this is the first work to enable
SAM2 for automatic video mirror segmentation. Experiments on the VMD and DVMD
benchmark demonstrate that MirrorSAM2 achieves SOTA performance, even under
challenging conditions such as small mirrors, weak boundaries, and strong
reflections.

</details>


### [119] [DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction](https://arxiv.org/abs/2509.17232)
*Bo Liu,Runlong Li,Li Zhou,Yan Zhou*

Main category: cs.CV

TL;DR: DT-NeRF方法通过结合扩散模型和Transformer，提升了3D场景重建中的细节恢复和多视角一致性，在稀疏视角和复杂几何场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF方法在稀疏视角下细节恢复不足，多视角一致性较差，需要一种能够有效恢复细节并保持高精度的3D重建方法。

Method: 提出DT-NeRF方法，将扩散模型与Transformer结合，利用扩散模型优化神经辐射场，增强细节恢复能力并保持多视角一致性。

Result: 在Matterport3D和ShapeNet数据集上，DT-NeRF在PSNR、SSIM、Chamfer Distance和Fidelity等指标上显著优于传统NeRF和其他先进方法。消融实验证实了扩散和Transformer模块的关键作用。

Conclusion: DT-NeRF展示了模块间的协同效应，为3D场景重建提供了高效准确的解决方案。未来研究可进一步优化模型，探索更先进的生成模型和网络架构以提升在大规模动态场景中的性能。

Abstract: This paper proposes a Diffusion Model-Optimized Neural Radiance Field
(DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency
in 3D scene reconstruction. By combining diffusion models with Transformers,
DT-NeRF effectively restores details under sparse viewpoints and maintains high
accuracy in complex geometric scenes. Experimental results demonstrate that
DT-NeRF significantly outperforms traditional NeRF and other state-of-the-art
methods on the Matterport3D and ShapeNet datasets, particularly in metrics such
as PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further
confirm the critical role of the diffusion and Transformer modules in the
model's performance, with the removal of either module leading to a decline in
performance. The design of DT-NeRF showcases the synergistic effect between
modules, providing an efficient and accurate solution for 3D scene
reconstruction. Future research may focus on further optimizing the model,
exploring more advanced generative models and network architectures to enhance
its performance in large-scale dynamic scenes.

</details>


### [120] [SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views](https://arxiv.org/abs/2509.17246)
*Ranran Huang,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: SPFSplatV2是一个无需地面真实相机位姿的高效3D高斯溅射框架，可从稀疏多视角图像实现高质量新视角合成


<details>
  <summary>Details</summary>
Motivation: 消除对地面真实相机位姿的依赖，使3D重建方法能够利用更大更多样化的数据集，解决极端视角变化和有限图像重叠的挑战

Method: 使用共享特征提取骨干网络，在规范空间中同时预测3D高斯基元和相机位姿；引入掩码注意力机制估计目标位姿，使用重投影损失强制像素对齐的高斯基元

Result: 在领域内和领域外的新视角合成任务中达到最先进性能，即使在极端视角变化和有限图像重叠情况下也表现优异，超越了依赖几何监督的相对位姿估计方法

Conclusion: 该方法通过消除对地面真实位姿的依赖，提供了扩展到更大更多样化数据集的潜力，展示了在无位姿监督下实现高质量3D重建的可能性

Abstract: We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian
splatting from sparse multi-view images, requiring no ground-truth poses during
training and inference. It employs a shared feature extraction backbone,
enabling simultaneous prediction of 3D Gaussian primitives and camera poses in
a canonical space from unposed inputs. A masked attention mechanism is
introduced to efficiently estimate target poses during training, while a
reprojection loss enforces pixel-aligned Gaussian primitives, providing
stronger geometric constraints. We further demonstrate the compatibility of our
training framework with different reconstruction architectures, resulting in
two model variants. Remarkably, despite the absence of pose supervision, our
method achieves state-of-the-art performance in both in-domain and
out-of-domain novel view synthesis, even under extreme viewpoint changes and
limited image overlap, and surpasses recent methods that rely on geometric
supervision for relative pose estimation. By eliminating dependence on
ground-truth poses, our method offers the scalability to leverage larger and
more diverse datasets. Code and pretrained models will be available on our
project page: https://ranrhuang.github.io/spfsplatv2/.

</details>


### [121] [Optimized Learned Image Compression for Facial Expression Recognition](https://arxiv.org/abs/2509.17262)
*Xiumei Li,Marc Windsheimer,Misha Sadeghi,Björn Eskofier,André Kaup*

Main category: cs.CV

TL;DR: 本文提出了一种端到端模型，通过定制损失函数平衡压缩和识别性能，在面部表情识别任务中同时提升压缩效率和分类准确率。


<details>
  <summary>Details</summary>
Motivation: 解决面部表情识别任务中，有损压缩导致特征退化和准确率下降的问题，需要在压缩效率和识别性能之间找到平衡。

Method: 采用端到端模型设计，引入定制损失函数来优化模型，平衡压缩和识别性能，并研究不同损失项权重对平衡的影响。

Result: 仅微调压缩模型使分类准确率提升0.71%，压缩效率提升49.32%；联合优化后准确率提升4.04%，效率提升89.12%。分类模型在压缩和未压缩数据上均保持高准确率，压缩模型在高压缩率下仍能可靠保留图像细节。

Conclusion: 提出的联合优化方法有效解决了压缩与识别性能的权衡问题，在保持高分类准确率的同时显著提升了压缩效率。

Abstract: Efficient data compression is crucial for the storage and transmission of
visual data. However, in facial expression recognition (FER) tasks, lossy
compression often leads to feature degradation and reduced accuracy. To address
these challenges, this study proposes an end-to-end model designed to preserve
critical features and enhance both compression and recognition performance. A
custom loss function is introduced to optimize the model, tailored to balance
compression and recognition performance effectively. This study also examines
the influence of varying loss term weights on this balance. Experimental
results indicate that fine-tuning the compression model alone improves
classification accuracy by 0.71% and compression efficiency by 49.32%, while
joint optimization achieves significant gains of 4.04% in accuracy and 89.12%
in efficiency. Moreover, the findings demonstrate that the jointly optimized
classification model maintains high accuracy on both compressed and
uncompressed data, while the compression model reliably preserves image
details, even at high compression rates.

</details>


### [122] [Task-Oriented Communications for 3D Scene Representation: Balancing Timeliness and Fidelity](https://arxiv.org/abs/2509.17282)
*Xiangmin Xu,Zhen Meng,Kan Chen,Jiaming Yang,Emma Li,Philip G. Zhao,David Flynn*

Main category: cs.CV

TL;DR: 本文提出了一种基于上下文bandit PPO框架的无线网络图像选择优化方法，通过平衡数据新鲜度和语义信息来提升实时3D场景表示的质量和延迟性能


<details>
  <summary>Details</summary>
Motivation: 实时3D场景表示在数字制造、VR/AR/MR和元宇宙等应用中至关重要，但如何在时间性和保真度之间取得平衡仍是一个挑战

Method: 采用上下文bandit近端策略优化(PPO)框架，结合信息年龄(AoI)和语义信息来优化图像选择，提出了ω-threshold和ω-wait两种策略

Result: 在标准数据集和基线3D场景表示模型上的实验结果表明，该方法在保持低延迟的同时提高了表示保真度

Conclusion: 该工作通过优化动态环境中时间性和保真度之间的权衡，推动了实时3D场景表示技术的发展

Abstract: Real-time Three-dimensional (3D) scene representation is a foundational
element that supports a broad spectrum of cutting-edge applications, including
digital manufacturing, Virtual, Augmented, and Mixed Reality (VR/AR/MR), and
the emerging metaverse. Despite advancements in real-time communication and
computing, achieving a balance between timeliness and fidelity in 3D scene
representation remains a challenge. This work investigates a wireless network
where multiple homogeneous mobile robots, equipped with cameras, capture an
environment and transmit images to an edge server over channels for 3D
representation. We propose a contextual-bandit Proximal Policy Optimization
(PPO) framework incorporating both Age of Information (AoI) and semantic
information to optimize image selection for representation, balancing data
freshness and representation quality. Two policies -- the $\omega$-threshold
and $\omega$-wait policies -- together with two benchmark methods are
evaluated, timeliness embedding and weighted sum, on standard datasets and
baseline 3D scene representation models. Experimental results demonstrate
improved representation fidelity while maintaining low latency, offering
insight into the model's decision-making process. This work advances real-time
3D scene representation by optimizing the trade-off between timeliness and
fidelity in dynamic environments.

</details>


### [123] [Automated Facility Enumeration for Building Compliance Checking using Door Detection and Large Language Models](https://arxiv.org/abs/2509.17283)
*Licheng Zhan,Bach Le,Naveed Akhtar,Tuan Ngo*

Main category: cs.CV

TL;DR: 本文提出了一种新的建筑合规检查任务：自动化设施枚举，通过结合门检测和基于大语言模型的推理来验证设施数量是否符合法规要求。


<details>
  <summary>Details</summary>
Motivation: 建筑合规检查中设施类型和空间分布的准确枚举是一个关键但被忽视的问题，手动执行耗时费力，而大语言模型的发展为自动化提供了新机会。

Method: 提出了一种新颖方法，将门检测与基于LLM的推理相结合，首次将LLMs应用于此任务，并通过思维链（CoT）管道进一步提升性能。

Result: 在真实世界和合成的平面图数据上的实验证明了该方法的有效性和鲁棒性，能够很好地泛化到不同的数据集和设施类型。

Conclusion: 该方法为建筑合规检查的自动化提供了有效的解决方案，填补了现有工作流中的关键空白。

Abstract: Building compliance checking (BCC) is a critical process for ensuring that
constructed facilities meet regulatory standards. A core component of BCC is
the accurate enumeration of facility types and their spatial distribution.
Despite its importance, this problem has been largely overlooked in the
literature, posing a significant challenge for BCC and leaving a critical gap
in existing workflows. Performing this task manually is time-consuming and
labor-intensive. Recent advances in large language models (LLMs) offer new
opportunities to enhance automation by combining visual recognition with
reasoning capabilities. In this paper, we introduce a new task for BCC:
automated facility enumeration, which involves validating the quantity of each
facility type against statutory requirements. To address it, we propose a novel
method that integrates door detection with LLM-based reasoning. We are the
first to apply LLMs to this task and further enhance their performance through
a Chain-of-Thought (CoT) pipeline. Our approach generalizes well across diverse
datasets and facility types. Experiments on both real-world and synthetic floor
plan data demonstrate the effectiveness and robustness of our method.

</details>


### [124] [DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking](https://arxiv.org/abs/2509.17323)
*Buyin Deng,Lingxin Huang,Kai Luo,Fei Teng,Kailun Yang*

Main category: cs.CV

TL;DR: DepTR-MOT是一个基于DETR的多目标跟踪方法，通过引入实例级深度信息来解决遮挡和近距离交互问题，在机器人环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的多目标跟踪方法主要依赖2D线索（如边界框和运动建模），在遮挡和近距离交互场景下表现不佳。深度信息有潜力缓解这些问题，但大多数MOT数据集缺乏深度标注，导致深度信息在该领域未被充分利用。

Method: 提出DepTR-MOT，一个基于DETR的检测器，包含两个关键创新：(1)基于基础模型的实例级软深度标签监督，用于细化深度预测；(2)密集深度图的蒸馏，以保持全局深度一致性。这些策略使模型能够在推理时输出实例级深度，无需基础模型且不增加计算成本。

Result: 在QuadTrack和DanceTrack数据集上的实验表明，该方法分别达到了27.59和44.47的HOTA分数。特别是在机器人平台数据集QuadTrack上，结果突出了该方法在处理遮挡和近距离交互挑战方面的优势。

Conclusion: 通过引入深度线索，DepTR-MOT增强了TBD范式的鲁棒性，有效解决了遮挡和近距离交互问题，在机器人跟踪场景中具有明显优势。

Abstract: Visual Multi-Object Tracking (MOT) is a crucial component of robotic
perception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D
cues, such as bounding boxes and motion modeling, which struggle under
occlusions and close-proximity interactions. Trackers relying on these 2D cues
are particularly unreliable in robotic environments, where dense targets and
frequent occlusions are common. While depth information has the potential to
alleviate these issues, most existing MOT datasets lack depth annotations,
leading to its underexploited role in the domain. To unveil the potential of
depth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based
detector enhanced with instance-level depth information. Specifically, we
propose two key innovations: (i) foundation model-based instance-level soft
depth label supervision, which refines depth prediction, and (ii) the
distillation of dense depth maps to maintain global depth consistency. These
strategies enable DepTR-MOT to output instance-level depth during inference,
without requiring foundation models and without additional computational cost.
By incorporating depth cues, our method enhances the robustness of the TBD
paradigm, effectively resolving occlusion and close-proximity challenges.
Experiments on both the QuadTrack and DanceTrack datasets demonstrate the
effectiveness of our approach, achieving HOTA scores of 27.59 and 44.47,
respectively. In particular, results on QuadTrack, a robotic platform MOT
dataset, highlight the advantages of our method in handling occlusion and
close-proximity challenges in robotic tracking. The source code will be made
publicly available at https://github.com/warriordby/DepTR-MOT.

</details>


### [125] [UIPro: Unleashing Superior Interaction Capability For GUI Agents](https://arxiv.org/abs/2509.17328)
*Hongxin Li,Jingran Su,Jingfan Chen,Zheng Ju,Yuntao Chen,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: UIPro是一个基于多模态理解能力的通用GUI代理，通过大规模多平台GUI交互数据和统一动作空间训练，在多个GUI任务基准上表现出色


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理方法受限于场景有限、数据规模不足和异构动作空间，阻碍了通用GUI代理的发展

Method: 首先构建包含2060万GUI理解任务的综合数据集进行预训练，然后建立统一动作空间整合异构GUI任务数据集，通过持续微调提升动作预测能力

Result: 实验结果表明UIPro在多个平台的各种GUI任务基准上表现出优越性能

Conclusion: UIPro通过大规模多平台数据和统一动作空间的方法，有效解决了通用GUI代理开发中的关键问题

Abstract: Building autonomous agents that perceive and operate graphical user
interfaces (GUIs) like humans has long been a vision in the field of artificial
intelligence. Central to these agents is the capability for GUI interaction,
which involves GUI understanding and planning capabilities. Existing methods
have tried developing GUI agents based on the multi-modal comprehension ability
of vision-language models (VLMs). However, the limited scenario, insufficient
size, and heterogeneous action spaces hinder the progress of building
generalist GUI agents. To resolve these issues, this paper proposes
\textbf{UIPro}, a novel generalist GUI agent trained with extensive
multi-platform and multi-task GUI interaction data, coupled with a unified
action space. We first curate a comprehensive dataset encompassing 20.6 million
GUI understanding tasks to pre-train UIPro, granting it a strong GUI grounding
capability, which is key to downstream GUI agent tasks. Subsequently, we
establish a unified action space to harmonize heterogeneous GUI agent task
datasets and produce a merged dataset to foster the action prediction ability
of UIPro via continued fine-tuning. Experimental results demonstrate UIPro's
superior performance across multiple GUI task benchmarks on various platforms,
highlighting the effectiveness of our approach.

</details>


### [126] [SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction](https://arxiv.org/abs/2509.17329)
*Neham Jain,Andrew Jong,Sebastian Scherer,Ioannis Gkioulekas*

Main category: cs.CV

TL;DR: SmokeSeer是一种从视频中同时进行3D场景重建和烟雾去除的方法，利用热成像和RGB图像，通过3D高斯泼溅技术分解烟雾和非烟雾成分，能够处理各种密度和动态变化的烟雾。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的烟雾会严重降低图像质量并阻碍可见性。现有的图像恢复方法要么依赖容易产生幻觉的数据驱动先验，要么仅限于静态低密度烟雾。

Method: 基于3D高斯泼溅技术，融合热成像和RGB图像信息，利用热成像中散射减少的特性透过烟雾观察场景，将场景明确分解为烟雾和非烟雾成分。

Result: 在合成数据和真实世界多视角烟雾数据集上验证了方法的有效性，能够处理广泛的烟雾密度范围并适应时间变化的烟雾。

Conclusion: SmokeSeer提供了一种有效的烟雾去除和3D场景重建解决方案，项目网站提供了开源代码和数据。

Abstract: Smoke in real-world scenes can severely degrade the quality of images and
hamper visibility. Recent methods for image restoration either rely on
data-driven priors that are susceptible to hallucinations, or are limited to
static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D
scene reconstruction and smoke removal from a video capturing multiple views of
a scene. Our method uses thermal and RGB images, leveraging the fact that the
reduced scattering in thermal images enables us to see through the smoke. We
build upon 3D Gaussian splatting to fuse information from the two image
modalities, and decompose the scene explicitly into smoke and non-smoke
components. Unlike prior approaches, SmokeSeer handles a broad range of smoke
densities and can adapt to temporally varying smoke. We validate our approach
on synthetic data and introduce a real-world multi-view smoke dataset with RGB
and thermal images. We provide open-source code and data at the project
website.

</details>


### [127] [Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model](https://arxiv.org/abs/2509.17365)
*Amanuel Tafese Dufera*

Main category: cs.CV

TL;DR: 本文提出了使用Transformer模型进行图像描述生成的方法，以解决传统CNN-LSTM方法在训练速度和信息保留方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统图像描述方法（如CNN-LSTM）存在训练速度慢、处理长序列时信息丢失的问题。Transformer的自注意力机制能够更好地捕捉数据中的短程和长程依赖关系，并支持高效的并行化处理。

Method: 采用Transformer架构，结合EfficientNetB0 CNN进行特征提取，在Flickr30k数据集上进行数据预处理和模型训练，并加入注意力机制。

Result: 该方法实现了高效的并行化训练和推理，克服了传统方法的局限性。

Conclusion: Transformer模型在图像描述任务中展现出优越的性能，特别是在处理长序列依赖和训练效率方面具有明显优势。

Abstract: Automatic image captioning, a multifaceted task bridging computer vision and
natural lan- guage processing, aims to generate descriptive textual content
from visual input. While Convolutional Neural Networks (CNNs) and Long
Short-Term Memory (LSTM) networks have achieved significant advancements, they
present limitations. The inherent sequential nature of RNNs leads to sluggish
training and inference times. LSTMs further struggle with retaining information
from earlier sequence elements when dealing with very long se- quences. This
project presents a comprehensive guide to constructing and comprehending
transformer models for image captioning. Transformers employ self-attention
mechanisms, capturing both short- and long-range dependencies within the data.
This facilitates efficient parallelization during both training and inference
phases. We leverage the well-established Transformer architecture, recognized
for its effectiveness in managing sequential data, and present a meticulous
methodology. Utilizing the Flickr30k dataset, we conduct data pre- processing,
construct a model architecture that integrates an EfficientNetB0 CNN for fea-
ture extraction, and train the model with attention mechanisms incorporated.
Our approach exemplifies the utilization of parallelization for efficient
training and inference. You can find the project on GitHub.

</details>


### [128] [Revisiting Vision Language Foundations for No-Reference Image Quality Assessment](https://arxiv.org/abs/2509.17374)
*Ankit Yadav,Ta Duc Huy,Lingqiao Liu*

Main category: cs.CV

TL;DR: 本文首次系统评估了六种预训练骨干网络（CLIP、SigLIP2、DINOv2、DINOv3、Perception、ResNet）在无参考图像质量评估任务中的表现，发现SigLIP2表现优异且激活函数选择对模型泛化能力至关重要，提出可学习的激活选择机制实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言预训练在无参考图像质量评估中显示出潜力，但现代Vision Transformer基础的相对优势尚未得到充分理解，需要系统评估不同预训练骨干的性能。

Method: 使用相同的轻量级MLP头对六种预训练骨干进行微调，发现sigmoid激活函数优于ReLU和GELU，进而提出可学习的激活选择机制，自适应确定每个通道的非线性函数。

Result: SigLIP2表现一致强劲，可学习激活选择机制在CLIVE、KADID10K和AGIQA3K数据集上实现了新的SOTA SRCC性能，消融实验证实了该方法的有效性。

Conclusion: 研究建立了强大且资源高效的NR-IQA基线，揭示了激活函数选择对图像质量评估模型泛化能力的关键作用，提出的自适应机制消除了手动设计激活函数的需求。

Abstract: Large-scale vision language pre-training has recently shown promise for
no-reference image-quality assessment (NR-IQA), yet the relative merits of
modern Vision Transformer foundations remain poorly understood. In this work,
we present the first systematic evaluation of six prominent pretrained
backbones, CLIP, SigLIP2, DINOv2, DINOv3, Perception, and ResNet, for the task
of No-Reference Image Quality Assessment (NR-IQA), each finetuned using an
identical lightweight MLP head. Our study uncovers two previously overlooked
factors: (1) SigLIP2 consistently achieves strong performance; and (2) the
choice of activation function plays a surprisingly crucial role, particularly
for enhancing the generalization ability of image quality assessment models.
Notably, we find that simple sigmoid activations outperform commonly used ReLU
and GELU on several benchmarks. Motivated by this finding, we introduce a
learnable activation selection mechanism that adaptively determines the
nonlinearity for each channel, eliminating the need for manual activation
design, and achieving new state-of-the-art SRCC on CLIVE, KADID10K, and
AGIQA3K. Extensive ablations confirm the benefits across architectures and
regimes, establishing strong, resource-efficient NR-IQA baselines.

</details>


### [129] [Diff-GNSS: Diffusion-based Pseudorange Error Estimation](https://arxiv.org/abs/2509.17397)
*Jiaqi Zhu,Shouyi Lu,Ziyao Li,Guirong Zhuo,Lu Xiong*

Main category: cs.CV

TL;DR: Diff-GNSS是一个基于条件扩散模型的GNSS伪距误差估计框架，通过粗到细的两阶段方法解决复杂误差分布问题，显著提高了城市环境下的定位精度。


<details>
  <summary>Details</summary>
Motivation: GNSS在城市定位中面临多径和非视距传播带来的大测量误差，现有学习方法受限于复杂误差分布，需要更有效的误差建模方法。

Method: 提出两阶段框架：1）基于Mamba模块的粗估计提供初始预测；2）条件去噪扩散层进行细粒度误差建模，使用三个GNSS测量质量特征作为条件指导反向去噪过程，并加入逐卫星不确定性建模。

Result: 在公开和自收集数据集上的实验表明，Diff-GNSS在多个指标上持续优于最先进的基线方法。

Conclusion: 这是扩散模型在伪距误差估计中的首次应用，提出的扩散精炼模块是即插即用的，可以轻松集成到现有网络中显著提高估计精度。

Abstract: Global Navigation Satellite Systems (GNSS) are vital for reliable urban
positioning. However, multipath and non-line-of-sight reception often introduce
large measurement errors that degrade accuracy. Learning-based methods for
predicting and compensating pseudorange errors have gained traction, but their
performance is limited by complex error distributions. To address this
challenge, we propose Diff-GNSS, a coarse-to-fine GNSS measurement
(pseudorange) error estimation framework that leverages a conditional diffusion
model to capture such complex distributions. Firstly, a Mamba-based module
performs coarse estimation to provide an initial prediction with appropriate
scale and trend. Then, a conditional denoising diffusion layer refines the
estimate, enabling fine-grained modeling of pseudorange errors. To suppress
uncontrolled generative diversity and achieve controllable synthesis, three key
features related to GNSS measurement quality are used as conditions to
precisely guide the reverse denoising process. We further incorporate
per-satellite uncertainty modeling within the diffusion stage to assess the
reliability of the predicted errors. We have collected and publicly released a
real-world dataset covering various scenes. Experiments on public and
self-collected datasets show that DiffGNSS consistently outperforms
state-of-the-art baselines across multiple metrics. To the best of our
knowledge, this is the first application of diffusion models to pseudorange
error estimation. The proposed diffusion-based refinement module is
plug-and-play and can be readily integrated into existing networks to markedly
improve estimation accuracy.

</details>


### [130] [Interpreting vision transformers via residual replacement model](https://arxiv.org/abs/2509.17401)
*Jinyeong Kim,Junhyeok Kim,Yumin Shim,Joohyeok Kim,Sunyoung Jung,Seong Jae Hwang*

Main category: cs.CV

TL;DR: 本文通过稀疏自编码器分析ViT的6.6K特征，提出残差替换模型来简化ViT计算，揭示特征从低层模式到高层语义的演变，以及曲线和空间位置编码机制，实现可解释的ViT机制理解。


<details>
  <summary>Details</summary>
Motivation: 探索视觉Transformer（ViT）如何表示和处理世界，解决长期以来对ViT内部工作机制理解不足的问题。

Method: 使用稀疏自编码器提取所有层的6.6K特征，引入残差替换模型将ViT计算替换为可解释特征，简化原始计算过程。

Result: 揭示了ViT特征从低层模式到高层语义的演变过程，发现了专门编码曲线和空间位置的特征类型，残差替换模型能忠实且简洁地再现ViT计算。

Conclusion: 该框架提供了直观理解ViT机制的途径，并在消除虚假相关性方面展示了实用价值，为ViT的可解释性研究提供了新方法。

Abstract: How do vision transformers (ViTs) represent and process the world? This paper
addresses this long-standing question through the first systematic analysis of
6.6K features across all layers, extracted via sparse autoencoders, and by
introducing the residual replacement model, which replaces ViT computations
with interpretable features in the residual stream. Our analysis reveals not
only a feature evolution from low-level patterns to high-level semantics, but
also how ViTs encode curves and spatial positions through specialized feature
types. The residual replacement model scalably produces a faithful yet
parsimonious circuit for human-scale interpretability by significantly
simplifying the original computations. As a result, this framework enables
intuitive understanding of ViT mechanisms. Finally, we demonstrate the utility
of our framework in debiasing spurious correlations.

</details>


### [131] [Real-Time Fish Detection in Indonesian Marine Ecosystems Using Lightweight YOLOv10-nano Architecture](https://arxiv.org/abs/2509.17406)
*Jonathan Wuntu,Muhamad Dwisnanto Putro,Rendy Syahputra*

Main category: cs.CV

TL;DR: 本研究探索了YOLOv10-nano深度学习模型在印尼水域实时海洋鱼类检测中的应用，结果显示该模型在保持低计算需求的同时实现了高检测精度，适合实时部署。


<details>
  <summary>Details</summary>
Motivation: 印尼海洋生态系统是全球公认的珊瑚三角区，生物多样性丰富，需要高效监测工具支持保护。传统鱼类检测方法耗时且需要专业知识，因此需要自动化解决方案。

Method: 使用YOLOv10-nano模型，其架构包含CSPNet骨干网络、PAN特征融合和金字塔空间注意力块，在Bunaken国家海洋公园的测试数据上进行评估，并在DeepFish和OpenImages V7-Fish数据集上进行验证。

Result: YOLOv10-nano实现了高检测精度（mAP50为0.966，mAP50:95为0.606），计算需求低（270万参数，8.4 GFLOPs），在CPU上的平均推理速度为29.29 FPS。

Conclusion: 该研究证明了YOLOv10-nano在数据有限环境中进行高效、可扩展的海洋鱼类监测和保护应用的潜力。

Abstract: Indonesia's marine ecosystems, part of the globally recognized Coral
Triangle, are among the richest in biodiversity, requiring efficient monitoring
tools to support conservation. Traditional fish detection methods are
time-consuming and demand expert knowledge, prompting the need for automated
solutions. This study explores the implementation of YOLOv10-nano, a
state-of-the-art deep learning model, for real-time marine fish detection in
Indonesian waters, using test data from Bunaken National Marine Park. YOLOv10's
architecture, featuring improvements like the CSPNet backbone, PAN for feature
fusion, and Pyramid Spatial Attention Block, enables efficient and accurate
object detection even in complex environments. The model was evaluated on the
DeepFish and OpenImages V7-Fish datasets. Results show that YOLOv10-nano
achieves a high detection accuracy with mAP50 of 0.966 and mAP50:95 of 0.606
while maintaining low computational demand (2.7M parameters, 8.4 GFLOPs). It
also delivered an average inference speed of 29.29 FPS on the CPU, making it
suitable for real-time deployment. Although OpenImages V7-Fish alone provided
lower accuracy, it complemented DeepFish in enhancing model robustness.
Overall, this study demonstrates YOLOv10-nano's potential for efficient,
scalable marine fish monitoring and conservation applications in data-limited
environments.

</details>


### [132] [Single-Image Depth from Defocus with Coded Aperture and Diffusion Posterior Sampling](https://arxiv.org/abs/2509.17427)
*Hodaka Kawachi,Jose Reinaldo Cunha Santos A. V. Silva Neto,Yasushi Yagi,Hajime Nagahara,Tomoya Nakamura*

Main category: cs.CV

TL;DR: 提出了一种基于学习扩散先验的单次快照离焦深度重建方法，用于编码孔径成像，通过优化框架结合可微分前向模型和扩散先验，无需配对训练数据且不依赖特定相机配置。


<details>
  <summary>Details</summary>
Motivation: 传统离焦深度重建方法依赖手工设计的先验，而基于U-Net的回归方法需要大量配对训练数据且受限于特定相机配置。本文旨在开发一种无需配对数据、配置无关的深度重建方法。

Method: 使用学习扩散先验作为正则化，通过优化框架强制测量一致性（可微分前向模型），在去噪图像域中利用扩散先验指导解，提高准确性和稳定性。

Result: 在综合仿真和原型相机实验中，该方法在不同噪声水平下均能实现稳定的RGBD重建，性能优于U-Net基线和传统编码孔径离焦深度方法。

Conclusion: 该方法成功将学习扩散先验应用于离焦深度重建，实现了无需配对训练数据、相机配置无关的高质量深度重建，为单次快照深度估计提供了新思路。

Abstract: We propose a single-snapshot depth-from-defocus (DFD) reconstruction method
for coded-aperture imaging that replaces hand-crafted priors with a learned
diffusion prior used purely as regularization. Our optimization framework
enforces measurement consistency via a differentiable forward model while
guiding solutions with the diffusion prior in the denoised image domain,
yielding higher accuracy and stability than clas- sical optimization. Unlike
U-Net-style regressors, our approach requires no paired defocus-RGBD training
data and does not tie training to a specific camera configuration. Experiments
on comprehensive simulations and a prototype camera demonstrate consistently
strong RGBD reconstructions across noise levels, outperforming both U-Net
baselines and a classical coded- aperture DFD method.

</details>


### [133] [Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration](https://arxiv.org/abs/2509.17429)
*Zhitao Zeng,Guojian Yuan,Junyuan Mao,Yuxuan Wang,Xiaoshuang Jia,Yueming Jin*

Main category: cs.CV

TL;DR: 本文提出了多尺度时间预测任务，通过增量生成和多智能体协作方法，在通用和手术场景中预测多个时间尺度和状态层次上的细粒度状态。


<details>
  <summary>Details</summary>
Motivation: 准确的时间预测是全面场景理解与具身人工智能之间的桥梁，但现有视觉语言模型难以在多个时间尺度上预测多个细粒度状态。

Method: 提出IG-MC方法，包含增量生成模块和多智能体协作框架。增量生成模块持续合成最新视觉预览，多智能体框架包含生成、启动和多状态评估智能体。

Result: 建立了首个MSTP基准测试，包含跨多个状态尺度和时间尺度的同步标注。

Conclusion: 该方法能够保持决策与生成视觉的同步，防止随着预测间隔延长而性能下降，平衡全局一致性和局部保真度。

Abstract: Accurate temporal prediction is the bridge between comprehensive scene
understanding and embodied artificial intelligence. However, predicting
multiple fine-grained states of a scene at multiple temporal scales is
difficult for vision-language models. We formalize the Multi-Scale Temporal
Prediction (MSTP) task in general and surgical scenes by decomposing
multi-scale into two orthogonal dimensions: the temporal scale, forecasting
states of humans and surgery at varying look-ahead intervals, and the state
scale, modeling a hierarchy of states in general and surgical scenes. For
example, in general scenes, states of contact relationships are finer-grained
than states of spatial relationships. In surgical scenes, medium-level steps
are finer-grained than high-level phases yet remain constrained by their
encompassing phase. To support this unified task, we introduce the first MSTP
Benchmark, featuring synchronized annotations across multiple state scales and
temporal scales. We further propose a method, Incremental Generation and
Multi-agent Collaboration (IG-MC), which integrates two key innovations. First,
we present a plug-and-play incremental generation module that continuously
synthesizes up-to-date visual previews at expanding temporal scales to inform
multiple decision-making agents, keeping decisions and generated visuals
synchronized and preventing performance degradation as look-ahead intervals
lengthen. Second, we present a decision-driven multi-agent collaboration
framework for multi-state prediction, comprising generation, initiation, and
multi-state assessment agents that dynamically trigger and evaluate prediction
cycles to balance global coherence and local fidelity.

</details>


### [134] [EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device](https://arxiv.org/abs/2509.17430)
*Gunjan Chhablani,Xiaomeng Ye,Muhammad Zubair Irshad,Zsolt Kira*

Main category: cs.CV

TL;DR: EmbodiedSplat提出了一种通过3D高斯溅射技术重建部署环境来个性化策略训练的新方法，显著提升了sim-to-real的迁移效果。


<details>
  <summary>Details</summary>
Motivation: 当前Embodied AI主要依赖仿真训练，但合成环境缺乏真实感，而高保真实世界重建成本高昂，导致sim-to-real迁移成为主要挑战。

Method: 利用iPhone捕获部署场景，通过3D高斯溅射技术重建网格，在Habitat-Sim模拟器中进行策略微调，实现真实场景的近似训练环境。

Result: 实验表明，EmbodiedSplat微调的智能体在真实世界图像导航任务上比HM3D和HSSD预训练的基线模型分别提升20%和40%的成功率，sim-vs-real相关性达到0.87-0.97。

Conclusion: 该方法能够以最小成本有效适应多样化环境，为sim-to-real迁移提供了高效解决方案。

Abstract: The field of Embodied AI predominantly relies on simulation for training and
evaluation, often using either fully synthetic environments that lack
photorealism or high-fidelity real-world reconstructions captured with
expensive hardware. As a result, sim-to-real transfer remains a major
challenge. In this paper, we introduce EmbodiedSplat, a novel approach that
personalizes policy training by efficiently capturing the deployment
environment and fine-tuning policies within the reconstructed scenes. Our
method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to
bridge the gap between realistic scene capture and effective training
environments. Using iPhone-captured deployment scenes, we reconstruct meshes
via GS, enabling training in settings that closely approximate real-world
conditions. We conduct a comprehensive analysis of training strategies,
pre-training datasets, and mesh reconstruction techniques, evaluating their
impact on sim-to-real predictivity in real-world scenarios. Experimental
results demonstrate that agents fine-tuned with EmbodiedSplat outperform both
zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and
synthetically generated datasets (HSSD), achieving absolute success rate
improvements of 20\% and 40\% on real-world Image Navigation task. Moreover,
our approach yields a high sim-vs-real correlation (0.87--0.97) for the
reconstructed meshes, underscoring its effectiveness in adapting policies to
diverse environments with minimal effort. Project page:
https://gchhablani.github.io/embodied-splat

</details>


### [135] [Emergent 3D Correspondence from Neural Shape Representation](https://arxiv.org/abs/2509.17431)
*Keyu Du,Jingyu Hu,Haipeng Li,Hao Xu,Haibing Huang,Chi-Wing Fu,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 提出了一种基于分层神经语义表示的3D语义对应估计方法，通过全局语义特征和局部几何特征的结合，实现准确鲁棒的3D语义对应。


<details>
  <summary>Details</summary>
Motivation: 为了解决3D语义对应估计中准确性和鲁棒性的挑战，特别是处理结构多样化形状时的泛化能力问题。

Method: 设计分层神经语义表示（HNSR），包含全局语义特征和多分辨率局部几何特征；采用渐进式全局到局部匹配策略；框架无需训练，兼容多种预训练3D生成模型。

Result: 在定性和定量评估中均优于现有最先进技术，支持形状共分割、关键点匹配和纹理迁移等应用，在跨类别场景下也表现良好。

Conclusion: 该方法通过分层表示和渐进匹配策略，实现了准确、鲁棒且泛化能力强的3D语义对应估计，为多种3D应用提供了有效解决方案。

Abstract: This paper presents a new approach to estimate accurate and robust 3D
semantic correspondence with the hierarchical neural semantic representation.
Our work has three key contributions. First, we design the hierarchical neural
semantic representation (HNSR), which consists of a global semantic feature to
capture high-level structure and multi-resolution local geometric features to
preserve fine details, by carefully harnessing 3D priors from pre-trained 3D
generative models. Second, we design a progressive global-to-local matching
strategy, which establishes coarse semantic correspondence using the global
semantic feature, then iteratively refines it with local geometric features,
yielding accurate and semantically-consistent mappings. Third, our framework is
training-free and broadly compatible with various pre-trained 3D generative
backbones, demonstrating strong generalization across diverse shape categories.
Our method also supports various applications, such as shape co-segmentation,
keypoint matching, and texture transfer, and generalizes well to structurally
diverse shapes, with promising results even in cross-category scenarios. Both
qualitative and quantitative evaluations show that our method outperforms
previous state-of-the-art techniques.

</details>


### [136] [Training-Free Label Space Alignment for Universal Domain Adaptation](https://arxiv.org/abs/2509.17452)
*Dujin Lee,Sojung An,Jungmyung Wi,Kuniaki Saito,Donghyun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉语言基础模型（如CLIP）的通用域自适应方法，专注于标签空间对齐而非视觉空间对齐，通过生成式视觉语言模型识别目标域中的未知类别，并构建通用分类器来提高域适应性能。


<details>
  <summary>Details</summary>
Motivation: 传统的通用域自适应方法主要关注视觉空间对齐，但由于内容差异导致的视觉模糊性限制了其鲁棒性和泛化能力。本文旨在利用视觉语言基础模型的零样本能力，通过标签空间对齐来提升域适应的稳定性。

Method: 首先使用生成式视觉语言模型识别目标域中的未知类别；然后提出无需训练标签空间对齐方法，过滤和精炼域间噪声标签；最后构建集成共享知识和目标私有类别信息的通用分类器。

Result: 在DomainBed基准测试中，该方法显著优于现有通用域自适应技术，H-score平均提升7.9%，H³-score平均提升6.1%。结合自训练后，H-score和H³-score进一步提升了1.6%。

Conclusion: 通过专注于标签空间对齐而非视觉空间对齐，并利用视觉语言基础模型的优势，该方法在通用域自适应任务中取得了显著改进，展示了标签空间对齐策略的有效性。

Abstract: Universal domain adaptation (UniDA) transfers knowledge from a labeled source
domain to an unlabeled target domain, where label spaces may differ and the
target domain may contain private classes. Previous UniDA methods primarily
focused on visual space alignment but often struggled with visual ambiguities
due to content differences, which limited their robustness and
generalizability. To overcome this, we introduce a novel approach that
leverages the strong \textit{zero-shot capabilities} of recent vision-language
foundation models (VLMs) like CLIP, concentrating solely on label space
alignment to enhance adaptation stability. CLIP can generate task-specific
classifiers based only on label names. However, adapting CLIP to UniDA is
challenging because the label space is not fully known in advance. In this
study, we first utilize generative vision-language models to identify unknown
categories in the target domain. Noise and semantic ambiguities in the
discovered labels -- such as those similar to source labels (e.g., synonyms,
hypernyms, hyponyms) -- complicate label alignment. To address this, we propose
a training-free label-space alignment method for UniDA (\ours). Our method
aligns label spaces instead of visual spaces by filtering and refining noisy
labels between the domains. We then construct a \textit{universal classifier}
that integrates both shared knowledge and target-private class information,
thereby improving generalizability under domain shifts. The results reveal that
the proposed method considerably outperforms existing UniDA techniques across
key DomainBed benchmarks, delivering an average improvement of
\textcolor{blue}{+7.9\%}in H-score and \textcolor{blue}{+6.1\%} in H$^3$-score.
Furthermore, incorporating self-training further enhances performance and
achieves an additional (\textcolor{blue}{+1.6\%}) increment in both H- and
H$^3$-scores.

</details>


### [137] [Explainable AI for Analyzing Person-Specific Patterns in Facial Recognition Tasks](https://arxiv.org/abs/2509.17457)
*Paweł Jakub Borsukiewicz,Jordan Samhi,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 本文提出LEAM技术，通过分析面部识别模型在个体层面的关键识别区域，为个性化隐私保护系统提供基础。研究发现不同模型在整体激活模式上具有相似性，且鼻部区域是重要的识别特征。


<details>
  <summary>Details</summary>
Motivation: 当前对抗性技术采用通用方法而非针对个体面部特征，限制了其有效性和隐蔽性。需要开发能够理解面部识别系统工作原理的技术来指导隐私保护研究。

Method: 引入Layer Embedding Activation Mapping (LEAM)技术，结合面部解析器，分析1000个个体在9个预训练面部识别模型中的数据，识别对识别贡献最大的面部区域。

Result: 研究发现面部识别模型优先关注面部中央区域（鼻部占关键识别区域的18.9-29.7%），且同一人的图像激活模式相似度显著高于不同人。仅使用1%最相关像素的验证遮挡实验证实了方法的有效性。

Conclusion: LEAM技术为未来基于个体化面部特征选择的隐私保护系统奠定了基础，其识别的重要区域在不同模型间具有可迁移性。

Abstract: The proliferation of facial recognition systems presents major privacy risks,
driving the need for effective countermeasures. Current adversarial techniques
apply generalized methods rather than adapting to individual facial
characteristics, limiting their effectiveness and inconspicuousness. In this
work, we introduce Layer Embedding Activation Mapping (LEAM), a novel technique
that identifies which facial areas contribute most to recognition at an
individual level. Unlike adversarial attack methods that aim to fool
recognition systems, LEAM is an explainability technique designed to understand
how these systems work, providing insights that could inform future privacy
protection research. We integrate LEAM with a face parser to analyze data from
1000 individuals across 9 pre-trained facial recognition models.
  Our analysis reveals that while different layers within facial recognition
models vary significantly in their focus areas, these models generally
prioritize similar facial regions across architectures when considering their
overall activation patterns, which show significantly higher similarity between
images of the same individual (Bhattacharyya Coefficient: 0.32-0.57) vs.
different individuals (0.04-0.13), validating the existence of person-specific
recognition patterns. Our results show that facial recognition models
prioritize the central region of face images (with nose areas accounting for
18.9-29.7% of critical recognition regions), while still distributing attention
across multiple facial fragments. Proper selection of relevant facial areas was
confirmed using validation occlusions, based on just 1% of the most relevant,
LEAM-identified, image pixels, which proved to be transferable across different
models. Our findings establish the foundation for future individually tailored
privacy protection systems centered around LEAM's choice of areas to be
perturbed.

</details>


### [138] [CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration](https://arxiv.org/abs/2509.17458)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,Shayan Baghayi Nejad,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: CARINOX是一个统一的框架，结合了噪声优化和探索方法，通过基于人类判断相关性的奖励选择程序，提高文本到图像扩散模型的组合对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型（如Stable Diffusion）在处理复杂对象关系、属性和空间排列时往往无法实现组合对齐。现有的推理时方法各有局限性：优化方法可能因初始化不良而停滞，探索方法则需要大量样本。

Method: CARINOX框架结合了噪声优化和探索策略，采用基于人类判断相关性的原则性奖励选择程序，解决了单一奖励指标或临时组合无法可靠捕捉所有组合性方面的问题。

Result: 在两个互补基准测试中，CARINOX将平均对齐分数提高了T2I-CompBench++上的+16%和HRS基准上的+11%，在所有主要类别中均优于最先进的优化和探索方法，同时保持图像质量和多样性。

Conclusion: CARINOX通过统一优化和探索策略，结合原则性奖励选择，有效解决了文本到图像扩散模型的组合对齐问题，显著提升了性能。

Abstract: Text-to-image diffusion models, such as Stable Diffusion, can produce
high-quality and diverse images but often fail to achieve compositional
alignment, particularly when prompts describe complex object relationships,
attributes, or spatial arrangements. Recent inference-time approaches address
this by optimizing or exploring the initial noise under the guidance of reward
functions that score text-image alignment without requiring model fine-tuning.
While promising, each strategy has intrinsic limitations when used alone:
optimization can stall due to poor initialization or unfavorable search
trajectories, whereas exploration may require a prohibitively large number of
samples to locate a satisfactory output. Our analysis further shows that
neither single reward metrics nor ad-hoc combinations reliably capture all
aspects of compositionality, leading to weak or inconsistent guidance. To
overcome these challenges, we present Category-Aware Reward-based Initial Noise
Optimization and Exploration (CARINOX), a unified framework that combines noise
optimization and exploration with a principled reward selection procedure
grounded in correlation with human judgments. Evaluations on two complementary
benchmarks covering diverse compositional challenges show that CARINOX raises
average alignment scores by +16% on T2I-CompBench++ and +11% on the HRS
benchmark, consistently outperforming state-of-the-art optimization and
exploration-based methods across all major categories, while preserving image
quality and diversity. The project page is available at
https://amirkasaei.com/carinox/{this URL}.

</details>


### [139] [CSDformer: A Conversion Method for Fully Spike-Driven Transformer](https://arxiv.org/abs/2509.17461)
*Yuhao Zhang,Chengjun Zhang,Di Wu,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: CSDformer是一种新型的完全脉冲驱动变压器转换方法，通过NReLU函数替换softmax、时间分解技术和延迟积分点火神经元，在超低延迟下实现高性能，同时大幅降低训练成本和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的脉冲变压器生成方法存在训练成本过高或硬件不友好的问题，需要一种既能保持高性能又能降低能耗的解决方案。

Method: 提出CSDformer转换方法：设计转换导向的变压器架构，用NReLU替换softmax，量化训练后通过时间分解技术转换为完全脉冲驱动模型，并使用延迟积分点火神经元减少转换误差。

Result: 在ImageNet上达到76.36% top-1准确率（7个时间步），相比现有方法减少75%计算资源和2-3倍训练加速。

Conclusion: 这是首个通过转换方法开发的完全脉冲驱动变压器模型，在超低延迟下实现高性能，显著降低了计算复杂度和训练开销。

Abstract: Spike-based transformer is a novel architecture aiming to enhance the
performance of spiking neural networks while mitigating the energy overhead
inherent to transformers. However, methods for generating these models suffer
from critical limitations: excessive training costs introduced by direct
training methods, or unavoidably hardware-unfriendly operations in existing
conversion methods. In this paper, we propose CSDformer, a novel conversion
method for fully spike-driven transformers. We tailor a conversion-oriented
transformer-based architecture and propose a new function NReLU to replace
softmax in self-attention. Subsequently, this model is quantized and trained,
and converted into a fully spike-driven model with temporal decomposition
technique. Also, we propose delayed Integrate-andFire neurons to reduce
conversion errors and improve the performance of spiking models. We evaluate
CSDformer on ImageNet, CIFAR-10 and CIFAR-100 datasets and achieve 76.36% top-1
accuracy under 7 time-steps on ImageNet, demonstrating superiority over
state-of-the-art models. Furthermore, CSDformer eliminates the need for
training SNNs, thereby reducing training costs (reducing computational resource
by 75% and accelerating training speed by 2-3$\times$). To the best of our
knowledge, this is the first fully spike-driven transformer-based model
developed via conversion method, achieving high performance under ultra-low
latency, while dramatically reducing both computational complexity and training
overhead.

</details>


### [140] [MAESTRO: Task-Relevant Optimization via Adaptive Feature Enhancement and Suppression for Multi-task 3D Perception](https://arxiv.org/abs/2509.17462)
*Changwon Kang,Jisong Kim,Hongjae Shin,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: MAESTRO是一个结构化框架，用于解决多任务3D感知中的任务冲突问题，通过生成任务特定特征和减轻特征干扰来提高3D目标检测、BEV地图分割和3D占据预测的性能。


<details>
  <summary>Details</summary>
Motivation: 多任务学习虽然能提高学习效率，但可能因不同任务目标之间的冲突导致性能下降。特别是在3D感知任务中，任务间的特征干扰问题尤为突出。

Method: MAESTRO包含三个组件：类原型生成器（CPG）将类别分为前景和背景组并生成组原型；任务特定特征生成器（TSFG）利用原型保留任务相关特征；场景原型聚合器（SPA）通过其他任务头的信息增强3D占据预测的原型。

Result: 在nuScenes和Occ3D基准测试上的广泛实验表明，MAESTRO在3D目标检测、BEV地图分割和3D占据预测任务上均优于现有方法。

Conclusion: MAESTRO通过结构化特征生成和原型管理，有效缓解了多任务3D感知中的特征干扰问题，实现了各任务的性能提升。

Abstract: The goal of multi-task learning is to learn to conduct multiple tasks
simultaneously based on a shared data representation. While this approach can
improve learning efficiency, it may also cause performance degradation due to
task conflicts that arise when optimizing the model for different objectives.
To address this challenge, we introduce MAESTRO, a structured framework
designed to generate task-specific features and mitigate feature interference
in multi-task 3D perception, including 3D object detection, bird's-eye view
(BEV) map segmentation, and 3D occupancy prediction. MAESTRO comprises three
components: the Class-wise Prototype Generator (CPG), the Task-Specific Feature
Generator (TSFG), and the Scene Prototype Aggregator (SPA). CPG groups class
categories into foreground and background groups and generates group-wise
prototypes. The foreground and background prototypes are assigned to the 3D
object detection task and the map segmentation task, respectively, while both
are assigned to the 3D occupancy prediction task. TSFG leverages these
prototype groups to retain task-relevant features while suppressing irrelevant
features, thereby enhancing the performance for each task. SPA enhances the
prototype groups assigned for 3D occupancy prediction by utilizing the
information produced by the 3D object detection head and the map segmentation
head. Extensive experiments on the nuScenes and Occ3D benchmarks demonstrate
that MAESTRO consistently outperforms existing methods across 3D object
detection, BEV map segmentation, and 3D occupancy prediction tasks.

</details>


### [141] [Stable Video-Driven Portraits](https://arxiv.org/abs/2509.17476)
*Mallikarjun B. R.,Fei Yin,Vikram Voleti,Nikita Drobyshev,Maksim Lapin,Aaryaman Vasishta,Varun Jampani*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的人像动画框架，通过使用驱动视频中掩码面部区域（眼睛、鼻子、嘴巴）作为强运动控制信号，结合跨身份监督和时空注意力机制，实现了高质量、可控的人像动画。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在表情表达能力、时间一致性和对未见身份或大姿态变化的泛化能力方面的局限性，特别是扩散模型在弱控制信号和架构限制下的约束。

Method: 采用掩码面部区域作为运动控制线索，使用跨身份监督防止外观泄露，引入最小新参数的架构以利用预训练扩散模型的强先验，设计时空注意力机制捕获细微运动并减少时间伪影，使用历史帧确保连续性，提出信号融合策略平衡运动保真度和身份保持。

Result: 实现了优越的时间一致性和准确的表情控制，能够生成高质量、可控的人像动画，适用于实际应用场景。

Conclusion: 该框架通过强运动控制信号、高效架构设计和时间一致性机制，显著提升了人像动画的质量和可控性，为实际应用提供了可行的解决方案。

Abstract: Portrait animation aims to generate photo-realistic videos from a single
source image by reenacting the expression and pose from a driving video. While
early methods relied on 3D morphable models or feature warping techniques, they
often suffered from limited expressivity, temporal inconsistency, and poor
generalization to unseen identities or large pose variations. Recent advances
using diffusion models have demonstrated improved quality but remain
constrained by weak control signals and architectural limitations. In this
work, we propose a novel diffusion based framework that leverages masked facial
regions specifically the eyes, nose, and mouth from the driving video as strong
motion control cues. To enable robust training without appearance leakage, we
adopt cross identity supervision. To leverage the strong prior from the
pretrained diffusion model, our novel architecture introduces minimal new
parameters that converge faster and help in better generalization. We introduce
spatial temporal attention mechanisms that allow inter frame and intra frame
interactions, effectively capturing subtle motions and reducing temporal
artifacts. Our model uses history frames to ensure continuity across segments.
At inference, we propose a novel signal fusion strategy that balances motion
fidelity with identity preservation. Our approach achieves superior temporal
consistency and accurate expression control, enabling high-quality,
controllable portrait animation suitable for real-world applications.

</details>


### [142] [ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding](https://arxiv.org/abs/2509.17481)
*Xingqi Wang,Yiming Cui,Xin Yao,Shijin Wang,Guoping Hu,Xiaoyu Qin*

Main category: cs.CV

TL;DR: ChartHal是一个针对图表理解中幻觉问题的细粒度基准测试，包含1,062个人工验证样本，评估显示当前最先进的大视觉语言模型在图表理解中存在严重幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大视觉语言模型取得了显著进展，但幻觉问题仍然是关键障碍，特别是在需要复杂感知认知能力和严格事实准确性的图表理解领域。现有研究独立探讨了幻觉和图表理解，但两者的交叉领域尚未充分研究。

Method: 提出了ChartHal基准测试，包含细粒度的图表理解幻觉场景分类和人工验证的数据集，对包括GPT-5和o4-mini在内的先进LVLMs进行了系统性评估。

Result: 评估结果显示最先进的LVLMs在ChartHal上存在严重幻觉，GPT-5和o4-mini的准确率分别仅为34.46%和22.79%。涉及图表中缺失或矛盾信息的问题特别容易引发幻觉。

Conclusion: 图表理解中的幻觉问题严重，迫切需要更鲁棒的缓解策略。ChartHal基准测试为研究和改进LVLMs在图表理解中的性能提供了重要工具。

Abstract: Large Vision-Language Models (LVLMs) have recently demonstrated remarkable
progress, yet hallucination remains a critical barrier, particularly in chart
understanding, which requires sophisticated perceptual and cognitive abilities
as well as rigorous factual accuracy. While prior work has investigated
hallucinations and chart comprehension independently, their intersection
remains largely unexplored. To address this gap, we present ChartHal, a
benchmark that features a fine-grained taxonomy of hallucination scenarios in
chart understanding, along with a human-validated dataset of 1,062 samples. Our
evaluation shows that state-of-the-art LVLMs suffer from severe hallucinations
on ChartHal, including proprietary models such as GPT-5 and o4-mini, which
achieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals
that questions involving information absent from or contradictory to charts are
especially likely to trigger hallucinations, underscoring the urgent need for
more robust mitigation strategies. Code and data are available at
https://github.com/ymcui/ChartHal .

</details>


### [143] [Multimodal Medical Image Classification via Synergistic Learning Pre-training](https://arxiv.org/abs/2509.17492)
*Qinghua Lin,Guang-Hai Liu,Zuoyong Li,Yang Li,Yuting Jiang,Xiang Wu*

Main category: cs.CV

TL;DR: 提出了一种用于多模态半监督医学图像分类的预训练+微调框架，通过一致性、重构和对齐学习实现模态融合，在标记数据稀缺的情况下提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 多模态病理图像在临床诊断中常见，但基于计算机视觉的多模态图像辅助诊断面临模态融合挑战，特别是在缺乏专家标注数据的情况下。

Method: 提出协同学习预训练框架（一致性、重构、对齐学习），将一种模态视为另一种模态的增强样本进行自监督预训练；设计多模态融合微调方法，使用不同编码器提取特征，并提出分布偏移方法缓解过拟合风险。

Result: 在公开胃镜图像数据集Kvasir和Kvasirv2上的实验表明，该方法在定量和定性结果上都优于当前最先进的分类方法。

Conclusion: 该方法有效解决了多模态医学图像在标签稀缺情况下的融合问题，为医学图像辅助诊断提供了可行的解决方案。

Abstract: Multimodal pathological images are usually in clinical diagnosis, but
computer vision-based multimodal image-assisted diagnosis faces challenges with
modality fusion, especially in the absence of expert-annotated data. To achieve
the modality fusion in multimodal images with label scarcity, we propose a
novel ``pretraining + fine-tuning" framework for multimodal semi-supervised
medical image classification. Specifically, we propose a synergistic learning
pretraining framework of consistency, reconstructive, and aligned learning. By
treating one modality as an augmented sample of another modality, we implement
a self-supervised learning pre-train, enhancing the baseline model's feature
representation capability. Then, we design a fine-tuning method for multimodal
fusion. During the fine-tuning stage, we set different encoders to extract
features from the original modalities and provide a multimodal fusion encoder
for fusion modality. In addition, we propose a distribution shift method for
multimodal fusion features, which alleviates the prediction uncertainty and
overfitting risks caused by the lack of labeled samples. We conduct extensive
experiments on the publicly available gastroscopy image datasets Kvasir and
Kvasirv2. Quantitative and qualitative results demonstrate that the proposed
method outperforms the current state-of-the-art classification methods. The
code will be released at: https://github.com/LQH89757/MICS.

</details>


### [144] [Vision-Based Driver Drowsiness Monitoring: Comparative Analysis of YOLOv5-v11 Models](https://arxiv.org/abs/2509.17498)
*Dilshara Herath,Chinthaka Abeyrathne,Prabhani Jayaweera*

Main category: cs.CV

TL;DR: 本文对基于计算机视觉的YOLO算法进行实时非侵入式驾驶员疲劳检测的全面评估，比较了7种YOLO变体的性能，发现YOLOv9c准确率最高，YOLOv11n在精度和推理效率间达到最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是导致道路事故的关键因素，每年造成数千人死亡和伤害，需要开发有效的实时检测方法。

Method: 使用UTA-RLDD公开数据集，对7种YOLO变体（v5s、v9c、v9t、v10n、v10l、v11n、v11l）进行微调，同时实现基于Dlib面部关键点的眼动比（EAR）方法。

Result: YOLOv9c达到最高准确率（mAP0.5为0.986，召回率为0.978），YOLOv11n在精度（0.954）和推理效率间达到最佳平衡。EAR方法计算量小但鲁棒性较差。

Conclusion: 研究揭示了准确性、延迟和资源需求之间的权衡关系，为自动驾驶和工业安全应用中的检测方法选择提供了实用指南。

Abstract: Driver drowsiness remains a critical factor in road accidents, accounting for
thousands of fatalities and injuries each year. This paper presents a
comprehensive evaluation of real-time, non-intrusive drowsiness detection
methods, focusing on computer vision based YOLO (You Look Only Once)
algorithms. A publicly available dataset namely, UTA-RLDD was used, containing
both awake and drowsy conditions, ensuring variability in gender, eyewear,
illumination, and skin tone. Seven YOLO variants (v5s, v9c, v9t, v10n, v10l,
v11n, v11l) are fine-tuned, with performance measured in terms of Precision,
Recall, mAP0.5, and mAP 0.5-0.95. Among these, YOLOv9c achieved the highest
accuracy (0.986 mAP 0.5, 0.978 Recall) while YOLOv11n strikes the optimal
balance between precision (0.954) and inference efficiency, making it highly
suitable for embedded deployment. Additionally, we implement an Eye Aspect
Ratio (EAR) approach using Dlib's facial landmarks, which despite its low
computational footprint exhibits reduced robustness under pose variation and
occlusions. Our findings illustrate clear trade offs between accuracy, latency,
and resource requirements, and offer practical guidelines for selecting or
combining detection methods in autonomous driving and industrial safety
applications.

</details>


### [145] [SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge](https://arxiv.org/abs/2509.17500)
*Yujie Xie,Hongyang Zhang,Zhihui Liu,Shihai Ruan*

Main category: cs.CV

TL;DR: SAMSON是ICCV 2025 MOSE赛道第三名的解决方案，通过集成长期记忆模块和SAM2Long后处理策略，有效处理长视频序列中的目标跟踪和分割挑战。


<details>
  <summary>Details</summary>
Motivation: 解决长视频目标分割中的目标重现、小目标、严重遮挡和拥挤场景等挑战，现有方法主要基于SAM2框架但存在局限性。

Method: 提出Segment Anything with Memory Strengthened Object Navigation (SAMSON)，集成最先进VOS模型的优势，包含长期记忆模块用于目标重识别，采用SAM2Long作为后处理策略减少误差累积。

Result: 在测试集排行榜上取得了0.8427的J&F分数。

Conclusion: SAMSON通过记忆增强的目标导航机制，在复杂的长视频目标分割任务中表现出色，验证了所提方法的有效性。

Abstract: Large-scale Video Object Segmentation (LSVOS) addresses the challenge of
accurately tracking and segmenting objects in long video sequences, where
difficulties stem from object reappearance, small-scale targets, heavy
occlusions, and crowded scenes. Existing approaches predominantly adopt
SAM2-based frameworks with various memory mechanisms for complex video mask
generation. In this report, we proposed Segment Anything with Memory
Strengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE
track of ICCV 2025, which integrates the strengths of stateof-the-art VOS
models into an effective paradigm. To handle visually similar instances and
long-term object disappearance in MOSE, we incorporate a long-term memorymodule
for reliable object re-identification. Additionly, we adopt SAM2Long as a
post-processing strategy to reduce error accumulation and enhance segmentation
stability in long video sequences. Our method achieved a final performance of
0.8427 in terms of J &F in the test-set leaderboard.

</details>


### [146] [4D-MoDe: Towards Editable and Scalable Volumetric Streaming via Motion-Decoupled 4D Gaussian Compression](https://arxiv.org/abs/2509.17506)
*Houqiang Zhong,Zihan Zheng,Qiang Hu,Yuan Tian,Ning Cao,Lan Xu,Xiaoyun Zhang,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: 4D-MoDe是一种运动解耦的4D高斯压缩框架，用于可扩展和可编辑的体积视频流传输，通过分层表示和前瞻性运动分解显著降低存储成本。


<details>
  <summary>Details</summary>
Motivation: 解决体积视频在6DoF导航和空间交互中的大规模传输挑战，现有表示方法数据量大、运动复杂且编辑性有限。

Method: 采用分层表示分离静态背景和动态前景，使用多分辨率运动估计网格和轻量级共享MLP捕捉连续运动轨迹，结合动态高斯补偿机制和自适应分组方案。

Result: 在多个数据集上实现竞争性重建质量，存储成本比现有方法低一个数量级（最低11.4KB/帧），支持背景替换和仅前景流传输等应用。

Conclusion: 4D-MoDe框架在保持高质量重建的同时显著降低了存储需求，为可扩展和可编辑的体积视频流传输提供了有效解决方案。

Abstract: Volumetric video has emerged as a key medium for immersive telepresence and
augmented/virtual reality, enabling six-degrees-of-freedom (6DoF) navigation
and realistic spatial interactions. However, delivering high-quality dynamic
volumetric content at scale remains challenging due to massive data volume,
complex motion, and limited editability of existing representations. In this
paper, we present 4D-MoDe, a motion-decoupled 4D Gaussian compression framework
designed for scalable and editable volumetric video streaming. Our method
introduces a layered representation that explicitly separates static
backgrounds from dynamic foregrounds using a lookahead-based motion
decomposition strategy, significantly reducing temporal redundancy and enabling
selective background/foreground streaming. To capture continuous motion
trajectories, we employ a multi-resolution motion estimation grid and a
lightweight shared MLP, complemented by a dynamic Gaussian compensation
mechanism to model emergent content. An adaptive grouping scheme dynamically
inserts background keyframes to balance temporal consistency and compression
efficiency. Furthermore, an entropy-aware training pipeline jointly optimizes
the motion fields and Gaussian parameters under a rate-distortion (RD)
objective, while employing range-based and KD-tree compression to minimize
storage overhead. Extensive experiments on multiple datasets demonstrate that
4D-MoDe consistently achieves competitive reconstruction quality with an order
of magnitude lower storage cost (e.g., as low as \textbf{11.4} KB/frame)
compared to state-of-the-art methods, while supporting practical applications
such as background replacement and foreground-only streaming.

</details>


### [147] [4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming](https://arxiv.org/abs/2509.17513)
*Zihan Zheng,Zhenlong Wu,Houqiang Zhong,Yuan Tian,Ning Cao,Lan Xu,Jiangchao Yao,Xiaoyun Zhang,Qiang Hu,Wenjun Zhang*

Main category: cs.CV

TL;DR: 4DGCPro是一个新颖的分层4D高斯压缩框架，通过渐进式体积视频流实现实时移动解码和高质量渲染，解决了现有方法缺乏灵活性、无法在单一模型中调整质量和比特率的问题。


<details>
  <summary>Details</summary>
Motivation: 现有体积视频压缩方法要么缺乏在单一模型中调整质量和比特率的灵活性，无法适应不同网络和设备的高效流媒体传输，要么在轻量级移动平台上难以实现实时解码和渲染。

Method: 提出了感知加权和压缩友好的分层4D高斯表示，采用运动感知自适应分组减少时间冗余、保持连贯性，并支持可扩展的多级细节流传输；同时设计了端到端熵优化训练方案，包含分层率失真监督和属性特定熵建模。

Result: 实验表明4DGCPro在单一模型中实现了灵活的质量和多种比特率调整，在移动设备上实现实时解码和渲染，并在多个数据集上的率失真性能优于现有方法。

Conclusion: 4DGCPro框架成功解决了体积视频压缩的关键挑战，为高质量体积视频的实时流媒体传输提供了有效的解决方案。

Abstract: Achieving seamless viewing of high-fidelity volumetric video, comparable to
2D video experiences, remains an open challenge. Existing volumetric video
compression methods either lack the flexibility to adjust quality and bitrate
within a single model for efficient streaming across diverse networks and
devices, or struggle with real-time decoding and rendering on lightweight
mobile platforms. To address these challenges, we introduce 4DGCPro, a novel
hierarchical 4D Gaussian compression framework that facilitates real-time
mobile decoding and high-quality rendering via progressive volumetric video
streaming in a single bitstream. Specifically, we propose a
perceptually-weighted and compression-friendly hierarchical 4D Gaussian
representation with motion-aware adaptive grouping to reduce temporal
redundancy, preserve coherence, and enable scalable multi-level detail
streaming. Furthermore, we present an end-to-end entropy-optimized training
scheme, which incorporates layer-wise rate-distortion (RD) supervision and
attribute-specific entropy modeling for efficient bitstream generation.
Extensive experiments show that 4DGCPro enables flexible quality and multiple
bitrate within a single model, achieving real-time decoding and rendering on
mobile devices while outperforming existing methods in RD performance across
multiple datasets. Project Page: https://mediax-sjtu.github.io/4DGCPro

</details>


### [148] [Unified Multimodal Coherent Field: Synchronous Semantic-Spatial-Vision Fusion for Brain Tumor Segmentation](https://arxiv.org/abs/2509.17520)
*Mingda Zhang,Yuyang Zheng,Ruixiang Tang,Jingru Qiu,Haiyan Ding*

Main category: cs.CV

TL;DR: 该论文提出了一种统一多模态相干场（UMCF）方法，用于脑肿瘤分割，通过同步融合视觉、语义和空间信息来解决传统方法在边界描绘和层次结构保持方面的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割需要准确识别层次区域（全肿瘤、肿瘤核心、增强肿瘤），但由于肿瘤组织异质性、边界模糊和MRI序列对比度变化，仅依赖视觉信息或后处理损失约束的方法在边界描绘和层次保持方面表现不稳定。

Method: UMCF方法在统一的3D潜在空间中实现视觉、语义和空间信息的同步交互融合，通过无参数不确定性门控自适应调整模态贡献，医学先验知识直接参与注意力计算，避免了传统的"先处理再拼接"分离架构。

Result: 在BraTS 2020和2021数据集上，UMCF+nnU-Net分别实现了0.8579和0.8977的平均Dice系数，相比主流架构平均提升了4.18%。

Conclusion: 通过深度整合临床知识与影像特征，UMCF为精准医学中的多模态信息融合提供了新的技术途径。

Abstract: Brain tumor segmentation requires accurate identification of hierarchical
regions including whole tumor (WT), tumor core (TC), and enhancing tumor (ET)
from multi-sequence magnetic resonance imaging (MRI) images. Due to tumor
tissue heterogeneity, ambiguous boundaries, and contrast variations across MRI
sequences, methods relying solely on visual information or post-hoc loss
constraints show unstable performance in boundary delineation and hierarchy
preservation. To address this challenge, we propose the Unified Multimodal
Coherent Field (UMCF) method. This method achieves synchronous interactive
fusion of visual, semantic, and spatial information within a unified 3D latent
space, adaptively adjusting modal contributions through parameter-free
uncertainty gating, with medical prior knowledge directly participating in
attention computation, avoiding the traditional "process-then-concatenate"
separated architecture. On Brain Tumor Segmentation (BraTS) 2020 and 2021
datasets, UMCF+nnU-Net achieves average Dice coefficients of 0.8579 and 0.8977
respectively, with an average 4.18% improvement across mainstream
architectures. By deeply integrating clinical knowledge with imaging features,
UMCF provides a new technical pathway for multimodal information fusion in
precision medicine.

</details>


### [149] [Chat-CBM: Towards Interactive Concept Bottleneck Models with Frozen Large Language Models](https://arxiv.org/abs/2509.17522)
*Hangzhou He,Lei Zhu,Kaiwen Li,Xinliang Zhang,Jiakui Hu,Ourui Fu,Zhengjian Yao,Yanye Lu*

Main category: cs.CV

TL;DR: Chat-CBM使用语言模型替代传统概念瓶颈模型中的线性分类器，通过在概念语义空间进行推理，实现了更丰富直观的用户干预方式，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统概念瓶颈模型使用固定的线性分类器，限制了用户干预只能手动调整数值，无法在测试时引入新概念或领域知识，特别是在无监督设置下概念激活噪声大、干预效果差。

Method: 用基于语言的分类器替代基于分数的分类器，直接在概念语义空间进行推理，利用冻结大语言模型的语言理解和少样本能力。

Result: 在九个数据集上的实验表明，Chat-CBM获得了更高的预测性能，显著改善了用户交互性，同时保持了概念瓶颈模型的可解释性。

Conclusion: Chat-CBM通过将预测建立在概念语义空间中，扩展了概念瓶颈模型的干预界面，使其超越数值编辑，在无监督设置下仍保持有效性。

Abstract: Concept Bottleneck Models (CBMs) provide inherent interpretability by first
predicting a set of human-understandable concepts and then mapping them to
labels through a simple classifier. While users can intervene in the concept
space to improve predictions, traditional CBMs typically employ a fixed linear
classifier over concept scores, which restricts interventions to manual value
adjustments and prevents the incorporation of new concepts or domain knowledge
at test time. These limitations are particularly severe in unsupervised CBMs,
where concept activations are often noisy and densely activated, making user
interventions ineffective. We introduce Chat-CBM, which replaces score-based
classifiers with a language-based classifier that reasons directly over concept
semantics. By grounding prediction in the semantic space of concepts, Chat-CBM
preserves the interpretability of CBMs while enabling richer and more intuitive
interventions, such as concept correction, addition or removal of concepts,
incorporation of external knowledge, and high-level reasoning guidance.
Leveraging the language understanding and few-shot capabilities of frozen large
language models, Chat-CBM extends the intervention interface of CBMs beyond
numerical editing and remains effective even in unsupervised settings.
Experiments on nine datasets demonstrate that Chat-CBM achieves higher
predictive performance and substantially improves user interactivity while
maintaining the concept-based interpretability of CBMs.

</details>


### [150] [SimToken: A Simple Baseline for Referring Audio-Visual Segmentation](https://arxiv.org/abs/2509.17537)
*Dian Jin,Yanghao Zhou,Jinxing Zhou,Jiaqi Ma,Ruohao Guo,Dan Guo*

Main category: cs.CV

TL;DR: SimToken是一个简单的框架，通过整合多模态大语言模型和Segment Anything Model来解决音频-视觉分割任务，利用特殊语义token指导视频对象分割


<details>
  <summary>Details</summary>
Motivation: 解决Referring Audio-Visual Segmentation任务中的跨模态推理和细粒度对象定位挑战

Method: 使用MLLM生成代表目标对象的特殊语义token，作为SAM的提示来分割视频帧中的对象，并引入目标一致语义对齐损失来改进语义学习

Result: 在Ref-AVS基准测试中表现出优于现有方法的性能

Conclusion: SimToken框架通过简单的token集成方法有效解决了复杂的跨模态分割问题

Abstract: Referring Audio-Visual Segmentation (Ref-AVS) aims to segment specific
objects in videos based on natural language expressions involving audio,
vision, and text information. This task poses significant challenges in
cross-modal reasoning and fine-grained object localization. In this paper, we
propose a simple framework, SimToken, that integrates a multimodal large
language model (MLLM) with the Segment Anything Model (SAM). The MLLM is guided
to generate a special semantic token representing the referred object. This
compact token, enriched with contextual information from all modalities, acts
as a prompt to guide SAM to segment objectsacross video frames. To further
improve semantic learning, we introduce a novel target-consistent semantic
alignment loss that aligns token embeddings from different expressions but
referring to the same object. Experiments on the Ref-AVS benchmark demonstrate
that our approach achieves superior performance compared to existing
methods.Code will be available at https://github.com/DianJin-HFUT/SimToken

</details>


### [151] [An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection](https://arxiv.org/abs/2509.17561)
*Edwine Nabahirwa,Wei Song,Minghua Zhang,Shufan Chen*

Main category: cs.CV

TL;DR: 本文对YOLO系列模型在水下物体检测中的鲁棒性进行了首次全面评估，发现在水下失真环境下YOLOv12表现最佳但易受噪声影响，并提出了轻量级训练策略来提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 水下物体检测面临严重挑战，因为水下失真会降低低级特征质量，影响最先进检测器的可靠性。目前缺乏对YOLO模型在这种独特挑战条件下鲁棒性的系统研究。

Method: 使用统一数据集（10,000张标注图像）在六个模拟水下环境中评估YOLOv8-YOLOv12变体，分析失真对纹理、边缘和颜色等关键低级特征的影响，并评估噪声感知样本注入和高级增强微调等轻量级训练策略。

Result: YOLOv12整体性能最强但对噪声高度敏感；噪声会破坏边缘和纹理特征；图像数量和实例频率是检测性能的主要驱动因素；噪声感知样本注入能提升鲁棒性，高级增强微调在增强域中提升准确性。

Conclusion: 研究结果为构建弹性和成本效益高的水下物体检测系统提供了实用指导，展示了轻量级训练策略在领域适应方面的强大潜力。

Abstract: Underwater object detection (UOD) remains a critical challenge in computer
vision due to underwater distortions which degrade low-level features and
compromise the reliability of even state-of-the-art detectors. While YOLO
models have become the backbone of real-time object detection, little work has
systematically examined their robustness under these uniquely challenging
conditions. This raises a critical question: Are YOLO models genuinely robust
when operating under the chaotic and unpredictable conditions of underwater
environments? In this study, we present one of the first comprehensive
evaluations of recent YOLO variants (YOLOv8-YOLOv12) across six simulated
underwater environments. Using a unified dataset of 10,000 annotated images
from DUO and Roboflow100, we not only benchmark model robustness but also
analyze how distortions affect key low-level features such as texture, edges,
and color. Our findings show that (1) YOLOv12 delivers the strongest overall
performance but is highly vulnerable to noise, and (2) noise disrupts edge and
texture features, explaining the poor detection performance in noisy images.
Class imbalance is a persistent challenge in UOD. Experiments revealed that (3)
image counts and instance frequency primarily drive detection performance,
while object appearance exerts only a secondary influence. Finally, we
evaluated lightweight training-aware strategies: noise-aware sample injection,
which improves robustness in both noisy and real-world conditions, and
fine-tuning with advanced enhancement, which boosts accuracy in enhanced
domains but slightly lowers performance in original data, demonstrating strong
potential for domain adaptation, respectively. Together, these insights provide
practical guidance for building resilient and cost-efficient UOD systems.

</details>


### [152] [Visual Instruction Pretraining for Domain-Specific Foundation Models](https://arxiv.org/abs/2509.17562)
*Yuxuan Li,Yicheng Zhang,Wenhao Tang,Yimian Dai,Ming-Ming Cheng,Xiang Li,Jian Yang*

Main category: cs.CV

TL;DR: 本文提出ViTP（Visual insTruction Pretraining）新范式，通过在视觉语言模型中嵌入ViT骨干网络，利用视觉指令数据端到端预训练，增强感知能力。


<details>
  <summary>Details</summary>
Motivation: 现代计算机视觉形成了感知、推理和生成的闭环，但高层推理对底层感知特征学习的影响尚未充分探索。本文旨在填补这一空白。

Method: ViTP将Vision Transformer嵌入视觉语言模型，使用目标下游领域的视觉指令数据进行端到端预训练，并采用Visual Robustness Learning（VRL）方法从稀疏视觉标记中学习鲁棒特征。

Result: 在16个具有挑战性的遥感影像和医学影像基准测试中，ViTP在多种下游任务上取得了最先进的性能。

Conclusion: ViTP通过将推理直接融入感知预训练过程，为下游领域的基础模型预训练提供了有效的新范式。

Abstract: Modern computer vision is converging on a closed loop in which perception,
reasoning and generation mutually reinforce each other. However, this loop
remains incomplete: the top-down influence of high-level reasoning on the
foundational learning of low-level perceptual features is not yet
underexplored. This paper addresses this gap by proposing a new paradigm for
pretraining foundation models in downstream domains. We introduce Visual
insTruction Pretraining (ViTP), a novel approach that directly leverages
reasoning to enhance perception. ViTP embeds a Vision Transformer (ViT)
backbone within a Vision-Language Model and pretrains it end-to-end using a
rich corpus of visual instruction data curated from target downstream domains.
ViTP is powered by our proposed Visual Robustness Learning (VRL), which compels
the ViT to learn robust and domain-relevant features from a sparse set of
visual tokens. Extensive experiments on 16 challenging remote sensing and
medical imaging benchmarks demonstrate that ViTP establishes new
state-of-the-art performance across a diverse range of downstream tasks. The
code is available at github.com/zcablii/ViTP.

</details>


### [153] [MRN: Harnessing 2D Vision Foundation Models for Diagnosing Parkinson's Disease with Limited 3D MR Data](https://arxiv.org/abs/2509.17566)
*Ding Shaodong,Liu Ziyang,Zhou Yijun,Liu Tao*

Main category: cs.CV

TL;DR: 本文提出了一种利用2D视觉基础模型进行帕金森病自动诊断的方法，通过多ROI处理和对比学习，在小型数据集上取得了86.0%的准确率，获得MICCAI 2025挑战赛第一名。


<details>
  <summary>Details</summary>
Motivation: 帕金森病的自动诊断具有重要临床需求，但现有方法面临数据量小导致的过拟合问题，以及3D医学模型预训练与微调时的模态不匹配挑战。

Method: 从NM和QSM图像中裁剪关键ROI，使用2D VFMs编码3D ROI的轴向切片，通过辅助分割头引导特征提取，并引入多ROI监督对比学习来提升诊断性能。

Result: 在仅300个标注QSM和NM-MRI扫描的数据集上训练，达到了86.0%的准确率，比第二名方法高出5.5%，获得MICCAI 2025挑战赛第一名。

Conclusion: 该方法展示了2D VFMs在3D MR图像临床分析中的潜力，为小样本医学图像诊断提供了有效解决方案。

Abstract: The automatic diagnosis of Parkinson's disease is in high clinical demand due
to its prevalence and the importance of targeted treatment. Current clinical
practice often relies on diagnostic biomarkers in QSM and NM-MRI images.
However, the lack of large, high-quality datasets makes training diagnostic
models from scratch prone to overfitting. Adapting pre-trained 3D medical
models is also challenging, as the diversity of medical imaging leads to
mismatches in voxel spacing and modality between pre-training and fine-tuning
data. In this paper, we address these challenges by leveraging 2D vision
foundation models (VFMs). Specifically, we crop multiple key ROIs from NM and
QSM images, process each ROI through separate branches to compress the ROI into
a token, and then combine these tokens into a unified patient representation
for classification. Within each branch, we use 2D VFMs to encode axial slices
of the 3D ROI volume and fuse them into the ROI token, guided by an auxiliary
segmentation head that steers the feature extraction toward specific brain
nuclei. Additionally, we introduce multi-ROI supervised contrastive learning,
which improves diagnostic performance by pulling together representations of
patients from the same class while pushing away those from different classes.
Our approach achieved first place in the MICCAI 2025 PDCADxFoundation
challenge, with an accuracy of 86.0% trained on a dataset of only 300 labeled
QSM and NM-MRI scans, outperforming the second-place method by 5.5%.These
results highlight the potential of 2D VFMs for clinical analysis of 3D MR
images.

</details>


### [154] [PRNU-Bench: A Novel Benchmark and Model for PRNU-Based Camera Identification](https://arxiv.org/abs/2509.17581)
*Florinel Alin Croitoru,Vlad Hondru,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 提出一个用于相机识别的PRNU估计新基准，包含13K张照片和120+相机，训练和测试在不同场景下进行，实现"野外"评估。同时提出一种基于PRNU的混合架构相机识别模型，使用去噪自编码器估计PRNU信号，卷积网络进行1:N验证，采用Hadamard乘积方法替代对比学习，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有相机识别方法在真实场景下的性能评估不足，需要开发能够在不同拍摄条件下进行准确相机识别的基准和方法。

Method: 构建包含13K照片、120+相机的基准数据集，提出混合架构模型：使用去噪自编码器估计PRNU信号，卷积网络进行设备验证，采用Hadamard乘积方法处理参考和查询PRNU信号。

Result: 该方法在"野外"评估中显著优于基于去噪自编码器和对比学习的最先进模型，取得了更好的相机识别效果。

Conclusion: 提出的基准和混合架构模型为相机识别提供了有效的解决方案，特别是在真实场景下的性能表现优异，为相关研究提供了有价值的资源和工具。

Abstract: We propose a novel benchmark for camera identification via Photo Response
Non-Uniformity (PRNU) estimation. The benchmark comprises 13K photos taken with
120+ cameras, where the training and test photos are taken in different
scenarios, enabling ``in-the-wild'' evaluation. In addition, we propose a novel
PRNU-based camera identification model that employs a hybrid architecture,
comprising a denoising autoencoder to estimate the PRNU signal and a
convolutional network that can perform 1:N verification of camera devices.
Instead of using a conventional approach based on contrastive learning, our
method takes the Hadamard product between reference and query PRNU signals as
input. This novel design leads to significantly better results compared with
state-of-the-art models based on denoising autoencoders and contrastive
learning. We release our dataset and code at:
https://github.com/CroitoruAlin/PRNU-Bench.

</details>


### [155] [Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models](https://arxiv.org/abs/2509.17588)
*Jinyeong Kim,Seil Kang,Jiwoo Park,Junhyeok Kim,Seong Jae Hwang*

Main category: cs.CV

TL;DR: 提出了一种名为head attribution的技术来分析大型视觉语言模型中图像到文本信息流的机制，发现特定注意力头在信息传递中起关键作用，且这些头的选择由图像语义内容而非视觉外观决定。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型通过注意力头实现图像到文本的信息传递，但由于众多注意力头同时运作，其底层机制难以解释，需要开发方法来理解信息流的结构化过程。

Method: 提出head attribution技术，基于组件归因方法识别在信息传递中起关键作用的注意力头模式，分析LVLMs如何依赖特定注意力头识别和回答关于图像主要对象的问题。

Result: 发现一个独特的注意力头子集促进图像到文本信息流，这些头的选择由输入图像的语义内容决定；文本信息先传播到角色相关标记和最终标记，然后接收图像信息；图像信息嵌入在对象相关和背景标记中。

Conclusion: 图像到文本信息流遵循结构化过程，在注意力头层面的分析为理解LVLMs机制提供了有前景的方向。

Abstract: Large Vision-Language Models (LVLMs) answer visual questions by transferring
information from images to text through a series of attention heads. While this
image-to-text information flow is central to visual question answering, its
underlying mechanism remains difficult to interpret due to the simultaneous
operation of numerous attention heads. To address this challenge, we propose
head attribution, a technique inspired by component attribution methods, to
identify consistent patterns among attention heads that play a key role in
information transfer. Using head attribution, we investigate how LVLMs rely on
specific attention heads to identify and answer questions about the main object
in an image. Our analysis reveals that a distinct subset of attention heads
facilitates the image-to-text information flow. Remarkably, we find that the
selection of these heads is governed by the semantic content of the input image
rather than its visual appearance. We further examine the flow of information
at the token level and discover that (1) text information first propagates to
role-related tokens and the final token before receiving image information, and
(2) image information is embedded in both object-related and background tokens.
Our work provides evidence that image-to-text information flow follows a
structured process, and that analysis at the attention-head level offers a
promising direction toward understanding the mechanisms of LVLMs.

</details>


### [156] [Domain Adaptive Object Detection for Space Applications with Real-Time Constraints](https://arxiv.org/abs/2509.17593)
*Samet Hicsonmez,Abd El Rahman Shabayek,Arunkumar Rathinam,Djamila Aouada*

Main category: cs.CV

TL;DR: 该论文研究了空间应用中目标检测的领域自适应问题，提出了一种结合领域不变特征学习和不变风险最小化的监督领域自适应方法，在轻量级和先进检测器上实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前空间应用中的目标检测模型通常在合成数据上训练，但在真实数据上性能显著下降。领域自适应目标检测在社区中被忽视，需要解决这一领域差距问题。

Method: 基于半监督自适应方法，结合CNN领域判别器的领域不变特征学习和使用领域无关回归头的不变风险最小化。在SSD+MobileNet和FCOS+ResNet-50架构上进行测试。

Result: 在SPEED+和SPARK数据集上评估，仅使用250张标注真实图像，平均精度(AP)提升了高达20个百分点。

Conclusion: 该方法有效缩小了合成数据与真实数据之间的领域差距，证明了监督领域自适应在空间目标检测中的重要性，为实时部署提供了可行解决方案。

Abstract: Object detection is essential in space applications targeting Space Domain
Awareness and also applications involving relative navigation scenarios.
Current deep learning models for Object Detection in space applications are
often trained on synthetic data from simulators, however, the model performance
drops significantly on real-world data due to the domain gap. However, domain
adaptive object detection is an overlooked problem in the community. In this
work, we first show the importance of domain adaptation and then explore
Supervised Domain Adaptation (SDA) to reduce this gap using minimal labeled
real data. We build on a recent semi-supervised adaptation method and tailor it
for object detection. Our approach combines domain-invariant feature learning
with a CNN-based domain discriminator and invariant risk minimization using a
domain-independent regression head. To meet real-time deployment needs, we test
our method on a lightweight Single Shot Multibox Detector (SSD) with MobileNet
backbone and on the more advanced Fully Convolutional One-Stage object detector
(FCOS) with ResNet-50 backbone. We evaluated on two space datasets, SPEED+ and
SPARK. The results show up to 20-point improvements in average precision (AP)
with just 250 labeled real images.

</details>


### [157] [COLA: Context-aware Language-driven Test-time Adaptation](https://arxiv.org/abs/2509.17598)
*Aiming Zhang,Tianyuan Yu,Liang Bai,Jun Tang,Yanming Guo,Yirun Ruan,Yun Zhou,Zhihe Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新的测试时自适应方法COLA，利用预训练的视觉语言模型（如CLIP）实现无需共享标签空间的多目标域自适应，通过轻量级上下文感知模块和类平衡伪标签策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时自适应方法通常假设源域模型和目标域共享相同的标签空间，这限制了其适用性。本文旨在解决更通用的场景，即源模型能够适应多个目标域而无需共享标签。

Method: 提出COLA方法，包含轻量级上下文感知模块（任务感知适配器、上下文感知单元和残差连接单元）和类平衡伪标签策略（CBPL），可无缝集成到冻结的VLM中。

Result: 该方法不仅在测试时自适应场景中有效，在类别泛化任务中也表现出色。

Conclusion: COLA方法通过利用VLM的潜力，提供了一种参数高效且通用的测试时自适应解决方案，解决了传统方法的局限性。

Abstract: Test-time adaptation (TTA) has gained increasing popularity due to its
efficacy in addressing ``distribution shift'' issue while simultaneously
protecting data privacy.
  However, most prior methods assume that a paired source domain model and
target domain sharing the same label space coexist, heavily limiting their
applicability.
  In this paper, we investigate a more general source model capable of
adaptation to multiple target domains without needing shared labels.
  This is achieved by using a pre-trained vision-language model (VLM), \egno,
CLIP, that can recognize images through matching with class descriptions.
  While the zero-shot performance of VLMs is impressive, they struggle to
effectively capture the distinctive attributes of a target domain.
  To that end, we propose a novel method -- Context-aware Language-driven TTA
(COLA).
  The proposed method incorporates a lightweight context-aware module that
consists of three key components: a task-aware adapter, a context-aware unit,
and a residual connection unit for exploring task-specific knowledge,
domain-specific knowledge from the VLM and prior knowledge of the VLM,
respectively.
  It is worth noting that the context-aware module can be seamlessly integrated
into a frozen VLM, ensuring both minimal effort and parameter efficiency.
  Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy
to mitigate the adverse effects caused by class imbalance.
  We demonstrate the effectiveness of our method not only in TTA scenarios but
also in class generalisation tasks.
  The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.

</details>


### [158] [Overview of PlantCLEF 2025: Multi-Species Plant Identification in Vegetation Quadrat Images](https://arxiv.org/abs/2509.17602)
*Giulio Martellucci,Herve Goeau,Pierre Bonnet,Fabrice Vinatier,Alexis Joly*

Main category: cs.CV

TL;DR: 该论文介绍了PlantCLEF 2025挑战赛，该挑战赛旨在利用AI技术加速植物生态学研究中的物种识别，通过多标签分类方法预测样方图像中的所有物种。


<details>
  <summary>Details</summary>
Motivation: 样方图像在生态学研究中至关重要，但人工识别物种耗时费力。集成AI技术可以帮助专家加速物种清单编制并扩大生态研究的空间覆盖范围。

Method: 将任务制定为（弱标记的）多标签分类问题，使用包含140万张单标签植物图像的大型训练集和预训练的视觉Transformer模型，预测样方图像中的所有物种。

Result: 提供了包含2,105张高分辨率多标签图像的测试集，覆盖约400个物种，并详细描述了参与者使用的方法和模型以及取得的成果。

Conclusion: PlantCLEF 2025挑战赛为评估AI在植物生态学研究中的应用进展提供了重要平台，有望推动该领域的技术发展。

Abstract: Quadrat images are essential for ecological studies, as they enable
standardized sampling, the assessment of plant biodiversity, long-term
monitoring, and large-scale field campaigns. These images typically cover an
area of fifty centimetres or one square meter, and botanists carefully identify
all the species present. Integrating AI could help specialists accelerate their
inventories and expand the spatial coverage of ecological studies. To assess
progress in this area, the PlantCLEF 2025 challenge relies on a new test set of
2,105 high-resolution multi-label images annotated by experts and covering
around 400 species. It also provides a large training set of 1.4 million
individual plant images, along with vision transformer models pre-trained on
this data. The task is formulated as a (weakly labelled) multi-label
classification problem, where the goal is to predict all species present in a
quadrat image using single-label training data. This paper provides a detailed
description of the data, the evaluation methodology, the methods and models
used by participants, and the results achieved.

</details>


### [159] [From Benchmarks to Reality: Advancing Visual Anomaly Detection by the VAND 3.0 Challenge](https://arxiv.org/abs/2509.17615)
*Lars Heckler-Kram,Ashwin Vaidya,Jan-Hendrik Neudeck,Ulla Scheler,Dick Ameln,Samet Akcay,Paula Ramos*

Main category: cs.CV

TL;DR: VAND 3.0挑战赛展示了异常检测在不同实际场景中的进展，重点关注对现实世界分布偏移的鲁棒性和视觉语言模型在少样本场景下的能力。


<details>
  <summary>Details</summary>
Motivation: 加强学术界与工业界的联系，解决异常检测领域的关键问题，展示当前在不同实践环境下的异常检测进展。

Method: 挑战赛设置两个赛道：一是开发对现实世界分布偏移具有鲁棒性的异常检测方法，二是探索视觉语言模型在少样本场景下的能力。参与者通过结合或调整现有方法并融合新颖流程来改进解决方案。

Result: 参与者的解决方案相比之前基线取得了显著改进，大型预训练视觉（语言）骨干网络在性能提升中发挥了关键作用。

Conclusion: 虽然大型预训练模型对性能提升至关重要，但未来研究需要更有效地扩展异常检测方法，以满足现场实时性和计算约束要求。

Abstract: Visual anomaly detection is a strongly application-driven field of research.
Consequently, the connection between academia and industry is of paramount
importance. In this regard, we present the VAND 3.0 Challenge to showcase
current progress in anomaly detection across different practical settings
whilst addressing critical issues in the field. The challenge hosted two
tracks, fostering the development of anomaly detection methods robust against
real-world distribution shifts (Category 1) and exploring the capabilities of
Vision Language Models within the few-shot regime (Category 2), respectively.
The participants' solutions reached significant improvements over previous
baselines by combining or adapting existing approaches and fusing them with
novel pipelines. While for both tracks the progress in large pre-trained vision
(language) backbones played a pivotal role for the performance increase,
scaling up anomaly detection methods more efficiently needs to be addressed by
future research to meet real-time and computational constraints on-site.

</details>


### [160] [Tensor-Based Self-Calibration of Cameras via the TrifocalCalib Method](https://arxiv.org/abs/2509.17620)
*Gregory Schroeder,Mohamed Sabry,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 提出了一种基于校准三焦张量的相机自标定方法TrifocalCalib，无需先验场景知识或标定目标，能够同时估计焦距和主点，在精度和鲁棒性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶和车辆编队等应用中，预先标定的设置不切实际，需要实时自适应能力，因此需要无需先验场景知识的相机内参估计方法。

Method: 基于校准三焦张量建立方程组，从最小图像数据实现投影相机自标定，无需标定目标，对相机运动无约束，可同时估计焦距和主点。

Result: 在程序生成的合成环境和结构化数据集场景中的评估表明，该方法在精度和鲁棒性上显著优于现有的基于学习和经典方法。

Conclusion: TrifocalCalib方法为无先验场景知识的相机自标定提供了有效的解决方案，代码已公开以支持可复现性。

Abstract: Estimating camera intrinsic parameters without prior scene knowledge is a
fundamental challenge in computer vision. This capability is particularly
important for applications such as autonomous driving and vehicle platooning,
where precalibrated setups are impractical and real-time adaptability is
necessary. To advance the state-of-the-art, we present a set of equations based
on the calibrated trifocal tensor, enabling projective camera self-calibration
from minimal image data. Our method, termed TrifocalCalib, significantly
improves accuracy and robustness compared to both recent learning-based and
classical approaches. Unlike many existing techniques, our approach requires no
calibration target, imposes no constraints on camera motion, and simultaneously
estimates both focal length and principal point. Evaluations in both
procedurally generated synthetic environments and structured dataset-based
scenarios demonstrate the effectiveness of our approach. To support
reproducibility, we make the code publicly available.

</details>


### [161] [Overview of PlantCLEF 2023: Image-based Plant Identification at Global Scale](https://arxiv.org/abs/2509.17622)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 本文介绍了PlantCLEF2023挑战赛，该挑战旨在利用深度学习技术解决植物物种自动识别问题，涉及80,000个植物物种的多图像分类任务。


<details>
  <summary>Details</summary>
Motivation: 面对生物多样性危机，植物识别对人类文明发展至关重要，但人工识别过程耗时且阻碍新数据积累。深度学习技术为解决这一问题提供了希望。

Method: PlantCLEF2023挑战赛采用多图像（和元数据）分类方法，处理大规模类别的植物物种识别，应对数据不平衡、识别错误、重复和质量不一等挑战。

Result: 深度学习技术已趋于成熟，有望在不久的将来开发出能够准确识别全球所有植物物种的识别系统。

Conclusion: PlantCLEF2023挑战赛通过大规模多图像分类任务，推动了植物自动识别技术的发展，为构建全球植物识别系统做出了重要贡献。

Abstract: The world is estimated to be home to over 300,000 species of vascular plants.
In the face of the ongoing biodiversity crisis, expanding our understanding of
these species is crucial for the advancement of human civilization,
encompassing areas such as agriculture, construction, and pharmacopoeia.
However, the labor-intensive process of plant identification undertaken by
human experts poses a significant obstacle to the accumulation of new data and
knowledge. Fortunately, recent advancements in automatic identification,
particularly through the application of deep learning techniques, have shown
promising progress. Despite challenges posed by data-related issues such as a
vast number of classes, imbalanced class distribution, erroneous
identifications, duplications, variable visual quality, and diverse visual
contents (such as photos or herbarium sheets), deep learning approaches have
reached a level of maturity which gives us hope that in the near future we will
have an identification system capable of accurately identifying all plant
species worldwide. The PlantCLEF2023 challenge aims to contribute to this
pursuit by addressing a multi-image (and metadata) classification problem
involving an extensive set of classes (80,000 plant species). This paper
provides an overview of the challenge's resources and evaluations, summarizes
the methods and systems employed by participating research groups, and presents
an analysis of key findings.

</details>


### [162] [OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models](https://arxiv.org/abs/2509.17627)
*Jinshu Chen,Xinghui Li,Xu Bai,Tianxiang Ma,Pengze Zhang,Zhuowei Chen,Gen Li,Lijie Liu,Songtao Zhao,Bingchuan Li,Qian He*

Main category: cs.CV

TL;DR: 本文提出了OmniInsert框架，用于解决无掩码视频插入任务中的三个关键挑战：数据稀缺、主体-场景平衡和插入协调。通过InsertPipe数据管道、条件特定特征注入机制和渐进式训练策略，实现了高质量的视频插入效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视频插入方法依赖复杂控制信号但难以保持主体一致性，限制了实际应用。本文旨在解决数据稀缺、主体-场景平衡和插入协调三大挑战。

Method: 提出InsertPipe数据管道自动构建多样化交叉对数据；开发OmniInsert统一框架，采用条件特定特征注入机制和渐进式训练策略；设计主体聚焦损失和插入偏好优化方法；引入上下文感知重表述模块。

Result: 在新建的InsertBench基准测试中，OmniInsert超越了最先进的闭源商业解决方案，证明了方法的有效性。

Conclusion: OmniInsert为无掩码视频插入提供了有效的解决方案，通过创新的数据管道、训练策略和优化方法，显著提升了视频插入的质量和实用性。

Abstract: Recent advances in video insertion based on diffusion models are impressive.
However, existing methods rely on complex control signals but struggle with
subject consistency, limiting their practical applicability. In this paper, we
focus on the task of Mask-free Video Insertion and aim to resolve three key
challenges: data scarcity, subject-scene equilibrium, and insertion
harmonization. To address the data scarcity, we propose a new data pipeline
InsertPipe, constructing diverse cross-pair data automatically. Building upon
our data pipeline, we develop OmniInsert, a novel unified framework for
mask-free video insertion from both single and multiple subject references.
Specifically, to maintain subject-scene equilibrium, we introduce a simple yet
effective Condition-Specific Feature Injection mechanism to distinctly inject
multi-source conditions and propose a novel Progressive Training strategy that
enables the model to balance feature injection from subjects and source video.
Meanwhile, we design the Subject-Focused Loss to improve the detailed
appearance of the subjects. To further enhance insertion harmonization, we
propose an Insertive Preference Optimization methodology to optimize the model
by simulating human preferences, and incorporate a Context-Aware Rephraser
module during reference to seamlessly integrate the subject into the original
scenes. To address the lack of a benchmark for the field, we introduce
InsertBench, a comprehensive benchmark comprising diverse scenes with
meticulously selected subjects. Evaluation on InsertBench indicates OmniInsert
outperforms state-of-the-art closed-source commercial solutions. The code will
be released.

</details>


### [163] [Overview of PlantCLEF 2022: Image-based plant identification at global scale](https://arxiv.org/abs/2509.17632)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: PlantCLEF2022挑战赛旨在通过多图像和元数据分类方法解决全球植物物种自动识别问题，涉及8万种植物物种的大规模分类任务。


<details>
  <summary>Details</summary>
Motivation: 全球有超过30万种维管植物，人工专家识别限制了新数据和知识的积累。在生物多样性危机背景下，开发自动识别技术对农业发展、建筑、药典等领域至关重要。

Method: 采用深度学习技术进行多图像和元数据分类，处理大规模类别（8万种植物物种）、类别不平衡、部分错误识别、重复数据、视觉质量不一等多种数据问题。

Result: 深度学习技术在处理大规模植物识别问题上已相对成熟，能够应对现实世界中复杂的植物识别挑战。

Conclusion: PlantCLEF2022挑战赛在推动全球植物生物多样性自动识别方面迈出了重要一步，为未来植物识别技术的发展提供了关键见解。

Abstract: It is estimated that there are more than 300,000 species of vascular plants
in the world. Increasing our knowledge of these species is of paramount
importance for the development of human civilization (agriculture,
construction, pharmacopoeia, etc.), especially in the context of the
biodiversity crisis. However, the burden of systematic plant identification by
human experts strongly penalizes the aggregation of new data and knowledge.
Since then, automatic identification has made considerable progress in recent
years as highlighted during all previous editions of PlantCLEF. Deep learning
techniques now seem mature enough to address the ultimate but realistic problem
of global identification of plant biodiversity in spite of many problems that
the data may present (a huge number of classes, very strongly unbalanced
classes, partially erroneous identifications, duplications, variable visual
quality, diversity of visual contents such as photos or herbarium sheets, etc).
The PlantCLEF2022 challenge edition proposes to take a step in this direction
by tackling a multi-image (and metadata) classification problem with a very
large number of classes (80k plant species). This paper presents the resources
and evaluations of the challenge, summarizes the approaches and systems
employed by the participating research groups, and provides an analysis of key
findings.

</details>


### [164] [A$^2$M$^2$-Net: Adaptively Aligned Multi-Scale Moment for Few-Shot Action Recognition](https://arxiv.org/abs/2509.17638)
*Zilin Gao,Qilong Wang,Bingbing Zhang,Qinghua Hu,Peihua Li*

Main category: cs.CV

TL;DR: 本文提出了A²M²-Net方法，通过自适应对齐的多尺度二阶矩网络来解决少样本动作识别中的时序不对齐问题，在多个基准测试中取得了有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的少样本动作识别方法通常忽视个体运动模式在比较中的作用，并且对视频动态特征统计的探索不足，特别是使用2D骨干网络时难以处理视频动态中的时序不对齐挑战。

Method: 提出A²M²-Net，包含两个核心组件：自适应对齐模块(A²)用于匹配，多尺度二阶矩模块(M²)用于生成强表征。M²模块在多个时空尺度上开发语义二阶描述符，A²模块自适应选择信息丰富的候选描述符并考虑个体运动模式。

Result: 在五个广泛使用的少样本动作识别基准测试上进行实验，结果显示A²M²-Net相比最先进方法取得了非常有竞争力的性能。

Conclusion: 该方法能够通过建立自适应对齐协议来处理时序不对齐问题，在多种少样本设置和不同度量标准下都表现出良好的泛化能力，证明了其有效性和通用性。

Abstract: Thanks to capability to alleviate the cost of large-scale annotation,
few-shot action recognition (FSAR) has attracted increased attention of
researchers in recent years. Existing FSAR approaches typically neglect the
role of individual motion pattern in comparison, and under-explore the feature
statistics for video dynamics. Thereby, they struggle to handle the challenging
temporal misalignment in video dynamics, particularly by using 2D backbones. To
overcome these limitations, this work proposes an adaptively aligned
multi-scale second-order moment network, namely A$^2$M$^2$-Net, to describe the
latent video dynamics with a collection of powerful representation candidates
and adaptively align them in an instance-guided manner. To this end, our
A$^2$M$^2$-Net involves two core components, namely, adaptive alignment (A$^2$
module) for matching, and multi-scale second-order moment (M$^2$ block) for
strong representation. Specifically, M$^2$ block develops a collection of
semantic second-order descriptors at multiple spatio-temporal scales.
Furthermore, A$^2$ module aims to adaptively select informative candidate
descriptors while considering the individual motion pattern. By such means, our
A$^2$M$^2$-Net is able to handle the challenging temporal misalignment problem
by establishing an adaptive alignment protocol for strong representation.
Notably, our proposed method generalizes well to various few-shot settings and
diverse metrics. The experiments are conducted on five widely used FSAR
benchmarks, and the results show our A$^2$M$^2$-Net achieves very competitive
performance compared to state-of-the-arts, demonstrating its effectiveness and
generalization.

</details>


### [165] [VideoArtGS: Building Digital Twins of Articulated Objects from Monocular Video](https://arxiv.org/abs/2509.17647)
*Yu Liu,Baoxiong Jia,Ruijie Lu,Chuyue Gan,Huayu Chen,Junfeng Ni,Song-Chun Zhu,Siyuan Huang*

Main category: cs.CV

TL;DR: VideoArtGS是一种从单目视频重建铰接物体数字孪生的新方法，通过运动先验引导和混合中心网格部件分配模块，显著提升了铰接参数和网格重建的精度。


<details>
  <summary>Details</summary>
Motivation: 从单目视频构建铰接物体的数字孪生是一个重要挑战，需要同时重建几何、部件分割和铰接参数。单目视频输入简单且可扩展，但仅凭视觉监督难以解耦物体几何和部件动力学，因为相机和部件的联合运动会导致估计问题不明确。

Method: 提出运动先验引导流程，分析3D轨迹、过滤噪声，并提供可靠的铰接参数初始化。设计混合中心网格部件分配模块，用于基于铰接的变形场，捕捉准确的部件运动。

Result: VideoArtGS在铰接和网格重建方面达到最先进性能，与现有方法相比将重建误差降低了约两个数量级。

Conclusion: VideoArtGS实现了从单目视频创建实用数字孪生的能力，为基于视频的铰接物体重建设立了新基准。

Abstract: Building digital twins of articulated objects from monocular video presents
an essential challenge in computer vision, which requires simultaneous
reconstruction of object geometry, part segmentation, and articulation
parameters from limited viewpoint inputs. Monocular video offers an attractive
input format due to its simplicity and scalability; however, it's challenging
to disentangle the object geometry and part dynamics with visual supervision
alone, as the joint movement of the camera and parts leads to ill-posed
estimation. While motion priors from pre-trained tracking models can alleviate
the issue, how to effectively integrate them for articulation learning remains
largely unexplored. To address this problem, we introduce VideoArtGS, a novel
approach that reconstructs high-fidelity digital twins of articulated objects
from monocular video. We propose a motion prior guidance pipeline that analyzes
3D tracks, filters noise, and provides reliable initialization of articulation
parameters. We also design a hybrid center-grid part assignment module for
articulation-based deformation fields that captures accurate part motion.
VideoArtGS demonstrates state-of-the-art performance in articulation and mesh
reconstruction, reducing the reconstruction error by about two orders of
magnitude compared to existing methods. VideoArtGS enables practical digital
twin creation from monocular video, establishing a new benchmark for
video-based articulated object reconstruction. Our work is made publicly
available at: https://videoartgs.github.io.

</details>


### [166] [Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming Visual Geometry Transformers](https://arxiv.org/abs/2509.17650)
*Soroush Mahdi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.CV

TL;DR: 提出一种训练免费、推理时的令牌驱逐策略，通过丢弃冗余令牌来限制内存增长，在保持准确性的同时显著减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 流式视觉变换器如StreamVGGT在3D感知方面表现强劲，但存在键值内存无限制增长的问题，限制了可扩展性。

Method: 使用推理时令牌驱逐策略，在保持最具信息量的令牌的同时丢弃冗余令牌，从而限制内存使用。

Result: 在7-Scenes长序列上，峰值内存从18.63GB降至9.39GB，准确性和完整性仅下降0.003。在严格内存预算下，驱逐策略允许更密集的帧采样，提高了重建准确性。

Conclusion: 该方法在视频深度估计、3D重建和相机姿态估计等多个任务上接近StreamVGGT性能，但内存使用大幅减少，使长序列流式推理更加实用。

Abstract: Streaming visual transformers like StreamVGGT achieve strong 3D perception
but suffer from unbounded growth of key value (KV) memory, which limits
scalability. We propose a training-free, inference-time token eviction policy
that bounds memory by discarding redundant tokens while keeping the most
informative ones. Our method uses significantly less memory with little to no
drop in accuracy: on 7-Scenes with long sequences it reduces peak memory from
18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under
strict memory budgets, eviction enables denser frame sampling, which improves
reconstruction accuracy compared to the baseline. Experiments across video
depth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and
camera pose estimation (Sintel, TUM-dynamics) show that our approach closely
matches StreamVGGT at a fraction of the memory and makes long-horizon streaming
inference more practical.

</details>


### [167] [SISMA: Semantic Face Image Synthesis with Mamba](https://arxiv.org/abs/2509.17651)
*Filippo Botti,Alex Ergasti,Tomaso Fontanini,Claudio Ferrari,Massimo Bertozzi,Andrea Prati*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mamba的新型架构SISMA，用于语义图像合成，相比基于Transformer的扩散模型，在保持高质量生成的同时显著降低了计算需求。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在语义图像合成中很受欢迎，但其训练和推理计算成本高，主要由于注意力层的二次复杂度。需要一种更轻量高效的替代方案。

Method: 基于最近提出的Mamba架构开发了SISMA模型，通过语义掩码控制生成图像的形状，同时降低计算需求。

Result: 在CelebAMask-HQ数据集上的实验表明，SISMA不仅获得了更好的FID分数，而且运行速度是现有最先进架构的三倍。

Conclusion: SISMA是基于Transformer模型的一个可行且轻量级的替代方案，在语义图像合成任务中表现出优越的性能和效率。

Abstract: Diffusion Models have become very popular for Semantic Image Synthesis (SIS)
of human faces. Nevertheless, their training and inference is computationally
expensive and their computational requirements are high due to the quadratic
complexity of attention layers. In this paper, we propose a novel architecture
called SISMA, based on the recently proposed Mamba. SISMA generates high
quality samples by controlling their shape using a semantic mask at a reduced
computational demand. We validated our approach through comprehensive
experiments with CelebAMask-HQ, revealing that our architecture not only
achieves a better FID score yet also operates at three times the speed of
state-of-the-art architectures. This indicates that the proposed design is a
viable, lightweight substitute to transformer-based models.

</details>


### [168] [Clothing agnostic Pre-inpainting Virtual Try-ON](https://arxiv.org/abs/2509.17654)
*Sehyun Kim,Hye Jun Lee,Jiwoo Lee,Taemin Lee*

Main category: cs.CV

TL;DR: CaP-VTON是一种基于扩散模型的虚拟试穿技术，通过多类别掩码和皮肤修复解决了现有方法在服装轮廓保持和皮肤修复方面的问题，在短袖合成准确率上比Leffa提升了15.4%。


<details>
  <summary>Details</summary>
Motivation: 现有的Leffa方法虽然改进了基于扩散模型的纹理失真问题，但在底部检测不准确和服装轮廓保留方面存在局限，需要开发更自然和一致的全身服装合成方法。

Method: 提出CaP-VTON方法，整合了基于Dress Code的多类别掩码和基于Stable Diffusion的皮肤修复，特别引入了生成皮肤模块来解决长袖转短袖/无袖时的皮肤修复问题。

Result: CaP-VTON在短袖合成准确率达到92.5%，比Leffa提升15.4%，在视觉评估中能一致地复现参考服装的风格和形状。

Conclusion: 该方法保持模型无关特性，可应用于各种基于扩散的虚拟试穿系统，对电商、定制造型和虚拟形象创建等高精度虚拟试穿应用有重要贡献。

Abstract: With the development of deep learning technology, virtual try-on technology
has become an important application value in the fields of e-commerce, fashion,
and entertainment. The recently proposed Leffa has improved the texture
distortion problem of diffu-sion-based models, but there are limitations in
that the bottom detection inaccuracy and the existing clothing silhouette
remain in the synthesis results. To solve this problem, this study proposes
CaP-VTON (Clothing agnostic Pre-inpainting Virtual Try-ON). CaP-VTON has
improved the naturalness and consistency of whole-body clothing syn-thesis by
integrating multi-category masking based on Dress Code and skin inpainting
based on Stable Diffusion. In particular, a generate skin module was introduced
to solve the skin restoration problem that occurs when long-sleeved images are
converted into short-sleeved or sleeveless ones, and high-quality restoration
was implemented consider-ing the human body posture and color. As a result,
CaP-VTON recorded 92.5\%, which is 15.4\% better than Leffa in short-sleeved
synthesis accuracy, and showed the performance of consistently reproducing the
style and shape of reference clothing in visual evaluation. These structures
maintain model-agnostic properties and are applicable to various
diffu-sion-based virtual inspection systems, and can contribute to applications
that require high-precision virtual wearing, such as e-commerce, custom
styling, and avatar creation.

</details>


### [169] [Development and validation of an AI foundation model for endoscopic diagnosis of esophagogastric junction adenocarcinoma: a cohort and deep learning study](https://arxiv.org/abs/2509.17660)
*Yikun Ma,Bo Li,Ying Chen,Zijie Yue,Shuchang Xu,Jingyao Li,Lei Ma,Liang Zhong,Duowu Zou,Leiming Xu,Yunshi Zhong,Xiaobo Li,Weiqun Ding,Minmin Zhang,Dongli He,Zhenghong Li,Ye Chen,Ye Zhao,Jialong Zhuo,Xiaofen Wu,Lisha Yi,Miaojing Shi,Huihui Sun*

Main category: cs.CV

TL;DR: 本文开发了一种基于AI基础模型的方法，用于食管胃结合部腺癌（EGJA）的筛查和分期诊断，使用DINOv2和ResNet50结合全局外观和局部细节特征，在多个测试集上表现出优于现有AI模型和专家的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: EGJA的早期检测对改善患者预后至关重要，但目前的诊断高度依赖操作者经验，需要开发更客观、准确的AI辅助诊断方法。

Method: 采用多中心研究设计，收集12,302张内镜图像，使用DINOv2（视觉基础模型）和ResNet50（卷积神经网络）提取图像特征，分别关注全局外观和局部细节，用于EGJA分期诊断。

Result: 模型在三个测试集上的准确率分别达到0.9256、0.8895和0.8956，优于最佳AI模型（ResNet50）和专家内镜医师。模型辅助下，各级医师的诊断准确率均有显著提升。

Conclusion: 这是基础模型在EGJA分期诊断中的首次应用，展示了在诊断准确性和效率方面的巨大潜力，有望成为临床辅助诊断的有效工具。

Abstract: The early detection of esophagogastric junction adenocarcinoma (EGJA) is
crucial for improving patient prognosis, yet its current diagnosis is highly
operator-dependent. This paper aims to make the first attempt to develop an
artificial intelligence (AI) foundation model-based method for both screening
and staging diagnosis of EGJA using endoscopic images. In this cohort and
learning study, we conducted a multicentre study across seven Chinese hospitals
between December 28, 2016 and December 30, 2024. It comprises 12,302 images
from 1,546 patients; 8,249 of them were employed for model training, while the
remaining were divided into the held-out (112 patients, 914 images), external
(230 patients, 1,539 images), and prospective (198 patients, 1,600 images) test
sets for evaluation. The proposed model employs DINOv2 (a vision foundation
model) and ResNet50 (a convolutional neural network) to extract features of
global appearance and local details of endoscopic images for EGJA staging
diagnosis. Our model demonstrates satisfactory performance for EGJA staging
diagnosis across three test sets, achieving an accuracy of 0.9256, 0.8895, and
0.8956, respectively. In contrast, among representative AI models, the best one
(ResNet50) achieves an accuracy of 0.9125, 0.8382, and 0.8519 on the three test
sets, respectively; the expert endoscopists achieve an accuracy of 0.8147 on
the held-out test set. Moreover, with the assistance of our model, the overall
accuracy for the trainee, competent, and expert endoscopists improves from
0.7035, 0.7350, and 0.8147 to 0.8497, 0.8521, and 0.8696, respectively. To our
knowledge, our model is the first application of foundation models for EGJA
staging diagnosis and demonstrates great potential in both diagnostic accuracy
and efficiency.

</details>


### [170] [SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models](https://arxiv.org/abs/2509.17664)
*Pingyi Chen,Yujing Lou,Shen Cao,Jinhui Guo,Lubin Fan,Yue Wu,Lin Yang,Lizhuang Ma,Jieping Ye*

Main category: cs.CV

TL;DR: SD-VLM是一个增强视觉语言模型3D空间理解能力的新框架，通过大规模空间测量数据集和深度位置编码方法，显著提升了VLMs的空间感知能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在2D语义理解方面表现出色，但在3D空间关系的定量推理能力方面存在不足，主要原因是2D图像缺乏足够的空间表示能力。

Method: 提出两个关键贡献：(1) MSMU数据集，包含精确的空间标注，涵盖700K问答对、250万物理数值标注和10K思维链增强样本；(2) 简单的深度位置编码方法，增强VLMs的空间感知能力。

Result: SD-VLM在MSMU-Bench上表现优异，比GPT-4o和Intern-VL3-78B分别提升26.91%和25.56%，在其他空间理解基准测试中也显示出良好的空间泛化能力。

Conclusion: SD-VLM是一个强大的通用视觉语言模型，展示了卓越的定量空间测量和理解能力，为解决VLMs在3D空间推理方面的局限性提供了有效方案。

Abstract: While vision language models (VLMs) excel in 2D semantic visual
understanding, their ability to quantitatively reason about 3D spatial
relationships remains under-explored, due to the deficiency of 2D images'
spatial representation ability. In this paper, we analyze the problem hindering
VLMs' spatial understanding abilities and propose SD-VLM, a novel framework
that significantly enhances fundamental spatial perception abilities of VLMs
through two key contributions: (1) propose Massive Spatial Measuring and
Understanding (MSMU) dataset with precise spatial annotations, and (2)
introduce a simple depth positional encoding method strengthening VLMs' spatial
awareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA
pairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented
samples. We have trained SD-VLM, a strong generalist VLM which shows superior
quantitative spatial measuring and understanding capability. SD-VLM not only
achieves state-of-the-art performance on our proposed MSMU-Bench, but also
shows spatial generalization abilities on other spatial understanding
benchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments
demonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and
25.56% respectively on MSMU-Bench. Code and models are released at
https://github.com/cpystan/SD-VLM.

</details>


### [171] [Tailored Transformation Invariance for Industrial Anomaly Detection](https://arxiv.org/abs/2509.17670)
*Mariette Schönfeld,Wannes Meert,Hendrik Blockeel*

Main category: cs.CV

TL;DR: LWinNN是一种基于局部窗口的工业异常检测方法，在kNN方法和复杂SOTA方法之间找到平衡点，通过有限平移不变性提高了准确率并减少了训练和测试时间。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测方法要么训练成本高（复杂SOTA方法），要么特征利用不足（传统kNN方法），且发现主流基准测试仅需对微小平移具有鲁棒性。

Method: 提出局部窗口方法LWinNN，在完全平移不变性（kNN）和无平移不变性（复杂方法）之间建立中间方案，通过有限平移不变性优化特征提取。

Result: 实验表明LWinNN显著提高了检测准确率，同时减少了训练和测试时间，缩小了kNN方法与SOTA方法之间的性能差距。

Conclusion: kNN方法的潜力尚未完全挖掘，有限平移不变性是关键改进方向；需要更具空间多样性的基准测试，LWinNN可作为新基线方法。

Abstract: Industrial Anomaly Detection (IAD) is a subproblem within Computer Vision
Anomaly Detection that has been receiving increasing amounts of attention due
to its applicability to real-life scenarios. Recent research has focused on how
to extract the most informative features, contrasting older kNN-based methods
that use only pretrained features. These recent methods are much more expensive
to train however and could complicate real-life application. Careful study of
related work with regards to transformation invariance leads to the idea that
popular benchmarks require robustness to only minor translations. With this
idea we then formulate LWinNN, a local window based approach that creates a
middle ground between kNN based methods that have either complete or no
translation invariance. Our experiments demonstrate that this small change
increases accuracy considerably, while simultaneously decreasing both train and
test time. This teaches us two things: first, the gap between kNN-based
approaches and more complex state-of-the-art methodology can still be narrowed
by effective usage of the limited data available. Second, our assumption of
requiring only limited translation invariance highlights potential areas of
interest for future work and the need for more spatially diverse benchmarks,
for which our method can hopefully serve as a new baseline. Our code can be
found at https://github.com/marietteschonfeld/LWinNN .

</details>


### [172] [DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning](https://arxiv.org/abs/2509.17684)
*ThankGod Egbe,Peng Wang,Zhihao Guo,Zidong Chen*

Main category: cs.CV

TL;DR: 本文评估了自监督视觉骨干网络DINOv3在机器人操作任务中的表现，发现其在微调后能够匹配或超越传统监督学习的ResNet-18，特别是在样本效率和鲁棒性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究自监督大规模视觉模型是否能够替代传统的监督预训练模型作为机器人操作中视觉运动扩散策略的感知前端，探索无标签预训练在机器人操作中的潜力。

Method: 在四个基准任务（Push-T、Lift、Can、Square）上，使用统一的FiLM条件扩散策略，比较DINOv3和ResNet-18在三种训练模式（从头训练、冻结、微调）下的性能。

Result: 微调后的DINOv3在多个任务上匹配或超越ResNet-18；冻结的DINOv3仍具竞争力；自监督特征提高了样本效率和鲁棒性；在Can任务上测试成功率提升10%。

Conclusion: 自监督大规模视觉模型可作为机器人操作中动作扩散策略的有效、可泛化感知前端，支持进一步探索可扩展的无标签预训练方法。

Abstract: This paper evaluates DINOv3, a recent large-scale self-supervised vision
backbone, for visuomotor diffusion policy learning in robotic manipulation. We
investigate whether a purely self-supervised encoder can match or surpass
conventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under
three regimes: training from scratch, frozen, and finetuned. Across four
benchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned
diffusion policy, we find that (i) finetuned DINOv3 matches or exceeds
ResNet-18 on several tasks, (ii) frozen DINOv3 remains competitive, indicating
strong transferable priors, and (iii) self-supervised features improve sample
efficiency and robustness. These results support self-supervised large visual
models as effective, generalizable perceptual front-ends for action diffusion
policies, motivating further exploration of scalable label-free pretraining in
robotic manipulation. Compared to using ResNet18 as a backbone, our approach
with DINOv3 achieves up to a 10% absolute increase in test-time success rates
on challenging tasks such as Can, and on-the-par performance in tasks like
Lift, PushT, and Square.

</details>


### [173] [Predicting Depth Maps from Single RGB Images and Addressing Missing Information in Depth Estimation](https://arxiv.org/abs/2509.17686)
*Mohamad Mofeed Chaar,Jamal Raiyn,Galia Weidl*

Main category: cs.CV

TL;DR: 本文提出了一种从单张RGB图像生成深度图像并修复深度图像中缺失信息的算法，通过多层训练方法在Cityscapes数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中深度成像至关重要，但深度图像常存在信息缺失问题，影响物体检测和测量精度。

Method: 采用多层训练方法开发算法，能够从单张RGB图像生成深度图像，并修复深度图像中的缺失信息。

Result: 在Cityscapes数据集上测试成功解决了深度图像中的信息缺失问题，证明了算法在真实城市场景中的有效性。

Conclusion: 该算法能够有效生成完整的深度图像并修复缺失信息，为自动驾驶系统的深度感知提供了可靠解决方案。

Abstract: Depth imaging is a crucial area in Autonomous Driving Systems (ADS), as it
plays a key role in detecting and measuring objects in the vehicle's
surroundings. However, a significant challenge in this domain arises from
missing information in Depth images, where certain points are not measurable
due to gaps or inconsistencies in pixel data. Our research addresses two key
tasks to overcome this challenge. First, we developed an algorithm using a
multi-layered training approach to generate Depth images from a single RGB
image. Second, we addressed the issue of missing information in Depth images by
applying our algorithm to rectify these gaps, resulting in Depth images with
complete and accurate data. We further tested our algorithm on the Cityscapes
dataset and successfully resolved the missing information in its Depth images,
demonstrating the effectiveness of our approach in real-world urban
environments.

</details>


### [174] [FROQ: Observing Face Recognition Models for Efficient Quality Assessment](https://arxiv.org/abs/2509.17689)
*Žiga Babnik,Deepak Kumar Jain,Peter Peer,Vitomir Štruc*

Main category: cs.CV

TL;DR: FROQ是一种半监督、无需训练的面部图像质量评估方法，结合了监督方法的高效性和无监督方法的无需训练优势，通过利用FR模型中的特定中间表示来估计面部图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前FIQA技术大多依赖大量监督训练，而无监督方法虽然无需训练但性能较低且速度较慢。需要一种既能保持高性能又无需显式训练的方法。

Method: FROQ利用FR模型中的特定中间表示进行质量估计，通过基于伪质量标签的简单校准步骤来发现适用于质量评估的表示。提出了一种基于样本扰动的新型无监督FIQA技术来生成伪标签。

Result: 在4个最先进的FR模型和8个基准数据集上的综合实验表明，FROQ相比最先进技术取得了极具竞争力的结果，实现了强性能和高效运行时间。

Conclusion: FROQ成功地将监督FIQA模型的效率与无监督方法的无需训练优势相结合，为面部识别系统提供了一种高效可靠的质量评估解决方案。

Abstract: Face Recognition (FR) plays a crucial role in many critical (high-stakes)
applications, where errors in the recognition process can lead to serious
consequences. Face Image Quality Assessment (FIQA) techniques enhance FR
systems by providing quality estimates of face samples, enabling the systems to
discard samples that are unsuitable for reliable recognition or lead to
low-confidence recognition decisions. Most state-of-the-art FIQA techniques
rely on extensive supervised training to achieve accurate quality estimation.
In contrast, unsupervised techniques eliminate the need for additional training
but tend to be slower and typically exhibit lower performance. In this paper,
we introduce FROQ (Face Recognition Observer of Quality), a semi-supervised,
training-free approach that leverages specific intermediate representations
within a given FR model to estimate face-image quality, and combines the
efficiency of supervised FIQA models with the training-free approach of
unsupervised methods. A simple calibration step based on pseudo-quality labels
allows FROQ to uncover specific representations, useful for quality assessment,
in any modern FR model. To generate these pseudo-labels, we propose a novel
unsupervised FIQA technique based on sample perturbations. Comprehensive
experiments with four state-of-the-art FR models and eight benchmark datasets
show that FROQ leads to highly competitive results compared to the
state-of-the-art, achieving both strong performance and efficient runtime,
without requiring explicit training.

</details>


### [175] [Depth Edge Alignment Loss: DEALing with Depth in Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2509.17702)
*Patrick Schmidt,Vasileios Belagiannis,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 提出一种模型无关的深度边缘对齐损失，用于改进弱监督语义分割模型，利用机器人系统中常见的深度信息来增强分割性能。


<details>
  <summary>Details</summary>
Motivation: 自主机器人系统在新领域应用时需要大量昂贵的像素级密集标签来训练语义分割模型。弱监督方法可以避免昂贵的标注过程，但传统计算机视觉中的弱监督方法缺乏深度信息的利用。

Method: 生成像素级语义标签从图像级监督中，并添加像素级深度信息作为额外监督模态，提出深度边缘对齐损失来改进分割模型。

Result: 在PASCAL VOC/MS COCO验证集和HOPE静态登机分割上，平均交并比分别提高了+5.439、+1.274和+16.416个点。

Conclusion: 该方法能有效提高分割性能，并可与其他损失函数结合获得更好效果，代码将公开提供。

Abstract: Autonomous robotic systems applied to new domains require an abundance of
expensive, pixel-level dense labels to train robust semantic segmentation
models under full supervision. This study proposes a model-agnostic Depth Edge
Alignment Loss to improve Weakly Supervised Semantic Segmentation models across
different datasets. The methodology generates pixel-level semantic labels from
image-level supervision, avoiding expensive annotation processes. While weak
supervision is widely explored in traditional computer vision, our approach
adds supervision with pixel-level depth information, a modality commonly
available in robotic systems. We demonstrate how our approach improves
segmentation performance across datasets and models, but can also be combined
with other losses for even better performance, with improvements up to +5.439,
+1.274 and +16.416 points in mean Intersection over Union on the PASCAL VOC /
MS COCO validation, and the HOPE static onboarding split, respectively. Our
code will be made publicly available.

</details>


### [176] [Neurodynamics-Driven Coupled Neural P Systems for Multi-Focus Image Fusion](https://arxiv.org/abs/2509.17704)
*Bo Li,Yunkuo Lei,Tingting Bao,Yaxian Wang,Lingling Zhang,Jun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经动力学驱动的耦合神经P系统（ND-CNPFuse）的多焦点图像融合方法，通过将源图像映射为可解释的脉冲矩阵来生成高质量决策图，在多个数据集上达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于启发式规则的方法和深度学习黑盒机制难以生成具有精确边界的高质量决策图，多焦点图像融合面临决策图边界精度不足的挑战。

Method: 引入第三代神经计算模型——神经动力学驱动的耦合神经P系统，通过分析模型神经动力学来确保神经元正常激活，将源图像映射为可解释的脉冲矩阵，通过比较脉冲数量直接生成准确决策图而无需后处理。

Result: 在Lytro、MFFW、MFI-WHU和Real-MFF四个经典多焦点图像融合数据集上的实验结果表明，ND-CNPFuse达到了新的最先进性能。

Conclusion: ND-CNPFuse通过神经动力学驱动的耦合神经P系统成功解决了多焦点图像融合中决策图边界精度问题，提供了一种可解释且无需后处理的高效融合方法。

Abstract: Multi-focus image fusion (MFIF) is a crucial technique in image processing,
with a key challenge being the generation of decision maps with precise
boundaries. However, traditional methods based on heuristic rules and deep
learning methods with black-box mechanisms are difficult to generate
high-quality decision maps. To overcome this challenge, we introduce
neurodynamics-driven coupled neural P (CNP) systems, which are third-generation
neural computation models inspired by spiking mechanisms, to enhance the
accuracy of decision maps. Specifically, we first conduct an in-depth analysis
of the model's neurodynamics to identify the constraints between the network
parameters and the input signals. This solid analysis avoids abnormal
continuous firing of neurons and ensures the model accurately distinguishes
between focused and unfocused regions, generating high-quality decision maps
for MFIF. Based on this analysis, we propose a
\textbf{N}eurodynamics-\textbf{D}riven \textbf{CNP} \textbf{F}usion model
(\textbf{ND-CNPFuse}) tailored for the challenging MFIF task. Unlike current
ideas of decision map generation, ND-CNPFuse distinguishes between focused and
unfocused regions by mapping the source image into interpretable spike
matrices. By comparing the number of spikes, an accurate decision map can be
generated directly without any post-processing. Extensive experimental results
show that ND-CNPFuse achieves new state-of-the-art performance on four
classical MFIF datasets, including Lytro, MFFW, MFI-WHU, and Real-MFF. The code
is available at https://github.com/MorvanLi/ND-CNPFuse.

</details>


### [177] [Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review](https://arxiv.org/abs/2509.17707)
*Emre Gülsoylu,Alhassan Abdelhalim,Derya Kara Boztas,Ole Grasse,Carlos Jahn,Simone Frintrop,Janick Edinger*

Main category: cs.CV

TL;DR: 本文回顾了63项关于计算机视觉在集装箱等联运装载单元识别中的实证研究，分析了该领域35年来的技术演进，指出了数据集缺乏和结果差异大的问题，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 联运装载单元的高效识别是港口运营的关键瓶颈，计算机视觉提供了成本效益高的解决方案，但缺乏公开数据集和标准化阻碍了该领域发展。

Method: 系统回顾了1990-2025年间63项实证研究，追踪了从早期数字图像处理和传统机器学习到当前深度学习主导的技术演进路径。

Result: 发现计算机视觉识别结果的端到端准确率差异巨大（5%-96%），主要受数据集限制影响；同时面临从字符识别向场景文本识别转变的新挑战。

Conclusion: 呼吁建立标准化术语、开放数据集和共享源代码，未来研究方向包括针对ISO6346代码的无上下文文本识别优化。

Abstract: The standardisation of Intermodal Loading Units (ILUs), such as containers,
semi-trailers and swap bodies, has revolutionised global trade yet their
efficient and robust identification remains a critical bottleneck in
high-throughput ports and terminals. This paper reviews 63 empirical studies
that propose computer vision (CV) based solutions. It covers the last 35 years
(1990-2025), tracing the field's evolution from early digital image processing
(DIP) and traditional machine learning (ML) to the current dominance of deep
learning (DL) techniques. While CV offers cost-effective alternatives for other
types of identification techniques, its development is hindered by the lack of
publicly available benchmarking datasets. This results in high variance for the
reported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond
dataset limitations, this review highlights the emerging challenges especially
introduced by the shift from character-based text recognition to scene-text
spotting and the integration of mobile cameras (e.g. drones, sensor equipped
ground vehicles) for dynamic terminal monitoring. To advance the field, the
paper calls for standardised terminology, open-access datasets, shared source
code, while outlining future research directions such as contextless text
recognition optimised for ISO6346 codes.

</details>


### [178] [RCTDistill: Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion](https://arxiv.org/abs/2509.17712)
*Geonho Bang,Minjae Seong,Jisong Kim,Geunju Baek,Daye Oh,Junhyung Kim,Junho Koh,Jun Won Choi*

Main category: cs.CV

TL;DR: RCTDistill是一种基于时序融合的跨模态知识蒸馏方法，通过三个关键模块解决雷达-相机融合中的不确定性，在nuScenes和VoD数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有雷达-相机融合方法在性能上仍落后于LiDAR方法，且未充分考虑物体运动和传感器特定误差带来的不确定性。

Method: 提出RCTDistill方法，包含三个模块：RAKD（考虑距离和方位角误差）、TKD（缓解动态物体的时序错位）、RDKD（增强特征区分能力）。

Result: 在nuScenes和View-of-Delft数据集上达到最先进的雷达-相机融合性能，推理速度达26.2 FPS。

Conclusion: 该方法通过有效的跨模态知识蒸馏和时序融合策略，显著提升了雷达-相机融合的3D目标检测性能。

Abstract: Radar-camera fusion methods have emerged as a cost-effective approach for 3D
object detection but still lag behind LiDAR-based methods in performance.
Recent works have focused on employing temporal fusion and Knowledge
Distillation (KD) strategies to overcome these limitations. However, existing
approaches have not sufficiently accounted for uncertainties arising from
object motion or sensor-specific errors inherent in radar and camera
modalities. In this work, we propose RCTDistill, a novel cross-modal KD method
based on temporal fusion, comprising three key modules: Range-Azimuth Knowledge
Distillation (RAKD), Temporal Knowledge Distillation (TKD), and
Region-Decoupled Knowledge Distillation (RDKD). RAKD is designed to consider
the inherent errors in the range and azimuth directions, enabling effective
knowledge transfer from LiDAR features to refine inaccurate BEV
representations. TKD mitigates temporal misalignment caused by dynamic objects
by aligning historical radar-camera BEV features with current LiDAR
representations. RDKD enhances feature discrimination by distilling relational
knowledge from the teacher model, allowing the student to differentiate
foreground and background features. RCTDistill achieves state-of-the-art
radar-camera fusion performance on both the nuScenes and View-of-Delft (VoD)
datasets, with the fastest inference speed of 26.2 FPS.

</details>


### [179] [Automated Labeling of Intracranial Arteries with Uncertainty Quantification Using Deep Learning](https://arxiv.org/abs/2509.17726)
*Javier Bisbal,Patrick Winter,Sebastian Jofre,Aaron Ponce,Sameer A. Ansari,Ramez Abdalla,Michael Markl,Oliver Welin Odeback,Sergio Uribe,Cristian Tejos,Julio Sotelo,Susanne Schnell,David Marlevi*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的框架，用于从3D TOF-MRA分割中自动标记颅内动脉，并引入不确定性量化以提高可解释性和可靠性。评估了三种CNN架构，其中nnUNet表现最佳，并通过血流速度验证了临床实用性。


<details>
  <summary>Details</summary>
Motivation: 颅内动脉的精确解剖标记对脑血管诊断和血流动力学分析至关重要，但目前仍耗时且存在操作者间变异性。

Method: 评估了三种卷积神经网络架构：带残差编码块的UNet、具有通道和空间注意力机制的CS-Net、以及自配置的nnUNet框架。实施了测试时间增强和新的坐标引导策略来量化不确定性。

Result: nnUNet实现了最高的标记性能（平均Dice得分：0.922；平均表面距离：0.387 mm），在解剖复杂血管中表现出更好的鲁棒性。不确定性图可靠地指示了解剖模糊、病理变异或手动标记不一致的区域。

Conclusion: 该框架为自动脑血管标记提供了可扩展、准确且具有不确定性感知的解决方案，支持下游血流动力学分析并促进临床整合。

Abstract: Accurate anatomical labeling of intracranial arteries is essential for
cerebrovascular diagnosis and hemodynamic analysis but remains time-consuming
and subject to interoperator variability. We present a deep learning-based
framework for automated artery labeling from 3D Time-of-Flight Magnetic
Resonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating
uncertainty quantification to enhance interpretability and reliability. We
evaluated three convolutional neural network architectures: (1) a UNet with
residual encoder blocks, reflecting commonly used baselines in vascular
labeling; (2) CS-Net, an attention-augmented UNet incorporating channel and
spatial attention mechanisms for enhanced curvilinear structure recognition;
and (3) nnUNet, a self-configuring framework that automates preprocessing,
training, and architectural adaptation based on dataset characteristics. Among
these, nnUNet achieved the highest labeling performance (average Dice score:
0.922; average surface distance: 0.387 mm), with improved robustness in
anatomically complex vessels. To assess predictive confidence, we implemented
test-time augmentation (TTA) and introduced a novel coordinate-guided strategy
to reduce interpolation errors during augmented inference. The resulting
uncertainty maps reliably indicated regions of anatomical ambiguity,
pathological variation, or manual labeling inconsistency. We further validated
clinical utility by comparing flow velocities derived from automated and manual
labels in co-registered 4D Flow MRI datasets, observing close agreement with no
statistically significant differences. Our framework offers a scalable,
accurate, and uncertainty-aware solution for automated cerebrovascular
labeling, supporting downstream hemodynamic analysis and facilitating clinical
integration.

</details>


### [180] [WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification](https://arxiv.org/abs/2509.17740)
*Yiwen Jiang,Deval Mehta,Siyuan Yan,Yaling Shen,Zimu Wang,Zongyuan Ge*

Main category: cs.CV

TL;DR: WISE方法通过弱监督引导的逐步解释，将概念瓶颈模型的概念表示转化为可解释的推理链，解决了现有多模态思维链方法忽视细粒度物体内部理解的问题。


<details>
  <summary>Details</summary>
Motivation: 现有MCoT方法依赖丰富的数据集且主要关注物体间推理，忽视了图像分类中关键的物体内部理解需求。

Method: 提出WISE方法，在弱监督下将概念瓶颈模型的概念表示重新表述为简洁可解释的推理链，为任何图像分类数据集生成MCoT。

Result: 在10个数据集上的实验表明，生成的MCoT不仅将可解释性提高了37%，而且在微调MLLM时还提升了分类准确率。

Conclusion: 该工作连接了基于概念的可解释性和生成式MCoT推理，为增强MLLM在细粒度视觉理解方面提供了通用框架。

Abstract: Multimodal Large Language Models (MLLMs) have shown promise in visual-textual
reasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly
enhancing interpretability. However, existing MCoT methods rely on
rationale-rich datasets and largely focus on inter-object reasoning,
overlooking the intra-object understanding crucial for image classification. To
address this gap, we propose WISE, a Weak-supervision-guided Step-by-step
Explanation method that augments any image classification dataset with MCoTs by
reformulating the concept-based representations from Concept Bottleneck Models
(CBMs) into concise, interpretable reasoning chains under weak supervision.
Experiments across ten datasets show that our generated MCoTs not only improve
interpretability by 37% but also lead to gains in classification accuracy when
used to fine-tune MLLMs. Our work bridges concept-based interpretability and
generative MCoT reasoning, providing a generalizable framework for enhancing
MLLMs in fine-grained visual understanding.

</details>


### [181] [Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA](https://arxiv.org/abs/2509.17743)
*Chenglin Li,Feng Han,FengTao,Ruilin Li,Qianglong Chen,Jingqi Tong,Yin Zhang,Jiaqi Wang*

Main category: cs.CV

TL;DR: FS-VisPR是一个自适应视觉程序推理框架，通过快速-慢速推理机制平衡简单和复杂视觉任务的效率与准确性，在长视频问答任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM在视觉任务中依赖闭源模型、缺乏系统推理能力、难以处理长视频问答的问题。

Method: 设计高效视觉模块，构建快速-慢速推理数据集，开发FS-LLM生成视觉程序工作流，实现自适应推理机制和参数搜索优化。

Result: 在LVBench上达到50.4%准确率，超越GPT-4o，在VideoMME上与Qwen2.5VL-72B性能相当。

Conclusion: FS-VisPR框架显著提升了视觉程序工作流的效率和可靠性，为长视频问答任务提供了有效解决方案。

Abstract: Large language models (LLMs) have shown promise in generating program
workflows for visual tasks. However, previous approaches often rely on
closed-source models, lack systematic reasoning, and struggle with long-form
video question answering (videoQA). To address these challenges, we introduce
the FS-VisPR framework, an adaptive visual program reasoning approach that
balances fast reasoning for simple queries with slow reasoning for difficult
ones. First, we design efficient visual modules (e.g., key clip retrieval and
subtitle retrieval) to support long-form video tasks. Then, we construct a
diverse and high-quality fast-slow reasoning dataset with a strong LLM to align
open-source language models' ability to generate visual program workflows as
FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple
queries are directly solved by VideoLLMs, while difficult ones invoke visual
program reasoning, motivated by human-like reasoning processes. During this
process, low-confidence fast-thinking answers will trigger a second-stage
slow-reasoning process, and a fallback mechanism to fast reasoning is activated
if the program execution fails. Moreover, we improve visual programs through
parameter search during both training and inference. By adjusting the
parameters of the visual modules within the program, multiple variants are
generated: during training, programs that yield correct answers are selected,
while during inference, the program with the highest confidence result is
applied. Experiments show that FS-VisPR improves both efficiency and
reliability in visual program workflows. It achieves 50.4% accuracy on LVBench,
surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.

</details>


### [182] [Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification](https://arxiv.org/abs/2509.17747)
*Sheng Huang,Jiexuan Yan,Beiyan Liu,Bo Liu,Richang Hong*

Main category: cs.CV

TL;DR: 提出HP-DVAL方法，利用视觉语言预训练模型解决多标签图像分类中的类别不平衡问题，通过双视角对齐学习和分层提示调优策略提升性能


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常呈现长尾分布和少样本场景，特别是在类别不平衡的多标签图像分类任务中，数据不平衡和多目标识别带来重大挑战

Method: 采用双视角对齐学习从VLP模型迁移强大特征表示能力，引入分层提示调优策略使用全局和局部提示学习任务特定知识，并设计语义一致性损失防止提示偏离VLP模型的通用知识

Result: 在MS-COCO和VOC2007基准测试中，长尾多标签图像分类任务mAP分别提升10.0%和5.2%，多标签少样本图像分类任务mAP分别提升6.8%和2.9%

Conclusion: HP-DVAL方法有效缓解了多标签图像分类中的类别不平衡问题，在多个基准测试中显著优于现有最优方法

Abstract: Real-world datasets often exhibit class imbalance across multiple categories,
manifesting as long-tailed distributions and few-shot scenarios. This is
especially challenging in Class-Imbalanced Multi-Label Image Classification
(CI-MLIC) tasks, where data imbalance and multi-object recognition present
significant obstacles. To address these challenges, we propose a novel method
termed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which
leverages multi-modal knowledge from vision-language pretrained (VLP) models to
mitigate the class-imbalance problem in multi-label settings. Specifically,
HP-DVAL employs dual-view alignment learning to transfer the powerful feature
representation capabilities from VLP models by extracting complementary
features for accurate image-text alignment. To better adapt VLP models for
CI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes
global and local prompts to learn task-specific and context-related prior
knowledge. Additionally, we design a semantic consistency loss during prompt
tuning to prevent learned prompts from deviating from general knowledge
embedded in VLP models. The effectiveness of our approach is validated on two
CI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results
demonstrate the superiority of our method over SOTA approaches, achieving mAP
improvements of 10.0\% and 5.2\% on the long-tailed multi-label image
classification task, and 6.8\% and 2.9\% on the multi-label few-shot image
classification task.

</details>


### [183] [Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained Semantic Guidance](https://arxiv.org/abs/2509.17757)
*Hongxing Fan,Lipeng Wang,Haohua Chen,Zehuan Huang,Jiangtao Wu,Lu Sheng*

Main category: cs.CV

TL;DR: 提出基于协作多智能体推理的框架，通过多智能体协作分析遮挡关系和边界扩展，结合细粒度语义指导，实现高质量的无模态图像补全。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在数据需求、泛化能力和渐进式管道中错误累积方面的挑战，实现更精确的遮挡对象补全。

Method: 使用多智能体协作推理框架分析遮挡关系，确定边界扩展需求；同时生成细粒度文本描述提供语义指导；基于扩散变换器的注意力图直接生成分层RGBA输出。

Result: 通过广泛评估证明该框架在视觉质量方面达到最先进水平，能够准确合成对象并防止遮挡物或其他不需要元素的再生。

Conclusion: 所提出的协作多智能体推理框架有效解决了无模态补全的关键挑战，在图像编辑和增强现实等应用中具有重要价值。

Abstract: Amodal completion, generating invisible parts of occluded objects, is vital
for applications like image editing and AR. Prior methods face challenges with
data needs, generalization, or error accumulation in progressive pipelines. We
propose a Collaborative Multi-Agent Reasoning Framework based on upfront
collaborative reasoning to overcome these issues. Our framework uses multiple
agents to collaboratively analyze occlusion relationships and determine
necessary boundary expansion, yielding a precise mask for inpainting.
Concurrently, an agent generates fine-grained textual descriptions, enabling
Fine-Grained Semantic Guidance. This ensures accurate object synthesis and
prevents the regeneration of occluders or other unwanted elements, especially
within large inpainting areas. Furthermore, our method directly produces
layered RGBA outputs guided by visible masks and attention maps from a
Diffusion Transformer, eliminating extra segmentation. Extensive evaluations
demonstrate our framework achieves state-of-the-art visual quality.

</details>


### [184] [Neural-MMGS: Multi-modal Neural Gaussian Splats for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2509.17762)
*Sitian Shen,Georgi Pramatarov,Yifu Tao,Daniele De Martini*

Main category: cs.CV

TL;DR: Neural-MMGS是一个新颖的神经3D高斯框架，用于多模态大规模场景重建，通过将多种感知模态融合到每个高斯的紧凑可学习嵌入中。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模场景重建工作虽然整合了LiDAR数据提供几何约束，但LiDAR的丰富物理属性和语义信息的潜力未被充分挖掘。传统方法将不同模态属性作为独立参数附加到高斯上，导致内存使用增加且模态间信息交换受限。

Method: 提出将图像、LiDAR和语义信息融合到紧凑的可学习嵌入中，隐式编码光学、物理和语义特征。训练轻量级神经解码器将这些嵌入映射到高斯参数，实现多模态重建。

Result: 在Oxford Spires数据集上获得更高质量的重建结果，在KITTI-360数据集上达到与当前LiDAR新视角合成方法竞争的结果，同时存储消耗更低。

Conclusion: Neural-MMGS通过多模态融合和紧凑嵌入表示，实现了更高效、高质量的大规模场景重建，证明了多模态信息融合在3D高斯重建中的优势。

Abstract: This paper proposes Neural-MMGS, a novel neural 3DGS framework for multimodal
large-scale scene reconstruction that fuses multiple sensing modalities in a
per-gaussian compact, learnable embedding. While recent works focusing on
large-scale scene reconstruction have incorporated LiDAR data to provide more
accurate geometric constraints, we argue that LiDAR's rich physical properties
remain underexplored. Similarly, semantic information has been used for object
retrieval, but could provide valuable high-level context for scene
reconstruction. Traditional approaches append these properties to Gaussians as
separate parameters, increasing memory usage and limiting information exchange
across modalities. Instead, our approach fuses all modalities -- image, LiDAR,
and semantics -- into a compact, learnable embedding that implicitly encodes
optical, physical, and semantic features in each Gaussian. We then train
lightweight neural decoders to map these embeddings to Gaussian parameters,
enabling the reconstruction of each sensing modality with lower memory overhead
and improved scalability. We evaluate Neural-MMGS on the Oxford Spires and
KITTI-360 datasets. On Oxford Spires, we achieve higher-quality
reconstructions, while on KITTI-360, our method reaches competitive results
with less storage consumption compared with current approaches in LiDAR-based
novel-view synthesis.

</details>


### [185] [Incorporating the Refractory Period into Spiking Neural Networks through Spike-Triggered Threshold Dynamics](https://arxiv.org/abs/2509.17769)
*Yang Li,Xinyi Zeng,Zhe Xue,Pinxian Zeng,Zikai Zhang,Yan Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为RPLIF的新型脉冲神经元模型，通过在LIF神经元中引入不应期机制来更好地模拟生物神经元特性，从而提升SNN的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有SNN中广泛使用的IF和LIF神经元模型忽略了生物神经元的不应期特性，这可能导致神经元过度兴奋和对异常信号的干扰。生物神经元在发放脉冲后会进入不应期，暂时无法响应后续刺激，这一机制对于防止过度兴奋和减少干扰至关重要。

Method: 提出RPLIF方法，通过脉冲触发的阈值动态机制将不应期整合到LIF神经元中。该方法计算效率高，能够确保每个脉冲准确编码神经信息，有效防止神经元在连续输入下的过度兴奋和异常输入的干扰。

Result: RPLIF在多个基准数据集上取得了最先进性能：Cifar10-DVS达到82.40%，N-Caltech101达到83.35%，且使用更少的时间步长。在DVS128 Gesture数据集上低延迟条件下达到97.22%的优异性能。

Conclusion: 将不应期机制整合到LIF神经元中是简单有效的，能够以可忽略的开销提升SNN的鲁棒性和效率，同时获得更好的性能表现。

Abstract: As the third generation of neural networks, spiking neural networks (SNNs)
have recently gained widespread attention for their biological plausibility,
energy efficiency, and effectiveness in processing neuromorphic datasets. To
better emulate biological neurons, various models such as Integrate-and-Fire
(IF) and Leaky Integrate-and-Fire (LIF) have been widely adopted in SNNs.
However, these neuron models overlook the refractory period, a fundamental
characteristic of biological neurons. Research on excitable neurons reveal that
after firing, neurons enter a refractory period during which they are
temporarily unresponsive to subsequent stimuli. This mechanism is critical for
preventing over-excitation and mitigating interference from aberrant signals.
Therefore, we propose a simple yet effective method to incorporate the
refractory period into spiking LIF neurons through spike-triggered threshold
dynamics, termed RPLIF. Our method ensures that each spike accurately encodes
neural information, effectively preventing neuron over-excitation under
continuous inputs and interference from anomalous inputs. Incorporating the
refractory period into LIF neurons is seamless and computationally efficient,
enhancing robustness and efficiency while yielding better performance with
negligible overhead. To the best of our knowledge, RPLIF achieves
state-of-the-art performance on Cifar10-DVS(82.40%) and N-Caltech101(83.35%)
with fewer timesteps and demonstrates superior performance on DVS128
Gesture(97.22%) at low latency.

</details>


### [186] [I2VWM: Robust Watermarking for Image to Video Generation](https://arxiv.org/abs/2509.17773)
*Guanjie Wang,Zehua Ma,Han Fang,Weiming Zhang*

Main category: cs.CV

TL;DR: 该论文提出了I2VWM框架，用于解决图像到视频生成中的跨模态水印问题，通过鲁棒扩散距离和光学流对齐模块提高水印在视频中的时间持久性。


<details>
  <summary>Details</summary>
Motivation: 随着图像引导视频生成技术的快速发展，其在错误信息和欺诈中的潜在滥用引发了担忧，迫切需要有效的数字水印技术。现有水印方法在单一模态内表现良好，但无法在I2V设置中追踪源图像。

Method: 提出鲁棒扩散距离概念来衡量生成视频中水印信号的时间持久性。I2VWM框架在训练时使用视频模拟噪声层，在推理时采用基于光学流的对齐模块。

Result: 在开源和商业I2V模型上的实验表明，I2VWM显著提高了水印的鲁棒性，同时保持了不可感知性。

Conclusion: I2VWM为生成视频时代的跨模态水印建立了新范式，代码已开源。

Abstract: The rapid progress of image-guided video generation (I2V) has raised concerns
about its potential misuse in misinformation and fraud, underscoring the urgent
need for effective digital watermarking. While existing watermarking methods
demonstrate robustness within a single modality, they fail to trace source
images in I2V settings. To address this gap, we introduce the concept of Robust
Diffusion Distance, which measures the temporal persistence of watermark
signals in generated videos. Building on this, we propose I2VWM, a cross-modal
watermarking framework designed to enhance watermark robustness across time.
I2VWM leverages a video-simulation noise layer during training and employs an
optical-flow-based alignment module during inference. Experiments on both
open-source and commercial I2V models demonstrate that I2VWM significantly
improves robustness while maintaining imperceptibility, establishing a new
paradigm for cross-modal watermarking in the era of generative video.
\href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code
Released.}

</details>


### [187] [Accurate and Efficient Low-Rank Model Merging in Core Space](https://arxiv.org/abs/2509.17786)
*Aniello Panariello,Daniel Marczak,Simone Magistri,Angelo Porrello,Bartłomiej Twardowski,Andrew D. Bagdanov,Simone Calderara,Joost van de Weijer*

Main category: cs.CV

TL;DR: 提出了Core Space框架，用于在共享对齐基上合并LoRA适配模型，保持低秩适配效率的同时显著提升跨任务准确性


<details>
  <summary>Details</summary>
Motivation: 现有LoRA模型合并方法往往牺牲效率，通过合并全尺寸权重矩阵，无法保持低秩适配的高效性

Method: Core Space合并框架，将模型投影到共同对齐基的核心空间，确保信息无损，提供形式化证明和复杂度分析

Result: 在视觉和语言任务上显著改进现有合并技术，达到最先进结果，同时仅使用少量计算资源

Conclusion: Core Space框架成功解决了LoRA模型合并的效率与准确性权衡问题，为参数高效适配提供了可行的合并方案

Abstract: In this paper, we address the challenges associated with merging low-rank
adaptations of large neural networks. With the rise of parameter-efficient
adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning
has become more accessible. While fine-tuning models with LoRA is highly
efficient, existing merging methods often sacrifice this efficiency by merging
fully-sized weight matrices. We propose the Core Space merging framework, which
enables the merging of LoRA-adapted models within a common alignment basis,
thereby preserving the efficiency of low-rank adaptation while substantially
improving accuracy across tasks. We further provide a formal proof that
projection into Core Space ensures no loss of information and provide a
complexity analysis showing the efficiency gains. Extensive empirical results
demonstrate that Core Space significantly improves existing merging techniques
and achieves state-of-the-art results on both vision and language tasks while
utilizing a fraction of the computational resources. Codebase is available at
https://github.com/apanariello4/core-space-merging.

</details>


### [188] [From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes](https://arxiv.org/abs/2509.17789)
*Guoxi Huang,Haoran Wang,Zipeng Qi,Wenjun Lu,David Bull,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: R-Splatting是一个统一框架，将水下图像恢复与3D高斯泼溅相结合，通过多增强视图集成、轻量级光照生成器和不确定性感知不透明度优化，提升水下3D重建的渲染质量和几何精度


<details>
  <summary>Details</summary>
Motivation: 水下图像退化对3D重建构成重大挑战，简化的物理模型在复杂场景中往往失效，需要更有效的方法来应对水下环境的复杂性

Method: 1) 将多种UIR模型产生的增强视图集成到单一重建流程；2) 使用轻量级光照生成器采样潜码支持多样化但一致的渲染；3) 对比损失确保解耦稳定的光照表示；4) 提出不确定性感知不透明度优化，将不透明度建模为随机函数来正则化训练

Result: 在Seathru-NeRF和新BlueCoral3D数据集上的实验表明，R-Splatting在渲染质量和几何精度方面均优于强基线方法

Conclusion: 该方法成功解决了水下3D重建中的图像退化问题，通过统一的框架实现了渲染质量和几何保真度的显著提升

Abstract: Underwater image degradation poses significant challenges for 3D
reconstruction, where simplified physical models often fail in complex scenes.
We propose \textbf{R-Splatting}, a unified framework that bridges underwater
image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both
rendering quality and geometric fidelity. Our method integrates multiple
enhanced views produced by diverse UIR models into a single reconstruction
pipeline. During inference, a lightweight illumination generator samples latent
codes to support diverse yet coherent renderings, while a contrastive loss
ensures disentangled and stable illumination representations. Furthermore, we
propose \textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models
opacity as a stochastic function to regularize training. This suppresses abrupt
gradient responses triggered by illumination variation and mitigates
overfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF
and our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong
baselines in both rendering quality and geometric accuracy.

</details>


### [189] [Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding](https://arxiv.org/abs/2509.17792)
*S M A Sharif,Abdur Rehman,Fayaz Ali Dharejo,Radu Timofte,Rizwan Ali Naqvi*

Main category: cs.CV

TL;DR: 本文提出了一种基于学习潜在先验推理的全能图像恢复方法，通过自适应特征选择、空间定位和退化语义三个结构化推理范式，实现了对多种退化类型的有效恢复。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像常受到雾霾、雨雪、低光照等多种空间多样化退化的影响，现有方法依赖外部文本提示或手工设计的架构先验，这些离散假设限制了方法对未见或混合退化的泛化能力。

Method: 将全能图像恢复重新定义为学习潜在先验推理，从输入中自动推断退化感知表示，设计轻量级解码模块利用这些潜在编码线索进行空间自适应恢复。

Result: 在六种常见退化任务、五种复合设置和未见退化上的实验表明，该方法优于现有最优方法，平均PSNR提升1.68 dB，同时效率提高三倍。

Conclusion: 通过将全能图像恢复构建为结构化推理范式，该方法在保持高效的同时显著提升了恢复性能，对多种退化类型具有良好的泛化能力。

Abstract: Real-world images often suffer from spatially diverse degradations such as
haze, rain, snow, and low-light, significantly impacting visual quality and
downstream vision tasks. Existing all-in-one restoration (AIR) approaches
either depend on external text prompts or embed hand-crafted architectural
priors (e.g., frequency heuristics); both impose discrete, brittle assumptions
that weaken generalization to unseen or mixed degradations. To address this
limitation, we propose to reframe AIR as learned latent prior inference, where
degradation-aware representations are automatically inferred from the input
without explicit task cues. Based on latent priors, we formulate AIR as a
structured reasoning paradigm: (1) which features to route (adaptive feature
selection), (2) where to restore (spatial localization), and (3) what to
restore (degradation semantics). We design a lightweight decoding module that
efficiently leverages these latent encoded cues for spatially-adaptive
restoration. Extensive experiments across six common degradation tasks, five
compound settings, and previously unseen degradations demonstrate that our
method outperforms state-of-the-art (SOTA) approaches, achieving an average
PSNR improvement of 1.68 dB while being three times more efficient.

</details>


### [190] [TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided Medical Time Series Classification](https://arxiv.org/abs/2509.17802)
*Qi'ao Xu,Pengfei Wang,Bo Zhong,Tianwen Qian,Xiaoling Wang,Ye Wang,Hong Yu*

Main category: cs.CV

TL;DR: TS-P^2CL是一个用于医学时间序列分类的即插即用框架，通过将1D生理信号转换为2D伪图像，利用预训练视觉模型的模式识别能力，采用双重对比学习策略来缓解跨个体异质性问题。


<details>
  <summary>Details</summary>
Motivation: 医学时间序列分类在智能医疗中至关重要，但由于跨个体异质性严重，现有方法受到模态特定归纳偏置的限制，无法学习普遍不变的表征。

Method: 提出TS-P^2CL框架：1）将1D生理信号转换为2D伪图像，建立与视觉领域的桥梁；2）采用双重对比学习策略：模态内一致性强制时间连贯性，跨模态对齐将时间序列动态与视觉语义对齐。

Result: 在六个医学时间序列数据集上的广泛实验表明，TS-P^2CL在受试者依赖和受试者独立设置下均优于14种方法。

Conclusion: 该框架通过视觉引导范式成功缓解了个体特异性偏差，学习了鲁棒的领域不变特征，为医学时间序列分类提供了有效解决方案。

Abstract: Medical time series (MedTS) classification is pivotal for intelligent
healthcare, yet its efficacy is severely limited by poor cross-subject
generation due to the profound cross-individual heterogeneity. Despite advances
in architectural innovations and transfer learning techniques, current methods
remain constrained by modality-specific inductive biases that limit their
ability to learn universally invariant representations. To overcome this, we
propose TS-P$^2$CL, a novel plug-and-play framework that leverages the
universal pattern recognition capabilities of pre-trained vision models. We
introduce a vision-guided paradigm that transforms 1D physiological signals
into 2D pseudo-images, establishing a bridge to the visual domain. This
transformation enables implicit access to rich semantic priors learned from
natural images. Within this unified space, we employ a dual-contrastive
learning strategy: intra-modal consistency enforces temporal coherence, while
cross-modal alignment aligns time-series dynamics with visual semantics,
thereby mitigating individual-specific biases and learning robust,
domain-invariant features. Extensive experiments on six MedTS datasets
demonstrate that TS-P$^2$CL consistently outperforms fourteen methods in both
subject-dependent and subject-independent settings.

</details>


### [191] [Selecting Optimal Camera Views for Gait Analysis: A Multi-Metric Assessment of 2D Projections](https://arxiv.org/abs/2509.17805)
*Dong Chen,Huili Peng,Yong Hu,Kenneth MC. Cheung*

Main category: cs.CV

TL;DR: 该研究系统量化了摄像机视角（正面vs侧面）对2D无标记步态分析准确性的影响，发现侧面视角在矢状面运动学参数上表现更优，而正面视角在躯干对称性参数上更准确。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏关于摄像机视角如何影响2D无标记步态分析准确性的系统性证据，需要为临床应用中摄像机部署提供数据驱动的指导。

Method: 使用18名受试者的步态数据，同时记录正面、侧面和3D运动捕捉数据，采用YOLOv8进行姿态估计，使用DTW、MCC、KLD和IE四种指标评估一致性，并进行统计显著性检验。

Result: 侧面视角在步长和膝关节旋转等矢状面运动学参数上显著优于正面视角，而正面视角在躯干旋转和腕-髋中距离等对称性参数上表现更好，效应量为中等至大。

Conclusion: 摄像机视角对步态参数准确性有重要影响，侧面视角适合矢状面运动学分析，正面视角适合躯干对称性分析，未来应基于疾病特点采用双视角设置。

Abstract: Objective: To systematically quantify the effect of the camera view (frontal
vs. lateral) on the accuracy of 2D markerless gait analysis relative to 3D
motion capture ground truth. Methods: Gait data from 18 subjects were recorded
simultaneously using frontal, lateral and 3D motion capture systems. Pose
estimation used YOLOv8. Four metrics were assessed to evaluate agreement:
Dynamic Time Warping (DTW) for temporal alignment, Maximum Cross-Correlation
(MCC) for signal similarity, Kullback-Leibler Divergence (KLD) for distribution
differences, and Information Entropy (IE) for complexity. Wilcoxon signed-rank
tests (significance: $p < 0.05$) and Cliff's delta ($\delta$) were used to
measure statistical differences and effect sizes. Results: Lateral views
significantly outperformed frontal views for sagittal plane kinematics: step
length (DTW: $53.08 \pm 24.50$ vs. $69.87 \pm 25.36$, $p = 0.005$) and knee
rotation (DTW: $106.46 \pm 38.57$ vs. $155.41 \pm 41.77$, $p = 0.004$). Frontal
views were superior for symmetry parameters: trunk rotation (KLD: $0.09 \pm
0.06$ vs. $0.30 \pm 0.19$, $p < 0.001$) and wrist-to-hipmid distance (MCC:
$105.77 \pm 29.72$ vs. $75.20 \pm 20.38$, $p = 0.003$). Effect sizes were
medium-to-large ($\delta: 0.34$--$0.76$). Conclusion: Camera view critically
impacts gait parameter accuracy. Lateral views are optimal for sagittal
kinematics; frontal views excel for trunk symmetry. Significance: This first
systematic evidence enables data-driven camera deployment in 2D gait analysis,
enhancing clinical utility. Future implementations should leverage both views
via disease-oriented setups.

</details>


### [192] [Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training](https://arxiv.org/abs/2509.17816)
*Brown Ebouky,Ajad Chhatkuli,Cristiano Malossi,Christoph Studer,Roy Assaf,Andrea Bartezzaghi*

Main category: cs.CV

TL;DR: GLARE是一种用于视觉基础模型的无监督、数据高效领域自适应方法，通过全局-局部-区域一致性约束来提升下游语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习模型主要基于通用数据集预训练，但在新领域特别是数据有限的情况下，针对密集预测任务的自适应方法研究不足。

Method: 提出GLARE持续自监督预训练任务，结合局部一致性、区域一致性约束，使用轻量级适配器模块进行参数高效更新。

Result: 在多个语义分割基准测试中，GLARE能够以最小的计算和参数开销持续提升下游任务性能。

Conclusion: GLARE为视觉基础模型在新领域的无监督自适应提供了有效的解决方案，特别适用于数据受限的密集预测任务。

Abstract: Self-supervised learning (SSL) has emerged as a central paradigm for training
foundation models by leveraging large-scale unlabeled datasets, often producing
representations with strong generalization capabilities. These models are
typically pre-trained on general-purpose datasets such as ImageNet and
subsequently adapted to various downstream tasks through finetuning. While
recent advances have explored parameter-efficient strategies for adapting
pre-trained models, extending SSL pre-training itself to new domains -
particularly under limited data regimes and for dense prediction tasks -
remains underexplored. In this work, we address the problem of adapting vision
foundation models to new domains in an unsupervised and data-efficient manner,
specifically targeting downstream semantic segmentation. We propose GLARE
(Global Local and Regional Enforcement), a novel continual self-supervised
pre-training task designed to enhance downstream segmentation performance.
GLARE introduces patch-level augmentations to encourage local consistency and
incorporates a regional consistency constraint that leverages spatial semantics
in the data. For efficient continual pre-training, we initialize Vision
Transformers (ViTs) with weights from existing SSL models and update only
lightweight adapter modules - specifically UniAdapter - while keeping the rest
of the backbone frozen. Experiments across multiple semantic segmentation
benchmarks on different domains demonstrate that GLARE consistently improves
downstream performance with minimal computational and parameter overhead.

</details>


### [193] [ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment](https://arxiv.org/abs/2509.17818)
*Yiyang Chen,Xuanhua He,Xiujun Ma,Yue Ma*

Main category: cs.CV

TL;DR: ContextFlow是一个无需训练的基于DiT的视频对象编辑框架，通过高阶Rectified Flow求解器和自适应上下文增强机制，解决了现有方法在保真度和时间一致性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的无需训练视频对象编辑方法在DiT架构中存在两个主要限制：一阶求解器导致的反转不准确，以及粗糙的"硬"特征替换引起的上下文冲突。DiT中先前的层选择启发式方法不适用，使得有效指导变得困难。

Method: 1. 使用高阶Rectified Flow求解器建立稳健的编辑基础；2. 提出自适应上下文增强机制，通过并行重建和编辑路径的Key-Value对连接来丰富自注意力上下文；3. 基于新的指导响应度指标进行数据驱动分析，识别任务特定的关键层。

Result: 大量实验表明，ContextFlow显著优于现有的无需训练方法，甚至超过了几种最先进的基于训练的方法，能够提供时间一致、高保真的结果。

Conclusion: ContextFlow通过系统化的层选择和动态信息融合机制，成功解决了DiT架构中视频对象编辑的挑战，为训练自由的视频编辑提供了有效的解决方案。

Abstract: Training-free video object editing aims to achieve precise object-level
manipulation, including object insertion, swapping, and deletion. However, it
faces significant challenges in maintaining fidelity and temporal consistency.
Existing methods, often designed for U-Net architectures, suffer from two
primary limitations: inaccurate inversion due to first-order solvers, and
contextual conflicts caused by crude "hard" feature replacement. These issues
are more challenging in Diffusion Transformers (DiTs), where the unsuitability
of prior layer-selection heuristics makes effective guidance challenging. To
address these limitations, we introduce ContextFlow, a novel training-free
framework for DiT-based video object editing. In detail, we first employ a
high-order Rectified Flow solver to establish a robust editing foundation. The
core of our framework is Adaptive Context Enrichment (for specifying what to
edit), a mechanism that addresses contextual conflicts. Instead of replacing
features, it enriches the self-attention context by concatenating Key-Value
pairs from parallel reconstruction and editing paths, empowering the model to
dynamically fuse information. Additionally, to determine where to apply this
enrichment (for specifying where to edit), we propose a systematic, data-driven
analysis to identify task-specific vital layers. Based on a novel Guidance
Responsiveness Metric, our method pinpoints the most influential DiT blocks for
different tasks (e.g., insertion, swapping), enabling targeted and highly
effective guidance. Extensive experiments show that ContextFlow significantly
outperforms existing training-free methods and even surpasses several
state-of-the-art training-based approaches, delivering temporally coherent,
high-fidelity results.

</details>


### [194] [Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous Tissue Synthesis in Histopathology](https://arxiv.org/abs/2509.17847)
*Saghir Alfasly,Wataru Uegami,MD Enamul Hoq,Ghazal Alabtah,H. R. Tizhoosh*

Main category: cs.CV

TL;DR: 提出一种用于组织病理学图像生成的潜在扩散模型，通过双条件方法结合语义分割图和特定组织视觉裁剪，生成具有精确区域注释的高保真异质性组织图像。


<details>
  <summary>Details</summary>
Motivation: 解决组织病理学合成数据生成面临的挑战：保持组织异质性、捕捉细微形态特征，以及在无标注数据集上的可扩展性。

Method: 使用潜在扩散模型，采用双条件方法结合语义分割图和原始组织裁剪。对于无标注数据，引入自监督扩展，使用基础模型嵌入将全切片图像聚类为100种组织类型，自动生成伪语义图进行训练。

Result: 在Camelyon16数据集上Fréchet距离降低6倍（从430.1降至72.0），在Panda和TCGA数据集上降低2-3倍。仅使用合成数据训练的DeepLabv3+模型在Camelyon16和Panda上分别达到0.71和0.95的IoU，接近真实数据基线（0.72和0.96）。

Conclusion: 该框架为生成多样化、标注的组织病理学数据提供了实用解决方案，解决了计算病理学中的关键瓶颈，能够扩展到11,765个TCGA全切片图像而无需手动标注。

Abstract: Synthetic data generation in histopathology faces unique challenges:
preserving tissue heterogeneity, capturing subtle morphological features, and
scaling to unannotated datasets. We present a latent diffusion model that
generates realistic heterogeneous histopathology images through a novel
dual-conditioning approach combining semantic segmentation maps with
tissue-specific visual crops. Unlike existing methods that rely on text prompts
or abstract visual embeddings, our approach preserves critical morphological
details by directly incorporating raw tissue crops from corresponding semantic
regions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches
ensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we
introduce a self-supervised extension that clusters whole-slide images into 100
tissue types using foundation model embeddings, automatically generating
pseudo-semantic maps for training. Our method synthesizes high-fidelity images
with precise region-wise annotations, achieving superior performance on
downstream segmentation tasks. When evaluated on annotated datasets, models
trained on our synthetic data show competitive performance to those trained on
real data, demonstrating the utility of controlled heterogeneous tissue
generation. In quantitative evaluation, prompt-guided synthesis reduces Frechet
Distance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower
FD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on
synthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within
1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA
whole-slide images without manual annotations, our framework offers a practical
solution for an urgent need for generating diverse, annotated histopathology
data, addressing a critical bottleneck in computational pathology.

</details>


### [195] [ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos](https://arxiv.org/abs/2509.17864)
*Shi Chen,Erik Sandström,Sandro Lombardi,Siyuan Li,Martin R. Oswald*

Main category: cs.CV

TL;DR: 提出了一种在线动态场景重建方法，通过SLAM系统分离静态和动态部分，使用运动掩码策略进行鲁棒姿态跟踪，并利用运动支架图进行动态部分重建。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM方法通常仅移除动态部分或需要RGB-D输入，离线方法无法扩展到长视频序列，而基于transformer的前馈方法缺乏全局一致性和外观细节。

Method: 在SLAM系统中分离静态和动态部分，采用新颖的运动掩码策略进行鲁棒姿态跟踪，动态部分通过渐进适应的运动支架图进行重建。

Result: 方法能够生成与离线方法相竞争的新视角渲染结果，并在跟踪性能上与最先进的动态SLAM方法相当。

Conclusion: 该方法实现了真正实用的动态3D重建，具备在线操作、全局姿态和地图一致性、详细外观建模能力，并能灵活处理RGB和RGB-D输入。

Abstract: Achieving truly practical dynamic 3D reconstruction requires online
operation, global pose and map consistency, detailed appearance modeling, and
the flexibility to handle both RGB and RGB-D inputs. However, existing SLAM
methods typically merely remove the dynamic parts or require RGB-D input, while
offline methods are not scalable to long video sequences, and current
transformer-based feedforward methods lack global consistency and appearance
details. To this end, we achieve online dynamic scene reconstruction by
disentangling the static and dynamic parts within a SLAM system. The poses are
tracked robustly with a novel motion masking strategy, and dynamic parts are
reconstructed leveraging a progressive adaptation of a Motion Scaffolds graph.
Our method yields novel view renderings competitive to offline methods and
achieves on-par tracking with state-of-the-art dynamic SLAM methods.

</details>


### [196] [Trainee Action Recognition through Interaction Analysis in CCATT Mixed-Reality Training](https://arxiv.org/abs/2509.17888)
*Divya Mereddy,Marcos Quinones-Grueiro,Ashwin T S,Eduardo Davalos,Gautam Biswas,Kent Etherton,Tyler Davis,Katelyn Kay,Jill Lear,Benjamin Goldberg*

Main category: cs.CV

TL;DR: 本研究提出了一个结合认知任务分析和多模态学习分析的系统化评估框架，用于客观评估重症监护航空运输团队在混合现实模拟训练中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的人工评估主观性强且可能遗漏关键事件，而现有的AI评估方法需要大量人工标注来训练算法评估复杂团队动态。需要更客观、可解释的性能评估方法。

Method: 开发了领域特定的认知任务分析模型和基于视觉的动作识别管道，使用微调的人-物交互模型（Cascade Disentangling Network）来检测和跟踪学员与设备的交互，自动生成性能指标并映射到分层认知任务分析模型。

Result: 建立了一个数据驱动的评估框架，能够自动生成反应时间、任务持续时间等性能指标，实现可解释的、领域相关的性能评估。

Conclusion: 该框架为重症监护航空运输团队培训提供了更客观、全面的评估方法，解决了传统评估方法的主观性和一致性限制问题。

Abstract: This study examines how Critical Care Air Transport Team (CCATT) members are
trained using mixed-reality simulations that replicate the high-pressure
conditions of aeromedical evacuation. Each team - a physician, nurse, and
respiratory therapist - must stabilize severely injured soldiers by managing
ventilators, IV pumps, and suction devices during flight. Proficient
performance requires clinical expertise and cognitive skills, such as
situational awareness, rapid decision-making, effective communication, and
coordinated task management, all of which must be maintained under stress.
Recent advances in simulation and multimodal data analytics enable more
objective and comprehensive performance evaluation. In contrast, traditional
instructor-led assessments are subjective and may overlook critical events,
thereby limiting generalizability and consistency. However, AI-based automated
and more objective evaluation metrics still demand human input to train the AI
algorithms to assess complex team dynamics in the presence of environmental
noise and the need for accurate re-identification in multi-person tracking. To
address these challenges, we introduce a systematic, data-driven assessment
framework that combines Cognitive Task Analysis (CTA) with Multimodal Learning
Analytics (MMLA). We have developed a domain-specific CTA model for CCATT
training and a vision-based action recognition pipeline using a fine-tuned
Human-Object Interaction model, the Cascade Disentangling Network (CDN), to
detect and track trainee-equipment interactions over time. These interactions
automatically yield performance indicators (e.g., reaction time, task
duration), which are mapped onto a hierarchical CTA model tailored to CCATT
operations, enabling interpretable, domain-relevant performance evaluations.

</details>


### [197] [Does Audio Matter for Modern Video-LLMs and Their Benchmarks?](https://arxiv.org/abs/2509.17901)
*Geewook Kim,Minjoon Seo*

Main category: cs.CV

TL;DR: 本文研究发现当前视频理解模型和评测基准过度依赖视觉信息，音频贡献有限。作者开发了包含音频编码器的视频-语言模型，并创建了更严格的音频敏感评测集。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型和评测基准大多忽略音频信息，但真实世界中音频对视频理解至关重要。作者希望评估音频在视频理解中的实际贡献，并开发能够有效处理音频的模型。

Method: 基于LLaVA-OneVision架构，集成语音/音频编码器（如Whisper），使用基于Mamba的状态空间标记压缩器解决音频标记爆炸问题，并创建了AVQA-Hard和Music-AVQA-Hard评测集。

Result: 音频在当前视频基准测试中贡献有限，但在音频敏感任务中起决定性作用。新评测集能更准确评估模型的音频理解能力。

Conclusion: 当前学术实践与真实世界期望存在差距，需要更重视音频在视频理解中的作用。作者提供了可扩展的音频-视觉视频-语言模型工具。

Abstract: Modern multimodal large language models often claim "video understanding,"
yet most evaluations use muted videos or simply discard audio. We ask a direct
question: how much does audio actually matter for contemporary Video-LLMs and
the benchmarks that certify them? We audit widely used suites and observe that
many items are even solvable from a single frame, rendering audio largely
redundant. Building on LLaVA-OneVision architecture, we attach a speech/audio
encoder (e.g., Whisper) and analyze when audio helps, while addressing audio
token explosion with a lightweight Mamba-based state-space token compressor. We
find that audio yields minimal gains on recent video benchmarks but is decisive
on curated, audio-sensitive subsets. To enable faithful evaluation, we release
AVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a
growing gap between current academic practice and real-world expectations, and
provide practical tools for scalable audio-visual Video-LLMs. We will fully
open-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.

</details>


### [198] [SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain Brain Tumor Segmentation in MRI](https://arxiv.org/abs/2509.17925)
*Yuanhan Wang,Yifei Chen,Shuo Jiang,Wenjing Yu,Mingxuan Liu,Beining Wu,Jinying Zong,Feiwei Qin,Changmiao Wang,Qiyuan Tian*

Main category: cs.CV

TL;DR: SmaRT是一个用于脑肿瘤MRI分割的源自由测试时自适应框架，通过风格调制增强、双分支动量策略和结构先验来应对域偏移问题，在低资源和儿科数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 解决脑肿瘤MRI分割模型在域偏移（扫描仪、协议、人群差异）下的泛化问题，特别是在低资源和儿科队列中传统自适应方法存在不稳定和结构不一致的问题

Method: 集成风格感知增强来减轻外观差异，双分支动量策略用于稳定伪标签细化，结构先验确保一致性、完整性和连通性

Result: 在撒哈拉以南非洲和儿科胶质瘤数据集上的广泛评估显示，SmaRT在Dice准确率和边界精度方面显著优于最先进方法

Conclusion: SmaRT弥合了算法进步与公平临床适用性之间的差距，支持MRI神经肿瘤工具在不同临床环境中的稳健部署

Abstract: Reliable brain tumor segmentation in MRI is indispensable for treatment
planning and outcome monitoring, yet models trained on curated benchmarks often
fail under domain shifts arising from scanner and protocol variability as well
as population heterogeneity. Such gaps are especially severe in low-resource
and pediatric cohorts, where conventional test-time or source-free adaptation
strategies often suffer from instability and structural inconsistency. We
propose SmaRT, a style-modulated robust test-time adaptation framework that
enables source-free cross-domain generalization. SmaRT integrates style-aware
augmentation to mitigate appearance discrepancies, a dual-branch momentum
strategy for stable pseudo-label refinement, and structural priors enforcing
consistency, integrity, and connectivity. This synergy ensures both adaptation
stability and anatomical fidelity under extreme domain shifts. Extensive
evaluations on sub-Saharan Africa and pediatric glioma datasets show that SmaRT
consistently outperforms state-of-the-art methods, with notable gains in Dice
accuracy and boundary precision. Overall, SmaRT bridges the gap between
algorithmic advances and equitable clinical applicability, supporting robust
deployment of MRI-based neuro-oncology tools in diverse clinical environments.
Our source code is available at https://github.com/baiyou1234/SmaRT.

</details>


### [199] [Multi-needle Localization for Pelvic Seed Implant Brachytherapy based on Tip-handle Detection and Matching](https://arxiv.org/abs/2509.17931)
*Zhuo Xiao,Fugen Zhou,Jingjing Wang,Chongyu He,Bo Liu,Haitao Sun,Zhe Ji,Yuliang Jiang,Junjie Wang,Qiuwen Wu*

Main category: cs.CV

TL;DR: 本文提出了一种基于HRNet的无锚网络方法，将针头定位重新定义为尖端-手柄检测和匹配问题，通过热图回归和极角预测来准确检测针尖和手柄，并使用贪婪匹配合并方法关联检测结果以重建3D针路径。


<details>
  <summary>Details</summary>
Motivation: 在盆腔种子植入近距离放射治疗中，术中CT图像的多针准确定位对优化种子放置至关重要，但由于图像对比度差和针头粘连，这一任务具有挑战性。

Method: 提出基于HRNet的无锚网络提取多尺度特征，通过分离的热图回归和极角预测分支检测针尖和手柄中心及方向；设计贪婪匹配合并方法解决不平衡分配问题，迭代选择最可能的尖端-手柄对并基于距离度量合并以重建3D针路径。

Result: 在100名患者数据集上的评估表明，该方法相比基于nnUNet模型的分割方法具有更高的精确度和F1分数。

Conclusion: 该方法为复杂临床场景中的针头定位提供了更稳健和准确的解决方案。

Abstract: Accurate multi-needle localization in intraoperative CT images is crucial for
optimizing seed placement in pelvic seed implant brachytherapy. However, this
task is challenging due to poor image contrast and needle adhesion. This paper
presents a novel approach that reframes needle localization as a tip-handle
detection and matching problem to overcome these difficulties. An anchor-free
network, based on HRNet, is proposed to extract multi-scale features and
accurately detect needle tips and handles by predicting their centers and
orientations using decoupled branches for heatmap regression and polar angle
prediction. To associate detected tips and handles into individual needles, a
greedy matching and merging (GMM) method designed to solve the unbalanced
assignment problem with constraints (UAP-C) is presented. The GMM method
iteratively selects the most probable tip-handle pairs and merges them based on
a distance metric to reconstruct 3D needle paths. Evaluated on a dataset of 100
patients, the proposed method demonstrates superior performance, achieving
higher precision and F1 score compared to a segmentation-based method utilizing
the nnUNet model,thereby offering a more robust and accurate solution for
needle localization in complex clinical scenarios.

</details>


### [200] [Can multimodal representation learning by alignment preserve modality-specific information?](https://arxiv.org/abs/2509.17943)
*Romain Thoreau,Jessie Levillain,Dawa Derksen*

Main category: cs.CV

TL;DR: 本文研究了多模态卫星数据融合中的对比学习方法，发现在追求模态间语义对齐的过程中可能导致任务相关信息的损失，并通过理论和实验验证了这一现象。


<details>
  <summary>Details</summary>
Motivation: 当前多模态遥感数据融合方法主要关注模态间的空间对齐来促进潜在空间的语义对齐，但这种方法可能无法保留模态特有的任务相关信息。作者旨在探索这种对齐策略在什么情况下会导致信息损失。

Method: 首先在简化假设下进行理论分析，证明对齐策略何时会导致信息损失；然后在更现实的设置下进行数值实验来支持理论发现。

Result: 理论和实验证据表明，现有的对比学习方法在追求模态间对齐时确实会损失一些任务相关信息，特别是在这些信息不跨模态共享的情况下。

Conclusion: 这项工作为多模态卫星数据对比学习的新发展提供了理论基础，强调了在模态对齐过程中需要考虑保留模态特有信息的重要性。

Abstract: Combining multimodal data is a key issue in a wide range of machine learning
tasks, including many remote sensing problems. In Earth observation, early
multimodal data fusion methods were based on specific neural network
architectures and supervised learning. Ever since, the scarcity of labeled data
has motivated self-supervised learning techniques. State-of-the-art multimodal
representation learning techniques leverage the spatial alignment between
satellite data from different modalities acquired over the same geographic area
in order to foster a semantic alignment in the latent space. In this paper, we
investigate how this methods can preserve task-relevant information that is not
shared across modalities. First, we show, under simplifying assumptions, when
alignment strategies fundamentally lead to an information loss. Then, we
support our theoretical insight through numerical experiments in more realistic
settings. With those theoretical and empirical evidences, we hope to support
new developments in contrastive learning for the combination of multimodal
satellite data. Our code and data is publicly available at
https://github.com/Romain3Ch216/alg_maclean_25.

</details>


### [201] [DragOSM: Extract Building Roofs and Footprints from Aerial Images by Aligning Historical Labels](https://arxiv.org/abs/2509.17951)
*Kai Li,Xingxing Weng,Yupeng Deng,Yu Meng,Chao Pang,Gui-Song Xia,Xiangyu Zhao*

Main category: cs.CV

TL;DR: 提出DragOSM方法，通过引入对齐令牌概念，将历史OpenStreetMap标签与遥感图像中的建筑屋顶和足迹进行对齐校正，解决倾斜图像中建筑标注的位置偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于分割的方法在倾斜遥感图像中表现不佳，因为屋顶和足迹存在显著位移，且立面像素与屋顶边界融合。虽然OpenStreetMap等开放矢量地图标注可用，但这些历史标签与新图像存在位置偏差且只有单一标注（屋顶或足迹），无法准确描述建筑结构。

Method: 提出DragOSM模型，引入对齐令牌编码校正向量来指导标签校正。将标签对齐建模为交互式去噪过程，将位置偏差建模为高斯分布。训练时通过随机高斯扰动模拟错位来学习校正误差，推理时迭代优化输入标签位置。

Result: 构建了包含179,265个建筑的新数据集ReBO，覆盖41个城市的5,473张图像。实验结果表明DragOSM方法有效。

Conclusion: DragOSM能够有效校正历史OpenStreetMap标签与遥感图像中建筑结构的位置偏差，为大规模城市分析提供了可靠的建筑轮廓提取解决方案。

Abstract: Extracting polygonal roofs and footprints from remote sensing images is
critical for large-scale urban analysis. Most existing methods rely on
segmentation-based models that assume clear semantic boundaries of roofs, but
these approaches struggle in off- nadir images, where the roof and footprint
are significantly displaced, and facade pixels are fused with the roof
boundary. With the increasing availability of open vector map annotations,
e.g., OpenStreetMap, utilizing historical labels for off-nadir image annotation
has become viable because remote sensing images are georeferenced once
captured. However, these historical labels commonly suffer from significant
positional discrepancies with new images and only have one annotation (roof or
footprint), which fails to describe the correct structures of a building. To
address these discrepancies, we first introduce a concept of an alignment
token, which encodes the correction vector to guide the label correction. Based
on this concept, we then propose Drag OpenStreetMap Labels (DragOSM), a novel
model designed to align dislocated historical labels with roofs and footprints.
Specifically, DragOSM formulates the label alignment as an interactive
denoising process, modeling the positional discrepancy as a Gaussian
distribution. During training, it learns to correct these errors by simulating
misalignment with random Gaussian perturbations; during inference, it
iteratively refines the positions of input labels. To validate our method, we
further present a new dataset, Repairing Buildings in OSM (ReBO), comprising
179,265 buildings with both OpenStreetMap and manually corrected annotations
across 5,473 images from 41 cities. Experimental results on ReBO demonstrate
the effectiveness of DragOSM. Code, dataset, and trained models are publicly
available at https://github.com/likaiucas/DragOSM.git.

</details>


### [202] [Breaking the Discretization Barrier of Continuous Physics Simulation Learning](https://arxiv.org/abs/2509.17955)
*Fan Xu,Hao Wu,Nan Wang,Lilan Peng,Kun Wang,Wei Gong,Xibin Zhao*

Main category: cs.CV

TL;DR: CoPS是一种纯数据驱动方法，用于从部分观测中有效建模连续物理模拟，通过多尺度图ODE和神经自校正模块实现时空连续建模。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法受限于固定的时空离散化，难以处理稀疏分布的非线性物理动力学观测问题。

Method: 使用乘法滤波器网络编码空间信息，定制几何网格并通过消息传递机制映射特征，设计多尺度图ODE建模连续时间动力学，引入基于马尔可夫的神经自校正模块。

Result: 综合实验表明CoPS在各种场景下的时空连续建模方面优于现有最先进方法。

Conclusion: CoPS成功克服了离散化限制，为复杂物理动力学的连续建模提供了有效解决方案。

Abstract: The modeling of complicated time-evolving physical dynamics from partial
observations is a long-standing challenge. Particularly, observations can be
sparsely distributed in a seemingly random or unstructured manner, making it
difficult to capture highly nonlinear features in a variety of scientific and
engineering problems. However, existing data-driven approaches are often
constrained by fixed spatial and temporal discretization. While some
researchers attempt to achieve spatio-temporal continuity by designing novel
strategies, they either overly rely on traditional numerical methods or fail to
truly overcome the limitations imposed by discretization. To address these, we
propose CoPS, a purely data-driven methods, to effectively model continuous
physics simulation from partial observations. Specifically, we employ
multiplicative filter network to fuse and encode spatial information with the
corresponding observations. Then we customize geometric grids and use
message-passing mechanism to map features from original spatial domain to the
customized grids. Subsequently, CoPS models continuous-time dynamics by
designing multi-scale graph ODEs, while introducing a Markov-based neural
auto-correction module to assist and constrain the continuous extrapolations.
Comprehensive experiments demonstrate that CoPS advances the state-of-the-art
methods in space-time continuous modeling across various scenarios.

</details>


### [203] [Visual Detector Compression via Location-Aware Discriminant Analysis](https://arxiv.org/abs/2509.17968)
*Qizhen Lan,Jung Im Choi,Qing Tian*

Main category: cs.CV

TL;DR: 提出了一种基于检测判别器的主动网络压缩方法，专门针对视觉检测器进行剪枝，利用目标定位信息，在保持检测性能的同时大幅降低模型复杂度


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法主要关注分类模型，对检测模型的关注有限，且缺乏对关键定位信息的利用。现有方法被动依赖预训练模型，难以在不损害有用组件的情况下移除无用组件

Method: 交替执行两个步骤：(1)最大化并压缩检测相关判别器，将其与检测头前的神经元/滤波器子集对齐；(2)跨层追踪检测相关判别能力，丢弃重要性较低的特征。两个步骤都利用了目标定位信息

Result: 在KITTI和COCO数据集上使用四种先进检测模型和四种竞争方法进行实验，压缩后的模型甚至能够超越原始基础模型，同时大幅降低复杂度

Conclusion: 该方法在视觉检测器压缩方面具有优越性，能够有效利用检测任务特有的定位信息，实现高性能的模型压缩

Abstract: Deep neural networks are powerful, yet their high complexity greatly limits
their potential to be deployed on billions of resource-constrained edge
devices. Pruning is a crucial network compression technique, yet most existing
methods focus on classification models, with limited attention to detection.
Even among those addressing detection, there is a lack of utilization of
essential localization information. Also, many pruning methods passively rely
on pre-trained models, in which useful and useless components are intertwined,
making it difficult to remove the latter without harming the former at the
neuron/filter level. To address the above issues, in this paper, we propose a
proactive detection-discriminants-based network compression approach for deep
visual detectors, which alternates between two steps: (1) maximizing and
compressing detection-related discriminants and aligning them with a subset of
neurons/filters immediately before the detection head, and (2) tracing the
detection-related discriminating power across the layers and discarding
features of lower importance. Object location information is exploited in both
steps. Extensive experiments, employing four advanced detection models and four
state-of-the-art competing methods on the KITTI and COCO datasets, highlight
the superiority of our approach. Remarkably, our compressed models can even
beat the original base models with a substantial reduction in complexity.

</details>


### [204] [StableGuard: Towards Unified Copyright Protection and Tamper Localization in Latent Diffusion Models](https://arxiv.org/abs/2509.17993)
*Haoxin Yang,Bangzhen Liu,Xuemiao Xu,Cheng Xu,Yuyang Yu,Zikai Huang,Yi Wang,Shengfeng He*

Main category: cs.CV

TL;DR: StableGuard是一个新颖的端到端框架，通过在潜在扩散模型中无缝集成二进制水印，实现版权保护和篡改定位，避免了后处理方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的进步增强了AI生成内容的真实性，但也引发了滥用担忧，需要强大的版权保护和篡改定位。现有方法依赖后处理，导致应用不便和取证可靠性降低。

Method: 开发了多路复用水印VAE（MPW-VAE），通过轻量级潜在残差适配器生成配对的水印和无水印图像；引入专家混合引导取证网络（MoE-GFN），动态整合全局水印模式、局部篡改痕迹和频域线索；两者以自监督端到端方式联合优化。

Result: 大量实验表明，StableGuard在图像保真度、水印验证和篡改定位方面始终优于最先进的方法。

Conclusion: StableGuard通过端到端设计成功解决了扩散模型的版权保护和篡改定位问题，实现了水印嵌入和取证准确性之间的互惠训练。

Abstract: The advancement of diffusion models has enhanced the realism of AI-generated
content but also raised concerns about misuse, necessitating robust copyright
protection and tampering localization. Although recent methods have made
progress toward unified solutions, their reliance on post hoc processing
introduces considerable application inconvenience and compromises forensic
reliability. We propose StableGuard, a novel framework that seamlessly
integrates a binary watermark into the diffusion generation process, ensuring
copyright protection and tampering localization in Latent Diffusion Models
through an end-to-end design. We develop a Multiplexing Watermark VAE (MPW-VAE)
by equipping a pretrained Variational Autoencoder (VAE) with a lightweight
latent residual-based adapter, enabling the generation of paired watermarked
and watermark-free images. These pairs, fused via random masks, create a
diverse dataset for training a tampering-agnostic forensic network. To further
enhance forensic synergy, we introduce a Mixture-of-Experts Guided Forensic
Network (MoE-GFN) that dynamically integrates holistic watermark patterns,
local tampering traces, and frequency-domain cues for precise watermark
verification and tampered region detection. The MPW-VAE and MoE-GFN are jointly
optimized in a self-supervised, end-to-end manner, fostering a reciprocal
training between watermark embedding and forensic accuracy. Extensive
experiments demonstrate that StableGuard consistently outperforms
state-of-the-art methods in image fidelity, watermark verification, and
tampering localization.

</details>


### [205] [Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization in Chest Radiographs](https://arxiv.org/abs/2509.18015)
*Advait Gosai,Arun Kavishwar,Stephanie L. McNamara,Soujanya Samineni,Renato Umeton,Alexander Chowdhury,William Lotter*

Main category: cs.CV

TL;DR: 本文评估了通用多模态大语言模型（GPT-4、GPT-5）和领域专用模型（MedGemma）在胸部X光片病理定位任务中的表现，发现它们虽然有一定潜力，但准确率仍低于专用CNN模型和放射科医生基准。


<details>
  <summary>Details</summary>
Motivation: 医学图像解读不仅需要诊断能力，还需要定位病理发现的能力。评估定位能力有助于了解模型对解剖结构和疾病空间关系的理解，具有临床和教育意义。

Method: 使用CheXlocalize数据集中的9种病理，通过空间网格提示管道获取基于坐标的预测，系统评估GPT-4、GPT-5和MedGemma的定位准确性。

Result: GPT-5定位准确率为49.7%，GPT-4为39.1%，MedGemma为17.7%，均低于专用CNN模型（59.9%）和放射科医生基准（80.1%）。GPT-5的预测在解剖学上合理但不精确，GPT-4对固定位置病理表现较好但存在不合理预测，MedGemma泛化能力有限。

Conclusion: 当前MLLMs在医学影像定位任务中表现出潜力但仍有局限，需要与任务专用工具结合才能实现可靠应用。

Abstract: Recent work has shown promising performance of frontier large language models
(LLMs) and their multimodal counterparts in medical quizzes and diagnostic
tasks, highlighting their potential for broad clinical utility given their
accessible, general-purpose nature. However, beyond diagnosis, a fundamental
aspect of medical image interpretation is the ability to localize pathological
findings. Evaluating localization not only has clinical and educational
relevance but also provides insight into a model's spatial understanding of
anatomy and disease. Here, we systematically assess two general-purpose MLLMs
(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to
localize pathologies on chest radiographs, using a prompting pipeline that
overlays a spatial grid and elicits coordinate-based predictions. Averaged
across nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a
localization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),
all lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark
(80.1%). Despite modest performance, error analysis revealed that GPT-5's
predictions were largely in anatomically plausible regions, just not always
precisely localized. GPT-4 performed well on pathologies with fixed anatomical
locations, but struggled with spatially variable findings and exhibited
anatomically implausible predictions more frequently. MedGemma demonstrated the
lowest performance on all pathologies, showing limited capacity to generalize
to this novel task. Our findings highlight both the promise and limitations of
current MLLMs in medical imaging and underscore the importance of integrating
them with task-specific tools for reliable use.

</details>


### [206] [NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning](https://arxiv.org/abs/2509.18041)
*Sahil Shah,S P Sharan,Harsh Goel,Minkyu Choi,Mustafa Munir,Manvik Pasula,Radu Marculescu,Sandeep Chinchali*

Main category: cs.CV

TL;DR: NeuS-QA是一个免训练的神经符号化管道，用于长视频问答，通过将问题转化为时序逻辑表达式，构建视频自动机，并应用模型检查来识别满足逻辑要求的视频片段，从而减少幻觉并提高推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型在长视频问答中面临挑战，无法处理复杂的时间推理和因果关系。现有方法缺乏明确的时序表示和逻辑验证，无法保证采样的上下文满足问题的组合或因果逻辑要求。

Method: NeuS-QA将自然语言问题转化为形式化的时序逻辑表达式，从帧级语义命题构建视频自动机，应用模型检查来严格识别满足问题逻辑要求的视频片段，仅将这些经过逻辑验证的片段提交给视觉语言模型。

Result: 在LongVideoBench和CinePile上的实验表明，NeuS-QA将性能提高了超过10%，特别是在涉及事件排序、因果关系和多步组合推理的问题上表现优异。

Conclusion: NeuS-QA通过神经符号化方法解决了长视频问答中的关键挑战，提供了更好的可解释性、减少幻觉，并实现了无需修改或微调模型的组合推理能力。

Abstract: Long-Form Video Question Answering (LVQA) poses challenges beyond traditional
visual question answering (VQA), which is often limited to static images or
short video clips. While current vision-language models (VLMs) perform well in
those settings, they struggle with complex queries in LVQA over long videos
involving multi-step temporal reasoning and causality. Vanilla approaches,
which sample frames uniformly and feed them to a VLM with the question, incur
significant token overhead, forcing severe downsampling. As a result, the model
often misses fine-grained visual structure, subtle event transitions, or key
temporal cues, ultimately leading to incorrect answers. To address these
limitations, recent works have explored query-adaptive frame sampling,
hierarchical keyframe selection, and agent-based iterative querying. However,
these methods remain fundamentally heuristic: they lack explicit temporal
representations and cannot enforce or verify logical event relationships. As a
result, there are no formal guarantees that the sampled context actually
encodes the compositional or causal logic demanded by the question. To address
these foundational gaps, we introduce NeuS-QA, a training-free, plug-and-play
neuro-symbolic pipeline for LVQA. NeuS-QA translates a natural language
question into a formal temporal logic expression, constructs a video automaton
from frame-level semantic propositions, and applies model checking to
rigorously identify video segments satisfying the question's logical
requirements. Only these logic-verified segments are submitted to the VLM, thus
improving interpretability, reducing hallucinations, and enabling compositional
reasoning without modifying or fine-tuning the model. Experiments on
LongVideoBench and CinePile show NeuS-QA improves performance by over 10%,
especially on questions involving event ordering, causality, and multi-step
compositional reasoning.

</details>


### [207] [TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs](https://arxiv.org/abs/2509.18056)
*Yunheng Li,Jing Cheng,Shaoyong Jia,Hangyi Kuang,Shaohui Jiao,Qibin Hou,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: TempSamp-R1是一个新的强化微调框架，用于改进多模态大语言模型在视频时序定位任务中的效果。它通过使用真实标注作为离策略监督来提供时序精确指导，并采用非线性软优势计算来稳定训练。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法（如GRPO）在大型时序搜索空间任务中效率低下且性能受限，因为它们依赖于策略内采样，往往无法找到时序准确的解决方案。

Method: TempSamp-R1利用真实标注作为离策略监督来提供时序精确指导，采用非线性软优势计算方法动态重塑奖励反馈，并通过混合思维链训练范式优化单一统一模型以支持思维链和非思维链推理模式。

Result: TempSamp-R1在基准数据集上超越了基于GRPO的基线方法，在Charades-STA（R1@0.7: 52.9%, +2.7%）、ActivityNet Captions（R1@0.5: 56.0%, +5.3%）和QVHighlights（mAP: 30.0%, +3.0%）上建立了新的最先进性能。

Conclusion: TempSamp-R1通过离策略监督和非线性软优势计算有效解决了视频时序定位任务中的效率问题，展示了强大的少样本泛化能力，为多模态大语言模型的时序定位任务提供了有效的强化微调解决方案。

Abstract: This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework
designed to improve the effectiveness of adapting multimodal large language
models (MLLMs) to video temporal grounding tasks. We reveal that existing
reinforcement learning methods, such as Group Relative Policy Optimization
(GRPO), rely on on-policy sampling for policy updates. However, in tasks with
large temporal search spaces, this strategy becomes both inefficient and
limited in performance, as it often fails to identify temporally accurate
solutions. To address this limitation, TempSamp-R1 leverages ground-truth
annotations as off-policy supervision to provide temporally precise guidance,
effectively compensating for the sparsity and misalignment in on-policy
solutions. To further stabilize training and reduce variance in reward-based
updates, TempSamp-R1 provides a non-linear soft advantage computation method
that dynamically reshapes the reward feedback via an asymmetric transformation.
By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1
optimizes a single unified model to support both CoT and non-CoT inference
modes, enabling efficient handling of queries with varying reasoning
complexity. Experimental results demonstrate that TempSamp-R1 outperforms
GRPO-based baselines, establishing new state-of-the-art performance on
benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions
(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,
TempSamp-R1 shows robust few-shot generalization capabilities under limited
data. Code: https://github.com/HVision-NKU/TempSamp-R1

</details>


### [208] [GraDeT-HTR: A Resource-Efficient Bengali Handwritten Text Recognition System utilizing Grapheme-based Tokenizer and Decoder-only Transformer](https://arxiv.org/abs/2509.18081)
*Md. Mahmudul Hasan,Ahmed Nesar Tahsin Choudhury,Mahmudul Hasan,Md. Mosaddek Khan*

Main category: cs.CV

TL;DR: GraDeT-HTR是一个基于Grapheme-aware Decoder-only Transformer架构的资源高效孟加拉语手写文本识别系统，通过集成基于字素的标记器显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为世界第六大语言，其手写文本识别系统严重不足。孟加拉语文字的复杂性（包含连字、变音符号和高度可变的书写风格）以及标注数据集的稀缺性使得这项任务特别具有挑战性。

Method: 使用基于字素的标记器增强仅解码器Transformer的性能，在大规模合成数据上进行预训练，然后在真实人工标注样本上进行微调。

Result: 在多个基准数据集上实现了最先进的性能。

Conclusion: GraDeT-HTR系统有效解决了孟加拉语手写文本识别的独特挑战，证明了基于字素的标记器相比传统子词标记器能显著提高识别准确率。

Abstract: Despite Bengali being the sixth most spoken language in the world,
handwritten text recognition (HTR) systems for Bengali remain severely
underdeveloped. The complexity of Bengali script--featuring conjuncts,
diacritics, and highly variable handwriting styles--combined with a scarcity of
annotated datasets makes this task particularly challenging. We present
GraDeT-HTR, a resource-efficient Bengali handwritten text recognition system
based on a Grapheme-aware Decoder-only Transformer architecture. To address the
unique challenges of Bengali script, we augment the performance of a
decoder-only transformer by integrating a grapheme-based tokenizer and
demonstrate that it significantly improves recognition accuracy compared to
conventional subword tokenizers. Our model is pretrained on large-scale
synthetic data and fine-tuned on real human-annotated samples, achieving
state-of-the-art performance on multiple benchmark datasets.

</details>


### [209] [GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction](https://arxiv.org/abs/2509.18090)
*Jiahe Li,Jiawei Zhang,Youmin Zhang,Xiao Bai,Jin Zheng,Xiaohan Yu,Lin Gu*

Main category: cs.CV

TL;DR: GeoSVR是一种基于稀疏体素的显式框架，用于实现精确、详细和完整的表面重建，通过体素不确定性深度约束和稀疏体素表面正则化来解决现有高斯溅射方法的表示瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于高斯溅射的主流方法受到表示瓶颈的限制，稀疏体素虽然能够保持覆盖完整性和几何清晰度，但在场景约束和局部表面细化方面存在挑战。

Method: 提出体素不确定性深度约束来最大化单目深度线索的效果，同时引入体素导向的不确定性以避免质量下降；设计稀疏体素表面正则化来增强微小体素的几何一致性，促进基于体素的锐利准确表面形成。

Result: 在多种挑战性场景下的广泛实验表明，该方法在几何精度、细节保持和重建完整性方面优于现有方法，同时保持高效率。

Conclusion: GeoSVR通过创新的体素不确定性深度约束和稀疏体素表面正则化，成功解决了稀疏体素在表面重建中的挑战，实现了卓越的重建性能。

Abstract: Reconstructing accurate surfaces with radiance fields has achieved remarkable
progress in recent years. However, prevailing approaches, primarily based on
Gaussian Splatting, are increasingly constrained by representational
bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based
framework that explores and extends the under-investigated potential of sparse
voxels for achieving accurate, detailed, and complete surface reconstruction.
As strengths, sparse voxels support preserving the coverage completeness and
geometric clarity, while corresponding challenges also arise from absent scene
constraints and locality in surface refinement. To ensure correct scene
convergence, we first propose a Voxel-Uncertainty Depth Constraint that
maximizes the effect of monocular depth cues while presenting a voxel-oriented
uncertainty to avoid quality degradation, enabling effective and robust scene
constraints yet preserving highly accurate geometries. Subsequently, Sparse
Voxel Surface Regularization is designed to enhance geometric consistency for
tiny voxels and facilitate the voxel-based formation of sharp and accurate
surfaces. Extensive experiments demonstrate our superior performance compared
to existing methods across diverse challenging scenarios, excelling in
geometric accuracy, detail preservation, and reconstruction completeness while
maintaining high efficiency. Code is available at
https://github.com/Fictionarry/GeoSVR.

</details>


### [210] [ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation](https://arxiv.org/abs/2509.18092)
*Guocheng Gordon Qian,Daniil Ostashev,Egor Nemchinov,Avihay Assouline,Sergey Tulyakov,Kuan-Chieh Jackson Wang,Kfir Aberman*

Main category: cs.CV

TL;DR: 提出了一种新的属性特定图像提示范式，通过多组参考图像分别控制人类外观的各个属性（如发型、服装、身份），实现可组合和分离的多属性控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化文本到图像合成中主要关注身份保持，但缺乏模块化能力，无法对特定视觉属性进行分离控制。

Method: 将不同参考图像编码为属性特定token，注入预训练的文本到图像扩散模型；采用跨参考训练数据集和多属性交叉参考训练策略，确保自然组合和鲁棒分离。

Result: 实验表明该方法在准确遵循视觉和文本提示方面达到最先进性能，支持单张图像中多个人的多属性控制。

Conclusion: 该框架通过视觉提示与文本驱动生成的结合，为更可配置的人类图像合成开辟了新途径。

Abstract: Generating high-fidelity images of humans with fine-grained control over
attributes such as hairstyle and clothing remains a core challenge in
personalized text-to-image synthesis. While prior methods emphasize identity
preservation from a reference image, they lack modularity and fail to provide
disentangled control over specific visual attributes. We introduce a new
paradigm for attribute-specific image prompting, in which distinct sets of
reference images are used to guide the generation of individual aspects of
human appearance, such as hair, clothing, and identity. Our method encodes
these inputs into attribute-specific tokens, which are injected into a
pre-trained text-to-image diffusion model. This enables compositional and
disentangled control over multiple visual factors, even across multiple people
within a single image. To promote natural composition and robust
disentanglement, we curate a cross-reference training dataset featuring
subjects in diverse poses and expressions, and propose a multi-attribute
cross-reference training strategy that encourages the model to generate
faithful outputs from misaligned attribute inputs while adhering to both
identity and textual conditioning. Extensive experiments show that our method
achieves state-of-the-art performance in accurately following both visual and
textual prompts. Our framework paves the way for more configurable human image
synthesis by combining visual prompting with text-driven generation. Webpage is
available at: https://snap-research.github.io/composeme/.

</details>


### [211] [UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning](https://arxiv.org/abs/2509.18094)
*Ye Liu,Zongyang Ma,Junfu Pu,Zhongang Qi,Yang Wu,Ying Shan,Chang Wen Chen*

Main category: cs.CV

TL;DR: UniPixel是一个大型多模态模型，能够灵活理解视觉提示输入并生成基于掩码的响应，将像素级感知与通用视觉理解能力无缝集成。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型主要关注整体图像和视频语言理解，而像素级细粒度理解能力的研究相对较少。之前的模型只能独立执行参考或分割任务，未能将这些细粒度感知能力整合到视觉推理中。

Method: UniPixel处理视觉提示并根据需求生成相关掩码，在推理过程中基于这些中间指针进行后续推理，从而实现细粒度的像素级推理。

Result: 该方法在10个基准测试中得到验证，涵盖像素级参考/分割和图像/视频中的对象中心理解等多种任务。还设计了新的PixelQA任务来验证方法的灵活性。

Conclusion: UniPixel成功地将像素级感知与通用视觉理解能力相结合，为细粒度像素级推理提供了有效的解决方案。

Abstract: Recent advances in Large Multi-modal Models (LMMs) have demonstrated their
remarkable success as general-purpose multi-modal assistants, with particular
focuses on holistic image- and video-language understanding. Conversely, less
attention has been given to scaling fine-grained pixel-level understanding
capabilities, where the models are expected to realize pixel-level alignment
between visual signals and language semantics. Some previous studies have
applied LMMs to related tasks such as region-level captioning and referring
expression segmentation. However, these models are limited to performing either
referring or segmentation tasks independently and fail to integrate these
fine-grained perception capabilities into visual reasoning. To bridge this gap,
we propose UniPixel, a large multi-modal model capable of flexibly
comprehending visual prompt inputs and generating mask-grounded responses. Our
model distinguishes itself by seamlessly integrating pixel-level perception
with general visual understanding capabilities. Specifically, UniPixel
processes visual prompts and generates relevant masks on demand, and performs
subsequent reasoning conditioning on these intermediate pointers during
inference, thereby enabling fine-grained pixel-level reasoning. The
effectiveness of our approach has been verified on 10 benchmarks across a
diverse set of tasks, including pixel-level referring/segmentation and
object-centric understanding in images/videos. A novel PixelQA task that
jointly requires referring, segmentation, and question answering is also
designed to verify the flexibility of our method.

</details>


### [212] [Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers](https://arxiv.org/abs/2509.18096)
*Chaehyun Kim,Heeseong Shin,Eunbeen Hong,Heeji Yoon,Anurag Arnab,Paul Hongsuck Seo,Sunghwan Hong,Seungryong Kim*

Main category: cs.CV

TL;DR: Seg4Diff是一个分析多模态扩散变换器（MM-DiT）注意力结构的框架，发现特定层能自然产生高质量语义分割掩码，并通过轻量微调提升分割和生成性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态扩散变换器通过联合自注意力实现了更丰富的跨模态对齐，但对注意力图如何具体贡献于图像生成的理解仍然有限。

Method: 引入Seg4Diff框架系统分析MM-DiT的注意力结构，识别语义接地专家层，并通过轻量微调方案增强语义分组能力。

Result: 发现语义分组是扩散变换器的涌现特性，可以通过选择性放大来提升分割性能和生成图像保真度。

Conclusion: 该研究为统一视觉感知和生成的模型铺平了道路，展示了语义分组能力可以同时提升分割和生成任务。

Abstract: Text-to-image diffusion models excel at translating language prompts into
photorealistic images by implicitly grounding textual concepts through their
cross-modal attention mechanisms. Recent multi-modal diffusion transformers
extend this by introducing joint self-attention over concatenated image and
text tokens, enabling richer and more scalable cross-modal alignment. However,
a detailed understanding of how and where these attention maps contribute to
image generation remains limited. In this paper, we introduce Seg4Diff
(Segmentation for Diffusion), a systematic framework for analyzing the
attention structures of MM-DiT, with a focus on how specific layers propagate
semantic information from text to image. Through comprehensive analysis, we
identify a semantic grounding expert layer, a specific MM-DiT block that
consistently aligns text tokens with spatially coherent image regions,
naturally producing high-quality semantic segmentation masks. We further
demonstrate that applying a lightweight fine-tuning scheme with mask-annotated
image data enhances the semantic grouping capabilities of these layers and
thereby improves both segmentation performance and generated image fidelity.
Our findings demonstrate that semantic grouping is an emergent property of
diffusion transformers and can be selectively amplified to advance both
segmentation and generation performance, paving the way for unified models that
bridge visual perception and generation.

</details>


### [213] [Preconditioned Deformation Grids](https://arxiv.org/abs/2509.18097)
*Julian Kaltheuner,Alexander Oebel,Hannah Droege,Patrick Stotko,Reinhard Klein*

Main category: cs.CV

TL;DR: 提出了一种基于预条件变形网格的新方法，直接从点云序列估计连贯的变形场，无需显式对应关系，在长序列重建中优于现有技术


<details>
  <summary>Details</summary>
Motivation: 解决现有动态表面重建方法需要多个正则化项或大量训练数据导致的精度妥协、过度平滑以及对未见物体和运动泛化能力差的问题

Method: 使用多分辨率体素网格捕捉不同空间尺度的整体运动，结合基于网格的Sobolev预条件梯度优化，应用Chamfer损失和弱等距损失确保变形精度和时间一致性

Result: 在广泛评估中，该方法在长序列重建方面取得了优于最先进技术的结果

Conclusion: Preconditioned Deformation Grids方法能够直接从非结构化点云序列获得准确的变形，无需显式对应关系，在动态表面重建领域具有显著优势

Abstract: Dynamic surface reconstruction of objects from point cloud sequences is a
challenging field in computer graphics. Existing approaches either require
multiple regularization terms or extensive training data which, however, lead
to compromises in reconstruction accuracy as well as over-smoothing or poor
generalization to unseen objects and motions. To address these lim- itations,
we introduce Preconditioned Deformation Grids, a novel technique for estimating
coherent deformation fields directly from unstructured point cloud sequences
without requiring or forming explicit correspondences. Key to our approach is
the use of multi-resolution voxel grids that capture the overall motion at
varying spatial scales, enabling a more flexible deformation representation. In
conjunction with incorporating grid-based Sobolev preconditioning into
gradient-based optimization, we show that applying a Chamfer loss between the
input point clouds as well as to an evolving template mesh is sufficient to
obtain accurate deformations. To ensure temporal consistency along the object
surface, we include a weak isometry loss on mesh edges which complements the
main objective without constraining deformation fidelity. Extensive evaluations
demonstrate that our method achieves superior results, particularly for long
sequences, compared to state-of-the-art techniques.

</details>
