<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 46]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Egocentric Bias in Vision-Language Models](https://arxiv.org/abs/2602.15892)
*Maijunxian Wang,Yijiang Li,Bingyang Wang,Tianwei Zhao,Ran Ji,Qingying Gao,Emmy Liu,Hokin Deng,Dezhi Luo*

Main category: cs.CV

TL;DR: FlipSet基准测试揭示视觉语言模型在二级视觉观点采择中存在系统性自我中心偏差，大多数模型表现低于随机水平，无法正确模拟他人视角的空间转换。


<details>
  <summary>Details</summary>
Motivation: 视觉观点采择是社会认知的基础能力，但当前视觉语言模型在这方面的能力尚未得到系统评估。研究者希望开发一个诊断性基准来测试模型从他人视角理解世界的能力。

Method: 引入FlipSet基准，要求模型模拟2D字符字符串的180度旋转（从另一个智能体视角），将空间转换与3D场景复杂性分离。评估了103个视觉语言模型，并进行控制实验分析组合能力缺陷。

Result: 发现系统性自我中心偏差：绝大多数模型表现低于随机水平，约四分之三的错误再现了相机视角。模型在单独的心智理论任务和心理旋转任务中表现良好，但需要整合时却完全失败，显示出组合能力缺陷。

Conclusion: 当前视觉语言模型缺乏将社会意识与空间操作结合的机制，在基于模型的空间推理方面存在根本性限制。FlipSet为诊断多模态系统中的观点采择能力提供了认知基础测试平台。

Abstract: Visual perspective taking--inferring how the world appears from another's viewpoint--is foundational to social cognition. We introduce FlipSet, a diagnostic benchmark for Level-2 visual perspective taking (L2 VPT) in vision-language models. The task requires simulating 180-degree rotations of 2D character strings from another agent's perspective, isolating spatial transformation from 3D scene complexity. Evaluating 103 VLMs reveals systematic egocentric bias: the vast majority perform below chance, with roughly three-quarters of errors reproducing the camera viewpoint. Control experiments expose a compositional deficit--models achieve high theory-of-mind accuracy and above-chance mental rotation in isolation, yet fail catastrophically when integration is required. This dissociation indicates that current VLMs lack the mechanisms needed to bind social awareness to spatial operations, suggesting fundamental limitations in model-based spatial reasoning. FlipSet provides a cognitively grounded testbed for diagnosing perspective-taking capabilities in multimodal systems.

</details>


### [2] [Detecting Deepfakes with Multivariate Soft Blending and CLIP-based Image-Text Alignment](https://arxiv.org/abs/2602.15903)
*Jingwei Li,Jiaxin Tong,Pengfei Wu*

Main category: cs.CV

TL;DR: 提出MSBA-CLIP框架，通过CLIP引导的伪造强度估计和多变量软混合增强，提升深度伪造检测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法面临准确率有限和泛化能力差的问题，主要原因是不同伪造技术生成的样本存在显著分布偏移。

Method: 提出MSBA-CLIP框架：1) 利用CLIP的多模态对齐能力捕捉细微伪造痕迹；2) 多变量软混合增强(MSBA)策略，通过随机权重混合多种伪造方法生成的图像；3) 多变量伪造强度估计(MFIE)模块，显式引导模型学习不同伪造模式和强度的特征。

Result: 在域内测试中，准确率和AUC分别比最佳基线提升3.32%和4.02%；在跨五个数据集的跨域评估中，平均AUC增益达3.27%；消融研究证实了两个组件的有效性。

Conclusion: 虽然依赖大型视觉语言模型带来较高计算成本，但该工作为实现更泛化和鲁棒的深度伪造检测迈出了重要一步。

Abstract: The proliferation of highly realistic facial forgeries necessitates robust detection methods. However, existing approaches often suffer from limited accuracy and poor generalization due to significant distribution shifts among samples generated by diverse forgery techniques. To address these challenges, we propose a novel Multivariate and Soft Blending Augmentation with CLIP-guided Forgery Intensity Estimation (MSBA-CLIP) framework. Our method leverages the multimodal alignment capabilities of CLIP to capture subtle forgery traces. We introduce a Multivariate and Soft Blending Augmentation (MSBA) strategy that synthesizes images by blending forgeries from multiple methods with random weights, forcing the model to learn generalizable patterns. Furthermore, a dedicated Multivariate Forgery Intensity Estimation (MFIE) module is designed to explicitly guide the model in learning features related to varied forgery modes and intensities. Extensive experiments demonstrate state-of-the-art performance. On in-domain tests, our method improves Accuracy and AUC by 3.32\% and 4.02\%, respectively, over the best baseline. In cross-domain evaluations across five datasets, it achieves an average AUC gain of 3.27\%. Ablation studies confirm the efficacy of both proposed components. While the reliance on a large vision-language model entails higher computational cost, our work presents a significant step towards more generalizable and robust deepfake detection.

</details>


### [3] [A Comprehensive Survey on Deep Learning-Based LiDAR Super-Resolution for Autonomous Driving](https://arxiv.org/abs/2602.15904)
*June Moh Goo,Zichao Zeng,Jan Boehm*

Main category: cs.CV

TL;DR: 本文首次全面综述了自动驾驶中的LiDAR超分辨率方法，将现有方法分为四类：基于CNN的架构、基于模型的深度展开、隐式表示方法以及Transformer和Mamba方法，并探讨了当前趋势和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 高分辨率LiDAR传感器昂贵，而低成本低分辨率传感器产生的稀疏点云会丢失关键细节。LiDAR超分辨率通过深度学习增强稀疏点云，弥合不同传感器类型之间的差距，实现实际部署中的跨传感器兼容性。

Method: 将现有方法系统性地分为四类：1) CNN-based architectures, 2) model-based deep unrolling, 3) implicit representation methods, 4) Transformer and Mamba-based approaches。建立了包括数据表示、问题公式化、基准数据集和评估指标在内的基本概念。

Result: 当前趋势包括：采用距离图像表示以提高处理效率、极端模型压缩、开发分辨率灵活的架构。最新研究优先考虑实时推理和跨传感器泛化以实现实际部署。

Conclusion: 本文首次对自动驾驶LiDAR超分辨率方法进行了全面综述，系统性地组织了现有方法，并指出了开放挑战和未来研究方向，包括进一步提升实时性能、跨传感器泛化能力以及开发更灵活的架构。

Abstract: LiDAR sensors are often considered essential for autonomous driving, but high-resolution sensors remain expensive while affordable low-resolution sensors produce sparse point clouds that miss critical details. LiDAR super-resolution addresses this challenge by using deep learning to enhance sparse point clouds, bridging the gap between different sensor types and enabling cross-sensor compatibility in real-world deployments. This paper presents the first comprehensive survey of LiDAR super-resolution methods for autonomous driving. Despite the importance of practical deployment, no systematic review has been conducted until now. We organize existing approaches into four categories: CNN-based architectures, model-based deep unrolling, implicit representation methods, and Transformer and Mamba-based approaches. We establish fundamental concepts including data representations, problem formulation, benchmark datasets and evaluation metrics. Current trends include the adoption of range image representation for efficient processing, extreme model compression and the development of resolution-flexible architectures. Recent research prioritizes real-time inference and cross-sensor generalization for practical deployment. We conclude by identifying open challenges and future research directions for advancing LiDAR super-resolution technology.

</details>


### [4] [MaS-VQA: A Mask-and-Select Framework for Knowledge-Based Visual Question Answering](https://arxiv.org/abs/2602.15915)
*Xianwei Mao,Kai Ye,Sheng Zhou,Nan Zhang,Haikuan Huang,Bin Li,Jiajun Bu*

Main category: cs.CV

TL;DR: 提出MaS-VQA框架，通过掩码选择机制联合过滤无关图像区域和弱相关知识片段，实现显式和隐式知识的互补建模，提升KB-VQA性能


<details>
  <summary>Details</summary>
Motivation: KB-VQA中检索的知识常存在噪声、部分不相关或与视觉内容不对齐，而内部模型知识难以控制和解释，简单的知识聚合限制了推理效果和答案准确性

Method: 提出MaS-VQA框架：1) 检索候选段落；2) 应用掩码选择机制联合修剪无关图像区域和弱相关知识片段，生成紧凑的高信号多模态知识；3) 过滤后的知识在受限语义空间中指导内部知识激活，实现显式和隐式知识的互补建模

Result: 在Encyclopedic-VQA和InfoSeek数据集上，多个MLLM骨干网络均获得一致的性能提升，消融实验验证选择机制能有效减少噪声并增强知识利用

Conclusion: MaS-VQA通过紧密耦合显式知识过滤和隐式知识推理，解决了KB-VQA中的知识噪声和对齐问题，提高了知识利用效率和答案准确性

Abstract: Knowledge-based Visual Question Answering (KB-VQA) requires models to answer questions by integrating visual information with external knowledge. However, retrieved knowledge is often noisy, partially irrelevant, or misaligned with the visual content, while internal model knowledge is difficult to control and interpret. Naive aggregation of these sources limits reasoning effectiveness and reduces answer accuracy. To address this, we propose MaS-VQA, a selection-driven framework that tightly couples explicit knowledge filtering with implicit knowledge reasoning. MaS-VQA first retrieves candidate passages and applies a Mask-and-Select mechanism to jointly prune irrelevant image regions and weakly relevant knowledge fragments, producing compact, high-signal multimodal knowledge . This filtered knowledge then guides the activation of internal knowledge in a constrained semantic space, enabling complementary co-modeling of explicit and implicit knowledge for robust answer prediction. Experiments on Encyclopedic-VQA and InfoSeek demonstrate consistent performance gains across multiple MLLM backbones, and ablations verify that the selection mechanism effectively reduces noise and enhances knowledge utilization.

</details>


### [5] [EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery](https://arxiv.org/abs/2602.15918)
*Zelin Xu,Yupu Zhang,Saugat Adhikari,Saiful Islam,Tingsong Xiao,Zibo Liu,Shigang Chen,Da Yan,Zhe Jiang*

Main category: cs.CV

TL;DR: EarthSpatialBench是一个用于评估多模态大语言模型在地球影像上空间推理能力的综合基准，包含超过32.5万个问答对，涵盖距离方向推理、拓扑关系、复杂几何对象等维度。


<details>
  <summary>Details</summary>
Motivation: 现有地球影像基准主要关注2D空间定位、图像描述和粗略空间关系，缺乏对定量方向距离推理、系统拓扑关系和复杂几何对象（如多边形）的支持，而这对具身AI等需要精确物理世界交互的系统至关重要。

Method: 提出EarthSpatialBench基准，包含超过325K个问答对，涵盖：1）空间距离和方向的定性与定量推理；2）系统拓扑关系；3）单对象、对象对和组合聚合组查询；4）通过文本描述、视觉覆盖和显式几何坐标（包括2D边界框、折线和多边形）的对象引用。

Result: 通过对开源和专有模型进行广泛实验，识别了MLLMs在空间推理方面的局限性。

Conclusion: EarthSpatialBench填补了地球影像空间推理评估的空白，为评估MLLMs在精确物理世界交互能力方面提供了全面基准，有助于推动具身AI等系统的发展。

Abstract: Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagged behind, as it uniquely involves grounding objects in georeferenced images and quantitatively reasoning about distances, directions, and topological relations using both visual cues and vector geometry coordinates (e.g., 2D bounding boxes, polylines, and polygons). Existing benchmarks for Earth imagery primarily focus on 2D spatial grounding, image captioning, and coarse spatial relations (e.g., simple directional or proximity cues). They lack support for quantitative direction and distance reasoning, systematic topological relations, and complex object geometries beyond bounding boxes. To fill this gap, we propose \textbf{EarthSpatialBench}, a comprehensive benchmark for evaluating spatial reasoning in MLLMs on Earth imagery. The benchmark contains over 325K question-answer pairs spanning: (1) qualitative and quantitative reasoning about spatial distance and direction; (2) systematic topological relations; (3) single-object queries, object-pair queries, and compositional aggregate group queries; and (4) object references expressed via textual descriptions, visual overlays, and explicit geometry coordinates, including 2D bounding boxes, polylines, and polygons. We conducted extensive experiments on both open-source and proprietary models to identify limitations in the spatial reasoning of MLLMs.

</details>


### [6] [A Study on Real-time Object Detection using Deep Learning](https://arxiv.org/abs/2602.15926)
*Ankita Bose,Jayasravani Bhumireddy,Naveen N*

Main category: cs.CV

TL;DR: 这篇论文综述了深度学习在实时目标检测中的应用，详细介绍了主流算法（如Faster R-CNN、YOLO、SSD等）、基准数据集、应用场景，并通过对比研究提供了分析结果和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 目标检测在多个领域（人机交互、安防监控、自动驾驶、医疗健康、AR/VR等）具有重要应用价值，实时目标检测能够提供动态视觉信息分析以支持即时决策。深度学习算法的发展为目标检测提供了更准确高效的解决方案，需要系统性地综述这些进展。

Method: 采用文献综述方法，详细分析深度学习算法在实时目标检测中的应用，包括：1）介绍主流目标检测模型（Faster R-CNN、Mask R-CNN、Cascade R-CNN、YOLO、SSD、RetinaNet等）；2）整理开放基准数据集；3）研究不同应用场景中的使用情况；4）通过控制性研究对比各种策略。

Result: 论文提供了深度学习算法在实时目标检测中的系统性综述，包括不同模型的性能对比分析、在各类应用中的实际效果评估，以及通过控制性研究得出的有启发性的发现。

Conclusion: 深度学习显著提升了实时目标检测的准确性和效率，但仍面临挑战。论文最后提出了多个有前景的研究方向和挑战，建议在相关深度学习方法和目标识别技术方面进行进一步探索。

Abstract: Object detection has compelling applications over a range of domains, including human-computer interfaces, security and video surveillance, navigation and road traffic monitoring, transportation systems, industrial automation healthcare, the world of Augmented Reality (AR) and Virtual Reality (VR), environment monitoring and activity identification. Applications of real time object detection in all these areas provide dynamic analysis of the visual information that helps in immediate decision making. Furthermore, advanced deep learning algorithms leverage the progress in the field of object detection providing more accurate and efficient solutions. There are some outstanding deep learning algorithms for object detection which includes, Faster R CNN(Region-based Convolutional Neural Network),Mask R-CNN, Cascade R-CNN, YOLO (You Only Look Once), SSD (Single Shot Multibox Detector), RetinaNet etc. This article goes into great detail on how deep learning algorithms are used to enhance real time object recognition. It provides information on the different object detection models available, open benchmark datasets, and studies on the use of object detection models in a range of applications. Additionally, controlled studies are provided to compare various strategies and produce some illuminating findings. Last but not least, a number of encouraging challenges and approaches are offered as suggestions for further investigation in both relevant deep learning approaches and object recognition.

</details>


### [7] [Visual Memory Injection Attacks for Multi-Turn Conversations](https://arxiv.org/abs/2602.15927)
*Christian Schlarmann,Matthias Hein*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的隐蔽视觉记忆注入（VMI）攻击，针对生成式大型视觉语言模型（LVLM），能够在多轮对话中实现长期记忆注入，使模型在触发提示下输出特定目标信息，用于对抗性营销或政治说服。


<details>
  <summary>Details</summary>
Motivation: 生成式大型视觉语言模型（LVLM）性能显著提升且用户快速增长，但其在多轮长上下文环境下的安全性研究不足。攻击者可通过上传篡改图像到网络/社交媒体，当良性用户下载并使用该图像作为LVLM输入时，可能被操纵。

Method: 提出隐蔽视觉记忆注入（VMI）攻击方法：攻击者上传经过处理的图像，正常提示下LVLM表现正常，但一旦用户给出触发提示，LVLM会输出预设的目标信息。相比之前单轮攻击，VMI在多轮长对话后仍有效。

Result: 在多个最新的开源权重LVLM上成功演示了VMI攻击，证明通过扰动图像在多轮对话环境中大规模操纵用户是可行的。

Conclusion: 研究表明LVLM在多轮对话环境中容易受到视觉记忆注入攻击，需要提高模型对这些攻击的鲁棒性。作者已开源代码。

Abstract: Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection

</details>


### [8] [Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families](https://arxiv.org/abs/2602.15950)
*Yuval Levental*

Main category: cs.CV

TL;DR: 视觉语言模型在识别二进制网格中无文本标识的填充单元格时存在根本性缺陷，当单元格缺乏文本符号时，模型的空间定位能力显著下降。


<details>
  <summary>Details</summary>
Motivation: 揭示视觉语言模型在空间推理中的一个基本限制：当视觉元素缺乏文本标识时，模型无法准确识别和定位二进制网格中的填充单元格。

Method: 生成15个15x15的二进制网格，填充密度从10.7%到41.8%，以两种图像类型呈现：文本符号（.和#）和无网格线的填充方块。使用三个前沿VLM模型（Claude Opus、ChatGPT 5.2、Gemini 3 Thinking）进行转录测试。

Result: 在文本符号条件下，Claude和ChatGPT达到约91%单元格准确率和84% F1分数，Gemini为84%准确率和63% F1。在填充方块条件下，所有模型准确率降至60-73%，F1分数降至29-39%。文本与方块之间的F1差距达到34-54个百分点。

Conclusion: 视觉语言模型在处理非文本视觉元素时存在严重空间定位缺陷，表明它们依赖高保真的文本识别路径进行空间推理，而原生视觉路径性能显著较差。

Abstract: We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text-symbol condition, Claude and ChatGPT achieve approximately 91% cell accuracy and 84% F1, while Gemini achieves 84% accuracy and 63% F1. In the filled-squares condition, all three models collapse to 60-73% accuracy and 29-39% F1. Critically, all conditions pass through the same visual encoder -- the text symbols are images, not tokenized text. The text-vs-squares F1 gap ranges from 34 to 54 points across models, demonstrating that VLMs behave as if they possess a high-fidelity text-recognition pathway for spatial reasoning that dramatically outperforms their native visual pathway. Each model exhibits a distinct failure mode in the squares condition -- systematic under-counting (Claude), massive over-counting (ChatGPT), and template hallucination (Gemini) -- but all share the same underlying deficit: severely degraded spatial localization for non-textual visual elements.

</details>


### [9] [Position-Aware Scene-Appearance Disentanglement for Bidirectional Photoacoustic Microscopy Registration](https://arxiv.org/abs/2602.15959)
*Yiwen Wang,Jiahao Qin*

Main category: cs.CV

TL;DR: GPEReg-Net：一种用于高速光学分辨率光声显微镜的双向扫描图像配准新方法，通过场景-外观解耦和全局位置编码实现高质量配准


<details>
  <summary>Details</summary>
Motivation: 高速OR-PAM双向扫描虽然提高了成像速度，但引入了域偏移和几何错位问题。现有配准方法受限于亮度恒定假设，配准质量有限；而生成方法虽然处理域偏移但缺乏跨帧的时间感知能力。

Method: 提出GPEReg-Net框架：1）使用自适应实例归一化（AdaIN）将域不变场景特征与域特定外观代码解耦，实现无需显式变形场估计的直接图像配准；2）引入全局位置编码（GPE）模块，结合可学习位置嵌入、正弦编码和跨帧注意力，利用相邻帧的上下文信息提升时间一致性。

Result: 在OR-PAM-Reg-4K基准测试（432个样本）上，GPEReg-Net达到NCC 0.953、SSIM 0.932、PSNR 34.49dB，在SSIM上超越SOTA 3.8%，PSNR提升1.99dB，同时保持有竞争力的NCC。

Conclusion: GPEReg-Net通过场景-外观解耦和时间感知的全局位置编码，有效解决了高速OR-PAM双向扫描中的域偏移和几何错位问题，显著提升了配准质量，为高速生物医学成像提供了有力工具。

Abstract: High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional raster scanning doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing registration methods, constrained by brightness constancy assumptions, achieve limited alignment quality, while recent generative approaches address domain shift through complex architectures that lack temporal awareness across frames. We propose GPEReg-Net, a scene-appearance disentanglement framework that separates domain-invariant scene features from domain-specific appearance codes via Adaptive Instance Normalization (AdaIN), enabling direct image-to-image registration without explicit deformation field estimation. To exploit temporal structure in sequential acquisitions, we introduce a Global Position Encoding (GPE) module that combines learnable position embeddings with sinusoidal encoding and cross-frame attention, allowing the network to leverage context from neighboring frames for improved temporal coherence. On the OR-PAM-Reg-4K benchmark (432 test samples), GPEReg-Net achieves NCC of 0.953, SSIM of 0.932, and PSNR of 34.49dB, surpassing the state-of-the-art by 3.8% in SSIM and 1.99dB in PSNR while maintaining competitive NCC. Code is available at https://github.com/JiahaoQin/GPEReg-Net.

</details>


### [10] [Automated Re-Identification of Holstein-Friesian Cattle in Dense Crowds](https://arxiv.org/abs/2602.15962)
*Phoenix Yu,Tilo Burghardt,Andrew W Dowsey,Neill W Campbell*

Main category: cs.CV

TL;DR: 提出新的检测-分割-识别管道，利用开放词汇无权重定位和Segment Anything模型，解决奶牛密集群聚时的检测失败问题，在真实农场数据上达到98.93%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法（包括基于YOLO的物种检测）在奶牛密集群聚时会失效，特别是对于具有轮廓破坏性毛色图案的物种。需要提升在这种密集场景下的检测效果和可迁移性。

Method: 提出新的检测-分割-识别管道，结合开放词汇无权重定位（Open-Vocabulary Weight-free Localisation）和Segment Anything模型作为预处理阶段，再配合Re-ID网络进行个体识别。

Result: 在真实奶牛场9天CCTV数据上，方法克服了密集动物群聚的检测失败，达到98.93%准确率，比现有定向边界框方法提升47.52%，比SAM物种检测基线提升27.13%。无监督对比学习可达到94.82%的Re-ID准确率。

Conclusion: 在拥挤场景下的Re-ID在真实农场环境中既实用又可靠，无需人工干预。提供了代码和数据集以确保可复现性。

Abstract: Holstein-Friesian detection and re-identification (Re-ID) methods capture individuals well when targets are spatially separate. However, existing approaches, including YOLO-based species detection, break down when cows group closely together. This is particularly prevalent for species which have outline-breaking coat patterns. To boost both effectiveness and transferability in this setting, we propose a new detect-segment-identify pipeline that leverages the Open-Vocabulary Weight-free Localisation and the Segment Anything models as pre-processing stages alongside Re-ID networks. To evaluate our approach, we publish a collection of nine days CCTV data filmed on a working dairy farm. Our methodology overcomes detection breakdown in dense animal groupings, resulting in a 98.93% accuracy. This significantly outperforms current oriented bounding box-driven, as well as SAM species detection baselines with accuracy improvements of 47.52% and 27.13%, respectively. We show that unsupervised contrastive learning can build on this to yield 94.82% Re-ID accuracy on our test data. Our work demonstrates that Re-ID in crowded scenarios is both practical as well as reliable in working farm settings with no manual intervention. Code and dataset are provided for reproducibility.

</details>


### [11] [Non-Contact Physiological Monitoring in Pediatric Intensive Care Units via Adaptive Masking and Self-Supervised Learning](https://arxiv.org/abs/2602.15967)
*Mohamed Khalil Ben Salah,Philippe Jouvet,Rita Noumeir*

Main category: cs.CV

TL;DR: 提出基于VisionMamba的自监督预训练框架，通过渐进式课程策略和师生蒸馏，在PICU环境中实现无接触心率监测，相比现有方法误差降低42%


<details>
  <summary>Details</summary>
Motivation: PICU中传统接触式传感器存在皮肤刺激、感染风险和患者不适等问题，而现有的远程光电容积描记技术(rPPG)在临床环境中面临运动伪影、遮挡、光照变化和领域偏移等挑战，缺乏标记临床数据限制了其应用

Method: 基于VisionMamba架构的自监督预训练框架，采用渐进式课程策略：1)自适应掩码机制，使用轻量级Mamba控制器分配时空重要性分数指导概率性补丁采样；2)师生蒸馏设置，利用公开数据集训练的监督专家模型提供潜在生理指导；3)三阶段课程：干净公开视频、合成遮挡场景、500名儿科患者的未标记视频

Result: 相比标准掩码自编码器平均绝对误差降低42%，优于PhysFormer 31%，最终MAE达到3.2 bpm。无需显式ROI提取，模型能持续关注脉搏丰富区域，在临床遮挡和噪声下表现出鲁棒性

Conclusion: 提出的自监督预训练框架有效解决了PICU环境中rPPG监测的挑战，通过渐进式课程策略和师生蒸馏，在缺乏标记临床数据的情况下实现了准确、鲁棒的无接触心率监测

Abstract: Continuous monitoring of vital signs in Pediatric Intensive Care Units (PICUs) is essential for early detection of clinical deterioration and effective clinical decision-making. However, contact-based sensors such as pulse oximeters may cause skin irritation, increase infection risk, and lead to patient discomfort. Remote photoplethysmography (rPPG) offers a contactless alternative to monitor heart rate using facial video, but remains underutilized in PICUs due to motion artifacts, occlusions, variable lighting, and domain shifts between laboratory and clinical data.
  We introduce a self-supervised pretraining framework for rPPG estimation in the PICU setting, based on a progressive curriculum strategy. The approach leverages the VisionMamba architecture and integrates an adaptive masking mechanism, where a lightweight Mamba-based controller assigns spatiotemporal importance scores to guide probabilistic patch sampling. This strategy dynamically increases reconstruction difficulty while preserving physiological relevance.
  To address the lack of labeled clinical data, we adopt a teacher-student distillation setup. A supervised expert model, trained on public datasets, provides latent physiological guidance to the student. The curriculum progresses through three stages: clean public videos, synthetic occlusion scenarios, and unlabeled videos from 500 pediatric patients.
  Our framework achieves a 42% reduction in mean absolute error relative to standard masked autoencoders and outperforms PhysFormer by 31%, reaching a final MAE of 3.2 bpm. Without explicit region-of-interest extraction, the model consistently attends to pulse-rich areas and demonstrates robustness under clinical occlusions and noise.

</details>


### [12] [LAND: A Longitudinal Analysis of Neuromorphic Datasets](https://arxiv.org/abs/2602.15973)
*Gregory Cohen,Alexandre Marcireau*

Main category: cs.CV

TL;DR: 本文回顾了423个神经形态数据集，分析了当前数据集存在的问题：缺乏标准化、访问困难、规模过大，以及合成数据集带来的潜在偏见风险。


<details>
  <summary>Details</summary>
Motivation: 神经形态工程面临数据问题：尽管过去十年数据集数量激增，但许多研究仍呼吁需要更多更大规模的数据集。现有数据集存在查找困难、目的不明确、任务定义模糊、下载使用不便等问题，同时合成数据集的兴起带来了新的挑战。

Method: 本文首先收集了423个现有神经形态数据集的快照，然后分析这些数据集的任务性质和数据结构。通过分析数据集规模、标准化程度、访问难度等维度，探讨合成数据集（通过模拟或视频转事件方法创建）的优缺点，并引入元数据集概念。

Result: 分析显示当前神经形态数据集面临多重困难：1）数据集规模过大；2）缺乏标准化；3）实际数据访问困难；4）单个数据集规模不断增长；5）合成数据集（模拟或视频转事件生成）的兴起可能对探索神经形态技术新应用带来潜在偏见。

Conclusion: 本文提出元数据集概念作为解决方案，通过利用现有数据集构建元数据集，既能减少对新数据的需求，又能消除因同时定义数据集和任务而产生的潜在偏见。这为解决神经形态工程的数据问题提供了新思路。

Abstract: Neuromorphic engineering has a data problem. Despite the meteoric rise in the number of neuromorphic datasets published over the past ten years, the conclusion of a significant portion of neuromorphic research papers still states that there is a need for yet more data and even larger datasets. Whilst this need is driven in part by the sheer volume of data required by modern deep learning approaches, it is also fuelled by the current state of the available neuromorphic datasets and the difficulties in finding them, understanding their purpose, and determining the nature of their underlying task. This is further compounded by practical difficulties in downloading and using these datasets. This review starts by capturing a snapshot of the existing neuromorphic datasets, covering over 423 datasets, and then explores the nature of their tasks and the underlying structure of the presented data. Analysing these datasets shows the difficulties arising from their size, the lack of standardisation, and difficulties in accessing the actual data. This paper also highlights the growth in the size of individual datasets and the complexities involved in working with the data. However, a more important concern is the rise of synthetic datasets, created by either simulation or video-to-events methods. This review explores the benefits of simulated data for testing existing algorithms and applications, highlighting the potential pitfalls for exploring new applications of neuromorphic technologies. This review also introduces the concepts of meta-datasets, created from existing datasets, as a way of both reducing the need for more data, and to remove potential bias arising from defining both the dataset and the task.

</details>


### [13] [SAM 3D Body: Robust Full-Body Human Mesh Recovery](https://arxiv.org/abs/2602.15989)
*Xitong Yang,Devansh Kukreja,Don Pinkus,Anushka Sagar,Taosha Fan,Jinhyung Park,Soyong Shin,Jinkun Cao,Jiawei Liu,Nicolas Ugrinovic,Matt Feiszli,Jitendra Malik,Piotr Dollar,Kris Kitani*

Main category: cs.CV

TL;DR: SAM 3D Body (3DB) 是一个基于提示的单图像全身3D人体网格恢复模型，采用新的参数化网格表示MHR，在多样化的真实场景中表现出最先进的性能和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D人体网格恢复模型在多样化真实场景中的泛化问题，提供更准确、一致的全身姿态估计（包括身体、脚部和手部），并支持用户引导的推理。

Method: 采用编码器-解码器架构，引入新的参数化网格表示Momentum Human Rig (MHR)来解耦骨骼结构和表面形状，支持2D关键点和掩码等辅助提示，通过多阶段标注流程获取高质量训练数据。

Result: 在传统定量分析和定性用户偏好研究中都优于现有方法，表现出卓越的泛化能力，特别是在罕见姿态和成像条件下的表现突出。

Conclusion: SAM 3D Body (3DB) 和 MHR 表示法为3D人体网格恢复提供了新的解决方案，具有优异的性能和泛化能力，两者均已开源。

Abstract: We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation, Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling user-guided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization, multi-view geometry, and dense keypoint detection. Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis. Both 3DB and MHR are open-source.

</details>


### [14] [BTReport: A Framework for Brain Tumor Radiology Report Generation with Clinically Relevant Features](https://arxiv.org/abs/2602.16006)
*Juampablo E. Heras Rivera,Dickson T. Chen,Tianyi Ren,Daniel K. Low,Asma Ben Abacha,Alberto Santamaria-Pang,Mehmet Kurt*

Main category: cs.CV

TL;DR: BTReport是一个用于脑肿瘤放射学报告生成的开源框架，它通过确定性特征提取和LLM结构化生成可解释的报告，并发布了BTReport-BraTS数据集。


<details>
  <summary>Details</summary>
Motivation: 神经肿瘤学领域缺乏开放的配对图像-报告数据集，限制了放射学报告生成的发展。现有方法依赖大型视觉语言模型，容易产生幻觉且缺乏可解释性。

Method: 将报告生成分为两个步骤：1）确定性特征提取进行图像分析；2）使用大型语言模型仅进行句法结构和叙述格式化。不同于现有方法同时处理图像解释和报告撰写。

Result: 生成的特征可预测关键临床结果（生存期和IDH突变状态），生成的报告比现有基线更接近参考临床报告。框架完全可解释且减少幻觉。

Conclusion: BTReport通过分离特征提取和报告生成，提供了可解释、可靠的脑肿瘤放射学报告生成方案，并发布了BTReport-BraTS数据集促进该领域发展。

Abstract: Recent advances in radiology report generation (RRG) have been driven by large paired image-text datasets; however, progress in neuro-oncology has been limited due to a lack of open paired image-report datasets. Here, we introduce BTReport, an open-source framework for brain tumor RRG that constructs natural language radiology reports using deterministically extracted imaging features. Unlike existing approaches that rely on large general-purpose or fine-tuned vision-language models for both image interpretation and report composition, BTReport performs deterministic feature extraction for image analysis and uses large language models only for syntactic structuring and narrative formatting. By separating RRG into a deterministic feature extraction step and a report generation step, the generated reports are completely interpretable and less prone to hallucinations. We show that the features used for report generation are predictive of key clinical outcomes, including survival and IDH mutation status, and reports generated by BTReport are more closely aligned with reference clinical reports than existing baselines for RRG. Finally, we introduce BTReport-BraTS, a companion dataset that augments BraTS imaging with synthetically generated radiology reports produced with BTReport. Code for this project can be found at  https://github.com/KurtLabUW/BTReport.

</details>


### [15] [MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval](https://arxiv.org/abs/2602.16019)
*Ahmad Elallaf,Yu Zhang,Yuktha Priya Masupalli,Jeong Yang,Young Lee,Zechun Cao,Gongbo Liang*

Main category: cs.CV

TL;DR: MedProbCLIP：用于胸部X光和放射学报告的贝叶斯视觉语言学习框架，通过高斯嵌入建模不确定性，在检索和分类任务中优于现有方法，并提高了可信度和安全性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言基础模型在生物医学等高风险应用中缺乏可靠性，确定性嵌入无法提供所需的不确定性建模，需要更可信的放射学图像-文本检索系统。

Method: 提出MedProbCLIP框架，将图像和文本表示为高斯嵌入，使用概率对比目标捕获不确定性和多对多对应关系；通过变分信息瓶颈防止过度自信预测；训练时使用多视图X光编码和多部分报告编码提供细粒度监督，推理时只需单张X光和单份报告。

Result: 在MIMIC-CXR数据集上，MedProbCLIP在检索和零样本分类任务中优于CLIP、CXR-CLIP、PCME++等确定性和概率基线方法，表现出更好的校准性、风险覆盖行为、选择性检索可靠性和对临床相关损坏的鲁棒性。

Conclusion: 概率视觉语言建模能显著提高放射学图像-文本检索系统的可信度和安全性，MedProbCLIP框架为高风险生物医学应用提供了更可靠的解决方案。

Abstract: Vision-language foundation models have emerged as powerful general-purpose representation learners with strong potential for multimodal understanding, but their deterministic embeddings often fail to provide the reliability required for high-stakes biomedical applications. This work introduces MedProbCLIP, a probabilistic vision-language learning framework for chest X-ray and radiology report representation learning and bidirectional retrieval. MedProbCLIP models image and text representations as Gaussian embeddings through a probabilistic contrastive objective that explicitly captures uncertainty and many-to-many correspondences between radiographs and clinical narratives. A variational information bottleneck mitigates overconfident predictions, while MedProbCLIP employs multi-view radiograph encoding and multi-section report encoding during training to provide fine-grained supervision for clinically aligned correspondence, yet requires only a single radiograph and a single report at inference. Evaluated on the MIMIC-CXR dataset, MedProbCLIP outperforms deterministic and probabilistic baselines, including CLIP, CXR-CLIP, and PCME++, in both retrieval and zero-shot classification. Beyond accuracy, MedProbCLIP demonstrates superior calibration, risk-coverage behavior, selective retrieval reliability, and robustness to clinically relevant corruptions, underscoring the value of probabilistic vision-language modeling for improving the trustworthiness and safety of radiology image-text retrieval systems.

</details>


### [16] [LGQ: Learning Discretization Geometry for Scalable and Stable Image Tokenization](https://arxiv.org/abs/2602.16086)
*Idil Bilge Altun,Mert Onur Cakiroglu,Elham Buxton,Mehmet Dalkilic,Hasan Kurban*

Main category: cs.CV

TL;DR: LGQ是一种可学习几何量化方法，通过温度控制软分配实现端到端可微训练，解决了现有量化器在几何灵活性与代码利用率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有离散图像标记化方法面临权衡：向量量化器几何灵活但优化有偏、代码利用率低；结构化标量量化器利用率稳定但几何固定、容量分配效率低。需要一种能学习离散化几何的端到端方法。

Method: LGQ用温度控制软分配替代硬最近邻查找，实现完全可微训练；使用各向同性高斯混合后验责任，最小化变分自由能目标；结合标记级峰值正则化和全局使用正则化，鼓励自信且平衡的代码利用。

Result: 在ImageNet上，16K码本大小下，LGQ比FSQ提升rFID 11.88%且减少49.96%活跃代码；比SimVQ提升rFID 6.06%且降低49.45%有效表示率，用更少活跃条目实现相当保真度。

Conclusion: LGQ通过可学习几何量化有效解决了现有量化器的权衡问题，实现了稳定优化和平衡利用，在保持语义结构的同时提高了离散容量使用效率。

Abstract: Discrete image tokenization is a key bottleneck for scalable visual generation: a tokenizer must remain compact for efficient latent-space priors while preserving semantic structure and using discrete capacity effectively. Existing quantizers face a trade-off: vector-quantized tokenizers learn flexible geometries but often suffer from biased straight-through optimization, codebook under-utilization, and representation collapse at large vocabularies. Structured scalar or implicit tokenizers ensure stable, near-complete utilization by design, yet rely on fixed discretization geometries that may allocate capacity inefficiently under heterogeneous latent statistics.
  We introduce Learnable Geometric Quantization (LGQ), a discrete image tokenizer that learns discretization geometry end-to-end. LGQ replaces hard nearest-neighbor lookup with temperature-controlled soft assignments, enabling fully differentiable training while recovering hard assignments at inference. The assignments correspond to posterior responsibilities of an isotropic Gaussian mixture and minimize a variational free-energy objective, provably converging to nearest-neighbor quantization in the low-temperature limit. LGQ combines a token-level peakedness regularizer with a global usage regularizer to encourage confident yet balanced code utilization without imposing rigid grids.
  Under a controlled VQGAN-style backbone on ImageNet across multiple vocabulary sizes, LGQ achieves stable optimization and balanced utilization. At 16K codebook size, LGQ improves rFID by 11.88% over FSQ while using 49.96% fewer active codes, and improves rFID by 6.06% over SimVQ with 49.45% lower effective representation rate, achieving comparable fidelity with substantially fewer active entries. Our GitHub repository is available at: https://github.com/KurbanIntelligenceLab/LGQ

</details>


### [17] [OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis](https://arxiv.org/abs/2602.16110)
*Tianwei Lin,Zhongwei Qiu,Wenqiao Zhang,Jiang Liu,Yihan Xie,Mingjian Gao,Zhenxuan Fan,Zhaocheng Li,Sijing Li,Zhongle Xie,Peng LU,Yueting Zhuang,Yingda Xia,Ling Zhang,Beng Chin Ooi*

Main category: cs.CV

TL;DR: OmniCT是一个统一的切片-体积大型视觉语言模型，通过空间一致性增强和器官级语义增强，解决了CT影像分析中切片驱动与体积驱动方法之间的割裂问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型在CT影像分析中存在碎片化问题：切片驱动模型缺乏跨切片空间一致性，而体积驱动模型粒度粗糙且与切片输入兼容性差。这种缺乏统一建模范式的问题阻碍了医学LVLMs的临床转化。

Method: 1. 空间一致性增强：结合体积切片组合与三轴位置嵌入引入体积一致性，MoE混合投影实现高效切片-体积适应；2. 器官级语义增强：通过分割和ROI定位显式对齐解剖区域，强调病变和器官级语义；3. MedEval-CT：最大的切片-体积CT数据集和混合基准，集成全面评估指标。

Result: OmniCT在各种临床任务中始终显著优于现有方法，同时满足微观细节敏感性和宏观空间推理需求，为跨模态医学影像理解建立了新范式。

Conclusion: OmniCT通过统一切片-体积建模解决了CT影像分析中的关键瓶颈，在保持细节敏感性的同时实现了空间一致性，为医学LVLMs的临床转化提供了有效解决方案。

Abstract: Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both slice-driven local features (e.g., sub-centimeter nodules, lesion boundaries) and volume-driven spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). However, existing Large Vision-Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. We present OmniCT, a powerful unified slice-volume LVLM for CT scenarios, which makes three contributions: (i) Spatial Consistency Enhancement (SCE): volumetric slice composition combined with tri-axial positional embedding that introduces volumetric consistency, and an MoE hybrid projection enables efficient slice-volume adaptation; (ii) Organ-level Semantic Enhancement (OSE): segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; (iii) MedEval-CT: the largest slice-volume CT dataset and hybrid benchmark integrates comprehensive metrics for unified evaluation. OmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks and satisfies both micro-level detail sensitivity and macro-level spatial reasoning. More importantly, it establishes a new paradigm for cross-modal medical imaging understanding.

</details>


### [18] [CHAI: CacHe Attention Inference for text2video](https://arxiv.org/abs/2602.16132)
*Joel Mathew Cherian,Ashutosh Muralidhara Bharadwaj,Vima Gupta,Anand Padmanabha Iyer*

Main category: cs.CV

TL;DR: CHAI通过跨推理缓存和Cache Attention机制，在保持视频质量的同时将文本到视频扩散模型的推理速度提升1.65-3.35倍，仅需8个去噪步骤。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频扩散模型因3D潜变量的顺序去噪而速度缓慢。现有加速方法要么需要昂贵的模型重训练，要么使用启发式步骤跳过，在减少去噪步骤时难以保持视频质量。

Method: 提出CHAI框架，采用跨推理缓存技术，引入Cache Attention机制，该机制能有效关注跨推理潜变量中的共享对象/场景，实现语义相关提示间缓存潜变量的有效重用。

Result: CHAI仅需8个去噪步骤即可生成高质量视频，集成到系统中后比基线OpenSora 1.2快1.65-3.35倍，同时保持视频质量，实现了高缓存命中率。

Conclusion: CHAI通过创新的跨推理缓存和Cache Attention机制，在显著加速文本到视频生成的同时保持了视频质量，为扩散模型的高效推理提供了有效解决方案。

Abstract: Text-to-video diffusion models deliver impressive results but remain slow because of the sequential denoising of 3D latents. Existing approaches to speed up inference either require expensive model retraining or use heuristic-based step skipping, which struggles to maintain video quality as the number of denoising steps decreases. Our work, CHAI, aims to use cross-inference caching to reduce latency while maintaining video quality. We introduce Cache Attention as an effective method for attending to shared objects/scenes across cross-inference latents. This selective attention mechanism enables effective reuse of cached latents across semantically related prompts, yielding high cache hit rates. We show that it is possible to generate high-quality videos using Cache Attention with as few as 8 denoising steps. When integrated into the overall system, CHAI is 1.65x - 3.35x faster than baseline OpenSora 1.2 while maintaining video quality.

</details>


### [19] [IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models](https://arxiv.org/abs/2602.16138)
*Parsa Madinei,Srijita Karmakar,Russell Cohen Hoffing,Felix Gervitz,Miguel P. Eckstein*

Main category: cs.CV

TL;DR: IRIS利用实时眼动数据解决开放视觉问答中的歧义问题，通过注视点分析将模糊问题的回答准确率从35.2%提升至77.2%


<details>
  <summary>Details</summary>
Motivation: 开放视觉问答中存在大量歧义问题，传统视觉语言模型难以准确理解用户意图，需要利用人类自然交互信息（如眼动数据）来解析用户真实意图

Method: 提出IRIS方法，无需训练，实时使用眼动追踪数据，特别关注用户开始提问时的注视点，这些注视点对歧义消解最有效

Result: 在500个独特图像-问题对的研究中，IRIS将模糊问题的回答准确率从35.2%提升至77.2%，同时在明确问题上保持性能，在各种先进视觉语言模型中都表现一致改进

Conclusion: 眼动数据是解决视觉问答歧义问题的有效信息源，IRIS方法无需训练即可显著提升模型性能，为此领域提供了新基准数据集、实时交互协议和评估套件

Abstract: We introduce IRIS (Intent Resolution via Inference-time Saccades), a novel training-free approach that uses eye-tracking data in real-time to resolve ambiguity in open-ended VQA. Through a comprehensive user study with 500 unique image-question pairs, we demonstrate that fixations closest to the time participants start verbally asking their questions are the most informative for disambiguation in Large VLMs, more than doubling the accuracy of responses on ambiguous questions (from 35.2% to 77.2%) while maintaining performance on unambiguous queries. We evaluate our approach across state-of-the-art VLMs, showing consistent improvements when gaze data is incorporated in ambiguous image-question pairs, regardless of architectural differences. We release a new benchmark dataset to use eye movement data for disambiguated VQA, a novel real-time interactive protocol, and an evaluation suite.

</details>


### [20] [Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing](https://arxiv.org/abs/2602.16149)
*Huichan Seo,Minki Hong,Sieun Choi,Jihie Kim,Jean Oh*

Main category: cs.CV

TL;DR: 研究发现图像到图像编辑中存在系统性的人口统计学偏见：相同编辑指令对不同人口群体的效果不同，表现为软擦除和刻板印象替换两种失败模式。


<details>
  <summary>Details</summary>
Motivation: 虽然文本到图像生成中的人口统计学偏见已有研究，但指令引导的图像到图像编辑中的人口统计学条件失败模式尚未充分探索。研究者希望探究相同编辑指令是否会对不同人口群体的主体产生系统性不同的结果。

Method: 研究者构建了一个受控基准测试，通过生成和编辑基于种族、性别和年龄条件的人像，使用诊断提示集来探测人口统计学条件行为。他们使用视觉语言模型评分和人工评估来评估多个编辑器的表现。

Result: 分析表明身份保留失败普遍存在，在不同人口群体中分布不均，并受到隐性社会先验的影响，包括职业驱动的性别推断。研究发现，无需模型更新的提示级身份约束可以显著减少少数群体的人口统计学变化，而对多数群体的人像影响较小，揭示了当前编辑器中存在不对称的身份先验。

Conclusion: 身份保留是图像到图像编辑中一个核心且人口统计学分布不均的失败模式。研究结果强调了开发人口统计学鲁棒编辑系统的必要性，并展示了通过简单的提示约束可以部分缓解这些问题。

Abstract: Demographic bias in text-to-image (T2I) generation is well studied, yet demographic-conditioned failures in instruction-guided image-to-image (I2I) editing remain underexplored. We examine whether identical edit instructions yield systematically different outcomes across subject demographics in open-weight I2I editors. We formalize two failure modes: Soft Erasure, where edits are silently weakened or ignored in the output image, and Stereotype Replacement, where edits introduce unrequested, stereotype-consistent attributes. We introduce a controlled benchmark that probes demographic-conditioned behavior by generating and editing portraits conditioned on race, gender, and age using a diagnostic prompt set, and evaluate multiple editors with vision-language model (VLM) scoring and human evaluation. Our analysis shows that identity preservation failures are pervasive, demographically uneven, and shaped by implicit social priors, including occupation-driven gender inference. Finally, we demonstrate that a prompt-level identity constraint, without model updates, can substantially reduce demographic change for minority groups while leaving majority-group portraits largely unchanged, revealing asymmetric identity priors in current editors. Together, our findings establish identity preservation as a central and demographically uneven failure mode in I2I editing and motivate demographic-robust editing systems. Project page: https://seochan99.github.io/i2i-demographic-bias

</details>


### [21] [Uncertainty-Guided Inference-Time Depth Adaptation for Transformer-Based Visual Tracking](https://arxiv.org/abs/2602.16160)
*Patrick Poggi,Divake Kumar,Theja Tulabandhula,Amit Ranjan Trivedi*

Main category: cs.CV

TL;DR: UncL-STARK：一种基于Transformer的跟踪器动态深度自适应方法，通过不确定性估计和反馈策略在保持精度的同时显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 现有Transformer跟踪器采用固定深度推理，无论视频帧的视觉复杂度如何都执行完整的编码器-解码器堆栈，在时间连贯性强的长视频序列中产生不必要的计算开销

Method: 1）使用随机深度训练和知识蒸馏对模型进行微调，使其在多个中间深度保持预测鲁棒性；2）运行时从角点定位热图中提取轻量级不确定性估计；3）基于预测置信度采用反馈驱动策略选择下一帧的编码器和解码器深度

Result: 在GOT-10k和LaSOT数据集上，实现高达12%的GFLOPs减少、8.9%的延迟降低和10.8%的能耗节省，同时跟踪精度保持在完整深度基线的0.2%以内

Conclusion: UncL-STARK能够在保持架构不变的情况下实现动态深度自适应，有效利用视频的时间连贯性，在保持跟踪精度的同时显著降低计算成本

Abstract: Transformer-based single-object trackers achieve state-of-the-art accuracy but rely on fixed-depth inference, executing the full encoder--decoder stack for every frame regardless of visual complexity, thereby incurring unnecessary computational cost in long video sequences dominated by temporally coherent frames. We propose UncL-STARK, an architecture-preserving approach that enables dynamic, uncertainty-aware depth adaptation in transformer-based trackers without modifying the underlying network or adding auxiliary heads. The model is fine-tuned to retain predictive robustness at multiple intermediate depths using random-depth training with knowledge distillation, thus enabling safe inference-time truncation. At runtime, we derive a lightweight uncertainty estimate directly from the model's corner localization heatmaps and use it in a feedback-driven policy that selects the encoder and decoder depth for the next frame based on the prediction confidence by exploiting temporal coherence in video. Extensive experiments on GOT-10k and LaSOT demonstrate up to 12\% GFLOPs reduction, 8.9\% latency reduction, and 10.8\% energy savings while maintaining tracking accuracy within 0.2\% of the full-depth baseline across both short-term and long-term sequences.

</details>


### [22] [DataCube: A Video Retrieval Platform via Natural Language Semantic Profiling](https://arxiv.org/abs/2602.16231)
*Yiming Ju,Hanyu Zhao,Quanyue Ma,Donglin Hao,Chengwei Wu,Ming Li,Songjing Wang,Tengfei Pan*

Main category: cs.CV

TL;DR: DataCube是一个智能视频处理平台，通过自动处理、多维度分析和查询驱动检索，帮助用户从大规模视频库中高效构建定制化数据集。


<details>
  <summary>Details</summary>
Motivation: 大规模视频库日益增多，但将原始视频转化为高质量、任务特定的数据集仍然成本高昂且效率低下。需要一种更高效的方式来处理和分析视频数据。

Method: DataCube平台通过自动视频处理、多维度分析和结构化语义表示构建，支持混合检索（神经重排序和深度语义匹配），并提供交互式Web界面。

Result: 开发了一个公开可访问的平台（https://datacube.baai.ac.cn/），用户可以从海量视频库中高效构建定制化视频子集，用于训练、分析和评估，并可在私有视频集合上构建可搜索系统。

Conclusion: DataCube提供了一个高效的解决方案，将原始视频转化为结构化语义表示，通过智能检索和交互界面显著提升了视频数据集构建的效率。

Abstract: Large-scale video repositories are increasingly available for modern video understanding and generation tasks. However, transforming raw videos into high-quality, task-specific datasets remains costly and inefficient. We present DataCube, an intelligent platform for automatic video processing, multi-dimensional profiling, and query-driven retrieval. DataCube constructs structured semantic representations of video clips and supports hybrid retrieval with neural re-ranking and deep semantic matching. Through an interactive web interface, users can efficiently construct customized video subsets from massive repositories for training, analysis, and evaluation, and build searchable systems over their own private video collections. The system is publicly accessible at https://datacube.baai.ac.cn/. Demo Video: https://baai-data-cube.ks3-cn-beijing.ksyuncs.com/custom/Adobe%20Express%20-%202%E6%9C%8818%E6%97%A5%20%281%29%281%29%20%281%29.mp4

</details>


### [23] [EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection](https://arxiv.org/abs/2602.16238)
*Hiroki Nakamura,Hiroto Iino,Masashi Okada,Tadahiro Taniguchi*

Main category: cs.CV

TL;DR: EasyControlEdge 通过适配图像生成基础模型进行边缘检测，利用预训练先验实现数据高效迁移，并通过迭代细化保留高频细节，在有限训练样本下生成清晰的边缘图。


<details>
  <summary>Details</summary>
Motivation: 在真实世界的边缘检测任务中（如平面图墙壁、卫星道路/建筑、医学器官边界），清晰度和数据效率至关重要。然而，在有限训练样本下生成清晰的原始边缘图仍然具有挑战性。虽然图像生成基础模型在许多下游任务中表现良好，但其用于数据高效迁移的预训练先验和用于高频细节保留的迭代细化能力在边缘检测中尚未得到充分利用。

Method: 1. 提出边缘专用的图像生成基础模型适配方法；2. 引入边缘导向目标函数和高效的像素空间损失；3. 在推理时引入基于无条件动态的引导，使单个模型能够通过引导尺度控制边缘密度。

Result: 在BSDS500、NYUDv2、BIPED和CubiCasa数据集上的实验表明，该方法相比最先进方法取得了一致性提升，特别是在无后处理的清晰度评估和有限训练数据情况下表现优异。

Conclusion: EasyControlEdge成功地将图像生成基础模型适配到边缘检测任务，利用其预训练先验实现数据高效迁移，并通过迭代细化生成清晰的边缘图，在多个边缘检测数据集上展现了优越性能。

Abstract: We propose EasyControlEdge, adapting an image-generation foundation model to edge detection. In real-world edge detection (e.g., floor-plan walls, satellite roads/buildings, and medical organ boundaries), crispness and data efficiency are crucial, yet producing crisp raw edge maps with limited training samples remains challenging. Although image-generation foundation models perform well on many downstream tasks, their pretrained priors for data-efficient transfer and iterative refinement for high-frequency detail preservation remain underexploited for edge detection. To enable crisp and data-efficient edge detection using these capabilities, we introduce an edge-specialized adaptation of image-generation foundation models. To better specialize the foundation model for edge detection, we incorporate an edge-oriented objective with an efficient pixel-space loss. At inference, we introduce guidance based on unconditional dynamics, enabling a single model to control the edge density through a guidance scale. Experiments on BSDS500, NYUDv2, BIPED, and CubiCasa compare against state-of-the-art methods and show consistent gains, particularly under no-post-processing crispness evaluation and with limited training data.

</details>


### [24] [HyPCA-Net: Advancing Multimodal Fusion in Medical Image Analysis](https://arxiv.org/abs/2602.16245)
*J. Dhar,M. K. Pandey,D. Chakladar,M. Haghighat,A. Alavi,S. Mistry,N. Zaidi*

Main category: cs.CV

TL;DR: 提出HyPCA-Net，一种混合并行融合级联注意力网络，用于解决多模态医学影像融合中的计算效率低和信息损失问题，在10个公开数据集上性能提升5.2%，计算成本降低73.1%。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法存在两大问题：1) 依赖计算昂贵的模型，在低资源环境中应用受限；2) 使用级联注意力模块可能导致信息损失，难以有效捕获跨模态的鲁棒共享表示，限制了在多疾病分析任务中的泛化能力。

Method: 提出HyPCA-Net，包含两个核心创新模块：1) 计算高效的残差自适应学习注意力块，用于捕获细化的模态特定表示；2) 双视图级联注意力块，旨在学习跨不同模态的鲁棒共享表示。

Result: 在10个公开数据集上的广泛实验表明，HyPCA-Net显著优于现有领先方法，性能提升最高达5.2%，计算成本降低最高达73.1%。

Conclusion: HyPCA-Net通过创新的混合并行融合架构有效解决了多模态医学影像融合中的计算效率和信息保留问题，为低资源环境下的多疾病分析提供了高效解决方案。

Abstract: Multimodal fusion frameworks, which integrate diverse medical imaging modalities (e.g., MRI, CT), have shown great potential in applications such as skin cancer detection, dementia diagnosis, and brain tumor prediction. However, existing multimodal fusion methods face significant challenges. First, they often rely on computationally expensive models, limiting their applicability in low-resource environments. Second, they often employ cascaded attention modules, which potentially increase risk of information loss during inter-module transitions and hinder their capacity to effectively capture robust shared representations across modalities. This restricts their generalization in multi-disease analysis tasks. To address these limitations, we propose a Hybrid Parallel-Fusion Cascaded Attention Network (HyPCA-Net), composed of two core novel blocks: (a) a computationally efficient residual adaptive learning attention block for capturing refined modality-specific representations, and (b) a dual-view cascaded attention block aimed at learning robust shared representations across diverse modalities. Extensive experiments on ten publicly available datasets exhibit that HyPCA-Net significantly outperforms existing leading methods, with improvements of up to 5.2% in performance and reductions of up to 73.1% in computational cost. Code: https://github.com/misti1203/HyPCA-Net.

</details>


### [25] [AFFMAE: Scalable and Efficient Vision Pretraining for Desktop Graphics Cards](https://arxiv.org/abs/2602.16249)
*David Smerkous,Zian Wang,Behzad Najafian*

Main category: cs.CV

TL;DR: AFFMAE是一个基于自适应非网格标记合并的掩码友好型分层预训练框架，通过丢弃掩码标记并仅在可见标记上执行动态合并，在保持分层可扩展性的同时移除了密集网格假设，显著降低了高分辨率训练的计算需求。


<details>
  <summary>Details</summary>
Motivation: 自监督预训练虽然改变了计算机视觉，但高分辨率训练通常需要服务器级基础设施，这限制了研究实验室开发领域特定基础模型。MAE通过仅编码可见标记来减少计算，但与分层下采样架构结合时存在结构挑战。

Method: 提出AFFMAE框架，采用自适应、非网格的标记合并技术，丢弃掩码标记并仅在可见标记上执行动态合并。开发了数值稳定的混合精度Flash风格集群注意力内核，并通过深度监督缓解稀疏阶段表示崩溃问题。

Result: 在高分辨率电子显微镜分割任务上，AFFMAE在相同参数数量下匹配ViT-MAE性能，同时将FLOPs减少高达7倍，内存使用减半，并在单个RTX 5090上实现更快的训练。

Conclusion: AFFMAE通过移除密集网格假设并采用自适应标记合并，为高分辨率视觉任务提供了计算高效的掩码友好型分层预训练框架，使研究实验室能够在有限硬件资源下开发领域特定基础模型。

Abstract: Self-supervised pretraining has transformed computer vision by enabling data-efficient fine-tuning, yet high-resolution training typically requires server-scale infrastructure, limiting in-domain foundation model development for many research laboratories. Masked Autoencoders (MAE) reduce computation by encoding only visible tokens, but combining MAE with hierarchical downsampling architectures remains structurally challenging due to dense grid priors and mask-aware design compromises. We introduce AFFMAE, a masking-friendly hierarchical pretraining framework built on adaptive, off-grid token merging. By discarding masked tokens and performing dynamic merging exclusively over visible tokens, AFFMAE removes dense-grid assumptions while preserving hierarchical scalability. We developed numerically stable mixed-precision Flash-style cluster attention kernels, and mitigate sparse-stage representation collapse via deep supervision. On high-resolution electron microscopy segmentation, AFFMAE matches ViT-MAE performance at equal parameter count while reducing FLOPs by up to 7x, halving memory usage, and achieving faster training on a single RTX 5090. Code available at https://github.com/najafian-lab/affmae.

</details>


### [26] [Breaking the Sub-Millimeter Barrier: Eyeframe Acquisition from Color Images](https://arxiv.org/abs/2602.16281)
*Manel Guzmán,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出基于计算机视觉的多视角眼框追踪算法，无需专用设备即可实现亚毫米精度测量


<details>
  <summary>Details</summary>
Motivation: 传统眼框追踪依赖机械工具，需要精确定位和校准，耗时且需要额外设备，导致验光师工作流程效率低下

Method: 基于计算机视觉的多视角方法，使用InVision系统采集图像，包括图像采集、框架分割、深度估计和多视角处理，整合RGB分割图像与深度数据进行精确轮廓测量

Result: 在真实数据上分析了不同配置和变体，仅使用彩色静态图像就能获得与其他解决方案相竞争的测量结果

Conclusion: 该方法消除了对专用追踪设备的需求，减少了光学技术人员的工作流程复杂性，提供了高效的眼框追踪解决方案

Abstract: Eyeframe lens tracing is an important process in the optical industry that requires sub-millimeter precision to ensure proper lens fitting and optimal vision correction. Traditional frame tracers rely on mechanical tools that need precise positioning and calibration, which are time-consuming and require additional equipment, creating an inefficient workflow for opticians. This work presents a novel approach based on artificial vision that utilizes multi-view information. The proposed algorithm operates on images captured from an InVision system. The full pipeline includes image acquisition, frame segmentation to isolate the eyeframe from background, depth estimation to obtain 3D spatial information, and multi-view processing that integrates segmented RGB images with depth data for precise frame contour measurement. To this end, different configurations and variants are proposed and analyzed on real data, providing competitive measurements from still color images with respect to other solutions, while eliminating the need for specialized tracing equipment and reducing workflow complexity for optical technicians.

</details>


### [27] [A Self-Supervised Approach for Enhanced Feature Representations in Object Detection Tasks](https://arxiv.org/abs/2602.16322)
*Santiago C. Vilabella,Pablo Pérez-Núñez,Beatriz Remeseiro*

Main category: cs.CV

TL;DR: 本文提出一种自监督学习方法，通过增强特征提取器，使模型能用更少的标注数据学习更有效的特征表示，在目标检测任务上超越ImageNet预训练模型。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型复杂度增加，获取标注数据成为重大挑战。目标检测等复杂问题需要大量时间和资源进行数据标注，给企业带来高昂成本。本文旨在证明增强特征提取器可以显著缓解这一问题。

Method: 采用自监督学习策略，在未标注数据上训练模型，增强特征提取器，使模型能够学习更有效的特征表示。

Result: 提出的模型在目标检测任务上超越了基于ImageNet预训练的最先进特征提取器，并且能够关注对象的最相关方面，获得更好的特征表示。

Conclusion: 增强特征提取器可以有效减少对标注数据的依赖，提高模型的可靠性和鲁棒性，为解决标注数据稀缺问题提供了有效途径。

Abstract: In the fast-evolving field of artificial intelligence, where models are increasingly growing in complexity and size, the availability of labeled data for training deep learning models has become a significant challenge. Addressing complex problems like object detection demands considerable time and resources for data labeling to achieve meaningful results. For companies developing such applications, this entails extensive investment in highly skilled personnel or costly outsourcing. This research work aims to demonstrate that enhancing feature extractors can substantially alleviate this challenge, enabling models to learn more effective representations with less labeled data. Utilizing a self-supervised learning strategy, we present a model trained on unlabeled data that outperforms state-of-the-art feature extractors pre-trained on ImageNet and particularly designed for object detection tasks. Moreover, the results demonstrate that our approach encourages the model to focus on the most relevant aspects of an object, thus achieving better feature representations and, therefore, reinforcing its reliability and robustness.

</details>


### [28] [Subtractive Modulative Network with Learnable Periodic Activations](https://arxiv.org/abs/2602.16337)
*Tiou Wang,Zhuoqian Yang,Markus Flierl,Mathieu Salzmann,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出Subtractive Modulative Network (SMN)，一种受经典减法合成启发的参数高效隐式神经表示架构，在图像重建和3D NeRF新视角合成任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 受到经典减法合成的启发，旨在设计一个参数高效且具有理论基础的隐式神经表示架构，以改善重建精度和参数效率。

Method: 设计了一个基于信号处理流程的架构，包含可学习的周期性激活层（振荡器）生成多频基，以及一系列调制掩码模块（滤波器）主动生成高阶谐波。

Result: 在两个图像数据集上达到40+ dB的PSNR，在重建精度和参数效率方面优于现有方法，在3D NeRF新视角合成任务上也表现出持续优势。

Conclusion: SMN是一种参数高效的隐式神经表示架构，通过受减法合成启发的设计，在图像重建和3D场景表示任务上取得了优异性能。

Abstract: We propose the Subtractive Modulative Network (SMN), a novel, parameter-efficient Implicit Neural Representation (INR) architecture inspired by classical subtractive synthesis. The SMN is designed as a principled signal processing pipeline, featuring a learnable periodic activation layer (Oscillator) that generates a multi-frequency basis, and a series of modulative mask modules (Filters) that actively generate high-order harmonics. We provide both theoretical analysis and empirical validation for our design. Our SMN achieves a PSNR of $40+$ dB on two image datasets, comparing favorably against state-of-the-art methods in terms of both reconstruction accuracy and parameter efficiency. Furthermore, consistent advantage is observed on the challenging 3D NeRF novel view synthesis task. Supplementary materials are available at https://inrainbws.github.io/smn/.

</details>


### [29] [SCAR: Satellite Imagery-Based Calibration for Aerial Recordings](https://arxiv.org/abs/2602.16349)
*Henry Hölzemann,Michael Schleiss*

Main category: cs.CV

TL;DR: SCAR是一种利用卫星影像作为全局参考的空中视觉惯性系统长期自动标定方法，无需人工干预即可在野外部署条件下检测和修正标定退化。


<details>
  <summary>Details</summary>
Motivation: 现有的空中视觉惯性系统标定方法依赖于专门的标定操作或人工测量的地面控制点，难以在长期野外部署中维持标定精度。需要一种能够利用外部地理空间数据自动检测和修正标定退化的方法。

Method: SCAR利用公开可用的正射影像和高程模型作为持久全局参考，通过将航拍图像与从卫星数据推导的2D-3D对应关系对齐，同时估计相机内参和外参参数。

Result: 在两年内六个大规模空中任务（涵盖不同季节和环境条件）的评估中，SCAR始终优于现有基线方法（Kalibr、COLMAP、VINS-Mono），大幅降低中值重投影误差，并将这些标定增益转化为显著降低的视觉定位旋转误差和更高的姿态精度。

Conclusion: SCAR能够在长期空中操作中提供准确、鲁棒且可重复的标定，无需人工干预，为空中视觉惯性系统的长期部署提供了实用的标定解决方案。

Abstract: We introduce SCAR, a method for long-term auto-calibration refinement of aerial visual-inertial systems that exploits georeferenced satellite imagery as a persistent global reference. SCAR estimates both intrinsic and extrinsic parameters by aligning aerial images with 2D--3D correspondences derived from publicly available orthophotos and elevation models. In contrast to existing approaches that rely on dedicated calibration maneuvers or manually surveyed ground control points, our method leverages external geospatial data to detect and correct calibration degradation under field deployment conditions. We evaluate our approach on six large-scale aerial campaigns conducted over two years under diverse seasonal and environmental conditions. Across all sequences, SCAR consistently outperforms established baselines (Kalibr, COLMAP, VINS-Mono), reducing median reprojection error by a large margin, and translating these calibration gains into substantially lower visual localization rotation errors and higher pose accuracy. These results demonstrate that SCAR provides accurate, robust, and reproducible calibration over long-term aerial operations without the need for manual intervention.

</details>


### [30] [Parameter-Free Adaptive Multi-Scale Channel-Spatial Attention Aggregation framework for 3D Indoor Semantic Scene Completion Toward Assisting Visually Impaired](https://arxiv.org/abs/2602.16385)
*Qi He,XiangXiang Wang,Jingtao Zhang,Yongbin Yu,Hongxiang Chu,Manping Fan,JingYe Cai,Zhenglin Yang*

Main category: cs.CV

TL;DR: AMAA框架通过自适应多尺度注意力聚合提升单目3D语义场景补全的性能，在NYUv2基准上取得SOTA结果，并能在嵌入式硬件上稳定运行。


<details>
  <summary>Details</summary>
Motivation: 现有单目SSC方法在2D-3D投影和多尺度融合中缺乏对体素特征可靠性的显式建模和跨尺度信息传播的调控，容易受到投影扩散和特征纠缠的影响，限制了结构稳定性。这影响了室内辅助感知系统对视觉障碍用户的安全性。

Method: 基于MonoScene管道构建自适应多尺度注意力聚合(AMAA)框架。通过并行通道-空间注意力聚合联合校准提升体素特征的语义和空间维度，同时通过分层自适应特征门控策略调控跨尺度的信息注入，稳定多尺度编码器-解码器融合。

Result: 在NYUv2基准测试中，AMAA实现了27.25%的SSC mIoU（提升0.31%）和43.10%的SC IoU（提升0.59%）。在NVIDIA Jetson平台上的系统级部署验证了完整框架能在嵌入式硬件上稳定执行。

Conclusion: AMAA提高了单目SSC的质量，为面向视觉障碍用户的室内辅助系统提供了一个可靠且可部署的感知框架，在不显著增加系统复杂性的情况下实现了性能提升。

Abstract: In indoor assistive perception for visually impaired users, 3D Semantic Scene Completion (SSC) is expected to provide structurally coherent and semantically consistent occupancy under strictly monocular vision for safety-critical scene understanding. However, existing monocular SSC approaches often lack explicit modeling of voxel-feature reliability and regulated cross-scale information propagation during 2D-3D projection and multi-scale fusion, making them vulnerable to projection diffusion and feature entanglement and thus limiting structural stability.To address these challenges, this paper presents an Adaptive Multi-scale Attention Aggregation (AMAA) framework built upon the MonoScene pipeline. Rather than introducing a heavier backbone, AMAA focuses on reliability-oriented feature regulation within a monocular SSC framework. Specifically, lifted voxel features are jointly calibrated in semantic and spatial dimensions through parallel channel-spatial attention aggregation, while multi-scale encoder-decoder fusion is stabilized via a hierarchical adaptive feature-gating strategy that regulates information injection across scales.Experiments on the NYUv2 benchmark demonstrate consistent improvements over MonoScene without significantly increasing system complexity: AMAA achieves 27.25% SSC mIoU (+0.31) and 43.10% SC IoU (+0.59). In addition, system-level deployment on an NVIDIA Jetson platform verifies that the complete AMAA framework can be executed stably on embedded hardware. Overall, AMAA improves monocular SSC quality and provides a reliable and deployable perception framework for indoor assistive systems targeting visually impaired users.

</details>


### [31] [ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding](https://arxiv.org/abs/2602.16412)
*Daichi Yashima,Shuhei Kurita,Yusuke Oda,Komei Sugiura*

Main category: cs.CV

TL;DR: ReMoRa是一个视频多模态大语言模型，通过直接处理压缩视频表示来解决长视频理解问题，使用关键帧保留外观信息，用运动表示编码时间动态，避免了处理完整RGB帧序列的计算负担。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在许多任务上表现出色，但长视频理解仍然是一个重大挑战。处理完整的RGB帧流在计算上不可行且高度冗余，因为自注意力机制具有序列长度的二次复杂度。

Method: ReMoRa直接处理视频的压缩表示：保留稀疏的RGB关键帧用于外观信息，将时间动态编码为运动表示，无需顺序RGB帧。引入去噪模块来细化块状运动的噪声和低保真度，并以线性序列长度复杂度压缩这些特征。

Result: ReMoRa在多个具有挑战性的长视频理解基准测试中超越了基线方法，包括LongVideoBench、NExT-QA和MLVU。

Conclusion: ReMoRa通过直接处理压缩视频表示，有效解决了长视频理解的计算挑战，在保持线性计算复杂度的同时，在多个基准测试中取得了优越性能。

Abstract: While multimodal large language models (MLLMs) have shown remarkable success across a wide range of tasks, long-form video understanding remains a significant challenge. In this study, we focus on video understanding by MLLMs. This task is challenging because processing a full stream of RGB frames is computationally intractable and highly redundant, as self-attention have quadratic complexity with sequence length. In this paper, we propose ReMoRa, a video MLLM that processes videos by operating directly on their compressed representations. A sparse set of RGB keyframes is retained for appearance, while temporal dynamics are encoded as a motion representation, removing the need for sequential RGB frames. These motion representations act as a compact proxy for optical flow, capturing temporal dynamics without full frame decoding. To refine the noise and low fidelity of block-based motions, we introduce a module to denoise and generate a fine-grained motion representation. Furthermore, our model compresses these features in a way that scales linearly with sequence length. We demonstrate the effectiveness of ReMoRa through extensive experiments across a comprehensive suite of long-video understanding benchmarks. ReMoRa outperformed baseline methods on multiple challenging benchmarks, including LongVideoBench, NExT-QA, and MLVU.

</details>


### [32] [Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems](https://arxiv.org/abs/2602.16430)
*Ali Faraz,Raja Kolla,Ashish Kulkarni,Shubham Agarwal*

Main category: cs.CV

TL;DR: 该论文研究了两种构建多语言OCR系统的训练策略，通过Chitrapathak系列在印度语境下实现最佳准确率-延迟权衡，并提出了专门针对9种印度政府文档的Parichay模型系列。


<details>
  <summary>Details</summary>
Motivation: 为印度设计OCR系统需要平衡语言多样性、文档异质性和部署约束。研究旨在找到构建多语言OCR系统的最佳训练策略，以应对印度复杂的语言环境和实际部署需求。

Method: 研究了两种训练策略：1) 采用流行的多模态方法，将通用视觉编码器与强大的多语言语言模型配对，进行端到端OCR训练；2) 微调现有的OCR模型，尽管该模型未针对目标语言进行训练。同时提出了专门针对9种印度政府文档的Parichay模型系列。

Result: 第二种策略（微调现有OCR模型）在准确率-延迟权衡方面表现更优。Chitrapathak-2相比前代实现了3-6倍加速，在泰卢固语上达到SOTA（6.69字符ANLS），在其他语言上排名第二。Parichay模型在9种政府文档上实现了89.8%的精确匹配分数，且推理速度更快。

Conclusion: 微调现有OCR模型的策略在印度多语言OCR系统中具有更好的准确率-延迟权衡。Chitrapathak和Parichay系列系统实现了SOTA性能，为在印度语境下构建生产级OCR管道提供了实用指导。

Abstract: Designing Optical Character Recognition (OCR) systems for India requires balancing linguistic diversity, document heterogeneity, and deployment constraints. In this paper, we study two training strategies for building multilingual OCR systems with Vision-Language Models through the Chitrapathak series. We first follow a popular multimodal approach, pairing a generic vision encoder with a strong multilingual language model and training the system end-to-end for OCR. Alternatively, we explore fine-tuning an existing OCR model, despite not being trained for the target languages. Through extensive evaluation on multilingual Indic OCR benchmarks and deployment-oriented metrics, we find that the second strategy consistently achieves better accuracy-latency trade-offs. Chitrapathak-2 achieves 3-6x speedup over its predecessor with being state-of-the-art (SOTA) in Telugu (6.69 char ANLS) and second best in the rest. In addition, we present Parichay, an independent OCR model series designed specifically for 9 Indian government documents to extract structured key fields, achieving 89.8% Exact Match score with a faster inference. Together, these systems achieve SOTA performance and provide practical guidance for building production-scale OCR pipelines in the Indian context.

</details>


### [33] [Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing](https://arxiv.org/abs/2602.16455)
*Jinsong Li,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Dahua Lin*

Main category: cs.CV

TL;DR: 提出Visual Self-Refine (VSR)范式，通过像素级定位反馈让模型自我纠正视觉感知错误，应用于图表解析任务


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在文本推理和自校正方面表现出色，但在视觉密集的图表解析任务中表现不佳，容易出现数据遗漏、错位和幻觉等问题。受人类使用手指作为"视觉锚点"确保准确性的启发，需要新的方法来解决视觉感知错误

Method: 提出VSR范式：让模型生成像素级定位输出，可视化这些定位，然后将可视化结果反馈给模型自身，使其能够直观检查和纠正视觉感知错误。在图表解析领域实例化为ChartVSR模型，将解析过程分解为两个阶段：精炼阶段（迭代使用视觉反馈确保所有数据点的像素级定位准确性）和解码阶段（使用已验证的定位作为精确视觉锚点解析最终结构化数据）

Result: 构建了ChartP-Bench这一新的高难度图表解析基准测试，展示了VSR作为一种通用视觉反馈机制在提升视觉中心任务准确性方面的潜力

Conclusion: VSR范式为增强视觉中心任务的准确性提供了有前景的新方向，通过视觉自我校正机制解决了现有模型在复杂视觉任务中的局限性

Abstract: While Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities for reasoning and self-correction at the textual level, these strengths provide minimal benefits for complex tasks centered on visual perception, such as Chart Parsing. Existing models often struggle with visually dense charts, leading to errors like data omission, misalignment, and hallucination. Inspired by the human strategy of using a finger as a ``visual anchor'' to ensure accuracy when reading complex charts, we propose a new paradigm named Visual Self-Refine (VSR). The core idea of VSR is to enable a model to generate pixel-level localization outputs, visualize them, and then feed these visualizations back to itself, allowing it to intuitively inspect and correct its own potential visual perception errors. We instantiate the VSR paradigm in the domain of Chart Parsing by proposing ChartVSR. This model decomposes the parsing process into two stages: a Refine Stage, where it iteratively uses visual feedback to ensure the accuracy of all data points' Pixel-level Localizations, and a Decode Stage, where it uses these verified localizations as precise visual anchors to parse the final structured data. To address the limitations of existing benchmarks, we also construct ChartP-Bench, a new and highly challenging benchmark for chart parsing. Our work also highlights VSR as a general-purpose visual feedback mechanism, offering a promising new direction for enhancing accuracy on a wide range of vision-centric tasks.

</details>


### [34] [MMA: Multimodal Memory Agent](https://arxiv.org/abs/2602.16493)
*Yihao Lu,Wanru Cheng,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: 提出MMA多模态记忆代理，通过动态可靠性评分解决检索记忆的过时、低可信度问题，并引入MMA-Bench基准测试，揭示了视觉安慰剂效应。


<details>
  <summary>Details</summary>
Motivation: 长视野多模态代理依赖外部记忆，但基于相似性的检索常返回过时、低可信度或冲突的信息，导致过度自信的错误。

Method: 提出MMA，为每个检索的记忆项分配动态可靠性评分，结合来源可信度、时间衰减和冲突感知网络共识，用此信号重新加权证据并在支持不足时弃权。

Result: 在FEVER上匹配基线准确率同时减少35.2%方差；在LoCoMo上提高可操作准确率；在MMA-Bench上视觉模式达到41.18% Type-B准确率，而基线在相同协议下崩溃为0.0%。

Conclusion: MMA通过动态可靠性评估有效解决多模态记忆检索中的可信度问题，揭示了RAG代理从基础模型中继承潜在视觉偏见的视觉安慰剂效应。

Abstract: Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the "Visual Placebo Effect", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.

</details>


### [35] [Benchmarking Adversarial Robustness and Adversarial Training Strategies for Object Detection](https://arxiv.org/abs/2602.16494)
*Alexis Winter,Jean-Vincent Martini,Romaric Audigier,Angelique Loesch,Bertrand Luvison*

Main category: cs.CV

TL;DR: 该论文提出了一个用于公平比较目标检测模型对抗攻击的基准框架，发现现代对抗攻击对Transformer架构的迁移性较差，并提出混合高扰动攻击的数据集进行对抗训练是最有效的防御策略。


<details>
  <summary>Details</summary>
Motivation: 目标检测模型在自动驾驶等系统中至关重要，但其对抗攻击敏感性带来严重安全风险。目前防御研究落后于分类任务，缺乏标准化评估方法，不同研究使用不同数据集、效率指标和扰动成本度量，难以公平比较攻击或防御方法。

Method: 提出统一的基准框架，专注于数字、非补丁式攻击。引入特定指标分离定位和分类错误，使用多种感知度量评估攻击成本。通过该基准对最先进的攻击和多种检测器进行广泛实验。

Result: 发现两个主要结论：1）现代对抗攻击对目标检测模型向Transformer架构的迁移性显著不足；2）最鲁棒的对抗训练策略是利用混合不同目标（如空间和语义）的高扰动攻击数据集，这优于任何单一攻击的训练。

Conclusion: 该研究填补了目标检测对抗攻击评估的空白，提出的基准框架为公平比较提供了标准，揭示了攻击迁移性的局限性，并确定了最有效的对抗训练策略，为目标检测模型的鲁棒性研究提供了重要指导。

Abstract: Object detection models are critical components of automated systems, such as autonomous vehicles and perception-based robots, but their sensitivity to adversarial attacks poses a serious security risk. Progress in defending these models lags behind classification, hindered by a lack of standardized evaluation. It is nearly impossible to thoroughly compare attack or defense methods, as existing work uses different datasets, inconsistent efficiency metrics, and varied measures of perturbation cost. This paper addresses this gap by investigating three key questions: (1) How can we create a fair benchmark to impartially compare attacks? (2) How well do modern attacks transfer across different architectures, especially from Convolutional Neural Networks to Vision Transformers? (3) What is the most effective adversarial training strategy for robust defense? To answer these, we first propose a unified benchmark framework focused on digital, non-patch-based attacks. This framework introduces specific metrics to disentangle localization and classification errors and evaluates attack cost using multiple perceptual metrics. Using this benchmark, we conduct extensive experiments on state-of-the-art attacks and a wide range of detectors. Our findings reveal two major conclusions: first, modern adversarial attacks against object detection models show a significant lack of transferability to transformer-based architectures. Second, we demonstrate that the most robust adversarial training strategy leverages a dataset composed of a mix of high-perturbation attacks with different objectives (e.g., spatial and semantic), which outperforms training on any single attack.

</details>


### [36] [DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images](https://arxiv.org/abs/2602.16502)
*Zeng Tao,Ying Jiang,Yunuo Chen,Tianyi Xie,Huamin Wang,Yingnian Wu,Yin Yang,Abishek Sampath Kumar,Kenji Tashiro,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: DressWild：从单张野外图像重建物理一致的2D缝纫图案和3D服装的feed-forward方法


<details>
  <summary>Details</summary>
Motivation: 现有前馈方法难以处理多样姿态和视角，而基于优化的方法计算成本高且难以扩展。服装建模和制造应用需要可编辑、可分离且仿真就绪的服装。

Method: 利用视觉语言模型(VLMs)在图像层面归一化姿态变化，提取姿态感知的3D服装特征，通过transformer编码器融合特征，预测可直接用于物理仿真、纹理合成和多层虚拟试穿的缝纫图案参数。

Result: 实验表明，该方法无需多视角输入或迭代优化，就能从野外图像稳健地恢复多样缝纫图案和对应3D服装，为逼真服装仿真和动画提供高效可扩展方案。

Conclusion: DressWild提出了一种新颖的前馈流程，能够从单张野外图像重建物理一致的2D缝纫图案和3D服装，解决了现有方法的局限性，为服装建模和制造应用提供了实用解决方案。

Abstract: Recent advances in garment pattern generation have shown promising progress. However, existing feed-forward methods struggle with diverse poses and viewpoints, while optimization-based approaches are computationally expensive and difficult to scale. This paper focuses on sewing pattern generation for garment modeling and fabrication applications that demand editable, separable, and simulation-ready garments. We propose DressWild, a novel feed-forward pipeline that reconstructs physics-consistent 2D sewing patterns and the corresponding 3D garments from a single in-the-wild image. Given an input image, our method leverages vision-language models (VLMs) to normalize pose variations at the image level, then extract pose-aware, 3D-informed garment features. These features are fused through a transformer-based encoder and subsequently used to predict sewing pattern parameters, which can be directly applied to physical simulation, texture synthesis, and multi-layer virtual try-on. Extensive experiments demonstrate that our approach robustly recovers diverse sewing patterns and the corresponding 3D garments from in-the-wild images without requiring multi-view inputs or iterative optimization, offering an efficient and scalable solution for realistic garment simulation and animation.

</details>


### [37] [Let's Split Up: Zero-Shot Classifier Edits for Fine-Grained Video Understanding](https://arxiv.org/abs/2602.16545)
*Kaiting Liu,Hazel Doughty*

Main category: cs.CV

TL;DR: 提出视频分类中的"类别分割"任务，将粗粒度类别细分为更精细的子类别，无需重新训练整个模型


<details>
  <summary>Details</summary>
Motivation: 现有视频识别模型通常在固定分类体系上训练，分类过于粗糙，将不同对象、方式或结果合并为单一标签。随着任务和定义演变，这些模型无法适应新出现的区分，而重新收集标注和训练成本高昂

Method: 提出零样本编辑方法，利用视频分类器的潜在组合结构来暴露细粒度区分，无需额外数据。同时展示少量样本微调虽然简单但非常有效，并能从零样本初始化中受益

Result: 在新构建的视频类别分割基准测试中，该方法显著优于视觉语言基线，在新增分割类别上提高准确性，同时不牺牲其他类别的性能

Conclusion: 类别分割任务和提出的方法能够有效将现有分类器中的粗粒度类别细分为更精细的子类别，为视频识别模型的适应性改进提供了高效途径

Abstract: Video recognition models are typically trained on fixed taxonomies which are often too coarse, collapsing distinctions in object, manner or outcome under a single label. As tasks and definitions evolve, such models cannot accommodate emerging distinctions and collecting new annotations and retraining to accommodate such changes is costly. To address these challenges, we introduce category splitting, a new task where an existing classifier is edited to refine a coarse category into finer subcategories, while preserving accuracy elsewhere. We propose a zero-shot editing method that leverages the latent compositional structure of video classifiers to expose fine-grained distinctions without additional data. We further show that low-shot fine-tuning, while simple, is highly effective and benefits from our zero-shot initialization. Experiments on our new video benchmarks for category splitting demonstrate that our method substantially outperforms vision-language baselines, improving accuracy on the newly split categories without sacrificing performance on the rest. Project page: https://kaitingliu.github.io/Category-Splitting/.

</details>


### [38] [Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face](https://arxiv.org/abs/2602.16569)
*Nicolò Di Domenico,Annalisa Franco,Matteo Ferrara,Davide Maltoni*

Main category: cs.CV

TL;DR: 本文提出了一种基于Arc2Face的新型人脸融合攻击技术，该技术利用身份条件化基础模型从紧凑身份表示合成逼真人脸图像，在多个数据集上展现出与传统地标方法相当的攻击潜力。


<details>
  <summary>Details</summary>
Motivation: 人脸融合攻击是电子身份文档人脸识别系统面临的最严峻威胁之一。这些攻击利用了护照注册程序中的关键漏洞——许多国家在采集面部图像时没有监督的实时捕获过程。现有方法存在局限性，需要更有效的融合技术来测试系统的安全性。

Method: 提出基于Arc2Face的深度学习方法，Arc2Face是一个身份条件化的人脸基础模型，能够从紧凑的身份表示合成逼真的人脸图像。该方法利用深度学习模型在融合生成过程中有效保持和管理身份信息。

Result: 在两个大规模隔离的人脸融合攻击检测数据集上，与多种最先进的融合方法相比，所提方法在融合攻击潜力指标上表现优异。在FEI和ONOT两个新融合人脸数据集上的实验结果显示，该方法达到了与传统地标技术相当的融合攻击潜力，而地标技术传统上被认为是最具挑战性的。

Conclusion: 提出的基于深度学习的方法能够有效保持和管理融合生成过程中的身份信息，其融合攻击潜力与传统地标技术相当，证实了该方法在生成高质量融合人脸方面的有效性，为测试人脸识别系统的安全性提供了有力工具。

Abstract: Face morphing attacks are widely recognized as one of the most challenging threats to face recognition systems used in electronic identity documents. These attacks exploit a critical vulnerability in passport enrollment procedures adopted by many countries, where the facial image is often acquired without a supervised live capture process. In this paper, we propose a novel face morphing technique based on Arc2Face, an identity-conditioned face foundation model capable of synthesizing photorealistic facial images from compact identity representations. We demonstrate the effectiveness of the proposed approach by comparing the morphing attack potential metric on two large-scale sequestered face morphing attack detection datasets against several state-of-the-art morphing methods, as well as on two novel morphed face datasets derived from FEI and ONOT. Experimental results show that the proposed deep learning-based approach achieves a morphing attack potential comparable to that of landmark-based techniques, which have traditionally been regarded as the most challenging. These findings confirm the ability of the proposed method to effectively preserve and manage identity information during the morph generation process.

</details>


### [39] [A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification](https://arxiv.org/abs/2602.16590)
*Qi You,Yitai Cheng,Zichao Zeng,James Haworth*

Main category: cs.CV

TL;DR: CLIP-MHAdapter：一种轻量级CLIP适配方法，通过多头自注意力机制增强对街景图像细粒度属性的分类能力，在保持低计算成本的同时达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 街景图像属性分类是自动驾驶、城市分析等高价值应用的关键任务，但现有方法计算成本高。虽然CLIP等预训练视觉语言模型提供了丰富的图像表示，但现有适配方法主要依赖全局图像嵌入，难以捕捉复杂街景中细粒度的局部属性。

Method: 提出CLIP-MHAdapter，在当前轻量级CLIP适配范式基础上，添加配备多头自注意力机制的瓶颈MLP，作用于补丁标记以建模补丁间依赖关系。该方法仅需约140万个可训练参数。

Result: 在Global StreetScapes数据集的8个属性分类任务上，CLIP-MHAdapter取得了优越或具有竞争力的准确率，达到了新的最先进结果，同时保持了低计算成本。

Conclusion: CLIP-MHAdapter通过多头自注意力机制有效增强了CLIP模型对街景图像细粒度属性的建模能力，在保持轻量级的同时实现了性能提升，为街景图像分析提供了高效解决方案。

Abstract: Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in complex, cluttered street scenes. To address this, we propose CLIP-MHAdapter, a variant of the current lightweight CLIP adaptation paradigm that appends a bottleneck MLP equipped with multi-head self-attention operating on patch tokens to model inter-patch dependencies. With approximately 1.4 million trainable parameters, CLIP-MHAdapter achieves superior or competitive accuracy across eight attribute classification tasks on the Global StreetScapes dataset, attaining new state-of-the-art results while maintaining low computational cost. The code is available at https://github.com/SpaceTimeLab/CLIP-MHAdapter.

</details>


### [40] [Unpaired Image-to-Image Translation via a Self-Supervised Semantic Bridge](https://arxiv.org/abs/2602.16664)
*Jiaming Liu,Felix Petersen,Yunhe Gao,Yabin Zhang,Hyojin Kim,Akshay S. Chaudhari,Yu Sun,Stefano Ermon,Sergios Gatidis*

Main category: cs.CV

TL;DR: SSB框架利用自监督视觉编码器学习外观不变但几何结构保持的表示，为扩散桥模型提供语义先验，实现无需跨域监督的空间保真图像翻译


<details>
  <summary>Details</summary>
Motivation: 现有对抗扩散和扩散反演方法在无配对图像翻译中存在局限：对抗方法需要目标域对抗损失训练，泛化到未见数据受限；扩散反演方法因不完美的噪声潜在表示反演导致翻译保真度低

Method: 提出自监督语义桥（SSB）框架，集成外部语义先验到扩散桥模型中。利用自监督视觉编码器学习外观不变但捕捉几何结构的表示，形成共享潜在空间来条件化扩散桥

Result: 在挑战性医学图像合成任务中，SSB在域内和域外设置下均优于现有方法，并能轻松扩展到高质量文本引导编辑

Conclusion: SSB通过集成语义先验到扩散桥模型，实现了无需跨域监督的空间保真图像翻译，在医学图像合成和文本引导编辑中表现出色

Abstract: Adversarial diffusion and diffusion-inversion methods have advanced unpaired image-to-image translation, but each faces key limitations. Adversarial approaches require target-domain adversarial loss during training, which can limit generalization to unseen data, while diffusion-inversion methods often produce low-fidelity translations due to imperfect inversion into noise-latent representations. In this work, we propose the Self-Supervised Semantic Bridge (SSB), a versatile framework that integrates external semantic priors into diffusion bridge models to enable spatially faithful translation without cross-domain supervision. Our key idea is to leverage self-supervised visual encoders to learn representations that are invariant to appearance changes but capture geometric structure, forming a shared latent space that conditions the diffusion bridges. Extensive experiments show that SSB outperforms strong prior methods for challenging medical image synthesis in both in-domain and out-of-domain settings, and extends easily to high-quality text-guided editing.

</details>


### [41] [PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction](https://arxiv.org/abs/2602.16669)
*Bo Lang,Nirav Savaliya,Zhihao Zheng,Jinglun Feng,Zheng-Hang Yeh,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: 提出一种用于在线高清矢量地图构建的端到端框架，通过联合执行地图实例跟踪和短期预测来解决现有方法的时间不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的方法通常采用随机查询初始化并依赖隐式时间建模，导致构建全局地图时出现时间不一致和不稳定问题。需要一种能够保持时间连续性的在线高清地图构建方法。

Method: 1) 语义感知查询生成器：用空间对齐的语义掩码初始化查询以捕获全局场景上下文；2) 历史栅格化地图记忆：为每个跟踪实例存储细粒度实例级地图；3) 历史地图引导模块：将栅格化地图信息整合到跟踪查询中；4) 短期未来引导模块：基于存储的历史轨迹预测地图实例的即时运动。

Result: 在nuScenes和Argoverse2数据集上的大量实验表明，该方法在保持良好效率的同时优于现有最先进方法。

Conclusion: 提出的框架通过显式历史先验和未来引导，有效解决了时间不一致问题，实现了更稳定和一致的在线高清矢量地图构建。

Abstract: High-definition (HD) maps are crucial to autonomous driving, providing structured representations of road elements to support navigation and planning. However, existing query-based methods often employ random query initialization and depend on implicit temporal modeling, which lead to temporal inconsistencies and instabilities during the construction of a global map. To overcome these challenges, we introduce a novel end-to-end framework for consistent online HD vectorized map construction, which jointly performs map instance tracking and short-term prediction. First, we propose a Semantic-Aware Query Generator that initializes queries with spatially aligned semantic masks to capture scene-level context globally. Next, we design a History Rasterized Map Memory to store fine-grained instance-level maps for each tracked instance, enabling explicit historical priors. A History-Map Guidance Module then integrates rasterized map information into track queries, improving temporal continuity. Finally, we propose a Short-Term Future Guidance module to forecast the immediate motion of map instances based on the stored history trajectories. These predicted future locations serve as hints for tracked instances to further avoid implausible predictions and keep temporal consistency. Extensive experiments on the nuScenes and Argoverse2 datasets demonstrate that our proposed method outperforms state-of-the-art (SOTA) methods with good efficiency.

</details>


### [42] [VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection](https://arxiv.org/abs/2602.16681)
*Yingyuan Yang,Tian Lan,Yifei Gao,Yimeng Lu,Wenjun He,Meng Wang,Chenghao Liu,Chen Zhang*

Main category: cs.CV

TL;DR: VETime是一个统一时间和视觉模态的时间序列异常检测框架，通过细粒度视觉-时间对齐和动态融合，解决了现有基础模型在点异常和上下文异常检测中的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测基础模型面临根本性权衡：一维时间模型能提供细粒度的点定位但缺乏全局上下文视角，而二维视觉模型能捕捉全局模式但存在信息瓶颈，缺乏时间对齐和细粒度检测能力。

Method: 提出VETime框架，包含可逆图像转换和补丁级时间对齐模块建立共享视觉-时间时间线，设计异常窗口对比学习机制和任务自适应多模态融合，自适应整合两种模态的互补感知优势。

Result: 在零样本场景下显著优于最先进模型，实现了更高的定位精度，同时计算开销低于当前基于视觉的方法。

Conclusion: VETime通过统一时间和视觉模态，解决了时间序列异常检测中细粒度定位和全局上下文感知的权衡问题，为TSAD提供了更有效的解决方案。

Abstract: Time-series anomaly detection (TSAD) requires identifying both immediate Point Anomalies and long-range Context Anomalies. However, existing foundation models face a fundamental trade-off: 1D temporal models provide fine-grained pointwise localization but lack a global contextual perspective, while 2D vision-based models capture global patterns but suffer from information bottlenecks due to a lack of temporal alignment and coarse-grained pointwise detection. To resolve this dilemma, we propose VETime, the first TSAD framework that unifies temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. VETime introduces a Reversible Image Conversion and a Patch-Level Temporal Alignment module to establish a shared visual-temporal timeline, preserving discriminative details while maintaining temporal sensitivity. Furthermore, we design an Anomaly Window Contrastive Learning mechanism and a Task-Adaptive Multi-Modal Fusion to adaptively integrate the complementary perceptual strengths of both modalities. Extensive experiments demonstrate that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving superior localization precision with lower computational overhead than current vision-based approaches. Code available at: https://github.com/yyyangcoder/VETime.

</details>


### [43] [Learning Situated Awareness in the Real World](https://arxiv.org/abs/2602.16682)
*Chuhan Li,Ruilin Han,Joy Hsu,Yongyuan Liang,Rajiv Dhawan,Jiajun Wu,Ming-Hsuan Yang,Xin Eric Wang*

Main category: cs.CV

TL;DR: SAW-Bench是一个评估多模态基础模型在真实世界视频中自我中心空间感知能力的新基准，包含786个智能眼镜拍摄的视频和2071个人工标注的问答对，揭示了当前最佳模型与人类表现存在37.66%的差距。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基础模型基准主要关注环境中心的空间关系（物体间关系），而忽视了需要从观察者视角、姿态和运动进行推理的自我中心关系。为了填补这一空白，需要建立评估自我中心空间感知能力的基准。

Method: 使用Ray-Ban Meta（第二代）智能眼镜录制786个真实世界视频，涵盖多样化的室内外环境，人工标注2071个问答对，包含6种不同的感知任务来评估模型的自我中心理解能力。

Result: 评估显示即使最佳模型Gemini 3 Flash与人类表现仍有37.66%的差距。深入分析发现：模型能利用自我中心视频中的部分几何线索，但常常无法推断一致的相机几何，导致系统性空间推理错误。

Conclusion: SAW-Bench作为空间智能基准，超越了被动观察，转向理解物理基础的、以观察者为中心的动态关系，为评估多模态基础模型的自我中心空间感知能力提供了重要工具。

Abstract: A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.

</details>


### [44] [Are Object-Centric Representations Better At Compositional Generalization?](https://arxiv.org/abs/2602.16689)
*Ferdinand Kapl,Amir Mohammad Karimi Mamaghan,Maximilian Seitzer,Karl Henrik Johansson,Carsten Marr,Stefan Bauer,Andrea Dittadi*

Main category: cs.CV

TL;DR: 该研究通过视觉问答基准测试发现，在视觉丰富场景中，面向对象的表示方法在困难的组合泛化任务上表现更优，而密集表示仅在简单任务上占优且需要更多计算资源。


<details>
  <summary>Details</summary>
Motivation: 组合泛化是人类认知的基础能力，但对机器学习仍是挑战。虽然面向对象表示被认为能支持这种泛化，但在视觉丰富环境中的系统性证据有限。研究旨在评估不同视觉编码器在未见过的物体属性组合上的泛化能力。

Method: 在三个受控视觉世界（CLEVRTex、Super-CLEVR、MOVi-C）上建立视觉问答基准测试。使用DINOv2和SigLIP2作为基础模型及其面向对象对应版本，严格控制训练数据多样性、样本量、表示大小、下游模型容量和计算资源等变量进行公平比较。

Result: 1) 面向对象方法在困难的组合泛化设置中表现更优；2) 原始密集表示仅在简单设置中超越面向对象方法，但通常需要显著更多的下游计算；3) 面向对象模型样本效率更高，用更少图像实现更强泛化，而密集编码器只有在足够数据和多样性时才能赶上或超越。

Conclusion: 当数据集大小、训练数据多样性或下游计算资源任一受限时，面向对象表示能提供更强的组合泛化能力。这为在资源受限场景中选择视觉表示方法提供了实证依据。

Abstract: Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.

</details>


### [45] [Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning](https://arxiv.org/abs/2602.16702)
*Mingjia Shi,Yinhan He,Yaochen Zhu,Jundong Li*

Main category: cs.CV

TL;DR: 本文提出SAP（显著性感知原则选择）方法，通过在高层次推理原则上操作而非令牌级轨迹，使视觉语言模型能够在推理时重新参考视觉证据，减少早期视觉基础错误累积，并支持多路径推理。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在推理时面临挑战：视觉输入仅在生成开始时提供一次，而文本推理是自回归生成的，导致推理逐渐被文本主导，早期视觉基础错误会累积。此外，推理时的视觉基础引导通常粗糙且嘈杂，难以在长文本推理中进行有效控制。

Method: 提出SAP（显著性感知原则选择）方法：1）在高层次推理原则上操作而非令牌级轨迹，实现噪声反馈下的稳定离散生成控制；2）允许后续推理步骤在需要时重新参考视觉证据；3）支持多路径推理，并行探索多样化的推理行为。该方法无需额外训练，模型无关且无需数据。

Result: 实证结果表明，在可比令牌生成预算下，SAP实现了有竞争力的性能，特别是在减少物体幻觉方面。相比CoT风格的顺序长推理，SAP产生更稳定的推理和更低的响应延迟。

Conclusion: SAP通过在高层次推理原则上操作，使视觉语言模型能够在推理过程中重新参考视觉证据，有效解决了视觉输入单次提供导致的错误累积问题，同时支持多路径推理，为视觉语言模型的推理时间计算扩展提供了有效方法。

Abstract: Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.

</details>


### [46] [TeCoNeRV: Leveraging Temporal Coherence for Compressible Neural Representations for Videos](https://arxiv.org/abs/2602.16711)
*Namitha Padmanabhan,Matthew Gwilliam,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: TeCoNeRV提出了一种基于超网络的隐式神经表示视频压缩方法，通过空间-时间分解、残差存储和时间相干性正则化，显著降低了内存需求，提高了压缩质量和速度。


<details>
  <summary>Details</summary>
Motivation: 现有的隐式神经表示(INR)视频压缩方法存在扩展性问题：每个视频需要单独训练INR，高分辨率下编码效率低；超网络方法虽然速度快但质量差、压缩率低、内存需求大。

Method: 1) 空间-时间分解：将视频片段分解为patch tubelets，减少预训练内存开销20倍；2) 残差存储方案：仅存储连续片段表示之间的差异，减少比特流大小；3) 时间相干性正则化：鼓励权重空间变化与视频内容相关。

Result: 在UVG数据集上，480p和720p分辨率下PSNR分别提升2.47dB和5.35dB，比特率降低36%，编码速度提升1.5-3倍。首次在480p、720p和1080p分辨率上展示超网络方法的结果。

Conclusion: TeCoNeRV通过创新的分解、存储和正则化技术，解决了超网络视频压缩的内存、质量和效率问题，为高分辨率视频压缩提供了可行的解决方案。

Abstract: Implicit Neural Representations (INRs) have recently demonstrated impressive performance for video compression. However, since a separate INR must be overfit for each video, scaling to high-resolution videos while maintaining encoding efficiency remains a significant challenge. Hypernetwork-based approaches predict INR weights (hyponetworks) for unseen videos at high speeds, but with low quality, large compressed size, and prohibitive memory needs at higher resolutions. We address these fundamental limitations through three key contributions: (1) an approach that decomposes the weight prediction task spatially and temporally, by breaking short video segments into patch tubelets, to reduce the pretraining memory overhead by 20$\times$; (2) a residual-based storage scheme that captures only differences between consecutive segment representations, significantly reducing bitstream size; and (3) a temporal coherence regularization framework that encourages changes in the weight space to be correlated with video content. Our proposed method, TeCoNeRV, achieves substantial improvements of 2.47dB and 5.35dB PSNR over the baseline at 480p and 720p on UVG, with 36% lower bitrates and 1.5-3$\times$ faster encoding speeds. With our low memory usage, we are the first hypernetwork approach to demonstrate results at 480p, 720p and 1080p on UVG, HEVC and MCL-JCV. Our project page is available at https://namithap10.github.io/teconerv/ .

</details>
