<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 28]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: GeoPredict是一个几何感知的视觉-语言-动作框架，通过预测3D关键点轨迹和几何信息来增强机器人操作的3D推理能力，在几何密集型任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型大多是反应式和2D中心的，在需要精确3D推理的任务中不可靠，特别是在机器人操作任务中。

Method: 提出GeoPredict框架，包含两个预测模块：1) 轨迹级模块编码运动历史并预测机器人手臂的多步3D关键点轨迹；2) 预测性3D高斯几何模块沿未来关键点轨迹预测工作空间几何。这些模块仅作为训练时的监督，通过基于深度的渲染实现，推理时只需轻量级查询令牌。

Result: 在RoboCasa Human-50、LIBERO和真实世界操作任务上的实验表明，GeoPredict始终优于强大的VLA基线，特别是在几何密集型和空间要求高的场景中。

Conclusion: GeoPredict通过整合预测性几何和运动先验，显著提升了视觉-语言-动作模型在需要精确3D推理的机器人操作任务中的性能。

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [2] [DenseBEV: Transforming BEV Grid Cells into 3D Objects](https://arxiv.org/abs/2512.16818)
*Marius Dähling,Sebastian Krebs,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 提出DenseBEV方法，使用BEV特征单元直接作为锚点进行多摄像头3D目标检测，通过两阶段锚点生成和BEV-NMS解决查询数量过多问题，在nuScenes和Waymo数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统BEV-based transformers使用随机查询作为锚点，近期研究用辅助网络检测补充或替代随机查询。本文提出更直观高效的方法：直接使用BEV特征单元作为锚点，避免额外网络开销。

Method: 1) 使用BEV特征单元直接作为对象查询锚点；2) 提出两阶段锚点生成方法；3) 使用BEV-based Non-Maximum Suppression解决大量查询的注意力缩放问题；4) 集成先验检测进行混合时序建模。

Result: 在nuScenes数据集上NDS和mAP显著提升，小目标检测效果明显，行人检测mAP提升3.8%；在Waymo数据集上达到SOTA性能，LET-mAP为60.7%，比之前最佳提升5.4%。

Conclusion: DenseBEV方法通过直接使用BEV特征作为锚点，提供了一种更直观高效的端到端多摄像头3D目标检测方案，在稀疏BEV网格下仍能取得优异性能，特别适合小目标检测。

Abstract: In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.

</details>


### [3] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 本研究评估了YOLOv8变体在车牌识别和字符识别任务上的性能，提出了一种优化的识别流水线：使用YOLOv8 Nano进行车牌检测，YOLOv8 Small进行字符识别，并引入基于x轴位置的字符排序方法，在保持计算效率的同时实现了高精度。


<details>
  <summary>Details</summary>
Motivation: 在智能交通系统中，车牌识别对于交通管理和车辆监控至关重要。然而，现有方法在多样化环境中的实时准确性和一致性仍然不足，需要开发更高效可靠的解决方案。

Method: 使用两个独立数据集分别训练和评估YOLOv8变体。YOLOv8 Nano用于车牌检测，YOLOv8 Small用于字符识别。提出了一种基于x轴位置的字符排序方法，构建了优化的识别流水线。

Result: YOLOv8 Nano在车牌识别任务上达到0.964的精确度和0.918的mAP50；YOLOv8 Small在字符识别任务上达到0.92的精确度和0.91的mAP50。提出的优化流水线在保持计算效率的同时实现了高精度。

Conclusion: 提出的YOLOv8 Nano（车牌检测）+ YOLOv8 Small（字符识别）流水线为智能交通系统提供了高效准确的解决方案，特别适合边缘设备部署，推动了智慧城市基础设施的发展。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [4] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: 提出一种紧凑的图像到文本架构，仅使用单张正面胸部X光图像生成报告发现部分，通过冻结DINOv3 ViT编码器和GPT-2解码器结合分层解剖注意力机制，在MIMIC-CXR数据集上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的放射学报告生成系统（如MAIRA-2和MedPaLM-M）依赖大规模多模态训练、临床元数据和多个成像视图，资源密集且难以普及。需要开发更紧凑、仅基于单张图像的解决方案。

Method: 结合冻结的DINOv3 ViT编码器和GPT-2解码器，通过分层解剖注意力机制整合肺部和心脏分割掩码。该机制使用分层高斯平滑将注意力偏向临床相关区域，不增加可训练参数。

Result: 在MIMIC-CXR数据集上评估，CheXpert指标显著提升：5种关键病理的Macro-F1提高168%（0.083→0.238），Micro-F1提高146%（0.137→0.337）。14种观察的总体性能提升86%（0.170→0.316）。RadGraph F1结构一致性提高9.7%。

Conclusion: 尽管模型规模小且仅基于图像条件，但解码器级解剖引导能改善空间定位并增强临床相关区域的连贯性，为资源受限环境提供有效的放射学报告生成方案。

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [5] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: OpenTouch是首个野外环境下的自我中心视角全手触觉数据集，包含5.1小时同步的视频-触觉-姿态数据和2900个带详细文本标注的剪辑片段，用于连接视觉感知与物理交互。


<details>
  <summary>Details</summary>
Motivation: 人类手是我们与物理世界的主要接口，但自我中心视角感知很少知道何时、何地以及如何施加接触力。现有的可穿戴触觉传感器稀缺，且没有野外数据集能将第一人称视频与全手触摸对齐。需要弥合视觉感知与物理交互之间的差距。

Method: 提出了OpenTouch数据集，包含同步的视频-触觉-姿态数据，并基于此数据集引入了检索和分类基准测试，探索触觉如何为感知和行动提供基础。

Result: 触觉信号为抓握理解提供了紧凑而强大的线索，增强了跨模态对齐，并能从野外视频查询中可靠地检索。数据集和基准测试已公开发布。

Conclusion: 通过发布这个带标注的视觉-触觉-姿态数据集和基准测试，旨在推进多模态自我中心感知、具身学习和接触丰富的机器人操作研究。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [6] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 本文指出文本到图像（T2I）模型评估存在基准漂移问题，以GenEval为例展示了其随时间推移与人类判断的偏差增大，提出了新基准GenEval 2和改进的评估方法Soft-TIFA来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 自动化T2I模型评估面临挑战：需要法官模型评分，测试提示需对当前T2I模型具有挑战性但对法官不难。这些约束可能导致基准随时间漂移，静态基准法官无法跟上新模型能力。本文发现GenEval这一流行T2I基准已出现显著漂移问题。

Method: 1) 分析GenEval基准的漂移问题，通过大规模人类研究验证其饱和状态；2) 提出新基准GenEval 2，改进原始视觉概念覆盖和组合性；3) 提出Soft-TIFA评估方法，结合视觉原语的判断，相比VQAScore等整体性法官更不易漂移。

Result: GenEval基准已严重漂移，与人类判断的绝对误差高达17.7%，表明其已饱和一段时间。GenEval 2对当前模型更具挑战性，Soft-TIFA评估方法与人类判断更一致，且更不易随时间漂移。

Conclusion: 基准漂移是T2I评估的重要问题，GenEval 2和Soft-TIFA提供了改进方案。但避免基准漂移远非易事，这项工作强调了T2I及相关自动化模型评估基准需要持续审计和改进的重要性。

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [7] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: RePlan是一个用于复杂指令图像编辑的规划-执行框架，通过视觉语言规划器分解指令并定位到目标区域，然后使用扩散编辑器进行精确的多区域并行编辑，无需迭代修复。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑模型在处理指令-视觉复杂性（IV-Complexity）时表现不佳，即当复杂指令遇到杂乱或模糊场景时，模型难以精确执行编辑任务。

Method: 提出RePlan框架：1）视觉语言规划器通过逐步推理分解指令并显式定位到目标区域；2）扩散编辑器使用无需训练的注意力区域注入机制进行编辑；3）采用GRPO强化学习增强规划能力；4）创建IV-Edit基准测试集。

Result: 在IV-Complex设置下，RePlan在区域精度和整体保真度方面持续优于使用更大数据集训练的强基线模型，特别是在细粒度定位和知识密集型编辑任务上表现优异。

Conclusion: RePlan通过规划-执行框架有效解决了复杂指令图像编辑的挑战，实现了精确的多区域并行编辑，为指令-视觉复杂场景下的图像编辑提供了有效解决方案。

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [8] [Pixel Seal: Adversarial-only training for invisible image and video watermarking](https://arxiv.org/abs/2512.16874)
*Tomáš Souček,Pierre Fernandez,Hady Elsahar,Sylvestre-Alvise Rebuffi,Valeriu Lacatusu,Tuan Tran,Tom Sander,Alexandre Mourachko*

Main category: cs.CV

TL;DR: Pixel Seal提出了一种新的图像和视频水印方法，通过对抗性训练、三阶段训练计划和高分辨率适应技术，解决了现有方法在鲁棒性、不可感知性和可扩展性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的水印模型训练困难，现有方法难以平衡鲁棒性和真正不可感知性。现有方法存在三个根本问题：1) 依赖MSE和LPIPS等代理感知损失，无法模拟人类感知，导致可见水印伪影；2) 目标冲突导致优化不稳定，需要大量超参数调优；3) 将模型扩展到高分辨率图像和视频时，水印的鲁棒性和不可感知性降低。

Method: 1) 提出仅对抗性训练范式，消除不可靠的像素级不可感知性损失；2) 引入三阶段训练计划，通过解耦鲁棒性和不可感知性来稳定收敛；3) 通过高分辨率适应解决分辨率差距，采用基于JND的衰减和训练时推理模拟来消除上采样伪影；4) 通过时间水印池化高效适应视频。

Result: Pixel Seal在图像和视频水印方面达到了新的最先进水平。在不同图像类型和各种变换下，对鲁棒性和不可感知性进行了全面评估，显示出明显优于现有方法的改进。模型通过时间水印池化高效适应视频。

Conclusion: Pixel Seal为现实世界图像和视频场景中的可靠来源追溯提供了一个实用且可扩展的解决方案，解决了现有水印方法的关键局限性。

Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.

</details>


### [9] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: ReMeDI-SAM3：一种无需训练的内存增强型SAM3扩展，通过相关性感知内存过滤、分段插值和特征重识别模块，显著提升内窥镜视频中手术器械分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 内窥镜视频中的手术器械分割对计算机辅助干预至关重要，但由于频繁遮挡、快速运动、镜面伪影和长期器械重新进入等挑战，现有方法（如SAM3）在手术场景中表现受限，存在内存更新不区分、固定内存容量和遮挡后身份恢复弱等问题。

Method: 提出ReMeDI-SAM3，包含三个组件：1）相关性感知内存过滤，配备专用的遮挡感知内存存储遮挡前帧；2）分段插值方案，扩展有效内存容量；3）基于特征的重识别模块，通过时间投票实现可靠的遮挡后身份消歧。

Result: 在EndoVis17和EndoVis18数据集上的零样本评估显示，相比原始SAM3分别获得约7%和16%的绝对mcIoU提升，甚至优于先前基于训练的方法。

Conclusion: ReMeDI-SAM3通过内存增强机制有效解决了SAM3在手术场景中的局限性，显著提升了内窥镜视频中手术器械分割的准确性和鲁棒性，特别是在遮挡恢复方面表现优异。

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [10] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: M-PhyGs：从视频中估计多材料复杂自然物体（如花朵）的材料组成和物理参数的新方法


<details>
  <summary>Details</summary>
Motivation: 现有方法假设物体是单一材料、预学习动力学或简单拓扑，而现实物体（如花朵）具有复杂的材料组成和几何结构，需要新的方法来估计其物理材料参数

Method: 提出Multi-material Physical Gaussians (M-PhyGs)，通过级联3D和2D损失函数以及时间小批量处理，从短视频中联合分割物体材料并恢复其连续介质力学参数

Result: 在Phlowers数据集（人与花朵交互视频）上验证了M-PhyGs在多材料物理参数估计任务中的准确性和有效性

Conclusion: M-PhyGs能够准确估计复杂自然物体的多材料物理参数，为现实世界物体的物理理解提供了新方法

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [11] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang,Yao Lu,Lichen Wang,Yunzhe Li,Daiwei Chen,Yunpeng Xu,Yun Fu*

Main category: cs.CV

TL;DR: LinkedOut提出了一种从视频大语言模型中提取世界知识表示的方法，用于视频推荐任务，解决了传统VLLM在部署时的延迟、多视频输入和视觉细节丢失等问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型在视频理解方面表现出色，但在实际部署到下游任务如视频推荐时面临挑战：生成式解码导致高延迟、不支持多视频输入、语言输出丢弃了视觉细节。这些限制源于缺乏既能保留像素级细节又能利用世界知识的表示。

Method: LinkedOut从原始视频帧中提取语义基础、知识感知的token，使用VLLM并受可提示查询和可选辅助模态指导。引入跨层知识融合MoE，从丰富的VLLM特征中选择适当的抽象层次，实现个性化、可解释、低延迟的推荐。

Result: LinkedOut是首个基于VLLM且无需手工标签的视频推荐方法，在标准基准测试中达到最先进结果。可解释性研究和消融实验证实了层多样性和分层融合的好处。

Conclusion: LinkedOut为充分利用VLLM世界知识先验和视觉推理进行下游视觉任务（如推荐）提供了一条实用路径，解决了VLLM在实际部署中的关键限制。

Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

</details>


### [12] [Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation](https://arxiv.org/abs/2512.16893)
*Kaiwen Jiang,Xueting Li,Seonwook Park,Ravi Ramamoorthi,Shalini De Mello,Koki Nagano*

Main category: cs.CV

TL;DR: Instant4D：通过知识蒸馏将2D扩散模型的表达能力注入3D前馈编码器，实现快速、3D一致且表情丰富的肖像动画


<details>
  <summary>Details</summary>
Motivation: 现有2D视频扩散模型虽然质量高，但缺乏3D一致性和速度；而3D面部动画前馈方法虽然3D一致且快速，但表情细节不足。需要结合两者优势，实现既快速又富有表现力的3D一致动画。

Method: 1. 从2D扩散方法中蒸馏知识到前馈编码器；2. 将单张图像转换为解耦的3D可动画表示；3. 采用轻量级局部融合策略替代计算密集的全局融合机制；4. 学习隐式运动表示，摆脱预定义参数模型的限制。

Result: 达到107.31 FPS的动画和姿态控制速度，动画质量与最先进方法相当，在速度和质量之间取得良好平衡，超越了需要牺牲速度换取质量或反之的设计。

Conclusion: Instant4D成功结合了2D扩散模型的表达能力和3D前馈方法的效率，实现了快速、3D一致且富有表现力的肖像动画，适用于数字孪生、远程呈现等实际应用场景。

Abstract: Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d

</details>


### [13] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: FlashPortrait：一种端到端的视频扩散Transformer，能够合成保持身份一致性的无限长度肖像动画，推理速度提升6倍


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的长肖像动画加速方法难以保证身份一致性，需要一种既能保持身份又能加速推理的解决方案

Method: 1. 使用现成提取器计算身份无关的面部表情特征；2. 引入标准化面部表情块，通过均值和方差归一化对齐面部特征与扩散潜在空间；3. 采用动态滑动窗口方案，在重叠区域进行加权混合；4. 基于特定时间步的潜在变化率和扩散层间导数幅度比，使用高阶潜在导数直接预测未来时间步的潜在表示

Result: 在基准测试中，FlashPortrait在质量和数量上都表现出有效性，能够合成身份保持的无限长度视频，推理速度提升高达6倍

Conclusion: FlashPortrait通过创新的面部特征对齐、动态滑动窗口和高阶潜在导数预测，成功解决了长肖像动画中的身份一致性和推理速度问题

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [14] [Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection](https://arxiv.org/abs/2512.16905)
*Kaixin Ding,Yang Zhou,Xi Chen,Miao Yang,Jiarong Ou,Rui Chen,Xin Tao,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Alchemist是一个基于元梯度的自动数据选择框架，用于从大规模文本-图像数据对中选择高质量子集，以提高T2I模型的训练效率和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型（如Imagen、Stable Diffusion、FLUX）的性能受限于训练数据质量。网络爬取和合成图像数据集常包含低质量或冗余样本，导致视觉保真度下降、训练不稳定和计算效率低下。现有方法依赖昂贵的人工筛选或基于单维特征的启发式评分，缺乏针对图像模态的元学习方法。

Method: Alchemist采用基于元梯度的框架，包含两个关键阶段：数据评分和数据剪枝。首先训练一个轻量级评分器，基于梯度信息并增强多粒度感知来估计每个样本的影响力；然后使用Shift-Gsampling策略选择信息丰富的子集进行高效模型训练。

Result: 实验表明，Alchemist在合成和网络爬取数据集上都能持续提升视觉质量和下游性能。使用Alchemist选择的50%数据训练，其性能可以超过使用完整数据集训练的效果。

Conclusion: Alchemist是首个针对T2I模型训练的自动、可扩展、基于元梯度的数据选择框架，能够有效提升数据效率和模型性能。

Abstract: Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.

</details>


### [15] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: VIVA是一个基于指令的视频编辑框架，通过VLM引导编码和奖励优化实现可扩展的视频编辑，解决了现有方法对复杂真实世界指令泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视频编辑方法通常在简单编辑操作的配对数据上训练，这从根本上限制了它们对多样复杂真实世界指令的泛化能力。需要解决这种泛化差距。

Method: 1) 引入基于VLM的指导器，将文本指令、源视频第一帧和可选参考图像编码为视觉基础指令表示；2) 提出后训练阶段Edit-GRPO，将组相对策略优化应用于视频编辑领域；3) 设计数据构建管道，合成生成多样高保真的基本编辑操作视频-指令配对数据。

Result: 大量实验表明，VIVA在指令遵循、泛化能力和编辑质量方面优于最先进的方法。

Conclusion: VIVA通过VLM引导编码和奖励优化，实现了对复杂真实世界指令的更好泛化，为基于指令的视频编辑提供了可扩展的解决方案。

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [16] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: 提出EgoMAN数据集和模型，用于结合语义推理与运动生成的3D手部轨迹预测


<details>
  <summary>Details</summary>
Motivation: 现有3D手部轨迹预测研究存在两个局限：数据集将运动与语义监督分离，模型对推理和动作的关联较弱

Method: 首先构建EgoMAN数据集（大规模第一人称数据集，包含219K 6DoF轨迹和3M结构化QA对），然后提出EgoMAN模型（通过轨迹-令牌接口连接视觉-语言推理和运动生成的推理到运动框架）

Result: 通过渐进式训练对齐推理与运动动态，该方法能生成准确且阶段感知的轨迹，并在真实场景中具有泛化能力

Conclusion: EgoMAN数据集和模型解决了现有研究的局限性，实现了语义推理与运动生成的紧密结合，为交互阶段感知的3D手部轨迹预测提供了有效解决方案

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [17] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: 提出SceneDiff Benchmark（首个多视角变化检测基准）和SceneDiff方法（无需训练的多视角物体变化检测方法），在多个基准上大幅超越现有方法


<details>
  <summary>Details</summary>
Motivation: 解决在不同时间拍摄的同一场景图像/视频中检测物体增减或移动的问题，这对机器人整理、施工进度监控等应用很重要。主要挑战是视角变化可能导致物体被错误识别为变化

Method: 提出SceneDiff方法：无需训练，利用预训练的3D、分割和图像编码模型；将捕获数据在3D中对齐，提取物体区域，比较空间和语义区域特征来检测变化

Result: 在多视角和双视角基准测试中，方法大幅超越现有方法（相对AP提升94%和37.4%）；建立了包含350个多样化视频对、数千个变化物体的SceneDiff Benchmark

Conclusion: 提出的SceneDiff Benchmark填补了多视角变化检测基准的空白，SceneDiff方法通过3D对齐和特征比较有效解决了视角变化带来的挑战，在多个基准上取得显著性能提升

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [18] [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](https://arxiv.org/abs/2512.16909)
*Yuanchen Ju,Yongyuan Liang,Yen-Jen Wang,Nandiraju Gireesh,Yuanliang Ju,Seungjae Lee,Qiao Gu,Elvis Hsieh,Furong Huang,Koushil Sreenath*

Main category: cs.CV

TL;DR: MomaGraph：一个统一的场景图表示，用于家庭移动操作机器人，结合空间-功能关系和部件级交互元素，并提供了数据集、评估基准和训练模型。


<details>
  <summary>Details</summary>
Motivation: 家庭移动操作机器人需要既能导航又能操作，这需要一个紧凑、语义丰富的场景表示。现有方法通常分离空间和功能关系，将场景视为静态快照，忽略对象状态和时间更新，且未关注当前任务最相关的信息。

Method: 提出MomaGraph统一场景表示，整合空间-功能关系和部件级交互元素。创建MomaGraph-Scenes大规模数据集和MomaGraph-Bench评估套件。基于此开发MomaGraph-R1，一个7B视觉语言模型，通过强化学习在MomaGraph-Scenes上训练，预测任务导向场景图并作为零样本任务规划器。

Result: 模型在基准测试中达到71.6%准确率（比最佳基线提升11.4%），在开源模型中达到最先进水平，能泛化到公共基准测试并有效迁移到真实机器人实验。

Conclusion: MomaGraph提供了一个有效的统一场景表示框架，其数据集、评估基准和模型为家庭移动操作机器人的场景理解和任务规划提供了全面解决方案。

Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

</details>


### [19] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: SFTok是一种新型离散图像分词器，通过多步迭代机制和自强制引导视觉重建技术，在64个token的高压缩率下实现了最先进的图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 当前离散分词器在图像重建质量上仍落后于连续分词器，限制了其在多模态系统中的采用。需要开发既能保持计算效率又能提供高质量重建的离散分词器。

Method: 提出SFTok离散分词器，采用多步迭代重建机制，结合自强制引导视觉重建和去偏拟合训练策略，解决了多步过程中的训练-推理不一致性问题。

Result: 在ImageNet上达到rFID=1.21的最优重建质量，在类别到图像生成任务中取得gFID=2.29的卓越性能，仅使用64个token的高压缩率。

Conclusion: SFTok通过创新的多步迭代机制和训练策略，显著提升了离散分词器的图像重建质量，为高分辨率图像生成提供了高效且高质量的tokenization解决方案。

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [20] [Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation](https://arxiv.org/abs/2512.16913)
*Xin Lin,Meixi Song,Dizhe Zhang,Wenxuan Lu,Haodong Li,Bo Du,Ming-Hsuan Yang,Truong Nguyen,Lu Qi*

Main category: cs.CV

TL;DR: 提出全景度量深度基础模型，通过数据循环范式结合多源数据，采用伪标签生成和三阶段优化策略，实现跨场景距离的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有深度估计方法在处理全景图像时难以适应多样化的场景距离（从室内到室外），且缺乏跨域泛化能力。需要构建一个能够处理不同距离范围、适应合成/真实数据差异的全景深度估计基础模型

Method: 1) 数据构建：结合公开数据集、UE5模拟器合成数据、文本到图像模型生成数据、网络真实全景图像；2) 伪标签生成：三阶段管道减少室内/室外和合成/真实数据间的域差距；3) 模型设计：采用DINOv3-Large作为骨干网络，引入可插拔范围掩码头、锐度中心优化和几何中心优化，增强距离鲁棒性和跨视图几何一致性

Result: 在多个基准测试（Stanford2D3D, Matterport3D, Deep360）上表现出色，具有强大的零样本泛化能力，在多样化真实场景中提供稳健且稳定的度量深度预测

Conclusion: 通过数据循环范式和精心设计的优化策略，成功构建了一个能够泛化到多样化场景距离的全景度量深度基础模型，为全景深度估计提供了有效的解决方案

Abstract: In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}

</details>


### [21] [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)
*Guibao Shen,Yihua Du,Wenhang Ge,Jing He,Chirui Chang,Donghao Zhou,Zhen Yang,Luozhou Wang,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出了UniStereo数据集和StereoPilot模型，用于解决单目到立体视频转换中的深度歧义和格式不一致问题，实现了高效高质量的立体视频生成。


<details>
  <summary>Details</summary>
Motivation: 立体显示设备快速增长，但高质量立体视频制作成本高且复杂。现有的多阶段"深度-扭曲-修复"流水线存在误差传播、深度歧义以及平行和汇聚立体格式不一致等问题。

Method: 1) 创建UniStereo数据集，统一包含两种立体格式；2) 提出StereoPilot模型，采用前馈架构直接合成目标视图，无需显式深度图或迭代扩散采样；3) 使用可学习的域切换器和循环一致性损失来适应不同立体格式并提升一致性。

Result: StereoPilot在视觉保真度和计算效率方面显著优于现有最先进方法，能够无缝适应不同立体格式并实现更好的一致性。

Conclusion: 通过统一的UniStereo数据集和高效的StereoPilot模型，解决了单目到立体视频转换的关键挑战，为高质量立体视频生成提供了有效解决方案。

Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.

</details>


### [22] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: AdaTooler-V是一个多模态大语言模型，通过自适应工具使用机制，只在真正需要视觉工具时才调用，减少不必要的推理开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态大语言模型存在盲目使用工具的问题，即使不需要视觉工具也会调用，这增加了推理开销并降低了模型性能。需要一种自适应工具使用的方法来优化模型效率。

Method: 1. 提出AT-GRPO强化学习算法，根据每个样本的工具效益分数自适应调整奖励尺度，鼓励模型只在工具能真正提升性能时才调用。2. 构建两个训练数据集：AdaTooler-V-CoT-100k用于SFT冷启动，AdaTooler-V-300k用于带可验证奖励的强化学习，涵盖单图像、多图像和视频数据。

Result: 在12个基准测试中表现出强大的推理能力，在多样化的视觉推理任务中优于现有方法。AdaTooler-V-7B在V*高分辨率基准上达到89.8%的准确率，超越了商业专有模型GPT-4o和Gemini 1.5 Pro。

Conclusion: AdaTooler-V通过自适应工具使用机制有效解决了多模态大语言模型中盲目调用视觉工具的问题，显著提升了推理效率和性能，在多个基准测试中达到最先进水平。

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [23] [DVGT: Driving Visual Geometry Transformer](https://arxiv.org/abs/2512.16919)
*Sicheng Zuo,Zixun Xie,Wenzhao Zheng,Shaoqing Xu,Fang Li,Shengyin Jiang,Long Chen,Zhi-Xin Yang,Jiwen Lu*

Main category: cs.CV

TL;DR: DVGT是一个用于自动驾驶的视觉几何Transformer，能够从无姿态的多视角图像序列中重建全局密集3D点云地图，无需精确相机参数或外部传感器对齐。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏能够适应不同场景和相机配置的驾驶场景密集几何感知模型，现有方法通常依赖精确的相机参数和外部传感器对齐。

Method: 使用DINO骨干网络提取视觉特征，通过交替的视图内局部注意力、跨视图空间注意力和跨帧时间注意力推断图像间的几何关系，然后解码生成第一帧自车坐标系下的全局点云地图和每帧的自车姿态。

Result: 在nuScenes、OpenScene、Waymo、KITTI、DDAD等多个驾驶数据集上训练，在各种场景下显著优于现有模型。

Conclusion: DVGT无需显式3D几何先验或精确相机参数，能够灵活处理任意相机配置，直接从图像序列预测度量尺度的几何信息，为自动驾驶提供了强大的几何感知解决方案。

Abstract: Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.

</details>


### [24] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: EasyV2V是一个简单有效的基于指令的视频编辑框架，通过数据组合、简化模型设计和统一控制机制，在视频编辑任务上达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 图像编辑技术发展迅速，但视频编辑仍面临一致性、控制和泛化性等挑战。本文旨在探索数据、架构和控制的设计空间，提出一个简单有效的视频编辑框架。

Method: 1) 数据方面：组合现有专家模型构建多样视频对，通过单帧监督和伪对提升图像编辑对到视频，挖掘密集标注的视频片段，添加过渡监督；2) 模型方面：利用预训练文本到视频模型的编辑能力，采用简单的序列拼接条件和轻量LoRA微调；3) 控制方面：通过单一掩码机制统一时空控制，支持可选参考图像。

Result: EasyV2V在视频编辑任务上达到最先进效果，超越了同期和商业系统，支持多种灵活输入（视频+文本、视频+掩码+文本、视频+掩码+参考图像+文本）。

Conclusion: EasyV2V是一个简单而强大的视频编辑框架，通过系统性的数据、模型和控制设计，有效解决了视频编辑中的关键挑战，为基于指令的视频编辑提供了实用解决方案。

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [25] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: AuditDM是一个自动化框架，通过强化学习训练MLLM作为审计员，生成挑战性问题和反事实图像来发现模型失败模式，从而改进模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的多模态大语言模型评估方法缺乏可解释性，且不足以充分揭示模型之间的能力差距。需要一种主动发现和纠正模型失败模式的方法。

Method: 通过强化学习微调MLLM作为审计员，生成最大化目标模型分歧的挑战性问题和反事实图像。审计员发现多样化的可解释示例，这些示例既揭示了模型弱点，又作为无需标注的数据用于模型修正。

Result: 应用于Gemma-3和PaliGemma-2等SOTA模型时，AuditDM发现了20多种不同的失败类型。在这些发现上微调模型后，所有模型在16个基准测试中均得到一致改进，并使一个3B模型超越了其28B对应模型。

Conclusion: 随着数据扩展达到收益递减阶段，有针对性的模型审计为模型诊断和改进提供了有效途径。AuditDM框架能够主动发现和纠正MLLM的失败模式。

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [26] [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/abs/2512.16922)
*Sihan Xu,Ziqiao Ma,Wenhao Chai,Xuweiyi Chen,Weiyang Jin,Joyce Chai,Saining Xie,Stella X. Yu*

Main category: cs.CV

TL;DR: 提出NEPA方法，通过预测未来patch嵌入而非像素或token，实现视觉自监督学习，在ImageNet和ADE20K上取得优秀结果


<details>
  <summary>Details</summary>
Motivation: 受自然语言生成式预训练成功的启发，探索是否可以将相同原则应用于视觉自监督学习，从学习表示转向学习模型

Method: 使用因果掩码和停止梯度，训练Transformer预测未来patch嵌入（Next-Embedding Predictive Autoregression），无需像素重建、离散token、对比损失或任务特定头

Result: 在ImageNet-1K上，ViT-B和ViT-L分别达到83.8%和85.3%的top-1准确率，在ADE20K语义分割上有效迁移

Conclusion: 基于嵌入的生成式预训练为视觉自监督学习提供了简单、可扩展且可能模态无关的替代方案

Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.

</details>


### [27] [Generative Refocusing: Flexible Defocus Control from a Single Image](https://arxiv.org/abs/2512.16923)
*Chun-Wei Tuan Mu,Jia-Bin Huang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 提出Generative Refocusing方法，通过DeblurNet恢复全焦图像，BokehNet生成可控散景，采用半监督训练结合合成数据和真实散景图像，实现单图像重聚焦和文本引导调整。


<details>
  <summary>Details</summary>
Motivation: 景深控制在摄影中很重要，但单图像重聚焦仍然困难。现有方法需要全焦输入、依赖合成数据、孔径控制有限，存在显著缺陷。

Method: 提出两阶段方法：1) DeblurNet从各种输入恢复全焦图像；2) BokehNet生成可控散景。采用半监督训练，结合合成配对数据和未配对的真实散景图像，利用EXIF元数据捕捉真实光学特性。

Result: 在去焦模糊、散景合成和重聚焦基准测试中达到最佳性能。支持文本引导调整和自定义孔径形状。

Conclusion: Generative Refocusing方法克服了现有方法的限制，通过半监督训练有效结合合成和真实数据，实现了高质量的单图像重聚焦和灵活的散景控制。

Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.

</details>


### [28] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas是一个多模态可提示世界事件框架，通过结合文本、轨迹和参考图像实现用户导向的丰富模拟，支持多智能体交互、对象进出、参考引导外观和反直觉事件生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于文本或轨迹控制，缺乏多模态组合能力，无法生成具有语义意图、视觉基础和运动控制的连贯可控世界事件。

Method: 结合轨迹（编码运动、时序和可见性）、自然语言（语义意图）和参考图像（对象身份视觉基础），实现多模态世界事件生成。

Result: 生成的视频不仅具有时间连贯性，还展现出涌现一致性，能保持对象身份和场景的连续性，即使对象暂时消失也能保持一致。

Conclusion: WorldCanvas将世界模型从被动预测器推进为交互式、用户可塑造的模拟器，支持表达性世界事件生成。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>
