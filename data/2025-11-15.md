<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 本文研究了不同规模的视觉语言模型对盲人和低视力用户视频描述质量的影响，评估了500M和2.2B参数的SmolVLM2变体，并提出了两个专门针对BLV可访问性评估的新框架。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然能够理解和生成视频描述，但其高内存、计算和部署需求限制了实际应用，特别是对于依赖详细、上下文感知描述的盲人和低视力用户。

Method: 使用500M和2.2B参数的SmolVLM2变体在两个多样化数据集上评估，引入两个新的评估框架：多上下文BLV框架和导航辅助框架，系统评估四种不同的提示设计策略，并在智能手机上部署模型评估FP32和INT8精度变体。

Result: 评估了模型在空间定向、社交互动、动作事件和环境上下文等方面的表现，以及移动性关键信息的描述质量，同时测试了在资源受限移动设备上的实际性能约束。

Conclusion: 通过系统评估不同规模模型和精度变体在移动设备上的性能，为开发更适合盲人和低视力用户实际需求的视觉语言模型提供了重要见解。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models](https://arxiv.org/abs/2511.10629)
*Aleksandr Razin,Danil Kazantsev,Ilya Makarov*

Main category: cs.CV

TL;DR: 提出Latent Upscaler Adapter (LUA)，在VAE解码前直接在潜在空间进行超分辨率，避免了传统像素空间超分辨率的伪影和额外延迟。


<details>
  <summary>Details</summary>
Motivation: 扩散模型难以扩展到训练分辨率之外，直接高分辨率采样缓慢昂贵，而像素空间超分辨率会在解码后引入伪影和额外延迟。

Method: LUA是一个轻量级模块，在生成器的潜在代码上进行超分辨率，使用共享Swin风格骨干网络和尺度特定的像素洗牌头，支持2倍和4倍缩放。

Result: LUA在感知质量上与基线相当，但解码和上采样时间降低近3倍（1024px生成仅增加0.42秒，而像素空间SR需要1.87秒），且在不同VAE的潜在空间上具有强泛化能力。

Conclusion: LUA提供了实用高效的路径，在保持高保真度的同时实现可扩展的高分辨率图像合成。

Abstract: Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.

</details>


### [3] [Depth Anything 3: Recovering the Visual Space from Any Views](https://arxiv.org/abs/2511.10647)
*Haotong Lin,Sili Chen,Junhao Liew,Donny Y. Chen,Zhenyu Li,Guang Shi,Jiashi Feng,Bingyi Kang*

Main category: cs.CV

TL;DR: Depth Anything 3 (DA3) 是一个能够从任意数量视觉输入中预测空间一致几何的模型，无需已知相机位姿，在多个几何任务上达到新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 追求最小化建模，探索单一普通transformer作为骨干网络是否足够，以及单一深度射线预测目标是否能替代复杂的多任务学习。

Method: 使用师生训练范式，采用单一普通transformer（如vanilla DINO编码器）作为骨干，使用单一深度射线预测目标，无需架构专业化或多任务学习。

Result: 在新建立的视觉几何基准上，DA3在所有任务上都达到了新的最先进水平，相机位姿精度比之前SOTA VGGT平均提升44.3%，几何精度提升25.1%，在单目深度估计上也超越了DA2。

Conclusion: 单一普通transformer骨干和单一深度射线预测目标足以实现高质量的几何预测，DA3在多个几何任务上建立了新的性能标准。

Abstract: We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.

</details>


### [4] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: 提出Self-Consistency Sampling (SCS)方法来解决多模态大语言模型强化学习中轨迹不忠实的问题，通过视觉扰动和轨迹重采样获得一致性分数，在多个基准测试上显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 在多选题设置中，基于结果奖励的强化学习面临一个被忽视的问题：即使推理链错误但猜对选项的轨迹与真正正确推理的轨迹获得相同奖励，这会影响模型学习效果。

Method: SCS方法：(i)引入小的视觉扰动；(ii)对初始轨迹进行重复截断和重采样；通过轨迹间的一致性产生可微的一致性分数，在策略更新时降低不可靠轨迹的权重。

Result: 在Qwen2.5-VL-7B-Instruct上，将SCS集成到RLOO、GRPO和REINFORCE++系列中，在6个多模态基准测试上准确率最高提升7.7个百分点，计算开销可忽略。在Qwen2.5-VL-3B-Instruct和InternVL3-8B上也取得显著提升。

Conclusion: SCS为多模态大语言模型的结果奖励强化学习提供了一个简单通用的解决方案，能有效纠正轨迹不忠实的问题。

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>
