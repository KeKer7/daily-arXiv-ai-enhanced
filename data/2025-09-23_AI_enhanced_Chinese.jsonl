{"id": "2509.16221", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16221", "abs": "https://arxiv.org/abs/2509.16221", "authors": ["Martin Prei\u00df"], "title": "Evaluation of Ensemble Learning Techniques for handwritten OCR Improvement", "comment": null, "summary": "For the bachelor project 2021 of Professor Lippert's research group,\nhandwritten entries of historical patient records needed to be digitized using\nOptical Character Recognition (OCR) methods. Since the data will be used in the\nfuture, a high degree of accuracy is naturally required. Especially in the\nmedical field this has even more importance. Ensemble Learning is a method that\ncombines several machine learning models and is claimed to be able to achieve\nan increased accuracy for existing methods. For this reason, Ensemble Learning\nin combination with OCR is investigated in this work in order to create added\nvalue for the digitization of the patient records. It was possible to discover\nthat ensemble learning can lead to an increased accuracy for OCR, which methods\nwere able to achieve this and that the size of the training data set did not\nplay a role here.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u96c6\u6210\u5b66\u4e60\u4e0eOCR\u7ed3\u5408\u5728\u5386\u53f2\u75c5\u5386\u6570\u5b57\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u96c6\u6210\u5b66\u4e60\u80fd\u63d0\u9ad8OCR\u51c6\u786e\u7387\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u96c6\u5927\u5c0f\u4e0d\u5f71\u54cd\u6548\u679c\u3002", "motivation": "\u4e3a\u6559\u6388\u7814\u7a76\u7ec4\u7684\u672c\u79d1\u9879\u76ee\u6570\u5b57\u5316\u5386\u53f2\u75c5\u5386\u624b\u5199\u8bb0\u5f55\uff0c\u533b\u7597\u9886\u57df\u9700\u8981\u9ad8\u7cbe\u5ea6OCR\uff0c\u96c6\u6210\u5b66\u4e60\u58f0\u79f0\u80fd\u63d0\u9ad8\u73b0\u6709\u65b9\u6cd5\u7684\u51c6\u786e\u7387\u3002", "method": "\u91c7\u7528\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0eOCR\u6280\u672f\uff0c\u7814\u7a76\u5176\u5bf9\u75c5\u5386\u6570\u5b57\u5316\u7684\u4ef7\u503c\u3002", "result": "\u53d1\u73b0\u96c6\u6210\u5b66\u4e60\u80fd\u63d0\u9ad8OCR\u51c6\u786e\u7387\uff0c\u786e\u5b9a\u4e86\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u96c6\u5927\u5c0f\u5bf9\u6b64\u65e0\u5f71\u54cd\u3002", "conclusion": "\u96c6\u6210\u5b66\u4e60\u4e0eOCR\u7ed3\u5408\u80fd\u4e3a\u75c5\u5386\u6570\u5b57\u5316\u521b\u9020\u9644\u52a0\u4ef7\u503c\uff0c\u662f\u6709\u6548\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2509.16343", "categories": ["cs.CV", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.16343", "abs": "https://arxiv.org/abs/2509.16343", "authors": ["Chung-En", "Yu", "Brian Jalaian", "Nathaniel D. Bastian"], "title": "Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute", "comment": null, "summary": "Developing trustworthy intelligent vision systems for high-stakes domains,\n\\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness\nwithout costly retraining. We propose \\textbf{Visual Reasoning Agent (VRA)}, a\ntraining-free, agentic reasoning framework that wraps off-the-shelf\nvision-language models \\emph{and} pure vision systems in a\n\\emph{Think--Critique--Act} loop. While VRA incurs significant additional\ntest-time computation, it achieves up to 40\\% absolute accuracy gains on\nchallenging visual reasoning benchmarks. Future work will optimize query\nrouting and early stopping to reduce inference overhead while preserving\nreliability in vision tasks.", "AI": {"tldr": "VRA\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u667a\u80fd\u89c6\u89c9\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7Think-Critique-Act\u5faa\u73af\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7eaf\u89c6\u89c9\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u5728\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b0\u9ad8\u8fbe40%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u9065\u611f\u548c\u533b\u7597\u8bca\u65ad\uff09\u5f00\u53d1\u53ef\u4fe1\u8d56\u7684\u667a\u80fd\u89c6\u89c9\u7cfb\u7edf\uff0c\u9700\u8981\u5728\u4e0d\u8fdb\u884c\u6602\u8d35\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5e7f\u6cdb\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faVisual Reasoning Agent (VRA)\u6846\u67b6\uff0c\u5c06\u73b0\u6210\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7eaf\u89c6\u89c9\u7cfb\u7edf\u5c01\u88c5\u5728Think-Critique-Act\u5faa\u73af\u4e2d\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u4f7f\u7528\u3002", "result": "VRA\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe40%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5c3d\u7ba1\u589e\u52a0\u4e86\u663e\u8457\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u672a\u6765\u5de5\u4f5c\u5c06\u4f18\u5316\u67e5\u8be2\u8def\u7531\u548c\u65e9\u505c\u673a\u5236\uff0c\u4ee5\u51cf\u5c11\u63a8\u7406\u5f00\u9500\uff0c\u540c\u65f6\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u4fdd\u6301\u53ef\u9760\u6027\u3002"}}
{"id": "2509.16346", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16346", "abs": "https://arxiv.org/abs/2509.16346", "authors": ["Juan Castorena", "E. Louise Loudermilk", "Scott Pokswinski", "Rodman Linn"], "title": "From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR", "comment": null, "summary": "The 3D structure of living and non-living components in ecosystems plays a\ncritical role in determining ecological processes and feedbacks from both\nnatural and human-driven disturbances. Anticipating the effects of wildfire,\ndrought, disease, or atmospheric deposition depends on accurate\ncharacterization of 3D vegetation structure, yet widespread measurement remains\nprohibitively expensive and often infeasible. We introduce ForestGen3D, a novel\ngenerative modeling framework that synthesizes high-fidelity 3D forest\nstructure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on\nconditional denoising diffusion probabilistic models (DDPMs) trained on\nco-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate\nTLS-like 3D point clouds conditioned on sparse ALS observations, effectively\nreconstructing occluded sub-canopy detail at scale. To ensure ecological\nplausibility, we introduce a geometric containment prior based on the convex\nhull of ALS observations and provide theoretical and empirical guarantees that\ngenerated structures remain spatially consistent. We evaluate ForestGen3D at\ntree, plot, and landscape scales using real-world data from mixed conifer\necosystems, and show that it produces high-fidelity reconstructions that\nclosely match TLS references in terms of geometric similarity and biophysical\nmetrics, such as tree height, DBH, crown diameter and crown volume.\nAdditionally, we demonstrate that the containment property can serve as a\npractical proxy for generation quality in settings where TLS ground truth is\nunavailable. Our results position ForestGen3D as a scalable tool for ecological\nmodeling, wildfire simulation, and structural fuel characterization in ALS-only\nenvironments.", "AI": {"tldr": "ForestGen3D\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u5efa\u6a21\u6846\u67b6\uff0c\u80fd\u591f\u4ec5\u4f7f\u7528\u822a\u7a7a\u6fc0\u5149\u96f7\u8fbe\uff08ALS\uff09\u8f93\u5165\u5408\u6210\u9ad8\u4fdd\u771f\u5ea6\u76843D\u68ee\u6797\u7ed3\u6784\uff0c\u901a\u8fc7\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u91cd\u5efa\u88ab\u906e\u6321\u7684\u51a0\u5c42\u4e0b\u7ec6\u8282\u3002", "motivation": "\u751f\u6001\u7cfb\u7edf\u4e2d\u76843D\u7ed3\u6784\u5bf9\u751f\u6001\u8fc7\u7a0b\u548c\u5e72\u6270\u54cd\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u89c4\u6a21\u6d4b\u91cf3D\u690d\u88ab\u7ed3\u6784\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4ece\u7a00\u758fALS\u6570\u636e\u91cd\u5efa\u8be6\u7ec63D\u7ed3\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPMs\uff09\uff0c\u4f7f\u7528\u914d\u51c6\u7684ALS/TLS\u6570\u636e\u8bad\u7ec3\uff0c\u5f15\u5165\u57fa\u4e8eALS\u89c2\u6d4b\u51f8\u5305\u7684\u51e0\u4f55\u5305\u542b\u5148\u9a8c\u6765\u786e\u4fdd\u751f\u6001\u5408\u7406\u6027\u3002", "result": "\u5728\u6df7\u5408\u9488\u53f6\u6797\u751f\u6001\u7cfb\u7edf\u4e2d\uff0cForestGen3D\u5728\u6811\u6728\u3001\u6837\u5730\u548c\u666f\u89c2\u5c3a\u5ea6\u4e0a\u4ea7\u751f\u9ad8\u4fdd\u771f\u91cd\u5efa\uff0c\u5728\u51e0\u4f55\u76f8\u4f3c\u6027\u548c\u751f\u7269\u7269\u7406\u6307\u6807\uff08\u5982\u6811\u9ad8\u3001\u80f8\u5f84\u3001\u51a0\u5e45\u7b49\uff09\u4e0a\u4e0eTLS\u53c2\u8003\u6570\u636e\u9ad8\u5ea6\u5339\u914d\u3002", "conclusion": "ForestGen3D\u6210\u4e3a\u5728\u4ec5\u4f7f\u7528ALS\u6570\u636e\u73af\u5883\u4e2d\u8fdb\u884c\u751f\u6001\u5efa\u6a21\u3001\u91ce\u706b\u6a21\u62df\u548c\u7ed3\u6784\u71c3\u6599\u8868\u5f81\u7684\u53ef\u6269\u5c55\u5de5\u5177\uff0c\u5176\u5305\u542b\u5c5e\u6027\u53ef\u5728\u6ca1\u6709TLS\u5730\u9762\u771f\u5b9e\u6570\u636e\u65f6\u4f5c\u4e3a\u751f\u6210\u8d28\u91cf\u7684\u5b9e\u7528\u4ee3\u7406\u3002"}}
{"id": "2509.16363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16363", "abs": "https://arxiv.org/abs/2509.16363", "authors": ["Hrishikesh Sharma"], "title": "Introducing Resizable Region Packing Problem in Image Generation, with a Heuristic Solution", "comment": null, "summary": "The problem of image data generation in computer vision has traditionally\nbeen a harder problem to solve, than discriminative problems. Such data\ngeneration entails placing relevant objects of appropriate sizes each, at\nmeaningful location in a scene canvas. There have been two classes of popular\napproaches to such generation: graphics based, and generative models-based.\nOptimization problems are known to lurk in the background for both these\nclasses of approaches. In this paper, we introduce a novel, practically useful\nmanifestation of the classical Bin Packing problem in the context of generation\nof synthetic image data. We conjecture that the newly introduced problem,\nResizable Anchored Region Packing(RARP) Problem, is NP-hard, and provide\ndetailed arguments about our conjecture. As a first solution, we present a\nnovel heuristic algorithm that is generic enough and therefore scales and packs\narbitrary number of arbitrary-shaped regions at arbitrary locations, into an\nimage canvas. The algorithm follows greedy approach to iteratively pack region\npairs in a careful way, while obeying the optimization constraints. The\nalgorithm is validated by an implementation that was used to generate a\nlarge-scale synthetic anomaly detection dataset, with highly varying degree of\nbin packing parameters per image sample i.e. RARP instance. Visual inspection\nof such data and checking of the correctness of each solution proves the\neffectiveness of our algorithm. With generative modeling being on rise in deep\nlearning, and synthetic data generation poised to become mainstream, we expect\nthat the newly introduced problem will be valued in the imaging scientific\ncommunity.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u95ee\u9898\u2014\u2014\u53ef\u8c03\u6574\u951a\u5b9a\u533a\u57df\u6253\u5305\u95ee\u9898\uff08RARP\uff09\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5408\u6210\u56fe\u50cf\u6570\u636e\u751f\u6210\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u8d2a\u5fc3\u542f\u53d1\u5f0f\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2aNP\u96be\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u5728\u751f\u6210\u5927\u89c4\u6a21\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u56fe\u50cf\u6570\u636e\u751f\u6210\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u662f\u4e00\u4e2a\u6bd4\u5224\u522b\u95ee\u9898\u66f4\u96be\u89e3\u51b3\u7684\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u5305\u62ec\u57fa\u4e8e\u56fe\u5f62\u5b66\u548c\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4f46\u90fd\u5b58\u5728\u4f18\u5316\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5728\u56fe\u50cf\u753b\u5e03\u4e0a\u6709\u610f\u4e49\u5730\u653e\u7f6e\u9002\u5f53\u5927\u5c0f\u7684\u76f8\u5173\u5bf9\u8c61\u8fd9\u4e00\u6838\u5fc3\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d2a\u5fc3\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u80fd\u591f\u901a\u7528\u5730\u7f29\u653e\u548c\u6253\u5305\u4efb\u610f\u6570\u91cf\u7684\u4efb\u610f\u5f62\u72b6\u533a\u57df\u5230\u56fe\u50cf\u753b\u5e03\u4e2d\u3002\u7b97\u6cd5\u901a\u8fc7\u4ed4\u7ec6\u5730\u8fed\u4ee3\u6253\u5305\u533a\u57df\u5bf9\uff0c\u540c\u65f6\u9075\u5b88\u4f18\u5316\u7ea6\u675f\u3002", "result": "\u901a\u8fc7\u5b9e\u73b0\u8be5\u7b97\u6cd5\u751f\u6210\u4e86\u5927\u89c4\u6a21\u5408\u6210\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u56fe\u50cf\u6837\u672c\u5177\u6709\u9ad8\u5ea6\u53d8\u5316\u7684\u6253\u5305\u53c2\u6570\u3002\u89c6\u89c9\u68c0\u67e5\u548c\u89e3\u51b3\u65b9\u6848\u6b63\u786e\u6027\u9a8c\u8bc1\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u6a21\u578b\u7684\u5174\u8d77\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u5373\u5c06\u6210\u4e3a\u4e3b\u6d41\uff0c\u65b0\u5f15\u5165\u7684RARP\u95ee\u9898\u9884\u8ba1\u5c06\u5728\u56fe\u50cf\u79d1\u5b66\u793e\u533a\u4e2d\u5f97\u5230\u91cd\u89c6\uff0c\u4e3a\u89e3\u51b3\u56fe\u50cf\u6570\u636e\u751f\u6210\u4e2d\u7684\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.16382", "categories": ["cs.CV", "cs.LG", "eess.IV", "I.2.1; I.5.2"], "pdf": "https://arxiv.org/pdf/2509.16382", "abs": "https://arxiv.org/abs/2509.16382", "authors": ["Saurabh Saini", "Kapil Ahuja", "Marc C. Steinbach", "Thomas Wick"], "title": "Accurate Thyroid Cancer Classification using a Novel Binary Pattern Driven Local Discrete Cosine Transform Descriptor", "comment": "15 Pages, 7 Figures, 5 Tables", "summary": "In this study, we develop a new CAD system for accurate thyroid cancer\nclassification with emphasis on feature extraction. Prior studies have shown\nthat thyroid texture is important for segregating the thyroid ultrasound images\ninto different classes. Based upon our experience with breast cancer\nclassification, we first conjuncture that the Discrete Cosine Transform (DCT)\nis the best descriptor for capturing textural features. Thyroid ultrasound\nimages are particularly challenging as the gland is surrounded by multiple\ncomplex anatomical structures leading to variations in tissue density. Hence,\nwe second conjuncture the importance of localization and propose that the Local\nDCT (LDCT) descriptor captures the textural features best in this context.\nAnother disadvantage of complex anatomy around the thyroid gland is scattering\nof ultrasound waves resulting in noisy and unclear textures. Hence, we third\nconjuncture that one image descriptor is not enough to fully capture the\ntextural features and propose the integration of another popular texture\ncapturing descriptor (Improved Local Binary Pattern, ILBP) with LDCT. ILBP is\nknown to be noise resilient as well. We term our novel descriptor as Binary\nPattern Driven Local Discrete Cosine Transform (BPD-LDCT). Final classification\nis carried out using a non-linear SVM. The proposed CAD system is evaluated on\nthe only two publicly available thyroid cancer datasets, namely TDID and AUITD.\nThe evaluation is conducted in two stages. In Stage I, thyroid nodules are\ncategorized as benign or malignant. In Stage II, the malignant cases are\nfurther sub-classified into TI-RADS (4) and TI-RADS (5). For Stage I\nclassification, our proposed model demonstrates exceptional performance of\nnearly 100% on TDID and 97% on AUITD. In Stage II classification, the proposed\nmodel again attains excellent classification of close to 100% on TDID and 99%\non AUITD.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u7532\u72b6\u817a\u764cCAD\u7cfb\u7edf\uff0c\u91cd\u70b9\u5728\u4e8e\u7279\u5f81\u63d0\u53d6\u3002\u63d0\u51fa\u4e86BPD-LDCT\uff08\u4e8c\u8fdb\u5236\u6a21\u5f0f\u9a71\u52a8\u7684\u5c40\u90e8\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff09\u63cf\u8ff0\u7b26\uff0c\u7ed3\u5408LDCT\u548cILBP\u6765\u6355\u83b7\u7eb9\u7406\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u975e\u7ebf\u6027SVM\u8fdb\u884c\u5206\u7c7b\u3002\u5728TDID\u548cAUITD\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u63a5\u8fd1100%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u7532\u72b6\u817a\u8d85\u58f0\u56fe\u50cf\u5206\u7c7b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7532\u72b6\u817a\u5468\u56f4\u590d\u6742\u7684\u89e3\u5256\u7ed3\u6784\u5bfc\u81f4\u7ec4\u7ec7\u5bc6\u5ea6\u53d8\u5316\u548c\u8d85\u58f0\u6ce2\u6563\u5c04\uff0c\u4ea7\u751f\u566a\u58f0\u548c\u6a21\u7cca\u7eb9\u7406\u3002\u57fa\u4e8e\u4e73\u817a\u764c\u5206\u7c7b\u7684\u7ecf\u9a8c\uff0c\u7814\u7a76\u8005\u8ba4\u4e3a\u9700\u8981\u66f4\u597d\u7684\u7eb9\u7406\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u6765\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002", "method": "1. \u63d0\u51faBPD-LDCT\u63cf\u8ff0\u7b26\uff0c\u7ed3\u5408\u5c40\u90e8\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08LDCT\uff09\u548c\u6539\u8fdb\u7684\u5c40\u90e8\u4e8c\u8fdb\u5236\u6a21\u5f0f\uff08ILBP\uff09\uff1b2. LDCT\u7528\u4e8e\u6355\u83b7\u5c40\u90e8\u7eb9\u7406\u7279\u5f81\uff1b3. ILBP\u7528\u4e8e\u566a\u58f0\u9c81\u68d2\u6027\uff1b4. \u4f7f\u7528\u975e\u7ebf\u6027SVM\u8fdb\u884c\u6700\u7ec8\u5206\u7c7b\uff1b5. \u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff08TDID\u548cAUITD\uff09\u4e0a\u8fdb\u884c\u4e24\u9636\u6bb5\u8bc4\u4f30\u3002", "result": "\u7b2c\u4e00\u9636\u6bb5\uff08\u826f\u6076\u6027\u5206\u7c7b\uff09\uff1aTDID\u6570\u636e\u96c6\u63a5\u8fd1100%\uff0cAUITD\u6570\u636e\u96c697%\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff08TI-RADS 4\u548c5\u4e9a\u5206\u7c7b\uff09\uff1aTDID\u6570\u636e\u96c6\u63a5\u8fd1100%\uff0cAUITD\u6570\u636e\u96c699%\u3002", "conclusion": "BPD-LDCT\u63cf\u8ff0\u7b26\u5728\u7532\u72b6\u817a\u764c\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u89e3\u5256\u7ed3\u6784\u5e26\u6765\u7684\u6311\u6218\uff0c\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.16415", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16415", "abs": "https://arxiv.org/abs/2509.16415", "authors": ["Zhengri Wu", "Yiran Wang", "Yu Wen", "Zeyu Zhang", "Biao Wu", "Hao Tang"], "title": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes", "comment": null, "summary": "Underwater stereo depth estimation provides accurate 3D geometry for robotics\ntasks such as navigation, inspection, and mapping, offering metric depth from\nlow-cost passive cameras while avoiding the scale ambiguity of monocular\nmethods. However, existing approaches face two critical challenges: (i)\nparameter-efficiently adapting large vision foundation encoders to the\nunderwater domain without extensive labeled data, and (ii) tightly fusing\nglobally coherent but scale-ambiguous monocular priors with locally metric yet\nphotometrically fragile stereo correspondences. To address these challenges, we\npropose StereoAdapter, a parameter-efficient self-supervised framework that\nintegrates a LoRA-adapted monocular foundation encoder with a recurrent stereo\nrefinement module. We further introduce dynamic LoRA adaptation for efficient\nrank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to\nenhance robustness under diverse underwater conditions. Comprehensive\nevaluations on both simulated and real-world benchmarks show improvements of\n6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,\nwhile real-world deployment with the BlueROV2 robot further demonstrates the\nconsistent robustness of our approach. Code:\nhttps://github.com/AIGeeksGroup/StereoAdapter. Website:\nhttps://aigeeksgroup.github.io/StereoAdapter.", "AI": {"tldr": "StereoAdapter\u662f\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u6c34\u4e0b\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\uff0c\u901a\u8fc7LoRA\u9002\u914d\u7684\u5355\u76ee\u57fa\u7840\u7f16\u7801\u5668\u548c\u5faa\u73af\u7acb\u4f53\u7ec6\u5316\u6a21\u5757\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6c34\u4e0b\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u53c2\u6570\u9ad8\u6548\u5730\u9002\u5e94\u5927\u578b\u89c6\u89c9\u57fa\u7840\u7f16\u7801\u5668\u5230\u6c34\u4e0b\u9886\u57df\uff0c\u4ee5\u53ca\u7d27\u5bc6\u878d\u5408\u5168\u5c40\u4e00\u81f4\u4f46\u5c3a\u5ea6\u6a21\u7cca\u7684\u5355\u76ee\u5148\u9a8c\u4e0e\u5c40\u90e8\u5ea6\u91cf\u4f46\u5149\u5ea6\u8106\u5f31\u7684\u7acb\u4f53\u5bf9\u5e94\u5173\u7cfb\u3002", "method": "\u63d0\u51faStereoAdapter\u6846\u67b6\uff0c\u5305\u542bLoRA\u9002\u914d\u7684\u5355\u76ee\u57fa\u7840\u7f16\u7801\u5668\u548c\u5faa\u73af\u7acb\u4f53\u7ec6\u5316\u6a21\u5757\uff0c\u91c7\u7528\u52a8\u6001LoRA\u9002\u914d\u8fdb\u884c\u9ad8\u6548\u79e9\u9009\u62e9\uff0c\u5e76\u5728\u5408\u6210UW-StereoDepth-40K\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728TartanAir\u548cSQUID\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u63d0\u53476.11%\u548c5.12%\uff0cBlueROV2\u673a\u5668\u4eba\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "StereoAdapter\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0b\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u5728\u6027\u80fd\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2509.16421", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16421", "abs": "https://arxiv.org/abs/2509.16421", "authors": ["Aiden Chang", "Celso De Melo", "Stephanie M. Lukin"], "title": "AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead", "comment": "Accepted at NeurIPS 2025, 32 pages, 5 figures", "summary": "Real-time understanding of continuous video streams is essential for\nintelligent agents operating in high-stakes environments, including autonomous\nvehicles, surveillance drones, and disaster response robots. Yet, most existing\nvideo understanding and highlight detection methods assume access to the entire\nvideo during inference, making them unsuitable for online or streaming\nscenarios. In particular, current models optimize for offline summarization,\nfailing to support step-by-step reasoning needed for real-time decision-making.\nWe introduce Aha, an autoregressive highlight detection framework that predicts\nthe relevance of each video frame against a task described in natural language.\nWithout accessing future video frames, Aha utilizes a multimodal\nvision-language model and lightweight, decoupled heads trained on a large,\ncurated dataset of human-centric video labels. To enable scalability, we\nintroduce the Dynamic SinkCache mechanism that achieves constant memory usage\nacross infinite-length streams without degrading performance on standard\nbenchmarks. This encourages the hidden representation to capture high-level\ntask objectives, enabling effective frame-level rankings for informativeness,\nrelevance, and uncertainty with respect to the natural language task. Aha\nachieves state-of-the-art (SOTA) performance on highlight detection benchmarks,\nsurpassing even prior offline, full-context approaches and video-language\nmodels by +5.9% on TVSum and +8.3% on Mr.Hisum in mAP (mean Average Precision).\nWe explore Aha's potential for real-world robotics applications given a\ntask-oriented natural language input and a continuous, robot-centric video.\nBoth experiments demonstrate Aha's potential effectiveness as a real-time\nreasoning module for downstream planning and long-horizon understanding.", "AI": {"tldr": "Aha\u662f\u4e00\u4e2a\u81ea\u56de\u5f52\u9ad8\u4eae\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u89c6\u9891\u6d41\u7406\u89e3\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u9884\u6d4b\u6bcf\u5e27\u89c6\u9891\u7684\u76f8\u5173\u6027\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u5047\u8bbe\u53ef\u4ee5\u8bbf\u95ee\u5b8c\u6574\u89c6\u9891\uff0c\u4e0d\u9002\u7528\u4e8e\u5728\u7ebf\u6216\u6d41\u5f0f\u573a\u666f\u3002\u9700\u8981\u652f\u6301\u5b9e\u65f6\u51b3\u7b56\u7684\u9010\u6b65\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u89e3\u8026\u5934\uff0c\u7ed3\u5408\u52a8\u6001SinkCache\u673a\u5236\u5b9e\u73b0\u6052\u5b9a\u5185\u5b58\u4f7f\u7528\uff0c\u65e0\u9700\u8bbf\u95ee\u672a\u6765\u89c6\u9891\u5e27\u3002", "result": "\u5728TVSum\u548cMr.Hisum\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u63d0\u53475.9%\u548c8.3%\u7684mAP\uff0c\u8d85\u8d8a\u4e4b\u524d\u7684\u6240\u6709\u79bb\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Aha\u5c55\u793a\u4e86\u4f5c\u4e3a\u5b9e\u65f6\u63a8\u7406\u6a21\u5757\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u652f\u6301\u4e0b\u6e38\u89c4\u5212\u548c\u957f\u671f\u7406\u89e3\u4efb\u52a1\u3002"}}
{"id": "2509.16423", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16423", "abs": "https://arxiv.org/abs/2509.16423", "authors": ["Maria Taktasheva", "Lily Goli", "Alessandro Fiorini", "Zhen", "Li", "Daniel Rebain", "Andrea Tagliasacchi"], "title": "3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction", "comment": null, "summary": "Recent advances in radiance fields and novel view synthesis enable creation\nof realistic digital twins from photographs. However, current methods struggle\nwith flat, texture-less surfaces, creating uneven and semi-transparent\nreconstructions, due to an ill-conditioned photometric reconstruction\nobjective. Surface reconstruction methods solve this issue but sacrifice visual\nquality. We propose a novel hybrid 2D/3D representation that jointly optimizes\nconstrained planar (2D) Gaussians for modeling flat surfaces and freeform (3D)\nGaussians for the rest of the scene. Our end-to-end approach dynamically\ndetects and refines planar regions, improving both visual fidelity and\ngeometric accuracy. It achieves state-of-the-art depth estimation on ScanNet++\nand ScanNetv2, and excels at mesh extraction without overfitting to a specific\ncamera model, showing its effectiveness in producing high-quality\nreconstruction of indoor scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u54082D/3D\u9ad8\u65af\u8868\u793a\u65b9\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u7ea6\u675f\u5e73\u9762\u9ad8\u65af\u548c\u81ea\u7531\u5f62\u5f0f\u9ad8\u65af\uff0c\u89e3\u51b3\u5e73\u5766\u65e0\u7eb9\u7406\u8868\u9762\u91cd\u5efa\u95ee\u9898", "motivation": "\u5f53\u524d\u8f90\u5c04\u573a\u65b9\u6cd5\u5728\u5e73\u5766\u65e0\u7eb9\u7406\u8868\u9762\u91cd\u5efa\u6548\u679c\u4e0d\u4f73\uff0c\u4ea7\u751f\u4e0d\u5747\u5300\u548c\u534a\u900f\u660e\u91cd\u5efa\u7ed3\u679c\uff0c\u800c\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u53c8\u727a\u7272\u4e86\u89c6\u89c9\u8d28\u91cf", "method": "\u7aef\u5230\u7aef\u65b9\u6cd5\u52a8\u6001\u68c0\u6d4b\u548c\u4f18\u5316\u5e73\u9762\u533a\u57df\uff0c\u8054\u5408\u4f18\u5316\u7ea6\u675f\u5e73\u9762\uff082D\uff09\u9ad8\u65af\u548c\u81ea\u7531\u5f62\u5f0f\uff083D\uff09\u9ad8\u65af", "result": "\u5728ScanNet++\u548cScanNetv2\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5728\u7f51\u683c\u63d0\u53d6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a\u76f8\u673a\u6a21\u578b", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5ba4\u5185\u573a\u666f\u91cd\u5efa\uff0c\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u51e0\u4f55\u7cbe\u5ea6\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347"}}
{"id": "2509.16429", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16429", "abs": "https://arxiv.org/abs/2509.16429", "authors": ["Itzik Waizman", "Yakov Gusakov", "Itay Benou", "Tammy Riklin Raviv"], "title": "TractoTransformer: Diffusion MRI Streamline Tractography using CNN and Transformer Networks", "comment": null, "summary": "White matter tractography is an advanced neuroimaging technique that\nreconstructs the 3D white matter pathways of the brain from diffusion MRI data.\nIt can be framed as a pathfinding problem aiming to infer neural fiber\ntrajectories from noisy and ambiguous measurements, facing challenges such as\ncrossing, merging, and fanning white-matter configurations. In this paper, we\npropose a novel tractography method that leverages Transformers to model the\nsequential nature of white matter streamlines, enabling the prediction of fiber\ndirections by integrating both the trajectory context and current diffusion MRI\nmeasurements. To incorporate spatial information, we utilize CNNs that extract\nmicrostructural features from local neighborhoods around each voxel. By\ncombining these complementary sources of information, our approach improves the\nprecision and completeness of neural pathway mapping compared to traditional\ntractography models. We evaluate our method with the Tractometer toolkit,\nachieving competitive performance against state-of-the-art approaches, and\npresent qualitative results on the TractoInferno dataset, demonstrating strong\ngeneralization to real-world data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65b0\u578b\u767d\u8d28\u7ea4\u7ef4\u675f\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u7ed3\u5408CNN\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\uff0c\u63d0\u9ad8\u795e\u7ecf\u901a\u8def\u6620\u5c04\u7684\u7cbe\u5ea6\u548c\u5b8c\u6574\u6027", "motivation": "\u767d\u8d28\u7ea4\u7ef4\u675f\u8ffd\u8e2a\u9762\u4e34\u4ea4\u53c9\u3001\u5408\u5e76\u548c\u6247\u5f62\u914d\u7f6e\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u566a\u58f0\u548c\u6a21\u7cca\u6d4b\u91cf\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5", "method": "\u4f7f\u7528Transformer\u5efa\u6a21\u767d\u8d28\u6d41\u7ebf\u7684\u5e8f\u5217\u7279\u6027\uff0c\u7ed3\u5408CNN\u63d0\u53d6\u5c40\u90e8\u5fae\u7ed3\u6784\u7279\u5f81\uff0c\u6574\u5408\u8f68\u8ff9\u4e0a\u4e0b\u6587\u548c\u5f53\u524d\u6269\u6563MRI\u6d4b\u91cf\u6765\u9884\u6d4b\u7ea4\u7ef4\u65b9\u5411", "result": "\u5728Tractometer\u5de5\u5177\u5305\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u5728TractoInferno\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u51fa\u5bf9\u771f\u5b9e\u6570\u636e\u7684\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "\u63d0\u51fa\u7684Transformer-CNN\u6df7\u5408\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u7ea4\u7ef4\u675f\u8ffd\u8e2a\u6a21\u578b\u80fd\u66f4\u7cbe\u786e\u548c\u5b8c\u6574\u5730\u6620\u5c04\u795e\u7ecf\u901a\u8def"}}
{"id": "2509.16436", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16436", "abs": "https://arxiv.org/abs/2509.16436", "authors": ["Zhejia Zhang", "Junjie Wang", "Le Zhang"], "title": "Improved mmFormer for Liver Fibrosis Staging via Missing-Modality Compensation", "comment": null, "summary": "In real-world clinical settings, magnetic resonance imaging (MRI) frequently\nsuffers from missing modalities due to equipment variability or patient\ncooperation issues, which can significantly affect model performance. To\naddress this issue, we propose a multimodal MRI classification model based on\nthe mmFormer architecture with an adaptive module for handling arbitrary\ncombinations of missing modalities. Specifically, this model retains the hybrid\nmodality-specific encoders and the modality-correlated encoder from mmFormer to\nextract consistent lesion features across available modalities. In addition, we\nintegrate a missing-modality compensation module which leverages zero-padding,\nmodality availability masks, and a Delta Function with learnable statistical\nparameters to dynamically synthesize proxy features for recovering missing\ninformation. To further improve prediction performance, we adopt a\ncross-validation ensemble strategy by training multiple models on different\nfolds and applying soft voting during inference. This method is evaluated on\nthe test set of Comprehensive Analysis & Computing of REal-world medical images\n(CARE) 2025 challenge, targeting the Liver Fibrosis Staging (LiFS) task based\non non-contrast dynamic MRI scans including T1-weighted imaging (T1WI),\nT2-weighted imaging (T2WI), and diffusion-weighted imaging (DWI). For Cirrhosis\nDetection and Substantial Fibrosis Detection on in-distribution vendors, our\nmodel obtains accuracies of 66.67%, and 74.17%, and corresponding area under\nthe curve (AUC) scores of 71.73% and 68.48%, respectively.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8emmFormer\u7684\u591a\u6a21\u6001MRI\u5206\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6a21\u5757\u5904\u7406\u4efb\u610f\u7f3a\u5931\u6a21\u6001\u7ec4\u5408\uff0c\u5728CARE 2025\u6311\u6218\u8d5b\u7684\u809d\u7ea4\u7ef4\u5316\u5206\u671f\u4efb\u52a1\u4e2d\u53d6\u5f97\u826f\u597d\u6027\u80fd", "motivation": "\u89e3\u51b3\u4e34\u5e8aMRI\u4e2d\u56e0\u8bbe\u5907\u5dee\u5f02\u6216\u60a3\u8005\u914d\u5408\u95ee\u9898\u5bfc\u81f4\u7684\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027", "method": "\u4fdd\u7559mmFormer\u7684\u6df7\u5408\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\u548c\u6a21\u6001\u76f8\u5173\u7f16\u7801\u5668\uff0c\u96c6\u6210\u7f3a\u5931\u6a21\u6001\u8865\u507f\u6a21\u5757\uff08\u96f6\u586b\u5145\u3001\u6a21\u6001\u53ef\u7528\u6027\u63a9\u7801\u3001\u53ef\u5b66\u4e60\u7edf\u8ba1\u53c2\u6570\u7684Delta\u51fd\u6570\uff09\uff0c\u91c7\u7528\u4ea4\u53c9\u9a8c\u8bc1\u96c6\u6210\u7b56\u7565", "result": "\u5728\u809d\u7ea4\u7ef4\u5316\u5206\u671f\u4efb\u52a1\u4e2d\uff0c\u809d\u786c\u5316\u68c0\u6d4b\u51c6\u786e\u738766.67%\uff08AUC 71.73%\uff09\uff0c\u663e\u8457\u7ea4\u7ef4\u5316\u68c0\u6d4b\u51c6\u786e\u738774.17%\uff08AUC 68.48%\uff09", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406MRI\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2509.16438", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16438", "abs": "https://arxiv.org/abs/2509.16438", "authors": ["Mohamed Eltahir", "Osamah Sarraj", "Abdulrahman Alfrihidi", "Taha Alshatiri", "Mohammed Khurd", "Mohammed Bremoo", "Tanveer Hussain"], "title": "AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks", "comment": "Accepted at ArabicNLP 2025 (EMNLP 2025 workshop)", "summary": "Video-to-text and text-to-video retrieval are dominated by English benchmarks\n(e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet\nArabic remains underserved, lacking localized evaluation metrics. We introduce\na three-stage framework, AutoArabic, utilizing state-of-the-art large language\nmodels (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic,\nreducing the manual revision required by nearly fourfold. The framework\nincorporates an error detection module that automatically flags potential\ntranslation errors with 97% accuracy. Applying the framework to DiDeMo, a video\nretrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent\nArabic descriptions. An analysis of the translation errors is provided and\norganized into an insightful taxonomy to guide future Arabic localization\nefforts. We train a CLIP-style baseline with identical hyperparameters on the\nArabic and English variants of the benchmark, finding a moderate performance\ngap (about 3 percentage points at Recall@1), indicating that Arabic\nlocalization preserves benchmark difficulty. We evaluate three post-editing\nbudgets (zero/ flagged-only/ full) and find that performance improves\nmonotonically with more post-editing, while the raw LLM output (zero-budget)\nremains usable. To ensure reproducibility to other languages, we made the code\navailable at https://github.com/Tahaalshatiri/AutoArabic.", "AI": {"tldr": "AutoArabic\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u975e\u963f\u62c9\u4f2f\u8bed\u89c6\u9891\u68c0\u7d22\u57fa\u51c6\u7ffb\u8bd1\u6210\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u4fee\u8ba2\u5de5\u4f5c\u91cf\uff0c\u5e76\u5f00\u53d1\u4e86\u963f\u62c9\u4f2f\u8bed\u89c6\u9891\u68c0\u7d22\u57fa\u51c6DiDeMo-AR\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u4e3b\u8981\u7531\u82f1\u8bed\u57fa\u51c6\u4e3b\u5bfc\uff0c\u963f\u62c9\u4f2f\u8bed\u7f3a\u4e4f\u672c\u5730\u5316\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u4f7f\u7528LLM\u8fdb\u884c\u7ffb\u8bd1\uff1b2\uff09\u96c6\u6210\u9519\u8bef\u68c0\u6d4b\u6a21\u5757\uff08\u51c6\u786e\u738797%\uff09\uff1b3\uff09\u751f\u6210\u963f\u62c9\u4f2f\u8bed\u57fa\u51c6DiDeMo-AR\uff0c\u5305\u542b40,144\u6761\u6d41\u7545\u963f\u62c9\u4f2f\u8bed\u63cf\u8ff0\u3002", "result": "\u5728\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u57fa\u51c6\u4e0a\u8bad\u7ec3CLIP\u98ce\u683c\u6a21\u578b\uff0c\u53d1\u73b0\u6027\u80fd\u5dee\u8ddd\u7ea6\u4e3a3\u4e2a\u767e\u5206\u70b9\uff08Recall@1\uff09\uff0c\u8868\u660e\u963f\u62c9\u4f2f\u8bed\u672c\u5730\u5316\u4fdd\u6301\u4e86\u57fa\u51c6\u96be\u5ea6\u3002\u4e0d\u540c\u540e\u7f16\u8f91\u9884\u7b97\u4e0b\u6027\u80fd\u5355\u8c03\u63d0\u5347\uff0c\u539f\u59cbLLM\u8f93\u51fa\u4ecd\u53ef\u4f7f\u7528\u3002", "conclusion": "AutoArabic\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u963f\u62c9\u4f2f\u8bed\u89c6\u9891\u68c0\u7d22\u57fa\u51c6\u7684\u7f3a\u4e4f\u95ee\u9898\uff0c\u9519\u8bef\u68c0\u6d4b\u6a21\u5757\u51c6\u786e\u7387\u9ad8\uff0c\u6846\u67b6\u53ef\u590d\u73b0\u5230\u5176\u4ed6\u8bed\u8a00\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.16452", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16452", "abs": "https://arxiv.org/abs/2509.16452", "authors": ["Son Hai Nguyen", "Diwei Wang", "Jinhyeok Jang", "Hyewon Seo"], "title": "KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models", "comment": null, "summary": "Accurate vision-based action recognition is crucial for developing autonomous\nrobots that can operate safely and reliably in complex, real-world\nenvironments. In this work, we advance video-based recognition of indoor daily\nactions for robotic perception by leveraging vision-language models (VLMs)\nenriched with domain-specific knowledge. We adapt a prompt-learning framework\nin which class-level textual descriptions of each action are embedded as\nlearnable prompts into a frozen pre-trained VLM backbone. Several strategies\nfor structuring and encoding these textual descriptions are designed and\nevaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our\nmethod, using only RGB video inputs at test time, achieves over 95\\% accuracy\nand outperforms state-of-the-art approaches. These results highlight the\neffectiveness of knowledge-augmented prompts in enabling robust action\nrecognition with minimal supervision.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u589e\u5f3a\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5ba4\u5185\u65e5\u5e38\u52a8\u4f5c\u8bc6\u522b\uff0c\u5728ETRI-Activity3D\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u5728\u590d\u6742\u771f\u5b9e\u73af\u5883\u4e2d\u5b89\u5168\u53ef\u9760\u8fd0\u884c\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u9700\u8981\u51c6\u786e\u7684\u57fa\u4e8e\u89c6\u89c9\u7684\u52a8\u4f5c\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u91c7\u7528\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u6bcf\u4e2a\u52a8\u4f5c\u7684\u7c7b\u522b\u7ea7\u6587\u672c\u63cf\u8ff0\u4f5c\u4e3a\u53ef\u5b66\u4e60\u63d0\u793a\u5d4c\u5165\u5230\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u6587\u672c\u63cf\u8ff0\u7ed3\u6784\u548c\u7f16\u7801\u7b56\u7565\u3002", "result": "\u5728ETRI-Activity3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528RGB\u89c6\u9891\u8f93\u5165\u5373\u53ef\u8fbe\u5230\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u77e5\u8bc6\u589e\u5f3a\u63d0\u793a\u65b9\u6cd5\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u76d1\u7763\u5b9e\u73b0\u9c81\u68d2\u7684\u52a8\u4f5c\u8bc6\u522b\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.16472", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16472", "abs": "https://arxiv.org/abs/2509.16472", "authors": ["Parth Agarwal", "Sangaa Chatterjee", "Md Faisal Kabir", "Suman Saha"], "title": "Explainable Gait Abnormality Detection Using Dual-Dataset CNN-LSTM Models", "comment": "The paper got accepted in ICMLA-2025. It is a camera-ready version", "summary": "Gait is a key indicator in diagnosing movement disorders, but most models\nlack interpretability and rely on single datasets. We propose a dual-branch\nCNN-LSTM framework a 1D branch on joint-based features from GAVD and a 3D\nbranch on silhouettes from OU-MVLP. Interpretability is provided by SHAP\n(temporal attributions) and Grad-CAM (spatial localization).On held-out sets,\nthe system achieves 98.6% accuracy with strong recall and F1. This approach\nadvances explainable gait analysis across both clinical and biometric domains.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5206\u652fCNN-LSTM\u6846\u67b6\uff0c\u7ed3\u54081D\u5173\u8282\u7279\u5f81\u548c3D\u8f6e\u5ed3\u7279\u5f81\uff0c\u901a\u8fc7SHAP\u548cGrad-CAM\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u6b65\u6001\u5206\u6790\u4e2d\u8fbe\u523098.6%\u51c6\u786e\u7387", "motivation": "\u5f53\u524d\u6b65\u6001\u8bca\u65ad\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u4f9d\u8d56\u5355\u4e00\u6570\u636e\u96c6\uff0c\u9700\u8981\u5f00\u53d1\u8de8\u6570\u636e\u96c6\u7684\u53ef\u89e3\u91ca\u6b65\u6001\u5206\u6790\u65b9\u6cd5", "method": "\u53cc\u5206\u652fCNN-LSTM\u6846\u67b6\uff1a1D\u5206\u652f\u5904\u7406GAVD\u7684\u5173\u8282\u7279\u5f81\uff0c3D\u5206\u652f\u5904\u7406OU-MVLP\u7684\u8f6e\u5ed3\u7279\u5f81\uff0c\u4f7f\u7528SHAP\u8fdb\u884c\u65f6\u95f4\u5f52\u56e0\u548cGrad-CAM\u8fdb\u884c\u7a7a\u95f4\u5b9a\u4f4d", "result": "\u5728\u4fdd\u7559\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523098.6%\u7684\u51c6\u786e\u7387\uff0c\u5177\u6709\u5f3a\u53ec\u56de\u7387\u548cF1\u5206\u6570", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e34\u5e8a\u548c\u751f\u7269\u8bc6\u522b\u9886\u57df\u63a8\u8fdb\u4e86\u53ef\u89e3\u91ca\u7684\u6b65\u6001\u5206\u6790"}}
{"id": "2509.16474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16474", "abs": "https://arxiv.org/abs/2509.16474", "authors": ["Gabrielle Chavez", "Laureano Moro-Velazquez", "Ankur Butala", "Najim Dehak", "Thomas Thebaud"], "title": "Cross-Corpus and Cross-domain Handwriting Assessment of NeuroDegenerative Diseases via Time-Series-to-Image Conversion", "comment": "5 pages, 2 figures, submitted to International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP)", "summary": "Handwriting is significantly affected by neurological disorders (ND) such as\nParkinson's disease (PD) and Alzheimer's disease (AD). Prior works have\nanalyzed handwriting tasks using feature-based approaches or computer-vision\ntechniques, but these methods have struggled to generalize across multiple\ndatasets, particularly between temporal features represented as time-series and\nimages. We propose a framework that leverages both time-series and images of\nhandwriting through a joint classifier, based on a ResNet50 pretrained on\nImageNet-1k. Binary classification experiments demonstrate state-of-the-art\nperformances on existing time-series and image datasets, with significant\nimprovement on specific drawing and writing tasks from the NeuroLogical Signals\n(NLS) dataset. In particular, the proposed model demonstrates improved\nperformance on Draw Clock and Spiral tasks. Additionally, cross-dataset and\nmulti-dataset experiments were consistently able to achieve high F1 scores, up\nto 98 for PD detection, highlighting the potential of the proposed model to\ngeneralize over different forms of handwriting signals, and enhance the\ndetection of motor deficits in ND.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\u7684\u8054\u5408\u5206\u7c7b\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u7b14\u8ff9\u5206\u6790\u68c0\u6d4b\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u5982\u5e15\u91d1\u68ee\u75c5\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7279\u5f81\u6216\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540c\u5f62\u5f0f\u7b14\u8ff9\u6570\u636e\uff08\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\uff09\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u5904\u7406\u591a\u79cd\u7b14\u8ff9\u4fe1\u53f7\u7684\u65b9\u6cd5\u6765\u6539\u5584\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u7684\u8fd0\u52a8\u7f3a\u9677\u68c0\u6d4b\u3002", "method": "\u57fa\u4e8eImageNet-1k\u9884\u8bad\u7ec3\u7684ResNet50\u6784\u5efa\u8054\u5408\u5206\u7c7b\u5668\uff0c\u540c\u65f6\u5229\u7528\u7b14\u8ff9\u7684\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\u6570\u636e\uff0c\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\u5b9e\u9a8c\u3002", "result": "\u5728\u73b0\u6709\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728NLS\u6570\u636e\u96c6\u7684\u753b\u949f\u548c\u87ba\u65cb\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u4ea4\u53c9\u6570\u636e\u96c6\u548c\u591a\u6570\u636e\u96c6\u5b9e\u9a8cF1\u5206\u6570\u9ad8\u8fbe98%\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u6cdb\u5316\u5230\u4e0d\u540c\u5f62\u5f0f\u7684\u7b14\u8ff9\u4fe1\u53f7\uff0c\u6709\u671b\u589e\u5f3a\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u4e2d\u8fd0\u52a8\u7f3a\u9677\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2509.16476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16476", "abs": "https://arxiv.org/abs/2509.16476", "authors": ["Qinyu Chen", "Jiawen Qi"], "title": "Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs", "comment": "11 pages", "summary": "Vision-Language Models (VLMs) deliver impressive performance in understanding\nvisual content with language instructions. However, redundancy in vision tokens\nresults in the degenerated inference efficiency of VLMs, which hinders\nreal-time use on edge consumer devices such as AR/VR devices. Existing\nefficiency methods commonly prune visual tokens using learned saliency, sparse\nattention schedules, or controller policies, but they often require\narchitectural modification or access to intermediate activations. These\npipelines add inference-time modules that increase compute and memory and often\nlead to an accuracy trade-off. Moreover, they also suffer from misalignment\nbetween the prompts and the region of interest in the images. Without human\nguidance, the model may focus on the wrong regions and miss small,\nhigh-frequency details when prompts or scenes change. In this paper, we propose\nGazeVLM, a training-free framework that uses the human eye gaze as a natural\nsupervisory signal to allocate computation where it matters. By extracting\ngaze-driven regions of interest (ROIs) and optionally combining them with a\nlow-resolution global view, GazeVLM mimics fovea-periphery perception to cut\nredundant visual tokens while preserving task-relevant details. We evaluate the\nvisual question answering tasks on Qwen2.5-VL-3B/7B on the VOILA-COCO benchmark\nwith human gaze. Quality of the answer is assessed by GPT-4o pairwise judging\nand a weighted score over coverage, accuracy, details, and fluency. Efficiency\nis measured by token counts and FLOPs. GazeVLM reduces visual tokens by up to\n93.1%, total tokens by up to 59.6%, and FLOPs by 50%, while keeping better\nanswer quality relative to full-resolution baselines. Our results show that\naligning model computation with human gaze offers a simple, plug-and-play path\ntoward efficient VLM inference on consumer devices.", "AI": {"tldr": "GazeVLM\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u5229\u7528\u4eba\u7c7b\u773c\u52a8\u6ce8\u89c6\u4f5c\u4e3a\u81ea\u7136\u76d1\u7763\u4fe1\u53f7\u6765\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u901a\u8fc7\u63d0\u53d6\u6ce8\u89c6\u9a71\u52a8\u7684\u611f\u5174\u8da3\u533a\u57df\u6765\u51cf\u5c11\u5197\u4f59\u89c6\u89c9token\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u76f8\u5173\u7ec6\u8282\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u89c6\u89c9token\u5197\u4f59\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\uff0c\u963b\u788d\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u67b6\u6784\u4fee\u6539\u6216\u4e2d\u95f4\u6fc0\u6d3b\u8bbf\u95ee\uff0c\u589e\u52a0\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\uff0c\u5e76\u4e14\u5b58\u5728\u63d0\u793a\u4e0e\u611f\u5174\u8da3\u533a\u57df\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faGazeVLM\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u4eba\u7c7b\u773c\u52a8\u6ce8\u89c6\u9a71\u52a8\u7684\u611f\u5174\u8da3\u533a\u57df\uff0c\u53ef\u9009\u5730\u7ed3\u5408\u4f4e\u5206\u8fa8\u7387\u5168\u5c40\u89c6\u56fe\uff0c\u6a21\u62df\u4e2d\u592e\u51f9-\u5468\u8fb9\u611f\u77e5\u673a\u5236\u6765\u51cf\u5c11\u5197\u4f59\u89c6\u89c9token\u3002", "result": "\u5728VOILA-COCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGazeVLM\u5c06\u89c6\u89c9token\u51cf\u5c11\u9ad8\u8fbe93.1%\uff0c\u603btoken\u51cf\u5c1159.6%\uff0cFLOPs\u51cf\u5c1150%\uff0c\u540c\u65f6\u76f8\u5bf9\u4e8e\u5168\u5206\u8fa8\u7387\u57fa\u7ebf\u4fdd\u6301\u66f4\u597d\u7684\u7b54\u6848\u8d28\u91cf\u3002", "conclusion": "\u5c06\u6a21\u578b\u8ba1\u7b97\u4e0e\u4eba\u7c7b\u773c\u52a8\u6ce8\u89c6\u5bf9\u9f50\uff0c\u4e3a\u6d88\u8d39\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548VLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16479", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16479", "abs": "https://arxiv.org/abs/2509.16479", "authors": ["Christopher Silver", "Thangarajah Akilan"], "title": "Thermal Imaging-based Real-time Fall Detection using Motion Flow and Attention-enhanced Convolutional Recurrent Architecture", "comment": null, "summary": "Falls among seniors are a major public health issue. Existing solutions using\nwearable sensors, ambient sensors, and RGB-based vision systems face challenges\nin reliability, user compliance, and practicality. Studies indicate that\nstakeholders, such as older adults and eldercare facilities, prefer\nnon-wearable, passive, privacy-preserving, and real-time fall detection systems\nthat require no user interaction. This study proposes an advanced thermal fall\ndetection method using a Bidirectional Convolutional Long Short-Term Memory\n(BiConvLSTM) model, enhanced with spatial, temporal, feature, self, and general\nattention mechanisms. Through systematic experimentation across hundreds of\nmodel variations exploring the integration of attention mechanisms, recurrent\nmodules, and motion flow, we identified top-performing architectures. Among\nthem, BiConvLSTM achieved state-of-the-art performance with a ROC-AUC of\n$99.7\\%$ on the TSF dataset and demonstrated robust results on TF-66, a newly\nemerged, diverse, and privacy-preserving benchmark. These results highlight the\ngeneralizability and practicality of the proposed model, setting new standards\nfor thermal fall detection and paving the way toward deployable,\nhigh-performance solutions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u5411\u5377\u79ef\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08BiConvLSTM\uff09\u7684\u70ed\u6210\u50cf\u8dcc\u5012\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728TSF\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.7%\u7684ROC-AUC\u6027\u80fd\uff0c\u4e3a\u8001\u5e74\u4eba\u8dcc\u5012\u68c0\u6d4b\u63d0\u4f9b\u4e86\u975e\u4fb5\u5165\u5f0f\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u8001\u5e74\u4eba\u8dcc\u5012\u662f\u4e00\u4e2a\u91cd\u5927\u7684\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u53ef\u9760\u6027\u3001\u7528\u6237\u4f9d\u4ece\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002\u5229\u76ca\u76f8\u5173\u8005\u66f4\u503e\u5411\u4e8e\u65e0\u9700\u7528\u6237\u4ea4\u4e92\u3001\u975e\u7a7f\u6234\u5f0f\u3001\u88ab\u52a8\u5f0f\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u5b9e\u65f6\u7684\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u53cc\u5411\u5377\u79ef\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08BiConvLSTM\uff09\u6a21\u578b\uff0c\u589e\u5f3a\u7a7a\u95f4\u3001\u65f6\u95f4\u3001\u7279\u5f81\u3001\u81ea\u6ce8\u610f\u529b\u548c\u901a\u7528\u6ce8\u610f\u529b\u673a\u5236\u3002\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u63a2\u7d22\u6ce8\u610f\u529b\u673a\u5236\u3001\u5faa\u73af\u6a21\u5757\u548c\u8fd0\u52a8\u6d41\u7684\u96c6\u6210\uff0c\u8bc6\u522b\u51fa\u6700\u4f18\u67b6\u6784\u3002", "result": "BiConvLSTM\u5728TSF\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.7%\u7684ROC-AUC\u6027\u80fd\uff0c\u5e76\u5728\u65b0\u51fa\u73b0\u7684\u591a\u6837\u5316\u9690\u79c1\u4fdd\u62a4\u57fa\u51c6TF-66\u4e0a\u8868\u73b0\u51fa\u7a33\u5065\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u6240\u63d0\u51fa\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u70ed\u6210\u50cf\u8dcc\u5012\u68c0\u6d4b\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u4e3a\u53ef\u90e8\u7f72\u7684\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.16483", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16483", "abs": "https://arxiv.org/abs/2509.16483", "authors": ["Xujia Zhang", "Brendan Crowe", "Christoffer Heckman"], "title": "Octree Latent Diffusion for Semantic 3D Scene Generation and Completion", "comment": null, "summary": "The completion, extension, and generation of 3D semantic scenes are an\ninterrelated set of capabilities that are useful for robotic navigation and\nexploration. Existing approaches seek to decouple these problems and solve them\noneoff. Additionally, these approaches are often domain-specific, requiring\nseparate models for different data distributions, e.g. indoor vs. outdoor\nscenes. To unify these techniques and provide cross-domain compatibility, we\ndevelop a single framework that can perform scene completion, extension, and\ngeneration in both indoor and outdoor scenes, which we term Octree Latent\nSemantic Diffusion. Our approach operates directly on an efficient dual octree\ngraph latent representation: a hierarchical, sparse, and memory-efficient\noccupancy structure. This technique disentangles synthesis into two stages: (i)\nstructure diffusion, which predicts binary split signals to construct a coarse\noccupancy octree, and (ii) latent semantic diffusion, which generates semantic\nembeddings decoded by a graph VAE into voxellevel semantic labels. To perform\nsemantic scene completion or extension, our model leverages inference-time\nlatent inpainting, or outpainting respectively. These inference-time methods\nuse partial LiDAR scans or maps to condition generation, without the need for\nretraining or finetuning. We demonstrate highquality structure, coherent\nsemantics, and robust completion from single LiDAR scans, as well as zero-shot\ngeneralization to out-of-distribution LiDAR data. These results indicate that\ncompletion-through-generation in a dual octree graph latent space is a\npractical and scalable alternative to regression-based pipelines for real-world\nrobotic perception tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Octree Latent Semantic Diffusion\u6846\u67b6\uff0c\u7edf\u4e00\u5904\u74063D\u8bed\u4e49\u573a\u666f\u7684\u8865\u5168\u3001\u6269\u5c55\u548c\u751f\u6210\u4efb\u52a1\uff0c\u652f\u6301\u5ba4\u5185\u5916\u573a\u666f\uff0c\u901a\u8fc7\u53cc\u516b\u53c9\u56fe\u6f5c\u5728\u8868\u793a\u548c\u4e24\u9636\u6bb5\u6269\u6563\u8fc7\u7a0b\u5b9e\u73b0\u9ad8\u6548\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c063D\u8bed\u4e49\u573a\u666f\u7684\u8865\u5168\u3001\u6269\u5c55\u548c\u751f\u6210\u95ee\u9898\u89e3\u8026\u5904\u7406\uff0c\u4e14\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u9886\u57df\uff08\u5982\u5ba4\u5185/\u5ba4\u5916\uff09\u9700\u8981\u4e0d\u540c\u6a21\u578b\u3002\u4e3a\u4e86\u7edf\u4e00\u8fd9\u4e9b\u6280\u672f\u5e76\u63d0\u4f9b\u8de8\u57df\u517c\u5bb9\u6027\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u901a\u7528\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u53cc\u516b\u53c9\u56fe\u6f5c\u5728\u8868\u793a\u4f5c\u4e3a\u9ad8\u6548\u7684\u5206\u5c42\u7a00\u758f\u5360\u7528\u7ed3\u6784\u3002\u65b9\u6cd5\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a(i)\u7ed3\u6784\u6269\u6563\u9884\u6d4b\u4e8c\u5206\u4fe1\u53f7\u6784\u5efa\u7c97\u7c92\u5ea6\u5360\u7528\u516b\u53c9\u6811\uff0c(ii)\u6f5c\u5728\u8bed\u4e49\u6269\u6563\u751f\u6210\u8bed\u4e49\u5d4c\u5165\uff0c\u901a\u8fc7\u56feVAE\u89e3\u7801\u4e3a\u4f53\u7d20\u7ea7\u8bed\u4e49\u6807\u7b7e\u3002\u5229\u7528\u63a8\u7406\u65f6\u6f5c\u5728\u4fee\u590d/\u5916\u7ed8\u8fdb\u884c\u573a\u666f\u8865\u5168/\u6269\u5c55\u3002", "result": "\u5c55\u793a\u4e86\u9ad8\u8d28\u91cf\u7684\u7ed3\u6784\u3001\u8fde\u8d2f\u7684\u8bed\u4e49\u548c\u4ece\u5355\u6b21LiDAR\u626b\u63cf\u7684\u9c81\u68d2\u8865\u5168\u80fd\u529b\uff0c\u4ee5\u53ca\u5bf9\u5206\u5e03\u5916LiDAR\u6570\u636e\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5728\u53cc\u516b\u53c9\u56fe\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u57fa\u4e8e\u751f\u6210\u7684\u8865\u5168\u662f\u5b9e\u9645\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u611f\u77e5\u4efb\u52a1\u3002"}}
{"id": "2509.16500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16500", "abs": "https://arxiv.org/abs/2509.16500", "authors": ["Tianyi Yan", "Wencheng Han", "Xia Zhou", "Xueyang Zhang", "Kun Zhan", "Cheng-zhong Xu", "Jianbing Shen"], "title": "RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation", "comment": "NeurIPS 2025", "summary": "Synthetic data is crucial for advancing autonomous driving (AD) systems, yet\ncurrent state-of-the-art video generation models, despite their visual realism,\nsuffer from subtle geometric distortions that limit their utility for\ndownstream perception tasks. We identify and quantify this critical issue,\ndemonstrating a significant performance gap in 3D object detection when using\nsynthetic versus real data. To address this, we introduce Reinforcement\nLearning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusion\nmodels by incorporating rewards from specialized latent-space AD perception\nmodels. Its core components include an efficient Latent-Space Windowing\nOptimization technique for targeted feedback during diffusion, and a\nHierarchical Geometric Reward (HGR) system providing multi-level rewards for\npoint-line-plane alignment, and scene occupancy coherence. To quantify these\ndistortions, we propose GeoScores. Applied to models like DiVE on nuScenes,\nRLGF substantially reduces geometric errors (e.g., VP error by 21\\%, Depth\nerror by 57\\%) and dramatically improves 3D object detection mAP by 12.7\\%,\nnarrowing the gap to real-data performance. RLGF offers a plug-and-play\nsolution for generating geometrically sound and reliable synthetic videos for\nAD development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRLGF\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u51e0\u4f55\u53cd\u9988\u6765\u6539\u8fdb\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u5408\u6210\u6570\u636e\u4e2d\u7684\u51e0\u4f55\u5931\u771f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u867d\u7136\u89c6\u89c9\u903c\u771f\uff0c\u4f46\u5b58\u5728\u7ec6\u5fae\u7684\u51e0\u4f55\u5931\u771f\uff0c\u9650\u5236\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u4e0b\u6e38\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002\u4f7f\u7528\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u57283D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u4e0e\u51e0\u4f55\u53cd\u9988\uff08RLGF\uff09\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u6f5c\u5728\u7a7a\u95f4\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u6a21\u578b\u63d0\u4f9b\u5956\u52b1\u6765\u4f18\u5316\u89c6\u9891\u6269\u6563\u6a21\u578b\u3002\u6838\u5fc3\u7ec4\u4ef6\u5305\u62ec\u9ad8\u6548\u7684\u6f5c\u5728\u7a7a\u95f4\u7a97\u53e3\u4f18\u5316\u6280\u672f\u548c\u5206\u5c42\u51e0\u4f55\u5956\u52b1\u7cfb\u7edf\uff0c\u7528\u4e8e\u70b9-\u7ebf-\u5e73\u9762\u5bf9\u9f50\u548c\u573a\u666f\u5360\u7528\u4e00\u81f4\u6027\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u7684DiVE\u6a21\u578b\u5e94\u7528RLGF\u540e\uff0c\u51e0\u4f55\u8bef\u5dee\u663e\u8457\u51cf\u5c11\uff08VP\u8bef\u5dee\u964d\u4f4e21%\uff0c\u6df1\u5ea6\u8bef\u5dee\u964d\u4f4e57%\uff09\uff0c3D\u76ee\u6807\u68c0\u6d4bmAP\u63d0\u534712.7%\uff0c\u7f29\u5c0f\u4e86\u4e0e\u771f\u5b9e\u6570\u636e\u6027\u80fd\u7684\u5dee\u8ddd\u3002", "conclusion": "RLGF\u63d0\u4f9b\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u751f\u6210\u51e0\u4f55\u6b63\u786e\u4e14\u53ef\u9760\u7684\u81ea\u52a8\u9a7e\u9a76\u5f00\u53d1\u7528\u5408\u6210\u89c6\u9891\u3002"}}
{"id": "2509.16506", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16506", "abs": "https://arxiv.org/abs/2509.16506", "authors": ["Joe Barrow"], "title": "CommonForms: A Large, Diverse Dataset for Form Field Detection", "comment": null, "summary": "This paper introduces CommonForms, a web-scale dataset for form field\ndetection. It casts the problem of form field detection as object detection:\ngiven an image of a page, predict the location and type (Text Input, Choice\nButton, Signature) of form fields. The dataset is constructed by filtering\nCommon Crawl to find PDFs that have fillable elements. Starting with 8 million\ndocuments, the filtering process is used to arrive at a final dataset of\nroughly 55k documents that have over 450k pages. Analysis shows that the\ndataset contains a diverse mixture of languages and domains; one third of the\npages are non-English, and among the 14 classified domains, no domain makes up\nmore than 25% of the dataset.\n  In addition, this paper presents a family of form field detectors,\nFFDNet-Small and FFDNet-Large, which attain a very high average precision on\nthe CommonForms test set. Each model cost less than $500 to train. Ablation\nresults show that high-resolution inputs are crucial for high-quality form\nfield detection, and that the cleaning process improves data efficiency over\nusing all PDFs that have fillable fields in Common Crawl. A qualitative\nanalysis shows that they outperform a popular, commercially available PDF\nreader that can prepare forms. Unlike the most popular commercially available\nsolutions, FFDNet can predict checkboxes in addition to text and signature\nfields. This is, to our knowledge, the first large scale dataset released for\nform field detection, as well as the first open source models. The dataset,\nmodels, and code will be released at https://github.com/jbarrow/commonforms", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CommonForms\u6570\u636e\u96c6\u548cFFDNet\u6a21\u578b\uff0c\u7528\u4e8e\u8868\u5355\u5b57\u6bb5\u68c0\u6d4b\u4efb\u52a1\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u5bf9\u8c61\u68c0\u6d4b\u95ee\u9898", "motivation": "\u9700\u8981\u89e3\u51b3\u8868\u5355\u5b57\u6bb5\u81ea\u52a8\u68c0\u6d4b\u7684\u95ee\u9898\uff0c\u73b0\u6709\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u529f\u80fd\u6709\u9650\u4e14\u7f3a\u4e4f\u5f00\u6e90\u6570\u636e\u96c6\u548c\u6a21\u578b", "method": "\u901a\u8fc7\u8fc7\u6ee4Common Crawl\u4e2d\u7684PDF\u6587\u6863\u6784\u5efa\u6570\u636e\u96c6\uff0c\u63d0\u51faFFDNet-Small\u548cFFDNet-Large\u4e24\u79cd\u6a21\u578b\u67b6\u6784", "result": "FFDNet\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u5f88\u9ad8\u7684\u5e73\u5747\u7cbe\u5ea6\uff0c\u8bad\u7ec3\u6210\u672c\u4f4e\u4e8e500\u7f8e\u5143\uff0c\u4f18\u4e8e\u5546\u4e1aPDF\u9605\u8bfb\u5668", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u8868\u5355\u5b57\u6bb5\u68c0\u6d4b\u6570\u636e\u96c6\u548c\u5f00\u6e90\u6a21\u578b\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90"}}
{"id": "2509.16507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16507", "abs": "https://arxiv.org/abs/2509.16507", "authors": ["Hanting Li", "Huaao Tang", "Jianhong Han", "Tianxiong Zhou", "Jiulong Cui", "Haizhen Xie", "Yan Chen", "Jie Hu"], "title": "OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution", "comment": null, "summary": "Recently, latent diffusion models has demonstrated promising performance in\nreal-world video super-resolution (VSR) task, which can reconstruct\nhigh-quality videos from distorted low-resolution input through multiple\ndiffusion steps. Compared to image super-resolution (ISR), VSR methods needs to\nprocess each frame in a video, which poses challenges to its inference\nefficiency. However, video quality and inference efficiency have always been a\ntrade-off for the diffusion-based VSR methods. In this work, we propose\nOne-Step Diffusion model for real-world Video Super-Resolution, namely\nOS-DiffVSR. Specifically, we devise a novel adjacent frame adversarial training\nparadigm, which can significantly improve the quality of synthetic videos.\nBesides, we devise a multi-frame fusion mechanism to maintain inter-frame\ntemporal consistency and reduce the flicker in video. Extensive experiments on\nseveral popular VSR benchmarks demonstrate that OS-DiffVSR can even achieve\nbetter quality than existing diffusion-based VSR methods that require dozens of\nsampling steps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOS-DiffVSR\uff0c\u4e00\u79cd\u5355\u6b65\u6269\u6563\u6a21\u578b\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684VSR\u65b9\u6cd5\u5728\u89c6\u9891\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u591a\u6b65\u6269\u6563\u8fc7\u7a0b\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u76f8\u90bb\u5e27\u5bf9\u6297\u8bad\u7ec3\u8303\u5f0f\u6765\u63d0\u5347\u5408\u6210\u89c6\u9891\u8d28\u91cf\uff0c\u5e76\u91c7\u7528\u591a\u5e27\u878d\u5408\u673a\u5236\u6765\u4fdd\u6301\u5e27\u95f4\u65f6\u95f4\u4e00\u81f4\u6027\u5e76\u51cf\u5c11\u89c6\u9891\u95ea\u70c1\u3002", "result": "\u5728\u591a\u4e2a\u6d41\u884cVSR\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOS-DiffVSR\u751a\u81f3\u80fd\u591f\u6bd4\u9700\u8981\u6570\u5341\u4e2a\u91c7\u6837\u6b65\u9aa4\u7684\u73b0\u6709\u6269\u6563\u57faVSR\u65b9\u6cd5\u83b7\u5f97\u66f4\u597d\u7684\u8d28\u91cf\u3002", "conclusion": "OS-DiffVSR\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u57faVSR\u65b9\u6cd5\u4e2d\u8d28\u91cf\u4e0e\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5355\u6b65\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u3002"}}
{"id": "2509.16509", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16509", "abs": "https://arxiv.org/abs/2509.16509", "authors": ["Haijin Zeng", "Xuan Lu", "Yurong Zhang", "Yongyong Chen", "Jingyong Su", "Jie Liu"], "title": "SlowFast-SCI: Slow-Fast Deep Unfolding Learning for Spectral Compressive Imaging", "comment": "12 pages", "summary": "Humans learn in two complementary ways: a slow, cumulative process that\nbuilds broad, general knowledge, and a fast, on-the-fly process that captures\nspecific experiences. Existing deep-unfolding methods for spectral compressive\nimaging (SCI) mirror only the slow component-relying on heavy pre-training with\nmany unfolding stages-yet they lack the rapid adaptation needed to handle new\noptical configurations. As a result, they falter on out-of-distribution\ncameras, especially in bespoke spectral setups unseen during training. This\ndepth also incurs heavy computation and slow inference. To bridge this gap, we\nintroduce SlowFast-SCI, a dual-speed framework seamlessly integrated into any\ndeep unfolding network beyond SCI systems. During slow learning, we pre-train\nor reuse a priors-based backbone and distill it via imaging guidance into a\ncompact fast-unfolding model. In the fast learning stage, lightweight\nadaptation modules are embedded within each block and trained self-supervised\nat test time via a dual-domain loss-without retraining the backbone. To the\nbest of our knowledge, SlowFast-SCI is the first test-time adaptation-driven\ndeep unfolding framework for efficient, self-adaptive spectral reconstruction.\nIts dual-stage design unites offline robustness with on-the-fly per-sample\ncalibration-yielding over 70% reduction in parameters and FLOPs, up to 5.79 dB\nPSNR improvement on out-of-distribution data, preserved cross-domain\nadaptability, and a 4x faster adaptation speed. In addition, its modularity\nintegrates with any deep-unfolding network, paving the way for self-adaptive,\nfield-deployable imaging and expanded computational imaging modalities. Code\nand models are available at https://github.com/XuanLu11/SlowFast-SCI.", "AI": {"tldr": "SlowFast-SCI\u662f\u4e00\u4e2a\u53cc\u901f\u5ea6\u6846\u67b6\uff0c\u5c06\u6162\u901f\u7d2f\u79ef\u5b66\u4e60\u548c\u5feb\u901f\u5728\u7ebf\u9002\u5e94\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u5149\u8c31\u538b\u7f29\u6210\u50cf\u7684\u6df1\u5ea6\u5c55\u5f00\u7f51\u7edc\uff0c\u663e\u8457\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\uff0c\u5e76\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5c55\u5f00\u65b9\u6cd5\u4ec5\u6a21\u4eff\u6162\u901f\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4f9d\u8d56\u5927\u91cf\u9884\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u5feb\u901f\u9002\u5e94\u65b0\u5149\u5b66\u914d\u7f6e\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u5728\u5206\u5e03\u5916\u76f8\u673a\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u8ba1\u7b97\u91cf\u5927\u3001\u63a8\u7406\u901f\u5ea6\u6162\u3002", "method": "\u63d0\u51fa\u53cc\u9636\u6bb5\u6846\u67b6\uff1a\u6162\u901f\u5b66\u4e60\u9636\u6bb5\u9884\u8bad\u7ec3\u6216\u91cd\u7528\u57fa\u4e8e\u5148\u9a8c\u7684\u4e3b\u5e72\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u6210\u50cf\u6307\u5bfc\u84b8\u998f\u5230\u7d27\u51d1\u7684\u5feb\u901f\u5c55\u5f00\u6a21\u578b\uff1b\u5feb\u901f\u5b66\u4e60\u9636\u6bb5\u5728\u6bcf\u4e2a\u5757\u4e2d\u5d4c\u5165\u8f7b\u91cf\u7ea7\u9002\u5e94\u6a21\u5757\uff0c\u901a\u8fc7\u53cc\u57df\u635f\u5931\u8fdb\u884c\u81ea\u76d1\u7763\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u4e3b\u5e72\u7f51\u7edc\u3002", "result": "\u5b9e\u73b0\u4e8670%\u4ee5\u4e0a\u7684\u53c2\u6570\u548cFLOPs\u51cf\u5c11\uff0c\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0aPSNR\u63d0\u5347\u9ad8\u8fbe5.79 dB\uff0c\u4fdd\u6301\u8de8\u57df\u9002\u5e94\u6027\uff0c\u9002\u5e94\u901f\u5ea6\u63d0\u53474\u500d\u3002", "conclusion": "SlowFast-SCI\u662f\u9996\u4e2a\u6d4b\u8bd5\u65f6\u9002\u5e94\u9a71\u52a8\u7684\u6df1\u5ea6\u5c55\u5f00\u6846\u67b6\uff0c\u5c06\u79bb\u7ebf\u9c81\u68d2\u6027\u4e0e\u5728\u7ebf\u6821\u51c6\u76f8\u7ed3\u5408\uff0c\u4e3a\u81ea\u9002\u5e94\u6027\u3001\u53ef\u90e8\u7f72\u6210\u50cf\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.16517", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.16517", "abs": "https://arxiv.org/abs/2509.16517", "authors": ["Burak Satar", "Zhixin Ma", "Patrick A. Irawan", "Wilfried A. Mulyawan", "Jing Jiang", "Ee-Peng Lim", "Chong-Wah Ngo"], "title": "Seeing Culture: A Benchmark for Visual Reasoning and Grounding", "comment": "Accepted to EMNLP 2025 Main Conference,\n  https://seeingculture-benchmark.github.io/", "summary": "Multimodal vision-language models (VLMs) have made substantial progress in\nvarious tasks that require a combined understanding of visual and textual\ncontent, particularly in cultural understanding tasks, with the emergence of\nnew cultural datasets. However, these datasets frequently fall short of\nproviding cultural reasoning while underrepresenting many cultures. In this\npaper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural\nreasoning with a novel approach that requires VLMs to reason on culturally rich\nimages in two stages: i) selecting the correct visual option with\nmultiple-choice visual question answering (VQA), and ii) segmenting the\nrelevant cultural artifact as evidence of reasoning. Visual options in the\nfirst stage are systematically organized into three types: those originating\nfrom the same country, those from different countries, or a mixed group.\nNotably, all options are derived from a singular category for each type.\nProgression to the second stage occurs only after a correct visual option is\nchosen. The SCB benchmark comprises 1,065 images that capture 138 cultural\nartifacts across five categories from seven Southeast Asia countries, whose\ndiverse cultures are often overlooked, accompanied by 3,178 questions, of which\n1,093 are unique and meticulously curated by human annotators. Our evaluation\nof various VLMs reveals the complexities involved in cross-modal cultural\nreasoning and highlights the disparity between visual reasoning and spatial\ngrounding in culturally nuanced scenarios. The SCB serves as a crucial\nbenchmark for identifying these shortcomings, thereby guiding future\ndevelopments in the field of cultural reasoning.\nhttps://github.com/buraksatar/SeeingCulture", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Seeing Culture Benchmark (SCB)\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u6587\u5316\u63a8\u7406\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u4e30\u5bcc\u7684\u56fe\u50cf\u4e0a\u8fdb\u884c\u4e24\u9636\u6bb5\u63a8\u7406\uff1a\u591a\u9009\u89c6\u89c9\u95ee\u7b54\u548c\u76f8\u5173\u6587\u5316\u6587\u7269\u7684\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u5316\u6570\u636e\u96c6\u5728\u63d0\u4f9b\u6587\u5316\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u8bb8\u591a\u6587\u5316\u4ee3\u8868\u6027\u4e0d\u8db3\u3002SCB\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u7ecf\u5e38\u88ab\u5ffd\u89c6\u7684\u4e1c\u5357\u4e9a\u6587\u5316\u3002", "method": "SCB\u91c7\u7528\u4e24\u9636\u6bb5\u8bc4\u4f30\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u591a\u9009\u89c6\u89c9\u95ee\u7b54\u9009\u62e9\u6b63\u786e\u7684\u89c6\u89c9\u9009\u9879\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5bf9\u76f8\u5173\u6587\u5316\u6587\u7269\u8fdb\u884c\u5206\u5272\u4f5c\u4e3a\u63a8\u7406\u8bc1\u636e\u3002\u6570\u636e\u96c6\u5305\u542b1,065\u5f20\u56fe\u50cf\uff0c\u6db5\u76d67\u4e2a\u4e1c\u5357\u4e9a\u56fd\u5bb6\u7684138\u79cd\u6587\u5316\u6587\u7269\u3002", "result": "\u5bf9\u5404\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u8de8\u6a21\u6001\u6587\u5316\u63a8\u7406\u7684\u590d\u6742\u6027\uff0c\u5e76\u7a81\u663e\u4e86\u5728\u6587\u5316\u7ec6\u5fae\u573a\u666f\u4e2d\u89c6\u89c9\u63a8\u7406\u4e0e\u7a7a\u95f4\u5b9a\u4f4d\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "SCB\u4f5c\u4e3a\u4e00\u4e2a\u5173\u952e\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u5f53\u524d\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u4e3a\u6587\u5316\u63a8\u7406\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2509.16518", "categories": ["cs.CV", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.16518", "abs": "https://arxiv.org/abs/2509.16518", "authors": ["Sankeerth Durvasula", "Kavya Sreedhar", "Zain Moustafa", "Suraj Kothawade", "Ashish Gondimalla", "Suvinay Subramanian", "Narges Shahidi", "Nandita Vijaykumar"], "title": "FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers", "comment": null, "summary": "Generating realistic videos with diffusion transformers demands significant\ncomputation, with attention layers the central bottleneck; even producing a\nshort clip requires running a transformer over a very long sequence of\nembeddings, e.g., more than 30K embeddings for a 5-second video, incurring\nsignificant latency. Prior work aims to mitigate this bottleneck by exploiting\nsparsity in the attention layers to reduce computation. However, these works\ntypically rely on block-sparse attention, which skips score computation only\nwhen all entries in a block of attention scores (corresponding to M queries and\nM keys, with M = 64 typically) are zero. This coarse-granular skipping of\nattention scores does not fully exploit sparsity in the attention map and\nleaves room for improvement. In this work, we propose FG-Attn, a sparse\nattention mechanism for long-context diffusion transformers that leverages\nsparsity at a fine granularity. Unlike block-sparse attention, which skips\nentire MxM blocks, our approach skips computations at the granularity of Mx1\nslices of the attention map. Each slice is produced by query-key dot products\nbetween a block of query vectors and a single key. To implement our proposed\nsparse attention mechanism, we develop a new efficient bulk-load operation\ncalled asynchronous-gather load. This load operation gathers a sparse set of\nrelevant key-value vectors from memory and arranges them into packed tiles in\nthe GPU's shared memory. Only a sparse set of keys relevant to those queries\nare loaded into shared memory when computing attention for a block of queries,\nin contrast to loading full blocks of key tokens in block-sparse attention. Our\nfine-grained sparse attention, applied to video diffusion models, achieves an\naverage 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average\n1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFG-Attn\uff0c\u4e00\u79cd\u7528\u4e8e\u957f\u4e0a\u4e0b\u6587\u6269\u6563\u53d8\u6362\u5668\u7684\u7ec6\u7c92\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5f02\u6b65\u805a\u96c6\u52a0\u8f7d\u64cd\u4f5c\u5b9e\u73b0\u6bd4\u5757\u7a00\u758f\u6ce8\u610f\u529b\u66f4\u7cbe\u7ec6\u7684\u8ba1\u7b97\u8df3\u8fc7\uff0c\u5728\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e0a\u5b9e\u73b01.41-1.65\u500d\u52a0\u901f\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668\u751f\u6210\u771f\u5b9e\u89c6\u9891\u9700\u8981\u5927\u91cf\u8ba1\u7b97\uff0c\u6ce8\u610f\u529b\u5c42\u662f\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709\u5757\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u8df3\u8fc7\u6574\u4e2aMxM\u5757\u7684\u8ba1\u7b97\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6ce8\u610f\u529b\u56fe\u4e2d\u7684\u7a00\u758f\u6027\u3002", "method": "\u63d0\u51faFG-Attn\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728Mx1\u5207\u7247\u7c92\u5ea6\u4e0a\u8df3\u8fc7\u8ba1\u7b97\uff0c\u5f00\u53d1\u5f02\u6b65\u805a\u96c6\u52a0\u8f7d\u64cd\u4f5c\u6765\u9ad8\u6548\u52a0\u8f7d\u7a00\u758f\u76f8\u5173\u7684\u952e\u503c\u5411\u91cf\u3002", "result": "\u5728\u5355H100 GPU\u4e0a\uff0c\u5bf95\u79d2480p\u89c6\u9891\u5b9e\u73b0\u5e73\u57471.55\u500d\uff08\u6700\u9ad81.65\u500d\uff09\u52a0\u901f\uff0c\u5bf95\u79d2720p\u89c6\u9891\u5b9e\u73b0\u5e73\u57471.41\u500d\uff08\u6700\u9ad81.49\u500d\uff09\u52a0\u901f\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u6bd4\u5757\u7a00\u758f\u6ce8\u610f\u529b\u66f4\u6709\u6548\u5730\u5229\u7528\u6ce8\u610f\u529b\u7a00\u758f\u6027\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2509.16519", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16519", "abs": "https://arxiv.org/abs/2509.16519", "authors": ["Yang Han"], "title": "PM25Vision: A Large-Scale Benchmark Dataset for Visual Estimation of Air Quality", "comment": null, "summary": "We introduce PM25Vision (PM25V), the largest and most comprehensive dataset\nto date for estimating air quality - specifically PM2.5 concentrations - from\nstreet-level images. The dataset contains over 11,114 images matched with\ntimestamped and geolocated PM2.5 readings across 3,261 AQI monitoring stations\nand 11 years, significantly exceeding the scale of previous benchmarks. The\nspatial accuracy of this dataset has reached 5 kilometers, far exceeding the\ncity-level accuracy of many datasets. We describe the data collection,\nsynchronization, and cleaning pipelines, and provide baseline model\nperformances using CNN and transformer architectures. Our dataset is publicly\navailable.", "AI": {"tldr": "PM25Vision (PM25V) \u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u6700\u5168\u9762\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4ece\u8857\u666f\u56fe\u50cf\u4f30\u8ba1\u7a7a\u6c14\u8d28\u91cf\uff08\u7279\u522b\u662fPM2.5\u6d53\u5ea6\uff09\uff0c\u5305\u542b11,114\u5f20\u56fe\u50cf\u4e0ePM2.5\u8bfb\u6570\u5339\u914d\uff0c\u7a7a\u95f4\u7cbe\u5ea6\u8fbe5\u516c\u91cc\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u4e14\u7a7a\u95f4\u7cbe\u5ea6\u4e0d\u8db3\uff08\u591a\u4e3a\u57ce\u5e02\u7ea7\u522b\uff09\uff0c\u9700\u8981\u66f4\u5927\u89c4\u6a21\u3001\u66f4\u9ad8\u7cbe\u5ea6\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u4ece\u8857\u666f\u56fe\u50cf\u51c6\u786e\u4f30\u8ba1PM2.5\u6d53\u5ea6\u7684\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u6570\u636e\u6536\u96c6\u3001\u540c\u6b65\u548c\u6e05\u6d17\u6d41\u7a0b\uff0c\u6536\u96c6\u4e8611\u5e74\u671f\u95f43,261\u4e2aAQI\u76d1\u6d4b\u7ad9\u768411,114\u5f20\u8857\u666f\u56fe\u50cf\u4e0e\u65f6\u95f4\u6233\u548c\u5730\u7406\u4f4d\u7f6e\u5339\u914d\u7684PM2.5\u8bfb\u6570\uff0c\u5e76\u4f7f\u7528CNN\u548cTransformer\u67b6\u6784\u8fdb\u884c\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u521b\u5efa\u4e86\u89c4\u6a21\u663e\u8457\u8d85\u8fc7\u5148\u524d\u57fa\u51c6\u7684\u6570\u636e\u96c6\uff0c\u7a7a\u95f4\u7cbe\u5ea6\u8fbe\u52305\u516c\u91cc\uff0c\u8fdc\u8d85\u8bb8\u591a\u6570\u636e\u96c6\u7684\u57ce\u5e02\u573a\u7ea7\u7cbe\u5ea6\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "PM25V\u6570\u636e\u96c6\u662f\u516c\u5f00\u53ef\u7528\u7684\uff0c\u4e3a\u4ece\u8857\u666f\u56fe\u50cf\u4f30\u8ba1PM2.5\u6d53\u5ea6\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u3001\u9ad8\u7cbe\u5ea6\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2509.16527", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16527", "abs": "https://arxiv.org/abs/2509.16527", "authors": ["Guangze Zheng", "Shijie Lin", "Haobo Zuo", "Si Si", "Ming-Shan Wang", "Changhong Fu", "Jia Pan"], "title": "Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity", "comment": "NeurIPS 2025. Project page: https://george-zhuang.github.io/lbm/", "summary": "This work proposes the Lattice Boltzmann Model (LBM) to learn real-world\npixel dynamicity for visual tracking. LBM decomposes visual representations\ninto dynamic pixel lattices and solves pixel motion states through\ncollision-streaming processes. Specifically, the high-dimensional distribution\nof the target pixels is acquired through a multilayer predict-update network to\nestimate the pixel positions and visibility. The predict stage formulates\nlattice collisions among the spatial neighborhood of target pixels and develops\nlattice streaming within the temporal visual context. The update stage\nrectifies the pixel distributions with online visual representations. Compared\nwith existing methods, LBM demonstrates practical applicability in an online\nand real-time manner, which can efficiently adapt to real-world visual tracking\ntasks. Comprehensive evaluations of real-world point tracking benchmarks such\nas TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of\nlarge-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B\nfurther demonstrates LBM's real-world practicality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u683c\u5b50\u73bb\u5c14\u5179\u66fc\u6a21\u578b\uff08LBM\uff09\u7684\u89c6\u89c9\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u89c6\u89c9\u8868\u793a\u4e3a\u52a8\u6001\u50cf\u7d20\u683c\u5b50\uff0c\u5e76\u5229\u7528\u78b0\u649e-\u6d41\u8fc7\u7a0b\u89e3\u51b3\u50cf\u7d20\u8fd0\u52a8\u72b6\u6001\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5728\u7ebf\u89c6\u89c9\u8ddf\u8e2a\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8ddf\u8e2a\u65b9\u6cd5\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u50cf\u7d20\u52a8\u6001\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u7ebf\u5b9e\u65f6\u9002\u5e94\u590d\u6742\u89c6\u89c9\u8ddf\u8e2a\u4efb\u52a1\u7684\u6709\u6548\u65b9\u6cd5\u3002", "method": "LBM\u901a\u8fc7\u591a\u5c42\u9884\u6d4b-\u66f4\u65b0\u7f51\u7edc\u83b7\u53d6\u76ee\u6807\u50cf\u7d20\u7684\u9ad8\u7ef4\u5206\u5e03\uff0c\u9884\u6d4b\u9636\u6bb5\u5728\u76ee\u6807\u50cf\u7d20\u7684\u7a7a\u95f4\u90bb\u57df\u5185\u8fdb\u884c\u683c\u5b50\u78b0\u649e\uff0c\u5728\u65f6\u95f4\u89c6\u89c9\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u683c\u5b50\u6d41\uff1b\u66f4\u65b0\u9636\u6bb5\u5229\u7528\u5728\u7ebf\u89c6\u89c9\u8868\u793a\u4fee\u6b63\u50cf\u7d20\u5206\u5e03\u3002", "result": "\u5728TAP-Vid\u548cRoboTap\u7b49\u771f\u5b9e\u4e16\u754c\u70b9\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86LBM\u7684\u6548\u7387\uff0c\u5728TAO\u3001BFT\u548cOVT-B\u7b49\u5927\u89c4\u6a21\u5f00\u653e\u4e16\u754c\u7269\u4f53\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "LBM\u5c55\u793a\u4e86\u5728\u5728\u7ebf\u5b9e\u65f6\u65b9\u5f0f\u4e0b\u7684\u5b9e\u9645\u9002\u7528\u6027\uff0c\u80fd\u591f\u9ad8\u6548\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u89c6\u89c9\u8ddf\u8e2a\u4efb\u52a1\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2509.16538", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16538", "abs": "https://arxiv.org/abs/2509.16538", "authors": ["Shubhashis Roy Dipta", "Tz-Ying Wu", "Subarna Tripathi"], "title": "Advancing Reference-free Evaluation of Video Captions with Factual Analysis", "comment": null, "summary": "Video captions offer concise snapshots of actors, objects, and actions within\na video, serving as valuable assets for applications such as question answering\nand event localization. However, acquiring human annotations for video captions\nis costly or even impractical, especially when dealing with diverse video\ndomains. Existing models trained on supervised datasets face challenges in\nevaluating performance across different domains due to the reliance on\nreference-based evaluation protocols, which necessitate ground truth captions.\nThis assumption is unrealistic for evaluating videos in the wild. To address\nthese limitations, we propose a reference-free evaluation framework that does\nnot require ground truth captions, focusing on factual grounding to ensure\naccurate assessment of caption quality. We introduce VC-Inspector, a novel\ncaption quality evaluator that is both reference-free and factually grounded.\nUtilizing large language models, we generate pseudo captions of varying quality\nbased on supervised data, which are subsequently used to train a multimodal\nmodel (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior\nalignment with human judgments on the VATEX-Eval dataset, outperforming\nexisting methods. The performance also generalizes to image caption datasets,\nFlickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos.\nOverall, VC-Inspector offers a scalable and generalizable solution for\nevaluating the factual accuracy of video captions, paving the way for more\neffective and objective assessment methodologies in diverse video domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u53c2\u8003\u57fa\u51c6\u7684\u65e0\u53c2\u8003\u89c6\u9891\u5b57\u5e55\u8bc4\u4f30\u6846\u67b6VC-Inspector\uff0c\u4e13\u6ce8\u4e8e\u4e8b\u5b9e\u51c6\u786e\u6027\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u5b57\u5e55\u7684\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5b57\u5e55\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u53c2\u8003\u5b57\u5e55\uff0c\u8fd9\u5728\u591a\u6837\u5316\u89c6\u9891\u9886\u57df\u4e2d\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u5207\u5b9e\u9645\u3002\u4f20\u7edf\u57fa\u4e8e\u53c2\u8003\u7684\u8bc4\u4f30\u534f\u8bae\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u7684\u5b57\u5e55\u8d28\u91cf\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0d\u540c\u8d28\u91cf\u7684\u4f2a\u5b57\u5e55\u4f5c\u4e3a\u76d1\u7763\u6570\u636e\uff0c\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578bQwen2.5-VL\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff0c\u5b9e\u73b0\u53c2\u8003\u65e0\u5173\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u8bc4\u4f30\u3002", "result": "\u5728VATEX-Eval\u6570\u636e\u96c6\u4e0a\uff0cVC-Inspector\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u56fe\u50cf\u5b57\u5e55\u6570\u636e\u96c6Flickr8K-Expert\u548cFlickr8K-CF\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VC-Inspector\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u591a\u6837\u5316\u89c6\u9891\u9886\u57df\u4e2d\u66f4\u6709\u6548\u548c\u5ba2\u89c2\u7684\u8bc4\u4f30\u65b9\u6cd5\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.16549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16549", "abs": "https://arxiv.org/abs/2509.16549", "authors": ["Zirui Wang", "Jiayi Zhang", "Tianwei Guan", "Yuhan Zhou", "Xingyuan Li", "Minjing Dong", "Jinyuan Liu"], "title": "Efficient Rectified Flow for Image Fusion", "comment": null, "summary": "Image fusion is a fundamental and important task in computer vision, aiming\nto combine complementary information from different modalities to fuse images.\nIn recent years, diffusion models have made significant developments in the\nfield of image fusion. However, diffusion models often require complex\ncomputations and redundant inference time, which reduces the applicability of\nthese methods. To address this issue, we propose RFfusion, an efficient\none-step diffusion model for image fusion based on Rectified Flow. We\nincorporate Rectified Flow into the image fusion task to straighten the\nsampling path in the diffusion model, achieving one-step sampling without the\nneed for additional training, while still maintaining high-quality fusion\nresults. Furthermore, we propose a task-specific variational autoencoder (VAE)\narchitecture tailored for image fusion, where the fusion operation is embedded\nwithin the latent space to further reduce computational complexity. To address\nthe inherent discrepancy between conventional reconstruction-oriented VAE\nobjectives and the requirements of image fusion, we introduce a two-stage\ntraining strategy. This approach facilitates the effective learning and\nintegration of complementary information from multi-modal source images,\nthereby enabling the model to retain fine-grained structural details while\nsignificantly enhancing inference efficiency. Extensive experiments demonstrate\nthat our method outperforms other state-of-the-art methods in terms of both\ninference speed and fusion quality. Code is available at\nhttps://github.com/zirui0625/RFfusion.", "AI": {"tldr": "RFfusion\u662f\u4e00\u79cd\u57fa\u4e8eRectified Flow\u7684\u9ad8\u6548\u4e00\u6b65\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u56fe\u50cf\u878d\u5408\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u76f4\u7ebf\u5316\u91c7\u6837\u8def\u5f84\u5b9e\u73b0\u4e00\u6b65\u91c7\u6837\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u540c\u65f6\u63d0\u51fa\u9488\u5bf9\u56fe\u50cf\u878d\u5408\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u67b6\u6784\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u5e76\u4fdd\u6301\u9ad8\u8d28\u91cf\u878d\u5408\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u878d\u5408\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u3001\u63a8\u7406\u65f6\u95f4\u5197\u4f59\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "1. \u5c06Rectified Flow\u5f15\u5165\u56fe\u50cf\u878d\u5408\u4efb\u52a1\uff0c\u76f4\u7ebf\u5316\u6269\u6563\u6a21\u578b\u91c7\u6837\u8def\u5f84\u5b9e\u73b0\u4e00\u6b65\u91c7\u6837\uff1b2. \u8bbe\u8ba1\u9488\u5bf9\u56fe\u50cf\u878d\u5408\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u5d4c\u5165\u878d\u5408\u64cd\u4f5c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b3. \u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u89e3\u51b3\u4f20\u7edfVAE\u91cd\u5efa\u76ee\u6807\u4e0e\u56fe\u50cf\u878d\u5408\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u901f\u5ea6\u548c\u878d\u5408\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u878d\u5408\u3002", "conclusion": "RFfusion\u901a\u8fc7Rectified Flow\u6280\u672f\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684VAE\u67b6\u6784\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u878d\u5408\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u56fe\u50cf\u878d\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16552", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16552", "abs": "https://arxiv.org/abs/2509.16552", "authors": ["Xiaoyang Yan", "Muleilan Pei", "Shaojie Shen"], "title": "ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting", "comment": null, "summary": "3D occupancy prediction is critical for comprehensive scene understanding in\nvision-centric autonomous driving. Recent advances have explored utilizing 3D\nsemantic Gaussians to model occupancy while reducing computational overhead,\nbut they remain constrained by insufficient multi-view spatial interaction and\nlimited multi-frame temporal consistency. To overcome these issues, in this\npaper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework\nto enhance both spatial and temporal modeling in existing Gaussian-based\npipelines. Specifically, we develop a guidance-informed spatial aggregation\nstrategy within a dual-mode attention mechanism to strengthen spatial\ninteraction in Gaussian representations. Furthermore, we introduce a\ngeometry-aware temporal fusion scheme that effectively leverages historical\ncontext to improve temporal continuity in scene completion. Extensive\nexperiments on the large-scale nuScenes occupancy prediction benchmark showcase\nthat our proposed approach not only achieves state-of-the-art performance but\nalso delivers markedly better temporal consistency compared to existing\nGaussian-based methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u7a7a\u9ad8\u65af\u6cfc\u6e85\uff08ST-GS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6a21\u5f0f\u6ce8\u610f\u529b\u673a\u5236\u548c\u51e0\u4f55\u611f\u77e5\u65f6\u5e8f\u878d\u5408\u65b9\u6848\uff0c\u589e\u5f3a\u57fa\u4e8e\u9ad8\u65af\u65b9\u6cd5\u76843D\u5360\u636e\u9884\u6d4b\u4e2d\u7684\u7a7a\u95f4\u4ea4\u4e92\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u76843D\u8bed\u4e49\u9ad8\u65af\u65b9\u6cd5\u5728\u89c6\u89c9\u4e2d\u5fc3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5efa\u6a21\u5360\u636e\u65f6\uff0c\u5b58\u5728\u591a\u89c6\u56fe\u7a7a\u95f4\u4ea4\u4e92\u4e0d\u8db3\u548c\u591a\u5e27\u65f6\u5e8f\u4e00\u81f4\u6027\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u6307\u5bfc\u4fe1\u606f\u7a7a\u95f4\u805a\u5408\u7b56\u7565\uff08\u53cc\u6a21\u5f0f\u6ce8\u610f\u529b\u673a\u5236\uff09\u6765\u589e\u5f3a\u9ad8\u65af\u8868\u793a\u7684\u7a7a\u95f4\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165\u4e86\u51e0\u4f55\u611f\u77e5\u65f6\u5e8f\u878d\u5408\u65b9\u6848\u6765\u5229\u7528\u5386\u53f2\u4e0a\u4e0b\u6587\u6539\u5584\u573a\u666f\u5b8c\u6210\u7684\u65f6\u5e8f\u8fde\u7eed\u6027\u3002", "result": "\u5728\u5927\u89c4\u6a21nuScenes\u5360\u636e\u9884\u6d4b\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u800c\u4e14\u76f8\u6bd4\u73b0\u6709\u57fa\u4e8e\u9ad8\u65af\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u663e\u8457\u66f4\u597d\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "conclusion": "ST-GS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u65af\u65b9\u6cd5\u4e2d\u7684\u7a7a\u95f4\u548c\u65f6\u5e8f\u5efa\u6a21\u9650\u5236\uff0c\u4e3a\u89c6\u89c9\u4e2d\u5fc3\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2509.16557", "categories": ["cs.CV", "cs.ET", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16557", "abs": "https://arxiv.org/abs/2509.16557", "authors": ["Muhammad Hamza", "Danish Hamid", "Muhammad Tahir Akram"], "title": "Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose", "comment": "21 pages, 8 figures, 7 tables. Preprint of a manuscript submitted to\n  CCF Transactions on Pervasive Computing and Interaction (Springer), currently\n  under review", "summary": "Human-Object Interaction Recognition (HOIR) and user identification play a\ncrucial role in advancing augmented reality (AR)-based personalized assistive\ntechnologies. These systems are increasingly being deployed in high-stakes,\nhuman-centric environments such as aircraft cockpits, aerospace maintenance,\nand surgical procedures. This research introduces I2S (Interact2Sign), a multi\nstage framework designed for unobtrusive user identification through human\nobject interaction recognition, leveraging 3D hand pose analysis in egocentric\nvideos. I2S utilizes handcrafted features extracted from 3D hand poses and per\nforms sequential feature augmentation: first identifying the object class,\nfollowed by HOI recognition, and ultimately, user identification. A\ncomprehensive feature extraction and description process was carried out for 3D\nhand poses, organizing the extracted features into semantically meaningful\ncategories: Spatial, Frequency, Kinematic, Orientation, and a novel descriptor\nintroduced in this work, the Inter-Hand Spatial Envelope (IHSE). Extensive\nablation studies were conducted to determine the most effective combination of\nfeatures. The optimal configuration achieved an impressive average F1-score of\n97.52% for user identification, evaluated on a bimanual object manipulation\ndataset derived from the ARCTIC and H2O datasets. I2S demonstrates\nstate-of-the-art performance while maintaining a lightweight model size of\nunder 4 MB and a fast inference time of 0.1 seconds. These characteristics make\nthe proposed framework highly suitable for real-time, on-device authentication\nin security-critical, AR-based systems.", "AI": {"tldr": "I2S\u662f\u4e00\u4e2a\u591a\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc73D\u624b\u90e8\u59ff\u6001\u5206\u6790\u5b9e\u73b0\u65e0\u5e72\u6270\u7684\u7528\u6237\u8eab\u4efd\u8bc6\u522b\uff0c\u5728AR\u73af\u5883\u4e2d\u8fbe\u523097.52%\u7684F1\u5206\u6570\uff0c\u6a21\u578b\u8f7b\u91cf\u4e14\u63a8\u7406\u5feb\u901f\u3002", "motivation": "\u5728\u98de\u673a\u9a7e\u9a76\u8231\u3001\u822a\u7a7a\u822a\u5929\u7ef4\u62a4\u548c\u624b\u672f\u7b49\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\uff0c\u9700\u8981\u53ef\u9760\u7684\u4eba\u673a\u4ea4\u4e92\u8bc6\u522b\u548c\u7528\u6237\u8eab\u4efd\u9a8c\u8bc1\u6280\u672f\u6765\u652f\u6301AR\u4e2a\u6027\u5316\u8f85\u52a9\u7cfb\u7edf\u3002", "method": "I2S\u91c7\u7528\u591a\u9636\u6bb5\u6846\u67b6\uff1a\u9996\u5148\u63d0\u53d63D\u624b\u90e8\u59ff\u6001\u7684\u624b\u5de5\u7279\u5f81\uff08\u7a7a\u95f4\u3001\u9891\u7387\u3001\u8fd0\u52a8\u5b66\u3001\u65b9\u5411\u7279\u5f81\u548c\u65b0\u578bIHSE\u63cf\u8ff0\u7b26\uff09\uff0c\u7136\u540e\u4f9d\u6b21\u8fdb\u884c\u7269\u4f53\u7c7b\u522b\u8bc6\u522b\u3001\u4eba\u673a\u4ea4\u4e92\u8bc6\u522b\uff0c\u6700\u7ec8\u5b9e\u73b0\u7528\u6237\u8eab\u4efd\u8bc6\u522b\u3002", "result": "\u5728ARCTIC\u548cH2O\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6700\u4f18\u914d\u7f6e\u8fbe\u523097.52%\u7684\u5e73\u5747F1\u5206\u6570\uff0c\u6a21\u578b\u5927\u5c0f\u5c0f\u4e8e4MB\uff0c\u63a8\u7406\u65f6\u95f4\u4ec50.1\u79d2\u3002", "conclusion": "I2S\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u6a21\u578b\u548c\u5feb\u901f\u63a8\u7406\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u975e\u5e38\u9002\u5408\u5b89\u5168\u5173\u952e\u7684AR\u7cfb\u7edf\u5b9e\u65f6\u8bbe\u5907\u8ba4\u8bc1\u3002"}}
{"id": "2509.16560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16560", "abs": "https://arxiv.org/abs/2509.16560", "authors": ["Ji Soo Lee", "Byungoh Ko", "Jaewon Cho", "Howoong Lee", "Jaewoon Byun", "Hyunwoo J. Kim"], "title": "Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization", "comment": "EMNLP 2025 Findings", "summary": "In text-video retrieval, auxiliary captions are often used to enhance video\nunderstanding, bridging the gap between the modalities. While recent advances\nin multi-modal large language models (MLLMs) have enabled strong zero-shot\ncaption generation, we observe that such captions tend to be generic and\nindistinguishable across visually similar videos, limiting their utility for\nfine-grained retrieval. Moreover, conventional captioning approaches are\ntypically evaluated using language generation metrics, such as BLEU, which are\nnot typically tailored for retrieval tasks that require making discriminative\ndistinctions between candidates. To address this, we propose\n$\\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes caption\ngeneration using retrieval relevance scores. At its core is Dual-Group Direct\nPreference Optimization (DG-DPO), a novel learning strategy that supervises\ncaptioning by modeling preferences across groups of distinct video and caption\npairs. In addition, we present an MLLM-based retrieval model that incorporates\nrole-embeddings to better distinguish between textual inputs with different\nfunctional roles, such as an auxiliary caption and a text query. Through\nextensive experiments, we demonstrate that CaRe-DPO significantly enhances\nretrieval performance by effectively leveraging auxiliary knowledge to generate\nfine-grained captions for retrieval. Code is available at\nhttps://github.com/mlvlab/CaReDPO.", "AI": {"tldr": "CaRe-DPO\u662f\u4e00\u4e2a\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u6027\u5206\u6570\u76f4\u63a5\u4f18\u5316\u5b57\u5e55\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5b57\u5e55\u5728\u7ec6\u7c92\u5ea6\u68c0\u7d22\u4e2d\u533a\u5206\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5b57\u5e55\u5728\u89c6\u89c9\u76f8\u4f3c\u89c6\u9891\u4e4b\u95f4\u7f3a\u4e4f\u533a\u5206\u5ea6\uff0c\u4e14\u4f20\u7edf\u5b57\u5e55\u8bc4\u4f30\u6307\u6807\uff08\u5982BLEU\uff09\u4e0d\u9002\u7528\u4e8e\u9700\u8981\u533a\u5206\u5019\u9009\u8005\u7684\u68c0\u7d22\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u53cc\u7ec4\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DG-DPO\uff09\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u5efa\u6a21\u4e0d\u540c\u89c6\u9891\u548c\u5b57\u5e55\u5bf9\u7ec4\u4e4b\u95f4\u7684\u504f\u597d\u6765\u76d1\u7763\u5b57\u5e55\u751f\u6210\uff1b\u540c\u65f6\u5f00\u53d1\u4e86\u57fa\u4e8e\u89d2\u8272\u5d4c\u5165\u7684MLLM\u68c0\u7d22\u6a21\u578b\uff0c\u66f4\u597d\u5730\u533a\u5206\u4e0d\u540c\u529f\u80fd\u89d2\u8272\u7684\u6587\u672c\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCaRe-DPO\u901a\u8fc7\u6709\u6548\u5229\u7528\u8f85\u52a9\u77e5\u8bc6\u751f\u6210\u7ec6\u7c92\u5ea6\u5b57\u5e55\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "conclusion": "CaRe-DPO\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u7ec6\u7c92\u5ea6\u6587\u672c-\u89c6\u9891\u68c0\u7d22\u4e2d\u5b57\u5e55\u751f\u6210\u7684\u533a\u5206\u5ea6\u95ee\u9898\uff0c\u4e3a\u68c0\u7d22\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5b57\u5e55\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2509.16567", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16567", "abs": "https://arxiv.org/abs/2509.16567", "authors": ["Nikolaos Spanos", "Maria Lymperaiou", "Giorgos Filandrianos", "Konstantinos Thomas", "Athanasios Voulodimos", "Giorgos Stamou"], "title": "V-CECE: Visual Counterfactual Explanations via Conceptual Edits", "comment": "Accepted in NeurIPS 2025", "summary": "Recent black-box counterfactual generation frameworks fail to take into\naccount the semantic content of the proposed edits, while relying heavily on\ntraining to guide the generation process. We propose a novel, plug-and-play\nblack-box counterfactual generation framework, which suggests step-by-step\nedits based on theoretical guarantees of optimal edits to produce human-level\ncounterfactual explanations with zero training. Our framework utilizes a\npre-trained image editing diffusion model, and operates without access to the\ninternals of the classifier, leading to an explainable counterfactual\ngeneration process. Throughout our experimentation, we showcase the explanatory\ngap between human reasoning and neural model behavior by utilizing both\nConvolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision\nLanguage Model (LVLM) classifiers, substantiated through a comprehensive human\nevaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5373\u63d2\u5373\u7528\u9ed1\u76d2\u53cd\u4e8b\u5b9e\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u8f91\u6269\u6563\u6a21\u578b\u751f\u6210\u4eba\u7c7b\u7ea7\u522b\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u63ed\u793a\u4e86\u4eba\u7c7b\u63a8\u7406\u4e0e\u795e\u7ecf\u7f51\u7edc\u884c\u4e3a\u4e4b\u95f4\u7684\u89e3\u91ca\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u9ed1\u76d2\u53cd\u4e8b\u5b9e\u751f\u6210\u6846\u67b6\u5ffd\u89c6\u8bed\u4e49\u5185\u5bb9\u4e14\u4e25\u91cd\u4f9d\u8d56\u8bad\u7ec3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ea7\u751f\u4eba\u7c7b\u53ef\u7406\u89e3\u89e3\u91ca\u7684\u96f6\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u8f91\u6269\u6563\u6a21\u578b\uff0c\u57fa\u4e8e\u7406\u8bba\u4fdd\u8bc1\u7684\u6700\u4f18\u7f16\u8f91\u5efa\u8bae\u9010\u6b65\u7f16\u8f91\uff0c\u65e0\u9700\u8bbf\u95ee\u5206\u7c7b\u5668\u5185\u90e8\u7ed3\u6784\u3002", "result": "\u901a\u8fc7CNN\u3001ViT\u548cLVLM\u5206\u7c7b\u5668\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u4eba\u7c7b\u63a8\u7406\u4e0e\u6a21\u578b\u884c\u4e3a\u4e4b\u95f4\u7684\u89e3\u91ca\u5dee\u8ddd\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u8bc1\u5b9e\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u53cd\u4e8b\u5b9e\u751f\u6210\u8fc7\u7a0b\uff0c\u80fd\u591f\u4ea7\u751f\u4eba\u7c7b\u7ea7\u522b\u7684\u89e3\u91ca\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u51b3\u7b56\u8fc7\u7a0b\u3002"}}
{"id": "2509.16582", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16582", "abs": "https://arxiv.org/abs/2509.16582", "authors": ["Antonio Scardace", "Lemuel Puglisi", "Francesco Guarnera", "Sebastiano Battiato", "Daniele Rav\u00ec"], "title": "A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis", "comment": null, "summary": "Deep generative models have emerged as a transformative tool in medical\nimaging, offering substantial potential for synthetic data generation. However,\nrecent empirical studies highlight a critical vulnerability: these models can\nmemorize sensitive training data, posing significant risks of unauthorized\npatient information disclosure. Detecting memorization in generative models\nremains particularly challenging, necessitating scalable methods capable of\nidentifying training data leakage across large sets of generated samples. In\nthis work, we propose DeepSSIM, a novel self-supervised metric for quantifying\nmemorization in generative models. DeepSSIM is trained to: i) project images\ninto a learned embedding space and ii) force the cosine similarity between\nembeddings to match the ground-truth SSIM (Structural Similarity Index) scores\ncomputed in the image space. To capture domain-specific anatomical features,\ntraining incorporates structure-preserving augmentations, allowing DeepSSIM to\nestimate similarity reliably without requiring precise spatial alignment. We\nevaluate DeepSSIM in a case study involving synthetic brain MRI data generated\nby a Latent Diffusion Model (LDM) trained under memorization-prone conditions,\nusing 2,195 MRI scans from two publicly available datasets (IXI and CoRR).\nCompared to state-of-the-art memorization metrics, DeepSSIM achieves superior\nperformance, improving F1 scores by an average of +52.03% over the best\nexisting method. Code and data of our approach are publicly available at the\nfollowing link: https://github.com/brAIn-science/DeepSSIM.", "AI": {"tldr": "DeepSSIM\u662f\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u751f\u6210\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u5316\u73b0\u8c61\uff0c\u5728\u533b\u5b66\u5f71\u50cf\u5408\u6210\u6570\u636e\u751f\u6210\u4e2d\u68c0\u6d4b\u8bad\u7ec3\u6570\u636e\u6cc4\u9732\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5F1\u5206\u6570\u5e73\u5747\u63d0\u534752.03%\u3002", "motivation": "\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u5b58\u5728\u8bb0\u5fc6\u654f\u611f\u8bad\u7ec3\u6570\u636e\u7684\u98ce\u9669\uff0c\u53ef\u80fd\u5bfc\u81f4\u60a3\u8005\u4fe1\u606f\u6cc4\u9732\u3002\u76ee\u524d\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u751f\u6210\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faDeepSSIM\u81ea\u76d1\u7763\u5ea6\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u6295\u5f71\u5230\u5b66\u4e60\u5d4c\u5165\u7a7a\u95f4\uff0c\u4f7f\u5d4c\u5165\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u56fe\u50cf\u7a7a\u95f4\u7684\u771f\u5b9eSSIM\u5206\u6570\u5339\u914d\u3002\u8bad\u7ec3\u65f6\u4f7f\u7528\u7ed3\u6784\u4fdd\u6301\u589e\u5f3a\u6280\u672f\u6765\u6355\u83b7\u9886\u57df\u7279\u5b9a\u7684\u89e3\u5256\u7279\u5f81\u3002", "result": "\u5728\u8111\u90e8MRI\u5408\u6210\u6570\u636e\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cDeepSSIM\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u8bb0\u5fc6\u5316\u5ea6\u91cf\u65b9\u6cd5\uff0cF1\u5206\u6570\u5e73\u5747\u63d0\u534752.03%\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "DeepSSIM\u4e3a\u68c0\u6d4b\u751f\u6210\u6a21\u578b\u4e2d\u7684\u8bad\u7ec3\u6570\u636e\u6cc4\u9732\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.16588", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16588", "abs": "https://arxiv.org/abs/2509.16588", "authors": ["Haiming Zhang", "Yiyao Zhu", "Wending Zhou", "Xu Yan", "Yingjie Cai", "Bingbing Liu", "Shuguang Cui", "Zhen Li"], "title": "SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving", "comment": "NeurIPS 2025 (Spotlight)", "summary": "Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes\nexplicit dense BEV or volumetric construction, enabling highly efficient\ncomputation and accelerated inference. In this paper, we introduce SQS, a novel\nquery-based splatting pre-training specifically designed to advance SPMs in\nautonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian\nrepresentations from sparse queries during pre-training, leveraging\nself-supervised splatting to learn fine-grained contextual features through the\nreconstruction of multi-view images and depth maps. During fine-tuning, the\npre-trained Gaussian queries are seamlessly integrated into downstream networks\nvia query interaction mechanisms that explicitly connect pre-trained queries\nwith task-specific queries, effectively accommodating the diverse requirements\nof occupancy prediction and 3D object detection. Extensive experiments on\nautonomous driving benchmarks demonstrate that SQS delivers considerable\nperformance gains across multiple query-based 3D perception tasks, notably in\noccupancy prediction and 3D object detection, outperforming prior\nstate-of-the-art pre-training approaches by a significant margin (i.e., +1.3\nmIoU on occupancy prediction and +1.0 NDS on 3D detection).", "AI": {"tldr": "SQS\u662f\u4e00\u79cd\u4e13\u4e3a\u7a00\u758f\u611f\u77e5\u6a21\u578b\u8bbe\u8ba1\u7684\u67e5\u8be2\u9a71\u52a8\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc73D\u9ad8\u65af\u8868\u793a\u548c\u81ea\u76d1\u7763\u6e85\u5c04\u5b66\u4e60\u7ec6\u7c92\u5ea6\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u7684\u5360\u7528\u9884\u6d4b\u548c3D\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7a00\u758f\u611f\u77e5\u6a21\u578b\u91c7\u7528\u67e5\u8be2\u9a71\u52a8\u8303\u5f0f\uff0c\u907f\u514d\u4e86\u663e\u5f0f\u7684\u5bc6\u96c6BEV\u6216\u4f53\u7d20\u6784\u5efa\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u8ba1\u7b97\u548c\u5feb\u901f\u63a8\u7406\u3002\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u9884\u8bad\u7ec3\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9SPMs\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "method": "SQS\u5f15\u5165\u4e86\u4e00\u4e2a\u63d2\u4ef6\u6a21\u5757\uff0c\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u4ece\u7a00\u758f\u67e5\u8be2\u9884\u6d4b3D\u9ad8\u65af\u8868\u793a\uff0c\u5229\u7528\u81ea\u76d1\u7763\u6e85\u5c04\u901a\u8fc7\u91cd\u5efa\u591a\u89c6\u89d2\u56fe\u50cf\u548c\u6df1\u5ea6\u56fe\u6765\u5b66\u4e60\u7ec6\u7c92\u5ea6\u4e0a\u4e0b\u6587\u7279\u5f81\u3002\u5728\u5fae\u8c03\u9636\u6bb5\uff0c\u9884\u8bad\u7ec3\u7684\u9ad8\u65af\u67e5\u8be2\u901a\u8fc7\u67e5\u8be2\u4ea4\u4e92\u673a\u5236\u4e0e\u4efb\u52a1\u7279\u5b9a\u67e5\u8be2\u663e\u5f0f\u8fde\u63a5\u3002", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSQS\u5728\u591a\u4e2a\u57fa\u4e8e\u67e5\u8be2\u76843D\u611f\u77e5\u4efb\u52a1\u4e2d\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u5360\u7528\u9884\u6d4b\uff08+1.3 mIoU\uff09\u548c3D\u76ee\u6807\u68c0\u6d4b\uff08+1.0 NDS\uff09\u65b9\u9762\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "SQS\u4e3a\u7a00\u758f\u611f\u77e5\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u67e5\u8be2\u9a71\u52a8\u7684\u6e85\u5c04\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u63d0\u5347\u4e863D\u611f\u77e5\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.16602", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16602", "abs": "https://arxiv.org/abs/2509.16602", "authors": ["Minji Heo", "Simon S. Woo"], "title": "FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection", "comment": null, "summary": "Multi-step or hybrid deepfakes, created by sequentially applying different\ndeepfake creation methods such as Face-Swapping, GAN-based generation, and\nDiffusion methods, can pose an emerging and unforseen technical challenge for\ndetection models trained on single-step forgeries. While prior studies have\nmainly focused on detecting isolated single manipulation, little is known about\nthe detection model behavior under such compositional, hybrid, and complex\nmanipulation pipelines. In this work, we introduce \\textbf{FakeChain}, a\nlarge-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using\nfive state-of-the-art representative generators. Using this approach, we\nanalyze detection performance and spectral properties across hybrid\nmanipulation at different step, along with varying generator combinations and\nquality settings. Surprisingly, our findings reveal that detection performance\nhighly depends on the final manipulation type, with F1-score dropping by up to\n\\textbf{58.83\\%} when it differs from training distribution. This clearly\ndemonstrates that detectors rely on last-stage artifacts rather than cumulative\nmanipulation traces, limiting generalization. Such findings highlight the need\nfor detection models to explicitly consider manipulation history and sequences.\nOur results highlight the importance of benchmarks such as FakeChain,\nreflecting growing synthesis complexity and diversity in real-world scenarios.\nOur sample code is available\nhere\\footnote{https://github.com/minjihh/FakeChain}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FakeChain\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u5728\u6df7\u5408\u591a\u6b65\u4f2a\u9020\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u68c0\u6d4b\u5668\u4e3b\u8981\u4f9d\u8d56\u6700\u540e\u4e00\u6b65\u7684\u4f2a\u9020\u75d5\u8ff9\u800c\u975e\u7d2f\u79ef\u75d5\u8ff9\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u6b65\u4f2a\u9020\uff0c\u800c\u73b0\u5b9e\u4e2d\u5b58\u5728\u901a\u8fc7\u591a\u79cd\u65b9\u6cd5\uff08\u5982Face-Swapping\u3001GAN\u3001Diffusion\uff09\u987a\u5e8f\u7ec4\u5408\u521b\u5efa\u7684\u6df7\u5408\u591a\u6b65\u4f2a\u9020\uff0c\u8fd9\u5bf9\u68c0\u6d4b\u6a21\u578b\u6784\u6210\u4e86\u65b0\u7684\u6280\u672f\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b1\u6b65\u30012\u6b65\u548c3\u6b65\u4f2a\u9020\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5FakeChain\uff0c\u4f7f\u7528\u4e94\u79cd\u6700\u5148\u8fdb\u7684\u751f\u6210\u5668\u5408\u6210\u4f2a\u9020\u6570\u636e\uff0c\u5206\u6790\u68c0\u6d4b\u6027\u80fd\u548c\u9891\u8c31\u7279\u6027\u5728\u4e0d\u540c\u6b65\u9aa4\u3001\u751f\u6210\u5668\u7ec4\u5408\u548c\u8d28\u91cf\u8bbe\u7f6e\u4e0b\u7684\u53d8\u5316\u3002", "result": "\u68c0\u6d4b\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6700\u7ec8\u4f2a\u9020\u7c7b\u578b\uff0c\u5f53\u4e0e\u8bad\u7ec3\u5206\u5e03\u4e0d\u540c\u65f6\uff0cF1\u5206\u6570\u6700\u591a\u4e0b\u964d58.83%\uff0c\u8868\u660e\u68c0\u6d4b\u5668\u4e3b\u8981\u4f9d\u8d56\u6700\u540e\u9636\u6bb5\u7684\u4f2a\u9020\u75d5\u8ff9\u800c\u975e\u7d2f\u79ef\u75d5\u8ff9\u3002", "conclusion": "\u68c0\u6d4b\u6a21\u578b\u9700\u8981\u660e\u786e\u8003\u8651\u4f2a\u9020\u5386\u53f2\u548c\u5e8f\u5217\uff0cFakeChain\u7b49\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u4e8e\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u4e2d\u65e5\u76ca\u590d\u6742\u7684\u5408\u6210\u573a\u666f\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.16609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16609", "abs": "https://arxiv.org/abs/2509.16609", "authors": ["Shipeng Liu", "Zhonglin Zhang", "Dengfeng Chen", "Liang Zhao"], "title": "Describe-to-Score: Text-Guided Efficient Image Complexity Assessment", "comment": null, "summary": "Accurately assessing image complexity (IC) is critical for computer vision,\nyet most existing methods rely solely on visual features and often neglect\nhigh-level semantic information, limiting their accuracy and generalization. We\nintroduce vision-text fusion for IC modeling. This approach integrates visual\nand textual semantic features, increasing representational diversity. It also\nreduces the complexity of the hypothesis space, which enhances both accuracy\nand generalization in complexity assessment. We propose the D2S\n(Describe-to-Score) framework, which generates image captions with a\npre-trained vision-language model. We propose the feature alignment and entropy\ndistribution alignment mechanisms, D2S guides semantic information to inform\ncomplexity assessment while bridging the gap between vision and text\nmodalities. D2S utilizes multi-modal information during training but requires\nonly the vision branch during inference, thereby avoiding multi-modal\ncomputational overhead and enabling efficient assessment. Experimental results\ndemonstrate that D2S outperforms existing methods on the IC9600 dataset and\nmaintains competitiveness on no-reference image quality assessment (NR-IQA)\nbenchmark, validating the effectiveness and efficiency of multi-modal fusion in\ncomplexity-related tasks. Code is available at:\nhttps://github.com/xauat-liushipeng/D2S", "AI": {"tldr": "\u63d0\u51faD2S\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u878d\u5408\u65b9\u6cd5\u8bc4\u4f30\u56fe\u50cf\u590d\u6742\u5ea6\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u4e49\u7279\u5f81\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u56fe\u50cf\u590d\u6742\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u9ad8\u5c42\u6b21\u8bed\u4e49\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b", "method": "D2S\u6846\u67b6\u4f7f\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u56fe\u50cf\u63cf\u8ff0\uff0c\u63d0\u51fa\u7279\u5f81\u5bf9\u9f50\u548c\u71b5\u5206\u5e03\u5bf9\u9f50\u673a\u5236\uff0c\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u8fdb\u884c\u590d\u6742\u5ea6\u8bc4\u4f30", "result": "\u5728IC9600\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b", "conclusion": "\u591a\u6a21\u6001\u878d\u5408\u5728\u590d\u6742\u5ea6\u76f8\u5173\u4efb\u52a1\u4e2d\u5177\u6709\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\uff0cD2S\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u591a\u6a21\u6001\u4fe1\u606f\u4f46\u63a8\u7406\u65f6\u4ec5\u9700\u89c6\u89c9\u5206\u652f\uff0c\u907f\u514d\u4e86\u591a\u6a21\u6001\u8ba1\u7b97\u5f00\u9500"}}
{"id": "2509.16617", "categories": ["cs.CV", "cs.AI", "I.2.6; I.5.4; I.6.8"], "pdf": "https://arxiv.org/pdf/2509.16617", "abs": "https://arxiv.org/abs/2509.16617", "authors": ["David Kreismann"], "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model", "comment": "12 pages, 4 figures, to appear in GI LNI (SKILL 2025)", "summary": "As urbanization and climate change progress, urban heat island effects are\nbecoming more frequent and severe. To formulate effective mitigation plans,\ncities require detailed air temperature data. However, predictive analytics\nmethods based on conventional machine learning models and limited data\ninfrastructure often provide inaccurate predictions, especially in underserved\nareas. In this context, geospatial foundation models trained on unstructured\nglobal data demonstrate strong generalization and require minimal fine-tuning,\noffering an alternative for predictions where traditional approaches are\nlimited. This study fine-tunes a geospatial foundation model to predict urban\nland surface temperatures under future climate scenarios and explores its\nresponse to land cover changes using simulated vegetation strategies. The\nfine-tuned model achieved pixel-wise downscaling errors below 1.74 {\\deg}C and\naligned with ground truth patterns, demonstrating an extrapolation capacity up\nto 3.62 {\\deg}C.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5fae\u8c03\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u672a\u6765\u6c14\u5019\u60c5\u666f\u4e0b\u7684\u57ce\u5e02\u5730\u8868\u6e29\u5ea6\uff0c\u5e76\u63a2\u7d22\u5176\u5bf9\u571f\u5730\u8986\u76d6\u53d8\u5316\u7684\u54cd\u5e94\u3002\u6a21\u578b\u5728\u50cf\u7d20\u7ea7\u964d\u5c3a\u5ea6\u8bef\u5dee\u4f4e\u4e8e1.74\u00b0C\uff0c\u5916\u63a8\u80fd\u529b\u8fbe3.62\u00b0C\u3002", "motivation": "\u968f\u7740\u57ce\u5e02\u5316\u548c\u6c14\u5019\u53d8\u5316\u52a0\u5267\uff0c\u57ce\u5e02\u70ed\u5c9b\u6548\u5e94\u65e5\u76ca\u4e25\u91cd\u3002\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u6709\u9650\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u5f80\u5f80\u9884\u6d4b\u4e0d\u51c6\u786e\uff0c\u7279\u522b\u662f\u5728\u670d\u52a1\u4e0d\u8db3\u5730\u533a\u3002\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u6700\u5c0f\u5fae\u8c03\u9700\u6c42\uff0c\u4e3a\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u7684\u60c5\u51b5\u63d0\u4f9b\u4e86\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5fae\u8c03\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u6765\u9884\u6d4b\u672a\u6765\u6c14\u5019\u60c5\u666f\u4e0b\u7684\u57ce\u5e02\u5730\u8868\u6e29\u5ea6\uff0c\u5e76\u4f7f\u7528\u6a21\u62df\u690d\u88ab\u7b56\u7565\u63a2\u7d22\u6a21\u578b\u5bf9\u571f\u5730\u8986\u76d6\u53d8\u5316\u7684\u54cd\u5e94\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u50cf\u7d20\u7ea7\u964d\u5c3a\u5ea6\u8bef\u5dee\u4f4e\u4e8e1.74\u00b0C\uff0c\u5e76\u4e0e\u5730\u9762\u771f\u5b9e\u6a21\u5f0f\u4fdd\u6301\u4e00\u81f4\uff0c\u5c55\u793a\u4e86\u9ad8\u8fbe3.62\u00b0C\u7684\u5916\u63a8\u80fd\u529b\u3002", "conclusion": "\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u5728\u57ce\u5e02\u6e29\u5ea6\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u7684\u533a\u57df\uff0c\u4e3a\u57ce\u5e02\u70ed\u5c9b\u6548\u5e94\u7f13\u89e3\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2509.16618", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16618", "abs": "https://arxiv.org/abs/2509.16618", "authors": ["Pengfei Hao", "Hongqiu Wang", "Shuaibo Li", "Zhaohu Xing", "Guang Yang", "Kaishun Wu", "Lei Zhu"], "title": "Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery", "comment": "Early accepted by MICCAI2025", "summary": "In recent years, Visual Question Localized-Answering in robotic surgery\n(Surgical-VQLA) has gained significant attention for its potential to assist\nmedical students and junior doctors in understanding surgical scenes. Recently,\nthe rapid development of Large Language Models (LLMs) has provided more\npromising solutions for this task. However, current methods struggle to\nestablish complex dependencies between text and visual details, and have\ndifficulty perceiving the spatial information of surgical scenes. To address\nthese challenges, we propose a novel method, Surgical-MambaLLM, which is the\nfirst to combine Mamba2 with LLM in the surgical domain, that leverages\nMamba2's ability to effectively capture cross-modal dependencies and perceive\nspatial information in surgical scenes, thereby enhancing the LLMs'\nunderstanding of surgical images. Specifically, we propose the Cross-modal\nBidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effective\nmultimodal fusion, with its cross-modal integration capabilities. Additionally,\ntailored to the geometric characteristics of surgical scenes, we design the\nSurgical Instrument Perception (SIP) scanning mode for Mamba2 to scan the\nsurgical images, enhancing the model's spatial understanding of the surgical\nscene. Extensive experiments demonstrate that our Surgical-MambaLLM model\noutperforms the state-of-the-art methods on the EndoVis17-VQLA and\nEndoVis18-VQLA datasets, significantly improving the performance of the\nSurgical-VQLA task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSurgical-MambaLLM\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06Mamba2\u4e0eLLM\u7ed3\u5408\u7528\u4e8e\u5916\u79d1\u624b\u672f\u89c6\u89c9\u95ee\u7b54\u5b9a\u4f4d\u4efb\u52a1\uff0c\u901a\u8fc7CBMI\u6a21\u5757\u5b9e\u73b0\u591a\u6a21\u6001\u878d\u5408\uff0c\u5e76\u8bbe\u8ba1SIP\u626b\u63cf\u6a21\u5f0f\u589e\u5f3a\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u5728EndoVis\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u96be\u4ee5\u5efa\u7acb\u6587\u672c\u4e0e\u89c6\u89c9\u7ec6\u8282\u4e4b\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e14\u5bf9\u5916\u79d1\u624b\u672f\u573a\u666f\u7684\u7a7a\u95f4\u4fe1\u606f\u611f\u77e5\u80fd\u529b\u4e0d\u8db3\u3002LLMs\u7684\u53d1\u5c55\u4e3a\u89e3\u51b3\u5916\u79d1\u624b\u672f\u89c6\u89c9\u95ee\u7b54\u5b9a\u4f4d\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\u3002", "method": "\u63d0\u51faSurgical-MambaLLM\u65b9\u6cd5\uff1a1\uff09CBMI\u6a21\u5757\u5229\u7528Mamba2\u8fdb\u884c\u8de8\u6a21\u6001\u878d\u5408\uff1b2\uff09SIP\u626b\u63cf\u6a21\u5f0f\u9488\u5bf9\u624b\u672f\u573a\u666f\u51e0\u4f55\u7279\u6027\u8bbe\u8ba1\uff0c\u589e\u5f3a\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5728EndoVis17-VQLA\u548cEndoVis18-VQLA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86Surgical-VQLA\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "Surgical-MambaLLM\u901a\u8fc7\u7ed3\u5408Mamba2\u548cLLM\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5916\u79d1\u624b\u672f\u89c6\u89c9\u95ee\u7b54\u5b9a\u4f4d\u4e2d\u7684\u8de8\u6a21\u6001\u4f9d\u8d56\u548c\u7a7a\u95f4\u611f\u77e5\u95ee\u9898\uff0c\u4e3a\u624b\u672f\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.16623", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16623", "abs": "https://arxiv.org/abs/2509.16623", "authors": ["Junjie Zhou", "Haijun Xiong", "Junhao Lu", "Ziyu Lin", "Bin Feng"], "title": "CGTGait: Collaborative Graph and Transformer for Gait Emotion Recognition", "comment": "Accepted by IJCB2025", "summary": "Skeleton-based gait emotion recognition has received significant attention\ndue to its wide-ranging applications. However, existing methods primarily focus\non extracting spatial and local temporal motion information, failing to capture\nlong-range temporal representations. In this paper, we propose\n\\textbf{CGTGait}, a novel framework that collaboratively integrates graph\nconvolution and transformers to extract discriminative spatiotemporal features\nfor gait emotion recognition. Specifically, CGTGait consists of multiple CGT\nblocks, where each block employs graph convolution to capture frame-level\nspatial topology and the transformer to model global temporal dependencies.\nAdditionally, we introduce a Bidirectional Cross-Stream Fusion (BCSF) module to\neffectively aggregate posture and motion spatiotemporal features, facilitating\nthe exchange of complementary information between the two streams. We evaluate\nour method on two widely used datasets, Emotion-Gait and ELMD, demonstrating\nthat our CGTGait achieves state-of-the-art or at least competitive performance\nwhile reducing computational complexity by approximately \\textbf{82.2\\%} (only\nrequiring 0.34G FLOPs) during testing. Code is available at\n\\small{https://github.com/githubzjj1/CGTGait.}", "AI": {"tldr": "\u63d0\u51faCGTGait\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u5377\u79ef\u548cTransformer\u8fdb\u884c\u6b65\u6001\u60c5\u611f\u8bc6\u522b\uff0c\u901a\u8fc7\u53cc\u5411\u8de8\u6d41\u878d\u5408\u6a21\u5757\u6574\u5408\u59ff\u6001\u548c\u8fd0\u52a8\u7279\u5f81\uff0c\u5728\u964d\u4f4e82.2%\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u6b65\u6001\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7a7a\u95f4\u548c\u5c40\u90e8\u65f6\u95f4\u4fe1\u606f\uff0c\u65e0\u6cd5\u6355\u6349\u957f\u8ddd\u79bb\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb", "method": "CGTGait\u6846\u67b6\u5305\u542b\u591a\u4e2aCGT\u5757\uff0c\u6bcf\u4e2a\u5757\u4f7f\u7528\u56fe\u5377\u79ef\u6355\u83b7\u5e27\u7ea7\u7a7a\u95f4\u62d3\u6251\uff0cTransformer\u5efa\u6a21\u5168\u5c40\u65f6\u95f4\u4f9d\u8d56\uff0c\u5e76\u5f15\u5165\u53cc\u5411\u8de8\u6d41\u878d\u5408\u6a21\u5757\u6574\u5408\u59ff\u6001\u548c\u8fd0\u52a8\u7279\u5f81", "result": "\u5728Emotion-Gait\u548cELMD\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6216\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e82.2%\uff08\u4ec5\u97000.34G FLOPs\uff09", "conclusion": "CGTGait\u80fd\u6709\u6548\u63d0\u53d6\u5224\u522b\u6027\u65f6\u7a7a\u7279\u5f81\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c"}}
{"id": "2509.16628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16628", "abs": "https://arxiv.org/abs/2509.16628", "authors": ["Janak Kapuriya", "Anwar Shaikh", "Arnav Goel", "Medha Hira", "Apoorv Singh", "Jay Saraf", "Sanjana", "Vaibhav Nauriyal", "Avinash Anand", "Zhengkui Wang", "Rajiv Ratn Shah"], "title": "Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning", "comment": null, "summary": "In this study, we introduce Vision-Caption aware Supervised FineTuning\n(VCASFT), a novel learning paradigm designed to enhance the performance of\nsmaller Vision Language Models(VLMs) on scientific visual question\nanswering(VQA) tasks. VCASFT leverages image captions as zero-shot prompts\nalongside question-answer pairs and instruction-tunes models to yield\nsignificant performance improvements. To comprehensively evaluate VCASFT, we\nbenchmark it on ScienceQA, which consists of questions across diverse\nlanguages, subjects, and fields, demonstrating its adaptability and\neffectiveness in a variety of educational contexts. Additionally, to further\ndemonstrate the effectiveness of this technique on lowresource languages, we\ndeveloped HiSciVQA, a dataset comprising 2,245 high-quality, hand-annotated\nHindi multimodal Q&A pairs. This dataset addresses the critical need for\nlow-resource language Q&A datasets and serves as a foundation for testing\nVCASFT. Additionally, we introduce a novel LLM-based evaluation scheme to\nevaluate VLMs on HiSciVQA which offers deeper insights into model effectiveness\nsurpassing traditional n-gram matching accuracy metrics. We are committed to\nadvancing the field by open-sourcing all code files and the HiSciVQA dataset\nfor the research community.", "AI": {"tldr": "VCASFT\u662f\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u5229\u7528\u56fe\u50cf\u63cf\u8ff0\u4f5c\u4e3a\u96f6\u6837\u672c\u63d0\u793a\u6765\u589e\u5f3a\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728ScienceQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5f00\u53d1\u4e86HiSciVQA\u6570\u636e\u96c6\u548c\u57fa\u4e8eLLM\u7684\u65b0\u8bc4\u4f30\u65b9\u6848\u3002", "motivation": "\u63d0\u5347\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u9700\u6c42\uff0c\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "VCASFT\u5b66\u4e60\u8303\u5f0f\uff0c\u4f7f\u7528\u56fe\u50cf\u63cf\u8ff0\u4f5c\u4e3a\u96f6\u6837\u672c\u63d0\u793a\uff0c\u7ed3\u5408\u95ee\u7b54\u5bf9\u8fdb\u884c\u6307\u4ee4\u8c03\u4f18\u3002\u5f00\u53d1\u4e86HiSciVQA\u6570\u636e\u96c6\uff082,245\u4e2a\u9ad8\u8d28\u91cf\u5370\u5730\u8bed\u591a\u6a21\u6001\u95ee\u7b54\u5bf9\uff09\u548c\u57fa\u4e8eLLM\u7684\u65b0\u8bc4\u4f30\u65b9\u6848\u3002", "result": "\u5728ScienceQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6559\u80b2\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u3002\u65b0\u8bc4\u4f30\u65b9\u6848\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edfn-gram\u5339\u914d\u66f4\u6df1\u5165\u7684\u6a21\u578b\u6548\u679c\u6d1e\u5bdf\u3002", "conclusion": "VCASFT\u662f\u4e00\u79cd\u6709\u6548\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5c0f\u578bVLMs\u5728\u79d1\u5b66VQA\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u6709\u91cd\u8981\u4ef7\u503c\u3002\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u4f9b\u7814\u7a76\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2509.16630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16630", "abs": "https://arxiv.org/abs/2509.16630", "authors": ["Yue Ma", "Zexuan Yan", "Hongyu Liu", "Hongfa Wang", "Heng Pan", "Yingqing He", "Junkun Yuan", "Ailing Zeng", "Chengfei Cai", "Heung-Yeung Shum", "Zhifeng Li", "Wei Liu", "Linfeng Zhang", "Qifeng Chen"], "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "comment": "accepted by IJCV2025. project\n  page:https://follow-your-emoji.github.io", "summary": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/.", "AI": {"tldr": "Follow-Your-Emoji-Faster\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u8096\u50cf\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7\u9762\u90e8\u5173\u952e\u70b9\u9a71\u52a8\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u4fdd\u6301\u3001\u8868\u60c5\u51c6\u786e\u8fc1\u79fb\u548c\u957f\u671f\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u8096\u50cf\u52a8\u753b\u4e2d\u8eab\u4efd\u4fdd\u6301\u3001\u8868\u60c5\u51c6\u786e\u8fc1\u79fb\u548c\u957f\u671f\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u6838\u5fc3\u6311\u6218\uff0c\u540c\u65f6\u786e\u4fdd\u751f\u6210\u6548\u7387\u3002", "method": "\u589e\u5f3aStable Diffusion\uff0c\u5f15\u5165\u8868\u60c5\u611f\u77e5\u5173\u952e\u70b9\u4f5c\u4e3a\u663e\u5f0f\u8fd0\u52a8\u4fe1\u53f7\u548c\u7ec6\u7c92\u5ea6\u9762\u90e8\u635f\u5931\uff1b\u91c7\u7528\u6e10\u8fdb\u751f\u6210\u7b56\u7565\u548c\u6cf0\u52d2\u63d2\u503c\u7f13\u5b58\u5b9e\u73b02.6\u500d\u65e0\u635f\u52a0\u901f\u3002", "result": "\u5728EmojiBench++\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u52a8\u753b\u8d28\u91cf\u548c\u53ef\u63a7\u6027\uff0c\u652f\u6301\u771f\u5b9e\u4eba\u8138\u3001\u5361\u901a\u3001\u96d5\u5851\u548c\u52a8\u7269\u7b49\u591a\u79cd\u8096\u50cf\u7c7b\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u52a8\u753b\u7ed3\u679c\uff0c\u7528\u6237\u53cb\u597d\u4e14\u6613\u4e8e\u8bbf\u95ee\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2509.16632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16632", "abs": "https://arxiv.org/abs/2509.16632", "authors": ["Weiran Chen", "Guiqian Zhu", "Ying Li", "Yi Ji", "Chunping Liu"], "title": "DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration", "comment": "Accepted by ACM MM 2025", "summary": "Few-shot font generation aims to create new fonts with a limited number of\nglyph references. It can be used to significantly reduce the labor cost of\nmanual font design. However, due to the variety and complexity of font styles,\nthe results generated by existing methods often suffer from visible defects,\nsuch as stroke errors, artifacts and blurriness. To address these issues, we\npropose DA-Font, a novel framework which integrates a Dual-Attention Hybrid\nModule (DAHM). Specifically, we introduce two synergistic attention blocks: the\ncomponent attention block that leverages component information from content\nimages to guide the style transfer process, and the relation attention block\nthat further refines spatial relationships through interacting the content\nfeature with both original and stylized component-wise representations. These\ntwo blocks collaborate to preserve accurate character shapes and stylistic\ntextures. Moreover, we also design a corner consistency loss and an elastic\nmesh feature loss to better improve geometric alignment. Extensive experiments\nshow that our DA-Font outperforms the state-of-the-art methods across diverse\nfont styles and characters, demonstrating its effectiveness in enhancing\nstructural integrity and local fidelity. The source code can be found at\n\\href{https://github.com/wrchen2001/DA-Font}{\\textit{https://github.com/wrchen2001/DA-Font}}.", "AI": {"tldr": "DA-Font\u662f\u4e00\u4e2a\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6ce8\u610f\u529b\u6df7\u5408\u6a21\u5757\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7684\u7b14\u753b\u9519\u8bef\u3001\u4f2a\u5f71\u548c\u6a21\u7cca\u7b49\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5b57\u7b26\u5f62\u72b6\u51c6\u786e\u6027\u548c\u98ce\u683c\u7eb9\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u5b57\u4f53\u8bbe\u8ba1\u7684\u6210\u672c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7531\u4e8e\u5b57\u4f53\u98ce\u683c\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff0c\u751f\u6210\u7ed3\u679c\u5e38\u5e38\u5b58\u5728\u53ef\u89c1\u7f3a\u9677\uff0c\u5982\u7b14\u753b\u9519\u8bef\u3001\u4f2a\u5f71\u548c\u6a21\u7cca\u3002", "method": "\u63d0\u51faDA-Font\u6846\u67b6\uff0c\u96c6\u6210\u53cc\u6ce8\u610f\u529b\u6df7\u5408\u6a21\u5757\uff08DAHM\uff09\uff0c\u5305\u542b\u7ec4\u4ef6\u6ce8\u610f\u529b\u5757\u548c\u5173\u7cfb\u6ce8\u610f\u529b\u5757\uff0c\u5206\u522b\u5229\u7528\u5185\u5bb9\u56fe\u50cf\u7684\u7ec4\u4ef6\u4fe1\u606f\u6307\u5bfc\u98ce\u683c\u8fc1\u79fb\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u4e0e\u539f\u59cb\u548c\u98ce\u683c\u5316\u7ec4\u4ef6\u8868\u793a\u4ea4\u4e92\u6765\u7ec6\u5316\u7a7a\u95f4\u5173\u7cfb\u3002\u8fd8\u8bbe\u8ba1\u4e86\u89d2\u70b9\u4e00\u81f4\u6027\u635f\u5931\u548c\u5f39\u6027\u7f51\u683c\u7279\u5f81\u635f\u5931\u6765\u6539\u5584\u51e0\u4f55\u5bf9\u9f50\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDA-Font\u5728\u591a\u79cd\u5b57\u4f53\u98ce\u683c\u548c\u5b57\u7b26\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u589e\u5f3a\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u5c40\u90e8\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "DA-Font\u901a\u8fc7\u53cc\u6ce8\u610f\u529b\u673a\u5236\u548c\u65b0\u7684\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u4e2d\u7684\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u5b57\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16633", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16633", "abs": "https://arxiv.org/abs/2509.16633", "authors": ["Abhirama Subramanyam Penamakuri", "Navlika Singh", "Piyush Arora", "Anand Mishra"], "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs", "comment": "Accepted to EMNLP (Main) 2025", "summary": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable\nperformance in various vision and language tasks, including visual question\nanswering (VQA). However, their high computational cost makes them impractical\nfor resource-constrained settings and inference-heavy applications. In\ncontrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer\nfrom a significant performance gap compared to their larger counterparts. In\nthis work, we introduce the Model Parity Aligner (MPA), a novel framework\ndesigned to systematically improve S-VLMs by leveraging unlabeled images and\neffective knowledge transfer from L-VLMs. Instead of traditional knowledge\ndistillation methods that rely on labeled training data, MPA employs a\nstrategic parity-based approach that precisely identifies the knowledge\ndisparities between S-VLMs and L-VLMs, and optimizes training by targeting only\nthese disparities. We conduct extensive experiments on four diverse VQA\nbenchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires\nspecialized reasoning capabilities such as text recognition, chart\ninterpretation, and commonsense and factual understanding. Our results\ndemonstrate that MPA consistently enhances the performance of S-VLMs on all\nbenchmarks, reducing the performance gap while maintaining computational\nefficiency. We make our code publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86MPA\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u6807\u7b7e\u56fe\u50cf\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u6765\u63d0\u5347\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u6027\u80fd\u5dee\u8ddd\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u4f18\u79c0\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5c0f\u578b\u6a21\u578b\u6548\u7387\u9ad8\u4f46\u6027\u80fd\u5dee\u8ddd\u5927\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd", "method": "MPA\u6846\u67b6\u91c7\u7528\u57fa\u4e8e\u5bf9\u7b49\u6027\u7684\u7b56\u7565\uff0c\u7cbe\u786e\u8bc6\u522b\u5927\u5c0f\u6a21\u578b\u4e4b\u95f4\u7684\u77e5\u8bc6\u5dee\u5f02\uff0c\u4ec5\u9488\u5bf9\u8fd9\u4e9b\u5dee\u5f02\u8fdb\u884c\u4f18\u5316\u8bad\u7ec3\uff0c\u4e0d\u4f9d\u8d56\u6807\u6ce8\u6570\u636e", "result": "\u5728TextVQA\u3001ST-VQ\u3001ChartQA\u548cOKVQA\u56db\u4e2aVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMPA\u4e00\u81f4\u63d0\u5347\u4e86\u5c0f\u578b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7f29\u5c0f\u4e86\u6027\u80fd\u5dee\u8ddd", "conclusion": "MPA\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u5927\u578b\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd"}}
{"id": "2509.16635", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16635", "abs": "https://arxiv.org/abs/2509.16635", "authors": ["Xulin Li", "Yan Lu", "Bin Liu", "Jiaze Li", "Qinhong Yang", "Tao Gong", "Qi Chu", "Mang Ye", "Nenghai Yu"], "title": "Towards Anytime Retrieval: A Benchmark for Anytime Person Re-Identification", "comment": "Accepted by IJCAI 2025", "summary": "In real applications, person re-identification (ReID) is expected to retrieve\nthe target person at any time, including both daytime and nighttime, ranging\nfrom short-term to long-term. However, existing ReID tasks and datasets can not\nmeet this requirement, as they are constrained by available time and only\nprovide training and evaluation for specific scenarios. Therefore, we\ninvestigate a new task called Anytime Person Re-identification (AT-ReID), which\naims to achieve effective retrieval in multiple scenarios based on variations\nin time. To address the AT-ReID problem, we collect the first large-scale\ndataset, AT-USTC, which contains 403k images of individuals wearing multiple\nclothes captured by RGB and IR cameras. Our data collection spans 21 months,\nand 270 volunteers were photographed on average 29.1 times across different\ndates or scenes, 4-15 times more than current datasets, providing conditions\nfor follow-up investigations in AT-ReID. Further, to tackle the new challenge\nof multi-scenario retrieval, we propose a unified model named Uni-AT, which\ncomprises a multi-scenario ReID (MS-ReID) framework for scenario-specific\nfeatures learning, a Mixture-of-Attribute-Experts (MoAE) module to alleviate\ninter-scenario interference, and a Hierarchical Dynamic Weighting (HDW)\nstrategy to ensure balanced training across all scenarios. Extensive\nexperiments show that our model leads to satisfactory results and exhibits\nexcellent generalization to all scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Anytime Person Re-identification (AT-ReID)\u4efb\u52a1\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709ReID\u65b9\u6cd5\u5728\u65f6\u95f4\u8de8\u5ea6\u5927\u3001\u573a\u666f\u591a\u53d8\u60c5\u51b5\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6AT-USTC\u548c\u7edf\u4e00\u6a21\u578bUni-AT\u3002", "motivation": "\u73b0\u6709ReID\u4efb\u52a1\u548c\u6570\u636e\u96c6\u53d7\u9650\u4e8e\u7279\u5b9a\u65f6\u95f4\u548c\u573a\u666f\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u5168\u5929\u5019\u3001\u957f\u65f6\u8de8\u5ea6\u7684\u884c\u4eba\u68c0\u7d22\u9700\u6c42\u3002", "method": "\u6536\u96c6\u4e86AT-USTC\u6570\u636e\u96c6\uff08403k\u56fe\u50cf\uff0c21\u4e2a\u6708\u8de8\u5ea6\uff09\uff0c\u63d0\u51faUni-AT\u6a21\u578b\u5305\u542b\u591a\u573a\u666fReID\u6846\u67b6\u3001\u5c5e\u6027\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u548c\u5206\u5c42\u52a8\u6001\u52a0\u6743\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e0b\u90fd\u53d6\u5f97\u4e86\u6ee1\u610f\u7ed3\u679c\u5e76\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AT-ReID\u4efb\u52a1\u5177\u6709\u91cd\u8981\u7814\u7a76\u4ef7\u503c\uff0c\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e3a\u89e3\u51b3\u591a\u573a\u666f\u884c\u4eba\u68c0\u7d22\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.16639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16639", "abs": "https://arxiv.org/abs/2509.16639", "authors": ["Shangzhuo Xie", "Qianqian Yang"], "title": "Unlocking Hidden Potential in Point Cloud Networks with Attention-Guided Grouping-Feature Coordination", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Point cloud analysis has evolved with diverse network architectures, while\nexisting works predominantly focus on introducing novel structural designs.\nHowever, conventional point-based architectures - processing raw points through\nsequential sampling, grouping, and feature extraction layers - demonstrate\nunderutilized potential. We notice that substantial performance gains can be\nunlocked through strategic module integration rather than structural\nmodifications. In this paper, we propose the Grouping-Feature Coordination\nModule (GF-Core), a lightweight separable component that simultaneously\nregulates both grouping layer and feature extraction layer to enable more\nnuanced feature aggregation. Besides, we introduce a self-supervised\npretraining strategy specifically tailored for point-based inputs to enhance\nmodel robustness in complex point cloud analysis scenarios. On ModelNet40\ndataset, our method elevates baseline networks to 94.0% accuracy, matching\nadvanced frameworks' performance while preserving architectural simplicity. On\nthree variants of the ScanObjectNN dataset, we obtain improvements of 2.96%,\n6.34%, and 6.32% respectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684Grouping-Feature\u534f\u8c03\u6a21\u5757(GF-Core)\uff0c\u901a\u8fc7\u540c\u65f6\u8c03\u63a7\u5206\u7ec4\u5c42\u548c\u7279\u5f81\u63d0\u53d6\u5c42\u6765\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u7279\u5f81\u805a\u5408\uff0c\u5e76\u5f15\u5165\u4e86\u4e13\u95e8\u9488\u5bf9\u70b9\u4e91\u8f93\u5165\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u5206\u6790\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u65b0\u9896\u7684\u7ed3\u6784\u8bbe\u8ba1\uff0c\u4f46\u4f20\u7edf\u7684\u57fa\u4e8e\u70b9\u7684\u67b6\u6784\uff08\u901a\u8fc7\u987a\u5e8f\u91c7\u6837\u3001\u5206\u7ec4\u548c\u7279\u5f81\u63d0\u53d6\u5c42\u5904\u7406\u539f\u59cb\u70b9\uff09\u663e\u793a\u51fa\u672a\u5145\u5206\u5229\u7528\u7684\u6f5c\u529b\u3002\u4f5c\u8005\u53d1\u73b0\u901a\u8fc7\u7b56\u7565\u6027\u7684\u6a21\u5757\u96c6\u6210\u800c\u975e\u7ed3\u6784\u4fee\u6539\u53ef\u4ee5\u89e3\u9501\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86Grouping-Feature\u534f\u8c03\u6a21\u5757(GF-Core)\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u53ef\u5206\u79bb\u7ec4\u4ef6\uff0c\u540c\u65f6\u8c03\u63a7\u5206\u7ec4\u5c42\u548c\u7279\u5f81\u63d0\u53d6\u5c42\u4ee5\u652f\u6301\u66f4\u7ec6\u81f4\u7684\u7279\u5f81\u805a\u5408\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e13\u95e8\u4e3a\u70b9\u4e91\u8f93\u5165\u8bbe\u8ba1\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728ModelNet40\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5c06\u57fa\u7ebf\u7f51\u7edc\u51c6\u786e\u7387\u63d0\u5347\u81f394.0%\uff0c\u4e0e\u5148\u8fdb\u6846\u67b6\u6027\u80fd\u76f8\u5f53\u4f46\u4fdd\u6301\u67b6\u6784\u7b80\u5355\u6027\u3002\u5728ScanObjectNN\u6570\u636e\u96c6\u7684\u4e09\u4e2a\u53d8\u4f53\u4e0a\uff0c\u5206\u522b\u83b7\u5f97\u4e862.96%\u30016.34%\u548c6.32%\u7684\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u6a21\u5757\u96c6\u6210\u7b56\u7565\u800c\u975e\u7ed3\u6784\u4fee\u6539\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u70b9\u4e91\u5206\u6790\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u4f20\u7edf\u70b9\u4e91\u67b6\u6784\u7684\u6f5c\u529b\u53ef\u4ee5\u901a\u8fc7\u66f4\u667a\u80fd\u7684\u6a21\u5757\u534f\u8c03\u6765\u5145\u5206\u91ca\u653e\u3002"}}
{"id": "2509.16645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16645", "abs": "https://arxiv.org/abs/2509.16645", "authors": ["Yichen Wang", "Hangtao Zhang", "Hewen Pan", "Ziqi Zhou", "Xianlong Wang", "Peijin Guo", "Lulu Xue", "Shengshan Hu", "Minghui Li", "Leo Yu Zhang"], "title": "ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents", "comment": null, "summary": "Vision-Language Models (VLMs), with their strong reasoning and planning\ncapabilities, are widely used in embodied decision-making (EDM) tasks in\nembodied agents, such as autonomous driving and robotic manipulation. Recent\nresearch has increasingly explored adversarial attacks on VLMs to reveal their\nvulnerabilities. However, these attacks either rely on overly strong\nassumptions, requiring full knowledge of the victim VLM, which is impractical\nfor attacking VLM-based agents, or exhibit limited effectiveness. The latter\nstems from disrupting most semantic information in the image, which leads to a\nmisalignment between the perception and the task context defined by system\nprompts. This inconsistency interrupts the VLM's reasoning process, resulting\nin invalid outputs that fail to affect interactions in the physical world. To\nthis end, we propose a fine-grained adversarial attack framework, ADVEDM, which\nmodifies the VLM's perception of only a few key objects while preserving the\nsemantics of the remaining regions. This attack effectively reduces conflicts\nwith the task context, making VLMs output valid but incorrect decisions and\naffecting the actions of agents, thus posing a more substantial safety threat\nin the physical world. We design two variants of based on this framework,\nADVEDM-R and ADVEDM-A, which respectively remove the semantics of a specific\nobject from the image and add the semantics of a new object into the image. The\nexperimental results in both general scenarios and EDM tasks demonstrate\nfine-grained control and excellent attack performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86ADVEDM\u6846\u67b6\uff0c\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u4fee\u6539\u5173\u952e\u5bf9\u8c61\u7684\u611f\u77e5\u6765\u5f71\u54cdVLM\u5728\u5177\u8eab\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u8f93\u51fa\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u533a\u57df\u7684\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u8981\u4e48\u9700\u8981\u5b8c\u5168\u4e86\u89e3\u53d7\u5bb3VLM\uff08\u4e0d\u5207\u5b9e\u9645\uff09\uff0c\u8981\u4e48\u56e0\u7834\u574f\u8fc7\u591a\u8bed\u4e49\u4fe1\u606f\u5bfc\u81f4\u4e0e\u4efb\u52a1\u4e0a\u4e0b\u6587\u4e0d\u4e00\u81f4\uff0c\u4ea7\u751f\u65e0\u6548\u8f93\u51fa\u3002\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u6765\u5f71\u54cd\u7269\u7406\u4e16\u754c\u4e2d\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u3002", "method": "\u8bbe\u8ba1\u4e86ADVEDM\u6846\u67b6\u53ca\u5176\u4e24\u4e2a\u53d8\u4f53\uff1aADVEDM-R\uff08\u4ece\u56fe\u50cf\u4e2d\u79fb\u9664\u7279\u5b9a\u5bf9\u8c61\u8bed\u4e49\uff09\u548cADVEDM-A\uff08\u5411\u56fe\u50cf\u4e2d\u6dfb\u52a0\u65b0\u5bf9\u8c61\u8bed\u4e49\uff09\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u611f\u77e5\u63a7\u5236\u3002", "result": "\u5728\u901a\u7528\u573a\u666f\u548c\u5177\u8eab\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u7ec6\u7c92\u5ea6\u63a7\u5236\u80fd\u529b\u548c\u4f18\u79c0\u7684\u653b\u51fb\u6027\u80fd\u3002", "conclusion": "ADVEDM\u6846\u67b6\u80fd\u591f\u6709\u6548\u5f71\u54cdVLM\u8f93\u51fa\u6709\u6548\u4f46\u9519\u8bef\u7684\u51b3\u7b56\uff0c\u5bf9\u7269\u7406\u4e16\u754c\u4e2d\u7684\u667a\u80fd\u4f53\u6784\u6210\u66f4\u5b9e\u8d28\u6027\u7684\u5b89\u5168\u5a01\u80c1\u3002"}}
{"id": "2509.16654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16654", "abs": "https://arxiv.org/abs/2509.16654", "authors": ["Xin Chen", "Jia He", "Maozheng Li", "Dongliang Xu", "Tianyu Wang", "Yixiao Chen", "Zhixin Lin", "Yue Yao"], "title": "Are VLMs Ready for Lane Topology Awareness in Autonomous Driving?", "comment": "5 pages, 5 figures", "summary": "Vision-Language Models (VLMs) have recently shown remarkable progress in\nmultimodal reasoning, yet their applications in autonomous driving remain\nlimited. In particular, the ability to understand road topology, a key\nrequirement for safe navigation, has received relatively little attention.\nWhile some recent works have begun to explore VLMs in driving contexts, their\nperformance on topology reasoning is far from satisfactory. In this work, we\nsystematically evaluate VLMs' capabilities in road topology understanding.\nSpecifically, multi-view images are projected into unified ground-plane\ncoordinate system and fused into bird's-eye-view (BEV) lanes. Based on these\nBEV lanes, we formulate four topology-related diagnostic VQA tasks, which\ntogether capture essential components of spatial topology reasoning. Through\nextensive evaluation, we find that while frontier closed-source models (e.g.,\nGPT-4o) achieve relatively high accuracy in some tasks, they still fail in some\ntemporal questions that humans can answer (e.g., GPT-4o achieve only 67.8% in\nvector, a two-class classification problem). Furthermore, we find open-source\nVLMs, even at 30B scale, struggle significantly. These results indicate that\nspatial reasoning remains a fundamental bottleneck for current VLMs. We also\nfind that the model's capability is positively correlated with model size,\nlength of reasoning tokens and shots provided as examples, showing direction\nfor future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9053\u8def\u62d3\u6251\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u74f6\u9888\uff0c\u7279\u522b\u662f\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u8f83\u5dee\uff0c\u800c\u6a21\u578b\u80fd\u529b\u4e0e\u6a21\u578b\u5927\u5c0f\u3001\u63a8\u7406\u6807\u8bb0\u957f\u5ea6\u548c\u793a\u4f8b\u6570\u91cf\u5448\u6b63\u76f8\u5173\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\u3002\u7279\u522b\u662f\u7406\u89e3\u9053\u8def\u62d3\u6251\u8fd9\u4e00\u5b89\u5168\u5bfc\u822a\u7684\u5173\u952e\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u73b0\u6709\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u5c3d\u5982\u4eba\u610f\u3002", "method": "\u5c06\u591a\u89c6\u89d2\u56fe\u50cf\u6295\u5f71\u5230\u7edf\u4e00\u7684\u9e1f\u77b0\u56fe\u5750\u6807\u7cfb\u4e2d\u878d\u5408\u6210BEV\u8f66\u9053\uff0c\u57fa\u4e8e\u8fd9\u4e9bBEV\u8f66\u9053\u8bbe\u8ba1\u4e86\u56db\u4e2a\u62d3\u6251\u76f8\u5173\u7684\u8bca\u65ad\u6027VQA\u4efb\u52a1\uff0c\u901a\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\u5206\u6790\u4e0d\u540c\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u524d\u6cbf\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u8f83\u9ad8\uff0c\u4f46\u5728\u65f6\u95f4\u6027\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u4ecd\u4e0d\u7406\u60f3\uff08\u5982GPT-4o\u5728\u4e8c\u5206\u7c7b\u95ee\u9898\u4e0a\u4ec5\u8fbe\u523067.8%\uff09\u3002\u5f00\u6e90\u6a21\u578b\u5373\u4f7f\u8fbe\u5230300\u4ebf\u53c2\u6570\u89c4\u6a21\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7a7a\u95f4\u63a8\u7406\u4ecd\u7136\u662f\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u672c\u74f6\u9888\uff0c\u6a21\u578b\u80fd\u529b\u4e0e\u6a21\u578b\u89c4\u6a21\u3001\u63a8\u7406\u6807\u8bb0\u957f\u5ea6\u548c\u793a\u4f8b\u6570\u91cf\u5448\u6b63\u76f8\u5173\uff0c\u8fd9\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.16673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16673", "abs": "https://arxiv.org/abs/2509.16673", "authors": ["Sinuo Wang", "Yutong Xie", "Yuyuan Liu", "Qi Wu"], "title": "MedCutMix: A Data-Centric Approach to Improve Radiology Vision-Language Pre-training with Disease Awareness", "comment": null, "summary": "Vision-Language Pre-training (VLP) is drawing increasing interest for its\nability to minimize manual annotation requirements while enhancing semantic\nunderstanding in downstream tasks. However, its reliance on image-text datasets\nposes challenges due to privacy concerns and the high cost of obtaining paired\nannotations. Data augmentation emerges as a viable strategy to address this\nissue, yet existing methods often fall short of capturing the subtle and\ncomplex variations in medical data due to limited diversity. To this end, we\npropose MedCutMix, a novel multi-modal disease-centric data augmentation\nmethod. MedCutMix performs diagnostic sentence CutMix within medical reports\nand establishes the cross-attention between the diagnostic sentence and medical\nimage to guide attentive manifold mix within the imaging modality. Our approach\nsurpasses previous methods across four downstream radiology diagnosis datasets,\nhighlighting its effectiveness in enhancing performance and generalizability in\nradiology VLP.", "AI": {"tldr": "\u63d0\u51fa\u4e86MedCutMix\u65b9\u6cd5\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u75be\u75c5\u4e2d\u5fc3\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u533b\u5b66\u62a5\u544a\u4e2d\u6267\u884c\u8bca\u65ad\u53e5\u5b50CutMix\uff0c\u5e76\u5efa\u7acb\u8bca\u65ad\u53e5\u5b50\u4e0e\u533b\u5b66\u56fe\u50cf\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u6765\u6307\u5bfc\u56fe\u50cf\u6a21\u6001\u7684\u6ce8\u610f\u529b\u6d41\u5f62\u6df7\u5408\u3002", "motivation": "\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u4f9d\u8d56\u56fe\u50cf-\u6587\u672c\u6570\u636e\u96c6\uff0c\u4f46\u9762\u4e34\u9690\u79c1\u62c5\u5fe7\u548c\u914d\u5bf9\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u533b\u5b66\u6570\u636e\u7684\u7ec6\u5fae\u590d\u6742\u53d8\u5316\uff0c\u591a\u6837\u6027\u6709\u9650\u3002", "method": "MedCutMix\u5728\u533b\u5b66\u62a5\u544a\u5185\u6267\u884c\u8bca\u65ad\u53e5\u5b50CutMix\uff0c\u5efa\u7acb\u8bca\u65ad\u53e5\u5b50\u4e0e\u533b\u5b66\u56fe\u50cf\u7684\u8de8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6307\u5bfc\u56fe\u50cf\u6a21\u6001\u7684\u6ce8\u610f\u529b\u6d41\u5f62\u6df7\u5408\u3002", "result": "\u5728\u56db\u4e2a\u4e0b\u6e38\u653e\u5c04\u5b66\u8bca\u65ad\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5148\u524d\u65b9\u6cd5\uff0c\u663e\u793a\u51fa\u5728\u653e\u5c04\u5b66\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u63d0\u5347\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u7684\u6709\u6548\u6027\u3002", "conclusion": "MedCutMix\u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u6a21\u6001\u75be\u75c5\u4e2d\u5fc3\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.16674", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16674", "abs": "https://arxiv.org/abs/2509.16674", "authors": ["Zengli Luo", "Canlong Zhang", "Xiaochun Lu", "Zhixin Li"], "title": "FitPro: A Zero-Shot Framework for Interactive Text-based Pedestrian Retrieval in Open World", "comment": "15pages,6 figures", "summary": "Text-based Pedestrian Retrieval (TPR) aims to retrieve specific target\npedestrians in visual scenes according to natural language descriptions.\nAlthough existing methods have achieved progress under constrained settings,\ninteractive retrieval in the open-world scenario still suffers from limited\nmodel generalization and insufficient semantic understanding. To address these\nchallenges, we propose FitPro, an open-world interactive zero-shot TPR\nframework with enhanced semantic comprehension and cross-scene adaptability.\nFitPro has three innovative components: Feature Contrastive Decoding (FCD),\nIncremental Semantic Mining (ISM), and Query-aware Hierarchical Retrieval\n(QHR). The FCD integrates prompt-guided contrastive decoding to generate\nhigh-quality structured pedestrian descriptions from denoised images,\neffectively alleviating semantic drift in zero-shot scenarios. The ISM\nconstructs holistic pedestrian representations from multi-view observations to\nachieve global semantic modeling in multi-turn interactions,thereby improving\nrobustness against viewpoint shifts and fine-grained variations in\ndescriptions. The QHR dynamically optimizes the retrieval pipeline according to\nquery types, enabling efficient adaptation to multi-modal and multi-view\ninputs. Extensive experiments on five public datasets and two evaluation\nprotocols demonstrate that FitPro significantly overcomes the generalization\nlimitations and semantic modeling constraints of existing methods in\ninteractive retrieval, paving the way for practical deployment. The code and\ndata will be released at https://github.com/\nlilo4096/FitPro-Interactive-Person-Retrieval.", "AI": {"tldr": "FitPro\u662f\u4e00\u4e2a\u9762\u5411\u5f00\u653e\u4e16\u754c\u7684\u4ea4\u4e92\u5f0f\u96f6\u6837\u672c\u6587\u672c\u884c\u4eba\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u5bf9\u6bd4\u89e3\u7801\u3001\u589e\u91cf\u8bed\u4e49\u6316\u6398\u548c\u67e5\u8be2\u611f\u77e5\u5206\u5c42\u68c0\u7d22\u4e09\u4e2a\u521b\u65b0\u7ec4\u4ef6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u578b\u6cdb\u5316\u6027\u548c\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u884c\u4eba\u68c0\u7d22\u65b9\u6cd5\u5728\u53d7\u9650\u573a\u666f\u4e0b\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u5f00\u653e\u4e16\u754c\u4ea4\u4e92\u68c0\u7d22\u4e2d\u9762\u4e34\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "FitPro\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u7279\u5f81\u5bf9\u6bd4\u89e3\u7801\uff08FCD\uff09- \u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u7684\u5bf9\u6bd4\u89e3\u7801\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u6784\u5316\u884c\u4eba\u63cf\u8ff0\uff1b2\uff09\u589e\u91cf\u8bed\u4e49\u6316\u6398\uff08ISM\uff09- \u4ece\u591a\u89c6\u89d2\u89c2\u5bdf\u6784\u5efa\u6574\u4f53\u884c\u4eba\u8868\u793a\uff1b3\uff09\u67e5\u8be2\u611f\u77e5\u5206\u5c42\u68c0\u7d22\uff08QHR\uff09- \u6839\u636e\u67e5\u8be2\u7c7b\u578b\u52a8\u6001\u4f18\u5316\u68c0\u7d22\u6d41\u7a0b\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u4e24\u79cd\u8bc4\u4f30\u534f\u8bae\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFitPro\u5728\u4ea4\u4e92\u68c0\u7d22\u4e2d\u663e\u8457\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u9650\u5236\u548c\u8bed\u4e49\u5efa\u6a21\u7ea6\u675f\u3002", "conclusion": "FitPro\u4e3a\u5b9e\u9645\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5c06\u5728GitHub\u4e0a\u53d1\u5e03\u3002"}}
{"id": "2509.16677", "categories": ["cs.CV", "cs.LG", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.16677", "abs": "https://arxiv.org/abs/2509.16677", "authors": ["Wenxin Li", "Kunyu Peng", "Di Wen", "Ruiping Liu", "Mengfei Duan", "Kai Luo", "Kailun Yang"], "title": "Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation Towards Embodied Intelligence", "comment": "The established benchmark and source code will be made publicly\n  available at https://github.com/mylwx/ActiSeg-NL", "summary": "Embodied intelligence relies on accurately segmenting objects actively\ninvolved in interactions. Action-based video object segmentation addresses this\nby linking segmentation with action semantics, but it depends on large-scale\nannotations and prompts that are costly, inconsistent, and prone to multimodal\nnoise such as imprecise masks and referential ambiguity. To date, this\nchallenge remains unexplored. In this work, we take the first step by studying\naction-based video object segmentation under label noise, focusing on two\nsources: textual prompt noise (category flips and within-category noun\nsubstitutions) and mask annotation noise (perturbed object boundaries to mimic\nimprecise supervision). Our contributions are threefold. First, we introduce\ntwo types of label noises for the action-based video object segmentation task.\nSecond, we build up the first action-based video object segmentation under a\nlabel noise benchmark ActiSeg-NL and adapt six label-noise learning strategies\nto this setting, and establish protocols for evaluating them under textual,\nboundary, and mixed noise. Third, we provide a comprehensive analysis linking\nnoise types to failure modes and robustness gains, and we introduce a Parallel\nMask Head Mechanism (PMHM) to address mask annotation noise. Qualitative\nevaluations further reveal characteristic failure modes, including boundary\nleakage and mislocalization under boundary perturbations, as well as occasional\nidentity substitutions under textual flips. Our comparative analysis reveals\nthat different learning strategies exhibit distinct robustness profiles,\ngoverned by a foreground-background trade-off where some achieve balanced\nperformance while others prioritize foreground accuracy at the cost of\nbackground precision. The established benchmark and source code will be made\npublicly available at https://github.com/mylwx/ActiSeg-NL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7814\u7a76\u4e86\u57fa\u4e8e\u52a8\u4f5c\u7684\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u5728\u6807\u7b7e\u566a\u58f0\u4e0b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u566a\u58f0\u7c7b\u578b\uff08\u6587\u672c\u63d0\u793a\u566a\u58f0\u548c\u63a9\u7801\u6807\u6ce8\u566a\u58f0\uff09\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u57fa\u51c6\u6570\u636e\u96c6ActiSeg-NL\uff0c\u5e76\u8bc4\u4f30\u4e86\u516d\u79cd\u6807\u7b7e\u566a\u58f0\u5b66\u4e60\u7b56\u7565\u3002", "motivation": "\u57fa\u4e8e\u52a8\u4f5c\u7684\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u6ce8\u91ca\u548c\u63d0\u793a\uff0c\u4f46\u8fd9\u4e9b\u6ce8\u91ca\u6210\u672c\u9ad8\u3001\u4e0d\u4e00\u81f4\u4e14\u5bb9\u6613\u53d7\u5230\u591a\u6a21\u6001\u566a\u58f0\uff08\u5982\u4e0d\u7cbe\u786e\u7684\u63a9\u7801\u548c\u6307\u4ee3\u6a21\u7cca\uff09\u7684\u5f71\u54cd\uff0c\u8fd9\u4e00\u6311\u6218\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u6807\u7b7e\u566a\u58f0\u7c7b\u578b\uff0c\u5efa\u7acbActiSeg-NL\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u516d\u79cd\u6807\u7b7e\u566a\u58f0\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u5e76\u884c\u63a9\u7801\u5934\u673a\u5236\uff08PMHM\uff09\u6765\u5904\u7406\u63a9\u7801\u6807\u6ce8\u566a\u58f0\u3002", "result": "\u4e0d\u540c\u5b66\u4e60\u7b56\u7565\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u9c81\u68d2\u6027\u7279\u5f81\uff0c\u53d7\u524d\u666f-\u80cc\u666f\u6743\u8861\u7684\u5236\u7ea6\u3002\u5b9a\u6027\u8bc4\u4f30\u63ed\u793a\u4e86\u8fb9\u754c\u6cc4\u6f0f\u3001\u5b9a\u4f4d\u9519\u8bef\u7b49\u7279\u5f81\u6027\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u5efa\u7acb\u4e86\u9996\u4e2a\u57fa\u4e8e\u52a8\u4f5c\u7684\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u5728\u6807\u7b7e\u566a\u58f0\u4e0b\u7684\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u566a\u58f0\u7c7b\u578b\u4e0e\u5931\u8d25\u6a21\u5f0f\u5206\u6790\uff0c\u63d0\u51fa\u7684PMHM\u673a\u5236\u80fd\u6709\u6548\u5904\u7406\u63a9\u7801\u6807\u6ce8\u566a\u58f0\u3002"}}
{"id": "2509.16678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16678", "abs": "https://arxiv.org/abs/2509.16678", "authors": ["Suorong Yang", "Hongchao Yang", "Suhan Guo", "Furao Shen", "Jian Zhao"], "title": "IPF-RDA: An Information-Preserving Framework for Robust Data Augmentation", "comment": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "summary": "Data augmentation is widely utilized as an effective technique to enhance the\ngeneralization performance of deep models. However, data augmentation may\ninevitably introduce distribution shifts and noises, which significantly\nconstrain the potential and deteriorate the performance of deep networks. To\nthis end, we propose a novel information-preserving framework, namely IPF-RDA,\nto enhance the robustness of data augmentations in this paper. IPF-RDA combines\nthe proposal of (i) a new class-discriminative information estimation algorithm\nthat identifies the points most vulnerable to data augmentation operations and\ncorresponding importance scores; And (ii) a new information-preserving scheme\nthat preserves the critical information in the augmented samples and ensures\nthe diversity of augmented data adaptively. We divide data augmentation methods\ninto three categories according to the operation types and integrate these\napproaches into our framework accordingly. After being integrated into our\nframework, the robustness of data augmentation methods can be enhanced and\ntheir full potential can be unleashed. Extensive experiments demonstrate that\nalthough being simple, IPF-RDA consistently improves the performance of\nnumerous commonly used state-of-the-art data augmentation methods with popular\ndeep models on a variety of datasets, including CIFAR-10, CIFAR-100,\nTiny-ImageNet, CUHK03, Market1501, Oxford Flower, and MNIST, where its\nperformance and scalability are stressed. The implementation is available at\nhttps://github.com/Jackbrocp/IPF-RDA.", "AI": {"tldr": "IPF-RDA\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u4fe1\u606f\u4fdd\u7559\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a\u6570\u636e\u589e\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u8bc6\u522b\u5bf9\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\u6700\u654f\u611f\u7684\u5173\u952e\u70b9\u5e76\u81ea\u9002\u5e94\u4fdd\u7559\u91cd\u8981\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u5347\u6df1\u5ea6\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u6570\u636e\u589e\u5f3a\u867d\u7136\u80fd\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4f1a\u5f15\u5165\u5206\u5e03\u504f\u79fb\u548c\u566a\u58f0\uff0c\u9650\u5236\u6df1\u5ea6\u7f51\u7edc\u7684\u6f5c\u529b\u5e76\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u7c7b\u5224\u522b\u4fe1\u606f\u4f30\u8ba1\u7b97\u6cd5\uff0c\u8bc6\u522b\u5bf9\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\u6700\u8106\u5f31\u7684\u70b9\u53ca\u5176\u91cd\u8981\u6027\u5206\u6570\uff1b(2)\u4fe1\u606f\u4fdd\u7559\u65b9\u6848\uff0c\u81ea\u9002\u5e94\u4fdd\u7559\u589e\u5f3a\u6837\u672c\u4e2d\u7684\u5173\u952e\u4fe1\u606f\u5e76\u786e\u4fdd\u6570\u636e\u591a\u6837\u6027\u3002\u5c06\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5206\u4e3a\u4e09\u7c7b\u5e76\u76f8\u5e94\u96c6\u6210\u5230\u6846\u67b6\u4e2d\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u3001Tiny-ImageNet\u3001CUHK03\u3001Market1501\u3001Oxford Flower\u548cMNIST\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cIPF-RDA\u80fd\u6301\u7eed\u6539\u8fdb\u591a\u79cd\u6700\u5148\u8fdb\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "IPF-RDA\u867d\u7136\u7b80\u5355\uff0c\u4f46\u80fd\u6709\u6548\u589e\u5f3a\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u5e76\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\uff0c\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.16680", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16680", "abs": "https://arxiv.org/abs/2509.16680", "authors": ["Xingjian Diao", "Weiyi Wu", "Keyi Kong", "Peijun Qing", "Xinwen Xu", "Ming Cheng", "Soroush Vosoughi", "Jiang Gui"], "title": "ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Visual Question Answering (VQA) is increasingly used in diverse applications\nranging from general visual reasoning to safety-critical domains such as\nmedical imaging and autonomous systems, where models must provide not only\naccurate answers but also explanations that humans can easily understand and\nverify. Prototype-based modeling has shown promise for interpretability by\ngrounding predictions in semantically meaningful regions for purely visual\nreasoning tasks, yet remains underexplored in the context of VQA. We present\nProtoVQA, a unified prototypical framework that (i) learns question-aware\nprototypes that serve as reasoning anchors, connecting answers to\ndiscriminative image regions, (ii) applies spatially constrained matching to\nensure that the selected evidence is coherent and semantically relevant, and\n(iii) supports both answering and grounding tasks through a shared prototype\nbackbone. To assess explanation quality, we propose the Visual-Linguistic\nAlignment Score (VLAS), which measures how well the model's attended regions\nalign with ground-truth evidence. Experiments on Visual7W show that ProtoVQA\nyields faithful, fine-grained explanations while maintaining competitive\naccuracy, advancing the development of transparent and trustworthy VQA systems.", "AI": {"tldr": "ProtoVQA\u662f\u4e00\u4e2a\u57fa\u4e8e\u539f\u578b\u7684\u53ef\u89e3\u91ca\u89c6\u89c9\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u95ee\u9898\u611f\u77e5\u539f\u578b\u6765\u8fde\u63a5\u7b54\u6848\u548c\u5224\u522b\u6027\u56fe\u50cf\u533a\u57df\uff0c\u63d0\u4f9b\u51c6\u786e\u7b54\u6848\u548c\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\u3002", "motivation": "\u968f\u7740VQA\u5728\u533b\u7597\u5f71\u50cf\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u6a21\u578b\u4e0d\u4ec5\u9700\u8981\u63d0\u4f9b\u51c6\u786e\u7b54\u6848\uff0c\u8fd8\u9700\u8981\u63d0\u4f9b\u4eba\u7c7b\u6613\u4e8e\u7406\u89e3\u548c\u9a8c\u8bc1\u7684\u89e3\u91ca\u3002\u539f\u578b\u5efa\u6a21\u5728\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u53ef\u89e3\u91ca\u6027\u6f5c\u529b\uff0c\u4f46\u5728VQA\u9886\u57df\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "ProtoVQA\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(i)\u5b66\u4e60\u95ee\u9898\u611f\u77e5\u539f\u578b\u4f5c\u4e3a\u63a8\u7406\u951a\u70b9\uff1b(ii)\u5e94\u7528\u7a7a\u95f4\u7ea6\u675f\u5339\u914d\u786e\u4fdd\u8bc1\u636e\u7684\u8fde\u8d2f\u6027\u548c\u8bed\u4e49\u76f8\u5173\u6027\uff1b(iii)\u901a\u8fc7\u5171\u4eab\u539f\u578b\u9aa8\u5e72\u540c\u65f6\u652f\u6301\u7b54\u6848\u751f\u6210\u548c\u5b9a\u4f4d\u4efb\u52a1\u3002", "result": "\u5728Visual7W\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cProtoVQA\u80fd\u591f\u4ea7\u751f\u5fe0\u5b9e\u3001\u7ec6\u7c92\u5ea6\u7684\u89e3\u91ca\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\u3002\u63d0\u51fa\u7684VLAS\u6307\u6807\u6709\u6548\u8bc4\u4f30\u4e86\u89e3\u91ca\u8d28\u91cf\u3002", "conclusion": "ProtoVQA\u63a8\u8fdb\u4e86\u900f\u660e\u53ef\u4fe1VQA\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u95ee\u7b54\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16684", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16684", "abs": "https://arxiv.org/abs/2509.16684", "authors": ["Qi Zhang", "Bin Li", "Antoni B. Chan", "Hui Huang"], "title": "Active View Selection for Scene-level Multi-view Crowd Counting and Localization with Limited Labels", "comment": "8 pages, 5 figures", "summary": "Multi-view crowd counting and localization fuse the input multi-views for\nestimating the crowd number or locations on the ground. Existing methods mainly\nfocus on accurately predicting on the crowd shown in the input views, which\nneglects the problem of choosing the `best' camera views to perceive all crowds\nwell in the scene. Besides, existing view selection methods require massive\nlabeled views and images, and lack the ability for cross-scene settings,\nreducing their application scenarios. Thus, in this paper, we study the view\nselection issue for better scene-level multi-view crowd counting and\nlocalization results with cross-scene ability and limited label demand, instead\nof input-view-level results. We first propose an independent view selection\nmethod (IVS) that considers view and scene geometries in the view selection\nstrategy and conducts the view selection, labeling, and downstream tasks\nindependently. Based on IVS, we also put forward an active view selection\nmethod (AVS) that jointly optimizes the view selection, labeling, and\ndownstream tasks. In AVS, we actively select the labeled views and consider\nboth the view/scene geometries and the predictions of the downstream task\nmodels in the view selection process. Experiments on multi-view counting and\nlocalization tasks demonstrate the cross-scene and the limited label demand\nadvantages of the proposed active view selection method (AVS), outperforming\nexisting methods and with wider application scenarios.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u591a\u89c6\u89d2\u4eba\u7fa4\u8ba1\u6570\u548c\u5b9a\u4f4d\u4e2d\u7684\u89c6\u89d2\u9009\u62e9\u95ee\u9898\uff0c\u63d0\u51fa\u72ec\u7acb\u89c6\u89d2\u9009\u62e9(IVS)\u548c\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9(AVS)\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0\u8de8\u573a\u666f\u80fd\u529b\u548c\u6709\u9650\u6807\u7b7e\u9700\u6c42\u4e0b\u7684\u573a\u666f\u7ea7\u6700\u4f18\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8f93\u5165\u89c6\u89d2\u7684\u51c6\u786e\u9884\u6d4b\uff0c\u5ffd\u7565\u4e86\u9009\u62e9\u6700\u4f73\u76f8\u673a\u89c6\u89d2\u6765\u5168\u9762\u611f\u77e5\u573a\u666f\u4e2d\u6240\u6709\u4eba\u7fa4\u7684\u95ee\u9898\u3002\u73b0\u6709\u89c6\u89d2\u9009\u62e9\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u89c6\u89d2\u548c\u56fe\u50cf\uff0c\u7f3a\u4e4f\u8de8\u573a\u666f\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5e94\u7528\u573a\u666f\u3002", "method": "\u63d0\u51faIVS\u65b9\u6cd5\u8003\u8651\u89c6\u89d2\u548c\u573a\u666f\u51e0\u4f55\u4fe1\u606f\u8fdb\u884c\u72ec\u7acb\u89c6\u89d2\u9009\u62e9\uff1b\u57fa\u4e8eIVS\u63d0\u51faAVS\u65b9\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u89c6\u89d2\u9009\u62e9\u3001\u6807\u6ce8\u548c\u4e0b\u6e38\u4efb\u52a1\uff0c\u5728\u89c6\u89d2\u9009\u62e9\u8fc7\u7a0b\u4e2d\u540c\u65f6\u8003\u8651\u51e0\u4f55\u4fe1\u606f\u548c\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u9884\u6d4b\u3002", "result": "\u5728\u591a\u89c6\u89d2\u8ba1\u6570\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\u65b9\u6cd5(AVS)\u5177\u6709\u8de8\u573a\u666f\u548c\u6709\u9650\u6807\u7b7e\u9700\u6c42\u7684\u4f18\u52bf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u5e94\u7528\u573a\u666f\u66f4\u5e7f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\u65b9\u6cd5\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u4eba\u7fa4\u5206\u6790\u4e2d\u7684\u89c6\u89d2\u9009\u62e9\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u573a\u666f\u7ea7\u6027\u80fd\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.16685", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16685", "abs": "https://arxiv.org/abs/2509.16685", "authors": ["Binbin Wen", "Yihang Wu", "Tareef Daqqaq", "Ahmad Chaddad"], "title": "Towards a Transparent and Interpretable AI Model for Medical Image Classifications", "comment": "Published in Cognitive Neurodynamics", "summary": "The integration of artificial intelligence (AI) into medicine is remarkable,\noffering advanced diagnostic and therapeutic possibilities. However, the\ninherent opacity of complex AI models presents significant challenges to their\nclinical practicality. This paper focuses primarily on investigating the\napplication of explainable artificial intelligence (XAI) methods, with the aim\nof making AI decisions transparent and interpretable. Our research focuses on\nimplementing simulations using various medical datasets to elucidate the\ninternal workings of the XAI model. These dataset-driven simulations\ndemonstrate how XAI effectively interprets AI predictions, thus improving the\ndecision-making process for healthcare professionals. In addition to a survey\nof the main XAI methods and simulations, ongoing challenges in the XAI field\nare discussed. The study highlights the need for the continuous development and\nexploration of XAI, particularly from the perspective of diverse medical\ndatasets, to promote its adoption and effectiveness in the healthcare domain.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u65b9\u6cd5\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u533b\u7597\u6570\u636e\u96c6\u6a21\u62df\u5c55\u793aXAI\u5982\u4f55\u63d0\u9ad8AI\u51b3\u7b56\u7684\u900f\u660e\u5ea6\uff0c\u5e76\u8ba8\u8bba\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "AI\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u590d\u6742AI\u6a21\u578b\u7684\u4e0d\u900f\u660e\u6027\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76XAI\u65b9\u6cd5\u6765\u63d0\u9ad8AI\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u533b\u7597\u6570\u636e\u96c6\u8fdb\u884c\u6a21\u62df\u5b9e\u9a8c\uff0c\u9610\u660eXAI\u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff0c\u5c55\u793aXAI\u5982\u4f55\u6709\u6548\u89e3\u91caAI\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u57fa\u4e8e\u6570\u636e\u96c6\u7684\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0cXAI\u80fd\u591f\u6709\u6548\u89e3\u91caAI\u9884\u6d4b\uff0c\u6539\u5584\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "conclusion": "\u9700\u8981\u6301\u7eed\u5f00\u53d1\u548c\u63a2\u7d22XAI\u6280\u672f\uff0c\u7279\u522b\u662f\u5728\u591a\u6837\u5316\u533b\u7597\u6570\u636e\u96c6\u65b9\u9762\u7684\u5e94\u7528\uff0c\u4ee5\u4fc3\u8fdb\u5176\u5728\u533b\u7597\u9886\u57df\u7684\u91c7\u7528\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2509.16690", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16690", "abs": "https://arxiv.org/abs/2509.16690", "authors": ["Xiaodong Wang", "Zijun He", "Ping Wang", "Lishun Wang", "Yanan Hu", "Xin Yuan"], "title": "Spectral Compressive Imaging via Chromaticity-Intensity Decomposition", "comment": null, "summary": "In coded aperture snapshot spectral imaging (CASSI), the captured measurement\nentangles spatial and spectral information, posing a severely ill-posed inverse\nproblem for hyperspectral images (HSIs) reconstruction. Moreover, the captured\nradiance inherently depends on scene illumination, making it difficult to\nrecover the intrinsic spectral reflectance that remains invariant to lighting\nconditions. To address these challenges, we propose a chromaticity-intensity\ndecomposition framework, which disentangles an HSI into a spatially smooth\nintensity map and a spectrally variant chromaticity cube. The chromaticity\nencodes lighting-invariant reflectance, enriched with high-frequency spatial\ndetails and local spectral sparsity. Building on this decomposition, we develop\nCIDNet, a Chromaticity-Intensity Decomposition unfolding network within a\ndual-camera CASSI system. CIDNet integrates a hybrid spatial-spectral\nTransformer tailored to reconstruct fine-grained and sparse spectral\nchromaticity and a degradation-aware, spatially-adaptive noise estimation\nmodule that captures anisotropic noise across iterative stages. Extensive\nexperiments on both synthetic and real-world CASSI datasets demonstrate that\nour method achieves superior performance in both spectral and chromaticity\nfidelity. Code and models will be publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8272\u5ea6-\u5f3a\u5ea6\u5206\u89e3\u6846\u67b6CIDNet\uff0c\u7528\u4e8e\u89e3\u51b3\u7f16\u7801\u5b54\u5f84\u5feb\u7167\u5149\u8c31\u6210\u50cf(CASSI)\u4e2d\u7684HSI\u91cd\u5efa\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u79bb\u5149\u7167\u4e0d\u53d8\u7684\u8272\u5ea6\u4fe1\u606f\u548c\u7a7a\u95f4\u5e73\u6ed1\u7684\u5f3a\u5ea6\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u5149\u8c31\u548c\u8272\u5ea6\u4fdd\u771f\u5ea6\u3002", "motivation": "CASSI\u6355\u83b7\u7684\u6d4b\u91cf\u503c\u7ea0\u7f20\u4e86\u7a7a\u95f4\u548c\u5149\u8c31\u4fe1\u606f\uff0c\u5bfc\u81f4HSI\u91cd\u5efa\u6210\u4e3a\u4e25\u91cd\u4e0d\u9002\u5b9a\u9006\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6355\u83b7\u7684\u8f90\u5c04\u5ea6\u4f9d\u8d56\u4e8e\u573a\u666f\u5149\u7167\uff0c\u96be\u4ee5\u6062\u590d\u5bf9\u5149\u7167\u6761\u4ef6\u4e0d\u53d8\u7684\u56fa\u6709\u5149\u8c31\u53cd\u5c04\u7387\u3002", "method": "\u63d0\u51fa\u8272\u5ea6-\u5f3a\u5ea6\u5206\u89e3\u6846\u67b6\uff0c\u5c06HSI\u5206\u89e3\u4e3a\u7a7a\u95f4\u5e73\u6ed1\u7684\u5f3a\u5ea6\u56fe\u548c\u5149\u8c31\u53d8\u5316\u7684\u8272\u5ea6\u7acb\u65b9\u4f53\u3002\u5f00\u53d1CIDNet\u7f51\u7edc\uff0c\u96c6\u6210\u6df7\u5408\u7a7a\u95f4-\u5149\u8c31Transformer\u91cd\u5efa\u7cbe\u7ec6\u8272\u5ea6\uff0c\u4ee5\u53ca\u9000\u5316\u611f\u77e5\u7684\u7a7a\u95f4\u81ea\u9002\u5e94\u566a\u58f0\u4f30\u8ba1\u6a21\u5757\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9eCASSI\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5149\u8c31\u548c\u8272\u5ea6\u4fdd\u771f\u5ea6\u65b9\u9762\u5747\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8272\u5ea6-\u5f3a\u5ea6\u5206\u89e3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86CASSI\u4e2d\u7684HSI\u91cd\u5efa\u6311\u6218\uff0c\u80fd\u591f\u6062\u590d\u5149\u7167\u4e0d\u53d8\u7684\u53cd\u5c04\u7387\u7279\u5f81\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2509.16691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16691", "abs": "https://arxiv.org/abs/2509.16691", "authors": ["Qiang Xiang", "Shuang Sun", "Binglei Li", "Dejia Song", "Huaxia Li", "Nemo Chen", "Xu Tang", "Yao Hu", "Junping Zhang"], "title": "InstanceAssemble: Layout-Aware Image Generation via Instance Assembling Attention", "comment": "Accepted in NeurIPS 2025", "summary": "Diffusion models have demonstrated remarkable capabilities in generating\nhigh-quality images. Recent advancements in Layout-to-Image (L2I) generation\nhave leveraged positional conditions and textual descriptions to facilitate\nprecise and controllable image synthesis. Despite overall progress, current L2I\nmethods still exhibit suboptimal performance. Therefore, we propose\nInstanceAssemble, a novel architecture that incorporates layout conditions via\ninstance-assembling attention, enabling position control with bounding boxes\n(bbox) and multimodal content control including texts and additional visual\ncontent. Our method achieves flexible adaption to existing DiT-based T2I models\nthrough light-weighted LoRA modules. Additionally, we propose a Layout-to-Image\nbenchmark, Denselayout, a comprehensive benchmark for layout-to-image\ngeneration, containing 5k images with 90k instances in total. We further\nintroduce Layout Grounding Score (LGS), an interpretable evaluation metric to\nmore precisely assess the accuracy of L2I generation. Experiments demonstrate\nthat our InstanceAssemble method achieves state-of-the-art performance under\ncomplex layout conditions, while exhibiting strong compatibility with diverse\nstyle LoRA modules.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86InstanceAssemble\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u4f8b\u7ec4\u88c5\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u57fa\u4e8e\u5e03\u5c40\u6761\u4ef6\u7684\u56fe\u50cf\u751f\u6210\uff0c\u652f\u6301\u8fb9\u754c\u6846\u4f4d\u7f6e\u63a7\u5236\u548c\u591a\u6a21\u6001\u5185\u5bb9\u63a7\u5236\uff0c\u5e76\u5728\u65b0\u57fa\u51c6Denselayout\u4e0a\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u6027\u80fd\u4ecd\u4e0d\u7406\u60f3\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u4f4d\u7f6e\u63a7\u5236\u548c\u5185\u5bb9\u63a7\u5236\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5b9e\u4f8b\u7ec4\u88c5\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7LoRA\u6a21\u5757\u9002\u914d\u73b0\u6709DiT-based T2I\u6a21\u578b\uff0c\u652f\u6301\u8fb9\u754c\u6846\u548c\u591a\u6a21\u6001\u5185\u5bb9\u63a7\u5236\u3002", "result": "\u5728\u5305\u542b5k\u56fe\u50cf\u300190k\u5b9e\u4f8b\u7684Denselayout\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6307\u6807LGS\u3002", "conclusion": "InstanceAssemble\u65b9\u6cd5\u5728\u590d\u6742\u5e03\u5c40\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u4e0e\u591a\u6837\u5f0fLoRA\u6a21\u5757\u517c\u5bb9\u6027\u5f3a\u3002"}}
{"id": "2509.16702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16702", "abs": "https://arxiv.org/abs/2509.16702", "authors": ["Chen Liu", "Haitao Wu", "Kafeng Wang", "Xiaowang Zhang"], "title": "Animalbooth: multimodal feature enhancement for animal subject personalization", "comment": null, "summary": "Personalized animal image generation is challenging due to rich appearance\ncues and large morphological variability. Existing approaches often exhibit\nfeature misalignment across domains, which leads to identity drift. We present\nAnimalBooth, a framework that strengthens identity preservation with an Animal\nNet and an adaptive attention module, mitigating cross domain alignment errors.\nWe further introduce a frequency controlled feature integration module that\napplies Discrete Cosine Transform filtering in the latent space to guide the\ndiffusion process, enabling a coarse to fine progression from global structure\nto detailed texture. To advance research in this area, we curate AnimalBench, a\nhigh resolution dataset for animal personalization. Extensive experiments show\nthat AnimalBooth consistently outperforms strong baselines on multiple\nbenchmarks and improves both identity fidelity and perceptual quality.", "AI": {"tldr": "AnimalBooth\u662f\u4e00\u4e2a\u7528\u4e8e\u4e2a\u6027\u5316\u52a8\u7269\u56fe\u50cf\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7Animal Net\u548c\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\u8eab\u4efd\u4fdd\u6301\uff0c\u5e76\u5f15\u5165\u9891\u7387\u63a7\u5236\u7279\u5f81\u96c6\u6210\u6a21\u5757\u6765\u6307\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u4ece\u5168\u5c40\u7ed3\u6784\u5230\u7ec6\u8282\u7eb9\u7406\u9010\u6b65\u751f\u6210\u3002", "motivation": "\u4e2a\u6027\u5316\u52a8\u7269\u56fe\u50cf\u751f\u6210\u9762\u4e34\u5916\u89c2\u7ebf\u7d22\u4e30\u5bcc\u548c\u5f62\u6001\u53d8\u5f02\u5927\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8de8\u57df\u7279\u5f81\u5bf9\u9f50\u9519\u8bef\u5bfc\u81f4\u8eab\u4efd\u6f02\u79fb\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faAnimalBooth\u6846\u67b6\uff0c\u5305\u542bAnimal Net\u548c\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6a21\u5757\u6765\u7f13\u89e3\u8de8\u57df\u5bf9\u9f50\u9519\u8bef\uff0c\u4ee5\u53ca\u9891\u7387\u63a7\u5236\u7279\u5f81\u96c6\u6210\u6a21\u5757\u5728\u6f5c\u5728\u7a7a\u95f4\u5e94\u7528\u79bb\u6563\u4f59\u5f26\u53d8\u6362\u6ee4\u6ce2\u6765\u6307\u5bfc\u6269\u6563\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAnimalBooth\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u8eab\u4efd\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "AnimalBooth\u901a\u8fc7\u521b\u65b0\u7684\u8eab\u4efd\u4fdd\u6301\u673a\u5236\u548c\u9891\u7387\u63a7\u5236\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u7269\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u6f02\u79fb\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86AnimalBench\u6570\u636e\u96c6\u63a8\u52a8\u8be5\u9886\u57df\u7814\u7a76\u3002"}}
{"id": "2509.16704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16704", "abs": "https://arxiv.org/abs/2509.16704", "authors": ["Pan Liu", "Jinshi Liu"], "title": "When Confidence Fails: Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation", "comment": null, "summary": "While significant advances exist in pseudo-label generation for\nsemi-supervised semantic segmentation, pseudo-label selection remains\nunderstudied. Existing methods typically use fixed confidence thresholds to\nretain high-confidence predictions as pseudo-labels. However, these methods\ncannot cope with network overconfidence tendency, where correct and incorrect\npredictions overlap significantly in high-confidence regions, making separation\nchallenging and amplifying model cognitive bias. Meanwhile, the direct\ndiscarding of low-confidence predictions disrupts spatial-semantic continuity,\ncausing critical context loss. We propose Confidence Separable Learning (CSL)\nto address these limitations. CSL formulates pseudo-label selection as a convex\noptimization problem within the confidence distribution feature space,\nestablishing sample-specific decision boundaries to distinguish reliable from\nunreliable predictions. Additionally, CSL introduces random masking of reliable\npixels to guide the network in learning contextual relationships from\nlow-reliability regions, thereby mitigating the adverse effects of discarding\nuncertain predictions. Extensive experimental results on the Pascal,\nCityscapes, and COCO benchmarks show that CSL performs favorably against\nstate-of-the-art methods. Code and model weights are available at\nhttps://github.com/PanLiuCSU/CSL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7f6e\u4fe1\u5ea6\u53ef\u5206\u5b66\u4e60\uff08CSL\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u4e2d\u4f2a\u6807\u7b7e\u9009\u62e9\u7684\u6311\u6218\uff0c\u901a\u8fc7\u51f8\u4f18\u5316\u5efa\u7acb\u6837\u672c\u7279\u5b9a\u51b3\u7b56\u8fb9\u754c\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u9608\u503c\u9009\u62e9\u4f2a\u6807\u7b7e\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u7f51\u7edc\u8fc7\u5ea6\u81ea\u4fe1\u503e\u5411\uff0c\u5bfc\u81f4\u6b63\u786e\u548c\u9519\u8bef\u9884\u6d4b\u5728\u9ad8\u7f6e\u4fe1\u5ea6\u533a\u57df\u91cd\u53e0\uff0c\u540c\u65f6\u4e22\u5f03\u4f4e\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u4f1a\u7834\u574f\u7a7a\u95f4\u8bed\u4e49\u8fde\u7eed\u6027\u3002", "method": "CSL\u5c06\u4f2a\u6807\u7b7e\u9009\u62e9\u5efa\u6a21\u4e3a\u7f6e\u4fe1\u5ea6\u5206\u5e03\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5efa\u7acb\u6837\u672c\u7279\u5b9a\u51b3\u7b56\u8fb9\u754c\uff0c\u5e76\u5f15\u5165\u53ef\u9760\u50cf\u7d20\u7684\u968f\u673a\u63a9\u7801\u6765\u5b66\u4e60\u4f4e\u53ef\u9760\u6027\u533a\u57df\u7684\u4e0a\u4e0b\u6587\u5173\u7cfb\u3002", "result": "\u5728Pascal\u3001Cityscapes\u548cCOCO\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCSL\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "CSL\u6709\u6548\u89e3\u51b3\u4e86\u4f2a\u6807\u7b7e\u9009\u62e9\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u51b3\u7b56\u8fb9\u754c\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u63d0\u5347\u4e86\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\u3002"}}
{"id": "2509.16721", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16721", "abs": "https://arxiv.org/abs/2509.16721", "authors": ["Haoyuan Li", "Rui Liu", "Hehe Fan", "Yi Yang"], "title": "Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding", "comment": "19 pages, 12 figures, 6 tables", "summary": "Enabling agents to understand and interact with complex 3D scenes is a\nfundamental challenge for embodied artificial intelligence systems. While\nMultimodal Large Language Models (MLLMs) have achieved significant progress in\n2D image understanding, extending such capabilities to 3D scenes remains\ndifficult: 1) 3D environment involves richer concepts such as spatial\nrelationships, affordances, physics, layout, and so on, 2) the absence of\nlarge-scale 3D vision-language datasets has posed a significant obstacle. In\nthis paper, we introduce Text-Scene, a framework that automatically parses 3D\nscenes into textual descriptions for scene understanding. Given a 3D scene, our\nmodel identifies object attributes and spatial relationships, and then\ngenerates a coherent summary of the whole scene, bridging the gap between 3D\nobservation and language without requiring human-in-the-loop intervention. By\nleveraging both geometric analysis and MLLMs, Text-Scene produces descriptions\nthat are accurate, detailed, and human-interpretable, capturing object-level\ndetails and global-level context. Experimental results on benchmarks\ndemonstrate that our textual parses can faithfully represent 3D scenes and\nbenefit downstream tasks. To evaluate the reasoning capability of MLLMs, we\npresent InPlan3D, a comprehensive benchmark for 3D task planning, consisting of\n3174 long-term planning tasks across 636 indoor scenes. We emphasize clarity\nand accessibility in our approach, aiming to make 3D scene content\nunderstandable through language. Code and datasets will be released.", "AI": {"tldr": "Text-Scene\u662f\u4e00\u4e2a\u81ea\u52a8\u5c063D\u573a\u666f\u89e3\u6790\u4e3a\u6587\u672c\u63cf\u8ff0\u4ee5\u8fdb\u884c\u573a\u666f\u7406\u89e3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u51c6\u786e\u3001\u8be6\u7ec6\u7684\u573a\u666f\u63cf\u8ff0\uff0c\u5e76\u63d0\u51fa\u4e86InPlan3D\u57fa\u51c6\u6765\u8bc4\u4f303D\u4efb\u52a1\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u57283D\u573a\u666f\u7406\u89e3\u65b9\u9762\u7684\u6311\u6218\uff0c\u5305\u62ec3D\u73af\u5883\u6d89\u53ca\u66f4\u4e30\u5bcc\u7684\u6982\u5ff5\uff08\u7a7a\u95f4\u5173\u7cfb\u3001\u529f\u80fd\u3001\u7269\u7406\u3001\u5e03\u5c40\u7b49\uff09\u4ee5\u53ca\u7f3a\u4e4f\u5927\u89c4\u6a213D\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1Text-Scene\u6846\u67b6\uff0c\u81ea\u52a8\u8bc6\u522b3D\u573a\u666f\u4e2d\u7684\u7269\u4f53\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\uff0c\u7136\u540e\u751f\u6210\u6574\u4e2a\u573a\u666f\u7684\u8fde\u8d2f\u6458\u8981\uff0c\u7ed3\u5408\u51e0\u4f55\u5206\u6790\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6587\u672c\u89e3\u6790\u80fd\u591f\u5fe0\u5b9e\u8868\u793a3D\u573a\u666f\u5e76\u6709\u76ca\u4e8e\u4e0b\u6e38\u4efb\u52a1\u3002\u63d0\u51fa\u4e86\u5305\u542b636\u4e2a\u5ba4\u5185\u573a\u666f\u4e2d3174\u4e2a\u957f\u671f\u89c4\u5212\u4efb\u52a1\u7684InPlan3D\u57fa\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bed\u8a00\u4f7f3D\u573a\u666f\u5185\u5bb9\u6613\u4e8e\u7406\u89e3\uff0c\u5f3a\u8c03\u6e05\u6670\u6027\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u53d1\u5e03\u3002"}}
{"id": "2509.16727", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16727", "abs": "https://arxiv.org/abs/2509.16727", "authors": ["Xin Lei Lin", "Soroush Mehraban", "Abhishek Moturu", "Babak Taati"], "title": "Pain in 3D: Generating Controllable Synthetic Faces for Automated Pain Assessment", "comment": null, "summary": "Automated pain assessment from facial expressions is crucial for\nnon-communicative patients, such as those with dementia. Progress has been\nlimited by two challenges: (i) existing datasets exhibit severe demographic and\nlabel imbalance due to ethical constraints, and (ii) current generative models\ncannot precisely control facial action units (AUs), facial structure, or\nclinically validated pain levels.\n  We present 3DPain, a large-scale synthetic dataset specifically designed for\nautomated pain assessment, featuring unprecedented annotation richness and\ndemographic diversity. Our three-stage framework generates diverse 3D meshes,\ntextures them with diffusion models, and applies AU-driven face rigging to\nsynthesize multi-view faces with paired neutral and pain images, AU\nconfigurations, PSPI scores, and the first dataset-level annotations of\npain-region heatmaps. The dataset comprises 82,500 samples across 25,000 pain\nexpression heatmaps and 2,500 synthetic identities balanced by age, gender, and\nethnicity.\n  We further introduce ViTPain, a Vision Transformer based cross-modal\ndistillation framework in which a heatmap-trained teacher guides a student\ntrained on RGB images, enhancing accuracy, interpretability, and clinical\nreliability. Together, 3DPain and ViTPain establish a controllable, diverse,\nand clinically grounded foundation for generalizable automated pain assessment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e863DPain\u5408\u6210\u6570\u636e\u96c6\u548cViTPain\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u75bc\u75db\u8bc4\u4f30\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u548c\u751f\u6210\u6a21\u578b\u63a7\u5236\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u75bc\u75db\u8bc4\u4f30\u5bf9\u4e8e\u975e\u6c9f\u901a\u6027\u60a3\u8005\uff08\u5982\u75f4\u5446\u75c7\u60a3\u8005\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u4e25\u91cd\u7684\u4eba\u53e3\u7edf\u8ba1\u548c\u6807\u7b7e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e14\u5f53\u524d\u751f\u6210\u6a21\u578b\u65e0\u6cd5\u7cbe\u786e\u63a7\u5236\u9762\u90e8\u52a8\u4f5c\u5355\u5143\u548c\u4e34\u5e8a\u9a8c\u8bc1\u7684\u75bc\u75db\u6c34\u5e73\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u751f\u6210\u591a\u6837\u53163D\u7f51\u683c\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7eb9\u7406\u5904\u7406\uff0c\u5e94\u7528AU\u9a71\u52a8\u7684\u9762\u90e8\u7ed1\u5b9a\u6280\u672f\u5408\u6210\u591a\u89c6\u89d2\u9762\u90e8\u56fe\u50cf\u3002\u540c\u65f6\u63d0\u51faViTPain\u6846\u67b6\uff0c\u901a\u8fc7\u70ed\u56fe\u8bad\u7ec3\u7684\u6559\u5e08\u6a21\u578b\u6307\u5bfcRGB\u56fe\u50cf\u8bad\u7ec3\u7684\u5b66\u751f\u6a21\u578b\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b82,500\u4e2a\u6837\u672c\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\uff0c\u6db5\u76d625,000\u4e2a\u75bc\u75db\u8868\u60c5\u70ed\u56fe\u548c2,500\u4e2a\u5408\u6210\u8eab\u4efd\uff0c\u5728\u5e74\u9f84\u3001\u6027\u522b\u548c\u79cd\u65cf\u65b9\u9762\u4fdd\u6301\u5e73\u8861\u3002ViTPain\u6846\u67b6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u53ef\u9760\u6027\u3002", "conclusion": "3DPain\u548cViTPain\u5171\u540c\u4e3a\u901a\u7528\u81ea\u52a8\u75bc\u75db\u8bc4\u4f30\u5efa\u7acb\u4e86\u53ef\u63a7\u3001\u591a\u6837\u5316\u548c\u4e34\u5e8a\u57fa\u7840\u7684\u65b9\u6cd5\u57fa\u7840\u3002"}}
{"id": "2509.16738", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16738", "abs": "https://arxiv.org/abs/2509.16738", "authors": ["Kai Jiang", "Zhengyan Shi", "Dell Zhang", "Hongyuan Zhang", "Xuelong Li"], "title": "Min: Mixture of Noise for Pre-Trained Model-Based Class-Incremental Learning", "comment": "Accepted by NeurIPS 2025. Source Code will be released in the next\n  version", "summary": "Class Incremental Learning (CIL) aims to continuously learn new categories\nwhile retaining the knowledge of old ones. Pre-trained models (PTMs) show\npromising capabilities in CIL. However, existing approaches that apply\nlightweight fine-tuning to backbones still induce parameter drift, thereby\ncompromising the generalization capability of pre-trained models. Parameter\ndrift can be conceptualized as a form of noise that obscures critical patterns\nlearned for previous tasks. However, recent researches have shown that noise is\nnot always harmful. For example, the large number of visual patterns learned\nfrom pre-training can be easily abused by a single task, and introducing\nappropriate noise can suppress some low-correlation features, thus leaving a\nmargin for future tasks. To this end, we propose learning beneficial noise for\nCIL guided by information theory and propose Mixture of Noise (Min), aiming to\nmitigate the degradation of backbone generalization from adapting new tasks.\nSpecifically, task-specific noise is learned from high-dimension features of\nnew tasks. Then, a set of weights is adjusted dynamically for optimal mixture\nof different task noise. Finally, Min embeds the beneficial noise into the\nintermediate features to mask the response of inefficient patterns. Extensive\nexperiments on six benchmark datasets demonstrate that Min achieves\nstate-of-the-art performance in most incremental settings, with particularly\noutstanding results in 50-steps incremental settings. This shows the\nsignificant potential for beneficial noise in continual learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMixture of Noise (Min)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u5b66\u4e60\u6709\u76ca\u566a\u58f0\u6765\u7f13\u89e3\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u5728\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\u4f1a\u5bfc\u81f4\u53c2\u6570\u6f02\u79fb\uff0c\u635f\u5bb3\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4f5c\u8005\u8ba4\u4e3a\u53c2\u6570\u6f02\u79fb\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u566a\u58f0\uff0c\u4f46\u566a\u58f0\u5e76\u975e\u603b\u662f\u6709\u5bb3\u7684\uff0c\u9002\u5f53\u7684\u566a\u58f0\u53ef\u4ee5\u6291\u5236\u4f4e\u76f8\u5173\u6027\u7279\u5f81\uff0c\u4e3a\u672a\u6765\u4efb\u52a1\u7559\u51fa\u7a7a\u95f4\u3002", "method": "Min\u65b9\u6cd5\u57fa\u4e8e\u4fe1\u606f\u7406\u8bba\u6307\u5bfc\uff0c\u4ece\u65b0\u4efb\u52a1\u7684\u9ad8\u7ef4\u7279\u5f81\u4e2d\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u566a\u58f0\uff0c\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u4efb\u52a1\u566a\u58f0\u7684\u6df7\u5408\u6743\u91cd\uff0c\u5e76\u5c06\u6709\u76ca\u566a\u58f0\u5d4c\u5165\u4e2d\u95f4\u7279\u5f81\u4ee5\u63a9\u76d6\u65e0\u6548\u6a21\u5f0f\u7684\u54cd\u5e94\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMin\u5728\u5927\u591a\u6570\u589e\u91cf\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u572850\u6b65\u589e\u91cf\u8bbe\u7f6e\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6709\u76ca\u566a\u58f0\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u9002\u5e94\u65b0\u4efb\u52a1\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u9000\u5316\u95ee\u9898\u3002"}}
{"id": "2509.16745", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16745", "abs": "https://arxiv.org/abs/2509.16745", "authors": ["Ritabrata Chakraborty", "Avijit Dasgupta", "Sandeep Chaurasia"], "title": "CAMBench-QR : A Structure-Aware Benchmark for Post-Hoc Explanations with QR Understanding", "comment": "9 pages, 5 figures, 6 tables", "summary": "Visual explanations are often plausible but not structurally faithful. We\nintroduce CAMBench-QR, a structure-aware benchmark that leverages the canonical\ngeometry of QR codes (finder patterns, timing lines, module grid) to test\nwhether CAM methods place saliency on requisite substructures while avoiding\nbackground. CAMBench-QR synthesizes QR/non-QR data with exact masks and\ncontrolled distortions, and reports structure-aware metrics (Finder/Timing Mass\nRatios, Background Leakage, coverage AUCs, Distance-to-Structure) alongside\ncausal occlusion, insertion/deletion faithfulness, robustness, and latency. We\nbenchmark representative, efficient CAMs (LayerCAM, EigenGrad-CAM, XGrad-CAM)\nunder two practical regimes of zero-shot and last-block fine-tuning. The\nbenchmark, metrics, and training recipes provide a simple, reproducible\nyardstick for structure-aware evaluation of visual explanations. Hence we\npropose that CAMBENCH-QR can be used as a litmus test of whether visual\nexplanations are truly structure-aware.", "AI": {"tldr": "CAMBench-QR\u662f\u4e00\u4e2a\u7ed3\u6784\u611f\u77e5\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5229\u7528QR\u7801\u7684\u89c4\u8303\u51e0\u4f55\u7ed3\u6784\u6765\u8bc4\u4f30CAM\u65b9\u6cd5\u662f\u5426\u5728\u5fc5\u8981\u5b50\u7ed3\u6784\u4e0a\u653e\u7f6e\u663e\u8457\u6027\uff0c\u540c\u65f6\u907f\u514d\u80cc\u666f\u5e72\u6270\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u89e3\u91ca\u65b9\u6cd5\u5f80\u5f80\u770b\u4f3c\u5408\u7406\u4f46\u7f3a\u4e4f\u7ed3\u6784\u5fe0\u5b9e\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6d4b\u8bd5\u65b9\u6cd5\u662f\u5426\u771f\u6b63\u5173\u6ce8\u56fe\u50cf\u7ed3\u6784\u7279\u5f81\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u901a\u8fc7\u5408\u6210QR\u7801/\u975eQR\u7801\u6570\u636e\uff0c\u4f7f\u7528\u7cbe\u786e\u63a9\u7801\u548c\u53d7\u63a7\u5931\u771f\uff0c\u5f00\u53d1\u7ed3\u6784\u611f\u77e5\u6307\u6807\uff08\u67e5\u627e\u5668/\u65f6\u5e8f\u7ebf\u8d28\u91cf\u6bd4\u3001\u80cc\u666f\u6cc4\u6f0f\u3001\u8986\u76d6AUC\u3001\u7ed3\u6784\u8ddd\u79bb\u7b49\uff09\uff0c\u5e76\u8bc4\u4f30\u4ee3\u8868\u6027\u9ad8\u6548CAM\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u548c\u6700\u540e\u5757\u5fae\u8c03\u4e24\u79cd\u5b9e\u7528\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u8bad\u7ec3\u65b9\u6848\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u80fd\u591f\u6709\u6548\u6d4b\u8bd5\u89c6\u89c9\u89e3\u91ca\u65b9\u6cd5\u7684\u7ed3\u6784\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "CAMBench-QR\u53ef\u4ee5\u4f5c\u4e3a\u89c6\u89c9\u89e3\u91ca\u662f\u5426\u771f\u6b63\u5177\u6709\u7ed3\u6784\u611f\u77e5\u80fd\u529b\u7684\u8bd5\u91d1\u77f3\uff0c\u4e3a\u89c6\u89c9\u89e3\u91ca\u65b9\u6cd5\u63d0\u4f9b\u7b80\u5355\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2509.16748", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16748", "abs": "https://arxiv.org/abs/2509.16748", "authors": ["Heyuan Li", "Kenkun Liu", "Lingteng Qiu", "Qi Zuo", "Keru Zheng", "Zilong Dong", "Xiaoguang Han"], "title": "HyPlaneHead: Rethinking Tri-plane-like Representations in Full-Head Image Synthesis", "comment": "Accepted by NeurIPS 2025", "summary": "Tri-plane-like representations have been widely adopted in 3D-aware GANs for\nhead image synthesis and other 3D object/scene modeling tasks due to their\nefficiency. However, querying features via Cartesian coordinate projection\noften leads to feature entanglement, which results in mirroring artifacts. A\nrecent work, SphereHead, attempted to address this issue by introducing\nspherical tri-planes based on a spherical coordinate system. While it\nsuccessfully mitigates feature entanglement, SphereHead suffers from uneven\nmapping between the square feature maps and the spherical planes, leading to\ninefficient feature map utilization during rendering and difficulties in\ngenerating fine image details. Moreover, both tri-plane and spherical tri-plane\nrepresentations share a subtle yet persistent issue: feature penetration across\nconvolutional channels can cause interference between planes, particularly when\none plane dominates the others. These challenges collectively prevent\ntri-plane-based methods from reaching their full potential. In this paper, we\nsystematically analyze these problems for the first time and propose innovative\nsolutions to address them. Specifically, we introduce a novel hybrid-plane\n(hy-plane for short) representation that combines the strengths of both planar\nand spherical planes while avoiding their respective drawbacks. We further\nenhance the spherical plane by replacing the conventional theta-phi warping\nwith a novel near-equal-area warping strategy, which maximizes the effective\nutilization of the square feature map. In addition, our generator synthesizes a\nsingle-channel unified feature map instead of multiple feature maps in separate\nchannels, thereby effectively eliminating feature penetration. With a series of\ntechnical improvements, our hy-plane representation enables our method,\nHyPlaneHead, to achieve state-of-the-art performance in full-head image\nsynthesis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u5e73\u9762\uff08hy-plane\uff09\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u5e73\u9762\u548c\u7403\u5f62\u5e73\u9762\u7684\u4f18\u70b9\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e09\u5e73\u9762\u8868\u793a\u4e2d\u7684\u7279\u5f81\u7ea0\u7f20\u3001\u7279\u5f81\u6620\u5c04\u4e0d\u5747\u5300\u548c\u7279\u5f81\u7a7f\u900f\u95ee\u9898\uff0c\u5728\u5934\u90e8\u56fe\u50cf\u5408\u6210\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4e09\u5e73\u9762\u8868\u793a\u5b58\u5728\u7279\u5f81\u7ea0\u7f20\u5bfc\u81f4\u955c\u50cf\u4f2a\u5f71\uff0c\u7403\u5f62\u4e09\u5e73\u9762\u867d\u7136\u7f13\u89e3\u4e86\u7279\u5f81\u7ea0\u7f20\u4f46\u5b58\u5728\u7279\u5f81\u6620\u5c04\u4e0d\u5747\u5300\u95ee\u9898\uff0c\u4e14\u4e24\u79cd\u65b9\u6cd5\u90fd\u5b58\u5728\u7279\u5f81\u7a7f\u900f\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u5e73\u9762\u8868\u793a\uff0c\u7ed3\u5408\u5e73\u9762\u548c\u7403\u5f62\u5e73\u9762\u7684\u4f18\u52bf\uff1b\u5f15\u5165\u8fd1\u7b49\u9762\u79ef\u626d\u66f2\u7b56\u7565\u66ff\u4ee3\u4f20\u7edf\u7684theta-phi\u626d\u66f2\uff0c\u6700\u5927\u5316\u7279\u5f81\u56fe\u5229\u7528\u7387\uff1b\u751f\u6210\u5668\u5408\u6210\u5355\u901a\u9053\u7edf\u4e00\u7279\u5f81\u56fe\u800c\u975e\u591a\u901a\u9053\u5206\u79bb\u7279\u5f81\u56fe\uff0c\u6d88\u9664\u7279\u5f81\u7a7f\u900f\u3002", "result": "HyPlaneHead\u65b9\u6cd5\u5728\u5934\u90e8\u56fe\u50cf\u5408\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u4e09\u5e73\u9762\u8868\u793a\u7684\u95ee\u9898\u5e76\u63d0\u51fa\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0chy-plane\u8868\u793a\u80fd\u591f\u5145\u5206\u53d1\u6325\u4e09\u5e73\u9762\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u4e3a3D\u611f\u77e5GANs\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8868\u793a\u65b9\u6cd5\u3002"}}
{"id": "2509.16767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16767", "abs": "https://arxiv.org/abs/2509.16767", "authors": ["Ozgur Kara", "Harris Nisar", "James M. Rehg"], "title": "DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation Conditioned on Natural Images", "comment": "Accepted to NeurIPS 2025", "summary": "Numerous models have been developed for scanpath and saliency prediction,\nwhich are typically trained on scanpaths, which model eye movement as a\nsequence of discrete fixation points connected by saccades, while the rich\ninformation contained in the raw trajectories is often discarded. Moreover,\nmost existing approaches fail to capture the variability observed among human\nsubjects viewing the same image. They generally predict a single scanpath of\nfixed, pre-defined length, which conflicts with the inherent diversity and\nstochastic nature of real-world visual attention. To address these challenges,\nwe propose DiffEye, a diffusion-based training framework designed to model\ncontinuous and diverse eye movement trajectories during free viewing of natural\nimages. Our method builds on a diffusion model conditioned on visual stimuli\nand introduces a novel component, namely Corresponding Positional Embedding\n(CPE), which aligns spatial gaze information with the patch-based semantic\nfeatures of the visual input. By leveraging raw eye-tracking trajectories\nrather than relying on scanpaths, DiffEye captures the inherent variability in\nhuman gaze behavior and generates high-quality, realistic eye movement\npatterns, despite being trained on a comparatively small dataset. The generated\ntrajectories can also be converted into scanpaths and saliency maps, resulting\nin outputs that more accurately reflect the distribution of human visual\nattention. DiffEye is the first method to tackle this task on natural images\nusing a diffusion model while fully leveraging the richness of raw eye-tracking\ndata. Our extensive evaluation shows that DiffEye not only achieves\nstate-of-the-art performance in scanpath generation but also enables, for the\nfirst time, the generation of continuous eye movement trajectories. Project\nwebpage: https://diff-eye.github.io/", "AI": {"tldr": "DiffEye\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u8fde\u7eed\u4e14\u591a\u6837\u5316\u7684\u773c\u52a8\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u89c6\u89c9\u6ce8\u610f\u529b\u591a\u6837\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u773c\u52a8\u9884\u6d4b\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u79bb\u6563\u7684\u6ce8\u89c6\u70b9\u5e8f\u5217\uff0c\u4e22\u5f03\u4e86\u539f\u59cb\u8f68\u8ff9\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u4e14\u65e0\u6cd5\u6355\u6349\u4e0d\u540c\u89c2\u5bdf\u8005\u4e4b\u95f4\u7684\u53d8\u5f02\u6027\uff0c\u901a\u5e38\u9884\u6d4b\u56fa\u5b9a\u957f\u5ea6\u7684\u5355\u4e00\u626b\u63cf\u8def\u5f84\u3002", "method": "\u63d0\u51faDiffEye\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528\u89c6\u89c9\u523a\u6fc0\u4f5c\u4e3a\u6761\u4ef6\uff0c\u5f15\u5165\u5bf9\u5e94\u4f4d\u7f6e\u5d4c\u5165(CPE)\u7ec4\u4ef6\u5c06\u7a7a\u95f4\u6ce8\u89c6\u4fe1\u606f\u4e0e\u89c6\u89c9\u8f93\u5165\u7684\u57fa\u4e8e\u8865\u4e01\u7684\u8bed\u4e49\u7279\u5f81\u5bf9\u9f50\uff0c\u5229\u7528\u539f\u59cb\u773c\u52a8\u8f68\u8ff9\u800c\u975e\u626b\u63cf\u8def\u5f84\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "DiffEye\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u771f\u5b9e\u7684\u773c\u52a8\u6a21\u5f0f\uff0c\u5728\u626b\u63cf\u8def\u5f84\u751f\u6210\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u8fde\u7eed\u773c\u52a8\u8f68\u8ff9\u7684\u751f\u6210\u3002", "conclusion": "DiffEye\u662f\u9996\u4e2a\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5e76\u5145\u5206\u5229\u7528\u539f\u59cb\u773c\u52a8\u6570\u636e\u4e30\u5bcc\u6027\u7684\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u8f68\u8ff9\u80fd\u66f4\u51c6\u786e\u5730\u53cd\u6620\u4eba\u7c7b\u89c6\u89c9\u6ce8\u610f\u529b\u7684\u5206\u5e03\u3002"}}
{"id": "2509.16768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16768", "abs": "https://arxiv.org/abs/2509.16768", "authors": ["Omid Bonakdar", "Nasser Mozayani"], "title": "MMPart: Harnessing Multi-Modal Large Language Models for Part-Aware 3D Generation", "comment": null, "summary": "Generative 3D modeling has advanced rapidly, driven by applications in VR/AR,\nmetaverse, and robotics. However, most methods represent the target object as a\nclosed mesh devoid of any structural information, limiting editing, animation,\nand semantic understanding. Part-aware 3D generation addresses this problem by\ndecomposing objects into meaningful components, but existing pipelines face\nchallenges: in existing methods, the user has no control over which objects are\nseparated and how model imagine the occluded parts in isolation phase. In this\npaper, we introduce MMPart, an innovative framework for generating part-aware\n3D models from a single image. We first use a VLM to generate a set of prompts\nbased on the input image and user descriptions. In the next step, a generative\nmodel generates isolated images of each object based on the initial image and\nthe previous step's prompts as supervisor (which control the pose and guide\nmodel how imagine previously occluded areas). Each of those images then enters\nthe multi-view generation stage, where a number of consistent images from\ndifferent views are generated. Finally, a reconstruction model converts each of\nthese multi-view images into a 3D model.", "AI": {"tldr": "MMPart\u662f\u4e00\u4e2a\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u90e8\u4ef6\u611f\u77e53D\u6a21\u578b\u7684\u521b\u65b0\u6846\u67b6\uff0c\u901a\u8fc7VLM\u751f\u6210\u63d0\u793a\u8bcd\uff0c\u5206\u9636\u6bb5\u751f\u6210\u9694\u79bb\u56fe\u50cf\u548c\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u6700\u7ec8\u91cd\u5efa\u4e3a3D\u6a21\u578b", "motivation": "\u73b0\u67093D\u751f\u6210\u65b9\u6cd5\u5c06\u76ee\u6807\u5bf9\u8c61\u8868\u793a\u4e3a\u5c01\u95ed\u7f51\u683c\uff0c\u7f3a\u4e4f\u7ed3\u6784\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u7f16\u8f91\u3001\u52a8\u753b\u548c\u8bed\u4e49\u7406\u89e3\u3002\u73b0\u6709\u90e8\u4ef6\u611f\u77e5\u65b9\u6cd5\u5b58\u5728\u7528\u6237\u65e0\u6cd5\u63a7\u5236\u5bf9\u8c61\u5206\u79bb\u65b9\u5f0f\u548c\u906e\u6321\u90e8\u5206\u60f3\u8c61\u7684\u95ee\u9898", "method": "1. \u4f7f\u7528VLM\u57fa\u4e8e\u8f93\u5165\u56fe\u50cf\u548c\u7528\u6237\u63cf\u8ff0\u751f\u6210\u63d0\u793a\u8bcd\uff1b2. \u751f\u6210\u6a21\u578b\u6839\u636e\u521d\u59cb\u56fe\u50cf\u548c\u63d0\u793a\u8bcd\u751f\u6210\u6bcf\u4e2a\u5bf9\u8c61\u7684\u9694\u79bb\u56fe\u50cf\uff1b3. \u591a\u89c6\u89d2\u751f\u6210\u9636\u6bb5\u751f\u6210\u4e00\u81f4\u7684\u591a\u89c6\u89d2\u56fe\u50cf\uff1b4. \u91cd\u5efa\u6a21\u578b\u5c06\u591a\u89c6\u89d2\u56fe\u50cf\u8f6c\u6362\u4e3a3D\u6a21\u578b", "result": "\u63d0\u51fa\u4e86MMPart\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u5177\u6709\u90e8\u4ef6\u7ed3\u6784\u76843D\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5bf9\u8c61\u5206\u79bb\u63a7\u5236\u548c\u906e\u6321\u90e8\u5206\u60f3\u8c61\u65b9\u9762\u7684\u5c40\u9650\u6027", "conclusion": "MMPart\u6846\u67b6\u4e3a\u751f\u6210\u90e8\u4ef6\u611f\u77e53D\u6a21\u578b\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5904\u7406\u5b9e\u73b0\u4e86\u5bf9\u5bf9\u8c61\u5206\u79bb\u548c\u906e\u6321\u90e8\u5206\u60f3\u8c61\u7684\u6709\u6548\u63a7\u5236"}}
{"id": "2509.16771", "categories": ["cs.CV", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2509.16771", "abs": "https://arxiv.org/abs/2509.16771", "authors": ["Xiaohan Chen", "Hongrui Gu", "Cunshi Wang", "Haiyang Mu", "Jie Zheng", "Junju Du", "Jing Ren", "Zhou Fan", "Jing Li"], "title": "Artificial Satellite Trails Detection Using U-Net Deep Neural Network and Line Segment Detector Algorithm", "comment": "15 pages, 7 figures, 2 tables, PASP accepted", "summary": "With the rapid increase in the number of artificial satellites, astronomical\nimaging is experiencing growing interference. When these satellites reflect\nsunlight, they produce streak-like artifacts in photometry images. Such\nsatellite trails can introduce false sources and cause significant photometric\nerrors. As a result, accurately identifying the positions of satellite trails\nin observational data has become essential. In this work, we propose a\nsatellite trail detection model that combines the U-Net deep neural network for\nimage segmentation with the Line Segment Detector (LSD) algorithm. The model is\ntrained on 375 simulated images of satellite trails, generated using data from\nthe Mini-SiTian Array. Experimental results show that for trails with a\nsignal-to-noise ratio (SNR) greater than 3, the detection rate exceeds 99.\nAdditionally, when applied to real observational data from the Mini-SiTian\nArray, the model achieves a recall of 79.57 and a precision of 74.56.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408U-Net\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u548cLSD\u7b97\u6cd5\u7684\u536b\u661f\u8f68\u8ff9\u68c0\u6d4b\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u5929\u6587\u89c2\u6d4b\u4e2d\u536b\u661f\u8f68\u8ff9\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u4eba\u9020\u536b\u661f\u6570\u91cf\u7684\u5feb\u901f\u589e\u957f\uff0c\u5929\u6587\u6210\u50cf\u53d7\u5230\u8d8a\u6765\u8d8a\u4e25\u91cd\u7684\u5e72\u6270\u3002\u536b\u661f\u53cd\u5c04\u9633\u5149\u4f1a\u5728\u6d4b\u5149\u56fe\u50cf\u4e2d\u4ea7\u751f\u6761\u7eb9\u72b6\u4f2a\u5f71\uff0c\u8fd9\u4e9b\u536b\u661f\u8f68\u8ff9\u4f1a\u5f15\u5165\u865a\u5047\u6e90\u5e76\u5bfc\u81f4\u663e\u8457\u7684\u5149\u5ea6\u6d4b\u91cf\u8bef\u5dee\u3002", "method": "\u4f7f\u7528U-Net\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u56fe\u50cf\u5206\u5272\uff0c\u7ed3\u5408Line Segment Detector (LSD)\u7b97\u6cd5\uff0c\u5728375\u5f20\u57fa\u4e8eMini-SiTian\u9635\u5217\u6570\u636e\u751f\u6210\u7684\u6a21\u62df\u536b\u661f\u8f68\u8ff9\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5bf9\u4e8e\u4fe1\u566a\u6bd4(SNR)\u5927\u4e8e3\u7684\u8f68\u8ff9\uff0c\u68c0\u6d4b\u7387\u8d85\u8fc799%\u3002\u5728Mini-SiTian\u9635\u5217\u771f\u5b9e\u89c2\u6d4b\u6570\u636e\u4e0a\uff0c\u6a21\u578b\u53ec\u56de\u7387\u8fbe\u523079.57%\uff0c\u7cbe\u786e\u7387\u8fbe\u523074.56%\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5929\u6587\u56fe\u50cf\u4e2d\u7684\u536b\u661f\u8f68\u8ff9\uff0c\u4e3a\u51cf\u5c11\u536b\u661f\u5e72\u6270\u5bf9\u5929\u6587\u89c2\u6d4b\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2509.16805", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16805", "abs": "https://arxiv.org/abs/2509.16805", "authors": ["Md. Atabuzzaman", "Ali Asgarov", "Chris Thomas"], "title": "Benchmarking and Mitigating MCQA Selection Bias of Large Vision-Language Models", "comment": "Accepted to EMNLP 2025 (Main Conference)", "summary": "Large Vision-Language Models (LVLMs) have achieved strong performance on\nvision-language tasks, particularly Visual Question Answering (VQA). While\nprior work has explored unimodal biases in VQA, the problem of selection bias\nin Multiple-Choice Question Answering (MCQA), where models may favor specific\noption tokens (e.g., \"A\") or positions, remains underexplored. In this paper,\nwe investigate both the presence and nature of selection bias in LVLMs through\nfine-grained MCQA benchmarks spanning easy, medium, and hard difficulty levels,\ndefined by the semantic similarity of the options. We further propose an\ninference-time logit-level debiasing method that estimates an ensemble bias\nvector from general and contextual prompts and applies confidence-adaptive\ncorrections to the model's output. Our method mitigates bias without retraining\nand is compatible with frozen LVLMs. Extensive experiments across several\nstate-of-the-art models reveal consistent selection biases that intensify with\ntask difficulty, and show that our mitigation approach significantly reduces\nbias while improving accuracy in challenging settings. This work offers new\ninsights into the limitations of LVLMs in MCQA and presents a practical\napproach to improve their robustness in fine-grained visual reasoning. Datasets\nand code are available at:\nhttps://github.com/Atabuzzaman/Selection-Bias-of-LVLMs", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9879\u9009\u62e9\u9898\u4e2d\u7684\u9009\u62e9\u504f\u89c1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u63a8\u7406\u65f6logit\u7ea7\u522b\u7684\u53bb\u504f\u89c1\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6709\u6548\u51cf\u8f7b\u504f\u89c1\u5e76\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u867d\u7136LVLMs\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u591a\u9879\u9009\u62e9\u9898\u4e2d\u7684\u9009\u62e9\u504f\u89c1\u95ee\u9898\uff08\u5982\u6a21\u578b\u504f\u597d\u7279\u5b9a\u9009\u9879\u6807\u8bb0\u6216\u4f4d\u7f6e\uff09\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u63a8\u7406\u65f6logit\u7ea7\u522b\u7684\u53bb\u504f\u89c1\u65b9\u6cd5\uff0c\u901a\u8fc7\u901a\u7528\u548c\u4e0a\u4e0b\u6587\u63d0\u793a\u4f30\u8ba1\u96c6\u6210\u504f\u89c1\u5411\u91cf\uff0c\u5e76\u5e94\u7528\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u6821\u6b63\u6765\u4fee\u6b63\u6a21\u578b\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLVLMs\u5b58\u5728\u4e00\u81f4\u7684\u9009\u62e9\u504f\u89c1\uff0c\u4e14\u968f\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\u800c\u52a0\u5267\uff1b\u6240\u63d0\u51fa\u7684\u53bb\u504f\u89c1\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u504f\u89c1\uff0c\u5e76\u5728\u6311\u6218\u6027\u8bbe\u7f6e\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86LVLMs\u5728MCQA\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u5176\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u63a8\u7406\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.16806", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16806", "abs": "https://arxiv.org/abs/2509.16806", "authors": ["Kacper Marzol", "Ignacy Kolton", "Weronika Smolak-Dy\u017cewska", "Joanna Kaleta", "Marcin Mazur", "Przemys\u0142aw Spurek"], "title": "MedGS: Gaussian Splatting for Multi-Modal 3D Medical Imaging", "comment": null, "summary": "Multi-modal three-dimensional (3D) medical imaging data, derived from\nultrasound, magnetic resonance imaging (MRI), and potentially computed\ntomography (CT), provide a widely adopted approach for non-invasive anatomical\nvisualization. Accurate modeling, registration, and visualization in this\nsetting depend on surface reconstruction and frame-to-frame interpolation.\nTraditional methods often face limitations due to image noise and incomplete\ninformation between frames. To address these challenges, we present MedGS, a\nsemi-supervised neural implicit surface reconstruction framework that employs a\nGaussian Splatting (GS)-based interpolation mechanism. In this framework,\nmedical imaging data are represented as consecutive two-dimensional (2D) frames\nembedded in 3D space and modeled using Gaussian-based distributions. This\nrepresentation enables robust frame interpolation and high-fidelity surface\nreconstruction across imaging modalities. As a result, MedGS offers more\nefficient training than traditional neural implicit methods. Its explicit\nGS-based representation enhances noise robustness, allows flexible editing, and\nsupports precise modeling of complex anatomical structures with fewer\nartifacts. These features make MedGS highly suitable for scalable and practical\napplications in medical imaging.", "AI": {"tldr": "MedGS\u662f\u4e00\u4e2a\u534a\u76d1\u7763\u795e\u7ecf\u9690\u5f0f\u8868\u9762\u91cd\u5efa\u6846\u67b6\uff0c\u4f7f\u7528\u9ad8\u65af\u6cfc\u6e85\u63d2\u503c\u673a\u5236\u5904\u7406\u591a\u6a21\u60013D\u533b\u5b66\u5f71\u50cf\u6570\u636e\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5e27\u95f4\u63d2\u503c\u548c\u9ad8\u8d28\u91cf\u8868\u9762\u91cd\u5efa\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u533b\u5b66\u5f71\u50cf\u8868\u9762\u91cd\u5efa\u548c\u5e27\u95f4\u63d2\u503c\u4e2d\u9762\u4e34\u56fe\u50cf\u566a\u58f0\u548c\u5e27\u95f4\u4fe1\u606f\u4e0d\u5b8c\u6574\u7684\u9650\u5236\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u533b\u5b66\u5f71\u50cf\u6570\u636e\u8868\u793a\u4e3a3D\u7a7a\u95f4\u4e2d\u8fde\u7eed\u76842D\u5e27\uff0c\u4f7f\u7528\u57fa\u4e8e\u9ad8\u65af\u5206\u5e03\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u65af\u6cfc\u6e85\u63d2\u503c\u673a\u5236\u8fdb\u884c\u534a\u76d1\u7763\u5b66\u4e60\u3002", "result": "MedGS\u6bd4\u4f20\u7edf\u795e\u7ecf\u9690\u5f0f\u65b9\u6cd5\u8bad\u7ec3\u66f4\u9ad8\u6548\uff0c\u5177\u6709\u66f4\u597d\u7684\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u652f\u6301\u7075\u6d3b\u7f16\u8f91\uff0c\u5e76\u80fd\u7cbe\u786e\u5efa\u6a21\u590d\u6742\u89e3\u5256\u7ed3\u6784\u4e14\u51cf\u5c11\u4f2a\u5f71\u3002", "conclusion": "MedGS\u6846\u67b6\u975e\u5e38\u9002\u5408\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u53ef\u6269\u5c55\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u4e3a\u591a\u6a21\u60013D\u533b\u5b66\u6210\u50cf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8868\u9762\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16822", "abs": "https://arxiv.org/abs/2509.16822", "authors": ["Townim Faisal Chowdhury", "Vu Minh Hieu Phan", "Kewen Liao", "Nanyu Dong", "Minh-Son To", "Anton Hengel", "Johan Verjans", "Zhibin Liao"], "title": "Looking in the mirror: A faithful counterfactual explanation method for interpreting deep image classification models", "comment": "Accepted at IEEE/CVF International Conference on Computer Vision\n  (ICCV), 2025", "summary": "Counterfactual explanations (CFE) for deep image classifiers aim to reveal\nhow minimal input changes lead to different model decisions, providing critical\ninsights for model interpretation and improvement. However, existing CFE\nmethods often rely on additional image encoders and generative models to create\nplausible images, neglecting the classifier's own feature space and decision\nboundaries. As such, they do not explain the intrinsic feature space and\ndecision boundaries learned by the classifier. To address this limitation, we\npropose Mirror-CFE, a novel method that generates faithful counterfactual\nexplanations by operating directly in the classifier's feature space, treating\ndecision boundaries as mirrors that ``reflect'' feature representations in the\nmirror. Mirror-CFE learns a mapping function from feature space to image space\nwhile preserving distance relationships, enabling smooth transitions between\nsource images and their counterfactuals. Through extensive experiments on four\nimage datasets, we demonstrate that Mirror-CFE achieves superior performance in\nvalidity while maintaining input resemblance compared to state-of-the-art\nexplanation methods. Finally, mirror-CFE provides interpretable visualization\nof the classifier's decision process by generating step-wise transitions that\nreveal how features evolve as classification confidence changes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMirror-CFE\u7684\u65b0\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728\u5206\u7c7b\u5668\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u5c06\u51b3\u7b56\u8fb9\u754c\u89c6\u4e3a\"\u955c\u5b50\"\u6765\u53cd\u5c04\u7279\u5f81\u8868\u793a\uff0c\u65e0\u9700\u4f9d\u8d56\u989d\u5916\u7684\u56fe\u50cf\u7f16\u7801\u5668\u548c\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u989d\u5916\u7684\u56fe\u50cf\u7f16\u7801\u5668\u548c\u751f\u6210\u6a21\u578b\u6765\u521b\u5efa\u5408\u7406\u7684\u56fe\u50cf\uff0c\u4f46\u5ffd\u7565\u4e86\u5206\u7c7b\u5668\u81ea\u8eab\u7684\u7279\u5f81\u7a7a\u95f4\u548c\u51b3\u7b56\u8fb9\u754c\uff0c\u65e0\u6cd5\u89e3\u91ca\u5206\u7c7b\u5668\u5b66\u4e60\u7684\u5185\u5728\u7279\u5f81\u7a7a\u95f4\u548c\u51b3\u7b56\u8fb9\u754c\u3002", "method": "Mirror-CFE\u65b9\u6cd5\u76f4\u63a5\u5728\u5206\u7c7b\u5668\u7279\u5f81\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u5c06\u51b3\u7b56\u8fb9\u754c\u4f5c\u4e3a\u955c\u5b50\u53cd\u5c04\u7279\u5f81\u8868\u793a\uff0c\u5b66\u4e60\u4ece\u7279\u5f81\u7a7a\u95f4\u5230\u56fe\u50cf\u7a7a\u95f4\u7684\u6620\u5c04\u51fd\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u8ddd\u79bb\u5173\u7cfb\uff0c\u5b9e\u73b0\u6e90\u56fe\u50cf\u4e0e\u5176\u53cd\u4e8b\u5b9e\u4e4b\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002", "result": "\u5728\u56db\u4e2a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMirror-CFE\u5728\u4fdd\u6301\u8f93\u5165\u76f8\u4f3c\u6027\u7684\u540c\u65f6\uff0c\u5728\u6709\u6548\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89e3\u91ca\u65b9\u6cd5\u3002", "conclusion": "Mirror-CFE\u901a\u8fc7\u751f\u6210\u9010\u6b65\u8fc7\u6e21\u7684\u53ef\u89c6\u5316\uff0c\u63ed\u793a\u4e86\u7279\u5f81\u5982\u4f55\u968f\u7740\u5206\u7c7b\u7f6e\u4fe1\u5ea6\u53d8\u5316\u800c\u6f14\u5316\uff0c\u4e3a\u5206\u7c7b\u5668\u7684\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u53ef\u89c6\u5316\u3002"}}
{"id": "2509.16832", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.16832", "abs": "https://arxiv.org/abs/2509.16832", "authors": ["Ziyang Xu", "Benedikt Schwab", "Yihui Yang", "Thomas H. Kolbe", "Christoph Holst"], "title": "L2M-Reg: Building-level Uncertainty-aware Registration of Outdoor LiDAR Point Clouds and Semantic 3D City Models", "comment": "submit to ISPRS Journal of Photogrammetry and Remote Sensing", "summary": "Accurate registration between LiDAR (Light Detection and Ranging) point\nclouds and semantic 3D city models is a fundamental topic in urban digital\ntwinning and a prerequisite for downstream tasks, such as digital construction,\nchange detection and model refinement. However, achieving accurate\nLiDAR-to-Model registration at individual building level remains challenging,\nparticularly due to the generalization uncertainty in semantic 3D city models\nat the Level of Detail 2 (LoD2). This paper addresses this gap by proposing\nL2M-Reg, a plane-based fine registration method that explicitly accounts for\nmodel uncertainty. L2M-Reg consists of three key steps: establishing reliable\nplane correspondence, building a pseudo-plane-constrained Gauss-Helmert model,\nand adaptively estimating vertical translation. Experiments on three real-world\ndatasets demonstrate that L2M-Reg is both more accurate and computationally\nefficient than existing ICP-based and plane-based methods. Overall, L2M-Reg\nprovides a novel building-level solution regarding LiDAR-to-Model registration\nwhen model uncertainty is present.", "AI": {"tldr": "L2M-Reg\u662f\u4e00\u79cd\u57fa\u4e8e\u5e73\u9762\u7684\u7cbe\u7ec6\u914d\u51c6\u65b9\u6cd5\uff0c\u4e13\u95e8\u89e3\u51b3LiDAR\u70b9\u4e91\u4e0e\u8bed\u4e493D\u57ce\u5e02\u6a21\u578b\u5728LoD2\u7ea7\u522b\u4e0b\u7684\u914d\u51c6\u95ee\u9898\uff0c\u901a\u8fc7\u8003\u8651\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u6765\u63d0\u9ad8\u914d\u51c6\u7cbe\u5ea6\u3002", "motivation": "LiDAR\u70b9\u4e91\u4e0e\u8bed\u4e493D\u57ce\u5e02\u6a21\u578b\u7684\u7cbe\u786e\u914d\u51c6\u662f\u57ce\u5e02\u6570\u5b57\u5b6a\u751f\u7684\u57fa\u7840\uff0c\u4f46\u5728LoD2\u7ea7\u522b\u7684\u5355\u4e2a\u5efa\u7b51\u5c42\u9762\u5b9e\u73b0\u7cbe\u786e\u914d\u51c6\u4ecd\u5177\u6311\u6218\u6027\uff0c\u4e3b\u8981\u7531\u4e8e\u8bed\u4e493D\u57ce\u5e02\u6a21\u578b\u5b58\u5728\u6cdb\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "method": "L2M-Reg\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\uff1a\u5efa\u7acb\u53ef\u9760\u7684\u5e73\u9762\u5bf9\u5e94\u5173\u7cfb\u3001\u6784\u5efa\u4f2a\u5e73\u9762\u7ea6\u675f\u7684Gauss-Helmert\u6a21\u578b\u3001\u81ea\u9002\u5e94\u4f30\u8ba1\u5782\u76f4\u5e73\u79fb\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cL2M-Reg\u6bd4\u73b0\u6709\u7684\u57fa\u4e8eICP\u548c\u57fa\u4e8e\u5e73\u9762\u7684\u65b9\u6cd5\u66f4\u51c6\u786e\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "L2M-Reg\u4e3a\u5b58\u5728\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684LiDAR\u5230\u6a21\u578b\u914d\u51c6\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5efa\u7b51\u7ea7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16853", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16853", "abs": "https://arxiv.org/abs/2509.16853", "authors": ["Jinhao Wang", "Cihan Ruan", "Nam Ling", "Wei Wang", "Wei Jiang"], "title": "ISCS: Parameter-Guided Channel Ordering and Grouping for Learned Image Compression", "comment": null, "summary": "Prior studies in learned image compression (LIC) consistently show that only\na small subset of latent channels is critical for reconstruction, while many\nothers carry limited information. Exploiting this imbalance could improve both\ncoding and computational efficiency, yet existing approaches often rely on\ncostly, dataset-specific ablation tests and typically analyze channels in\nisolation, ignoring their interdependencies.\n  We propose a generalizable, dataset-agnostic method to identify and organize\nimportant channels in pretrained VAE-based LIC models. Instead of brute-force\nempirical evaluations, our approach leverages intrinsic parameter\nstatistics-weight variances, bias magnitudes, and pairwise correlations-to\nestimate channel importance. This analysis reveals a consistent organizational\nstructure, termed the Invariant Salient Channel Space (ISCS), where\nSalient-Core channels capture dominant structures and Salient-Auxiliary\nchannels provide complementary details. Building on ISCS, we introduce a\ndeterministic channel ordering and grouping strategy that enables\nslice-parallel decoding, reduces redundancy, and improves bitrate efficiency.\n  Experiments across multiple LIC architectures demonstrate that our method\neffectively reduces bitrate and computation while maintaining reconstruction\nquality, providing a practical and modular enhancement to existing learned\ncompression frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3VAE\u7684LIC\u6a21\u578b\u4e2d\u8bc6\u522b\u548c\u7ec4\u7ec7\u91cd\u8981\u901a\u9053\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u7edf\u8ba1\u7279\u6027\u6765\u4f30\u8ba1\u901a\u9053\u91cd\u8981\u6027\uff0c\u53d1\u73b0\u4e86\u4e0d\u53d8\u663e\u8457\u901a\u9053\u7a7a\u95f4(ISCS)\u7ed3\u6784\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u786e\u5b9a\u6027\u901a\u9053\u6392\u5e8f\u548c\u5206\u7ec4\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5207\u7247\u5e76\u884c\u89e3\u7801\uff0c\u63d0\u9ad8\u4e86\u7f16\u7801\u6548\u7387\u3002", "motivation": "\u73b0\u6709LIC\u7814\u7a76\u4e2d\u53d1\u73b0\u53ea\u6709\u5c11\u91cf\u6f5c\u5728\u901a\u9053\u5bf9\u91cd\u5efa\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u6570\u636e\u96c6\u7279\u5b9a\u6d88\u878d\u6d4b\u8bd5\u4e14\u5b64\u7acb\u5206\u6790\u901a\u9053\uff0c\u5ffd\u7565\u4e86\u901a\u9053\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u5229\u7528\u6743\u91cd\u65b9\u5dee\u3001\u504f\u7f6e\u5927\u5c0f\u548c\u6210\u5bf9\u76f8\u5173\u6027\u7b49\u5185\u5728\u53c2\u6570\u7edf\u8ba1\u7279\u6027\u6765\u4f30\u8ba1\u901a\u9053\u91cd\u8981\u6027\uff0c\u8bc6\u522b\u51faISCS\u7ed3\u6784\uff08\u663e\u8457\u6838\u5fc3\u901a\u9053\u548c\u663e\u8457\u8f85\u52a9\u901a\u9053\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u786e\u5b9a\u6027\u901a\u9053\u6392\u5e8f\u548c\u5206\u7ec4\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2aLIC\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u6bd4\u7279\u7387\u548c\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u73b0\u6709\u5b66\u4e60\u538b\u7f29\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u6a21\u5757\u5316\u7684\u589e\u5f3a\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u7f16\u7801\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2509.16863", "categories": ["cs.CV", "68T20, 68U20"], "pdf": "https://arxiv.org/pdf/2509.16863", "abs": "https://arxiv.org/abs/2509.16863", "authors": ["Amanuel T. Dufera", "Yuan-Li Cai"], "title": "ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM", "comment": null, "summary": "We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM\nsystem for robust, highfidelity RGB-only reconstruction. Addressing geometric\ninaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable\ndepth estimation, ConfidentSplat incorporates a core innovation: a\nconfidence-weighted fusion mechanism. This mechanism adaptively integrates\ndepth cues from multiview geometry with learned monocular priors (Omnidata\nViT), dynamically weighting their contributions based on explicit reliability\nestimates-derived predominantly from multi-view geometric consistency-to\ngenerate high-fidelity proxy depth for map supervision. The resulting proxy\ndepth guides the optimization of a deformable 3DGS map, which efficiently\nadapts online to maintain global consistency following pose updates from a\nDROID-SLAM-inspired frontend and backend optimizations (loop closure, global\nbundle adjustment). Extensive validation on standard benchmarks (TUM-RGBD,\nScanNet) and diverse custom mobile datasets demonstrates significant\nimprovements in reconstruction accuracy (L1 depth error) and novel view\nsynthesis fidelity (PSNR, SSIM, LPIPS) over baselines, particularly in\nchallenging conditions. ConfidentSplat underscores the efficacy of principled,\nconfidence-aware sensor fusion for advancing state-of-the-art dense visual\nSLAM.", "AI": {"tldr": "ConfidentSplat\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684RGB-only SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u52a0\u6743\u878d\u5408\u673a\u5236\u7ed3\u5408\u591a\u89c6\u89d2\u51e0\u4f55\u548c\u5355\u76ee\u6df1\u5ea6\u5148\u9a8c\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u56e0\u4e0d\u53ef\u9760\u6df1\u5ea6\u4f30\u8ba1\u5bfc\u81f4\u7684\u51e0\u4f55\u4e0d\u51c6\u786e\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RGB-only 3DGS SLAM\u65b9\u6cd5\u7531\u4e8e\u6df1\u5ea6\u4f30\u8ba1\u4e0d\u53ef\u9760\u5bfc\u81f4\u51e0\u4f55\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u878d\u5408\u591a\u6e90\u6df1\u5ea6\u4fe1\u606f\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u7f6e\u4fe1\u5ea6\u52a0\u6743\u878d\u5408\u673a\u5236\uff0c\u52a8\u6001\u6574\u5408\u591a\u89c6\u89d2\u51e0\u4f55\u6df1\u5ea6\u548cOmnidata ViT\u5355\u76ee\u6df1\u5ea6\u5148\u9a8c\uff0c\u57fa\u4e8e\u591a\u89c6\u89d2\u51e0\u4f55\u4e00\u81f4\u6027\u751f\u6210\u53ef\u9760\u6027\u4f30\u8ba1\uff0c\u6307\u5bfc\u53ef\u53d8\u5f623D\u9ad8\u65af\u6cfc\u6e85\u5730\u56fe\u7684\u4f18\u5316\u3002", "result": "\u5728TUM-RGBD\u3001ScanNet\u7b49\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u548c\u79fb\u52a8\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u91cd\u5efa\u7cbe\u5ea6\uff08L1\u6df1\u5ea6\u8bef\u5dee\uff09\u548c\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\uff08PSNR\u3001SSIM\u3001LPIPS\uff09\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63a8\u8fdb\u5bc6\u96c6\u89c6\u89c9SLAM\u6280\u672f\u7684\u5148\u8fdb\u6c34\u5e73\uff0c\u7279\u522b\u662f\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.16873", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16873", "abs": "https://arxiv.org/abs/2509.16873", "authors": ["Yuanzhi Li", "Lebin Zhou", "Nam Ling", "Zhenghao Chen", "Wei Wang", "Wei Jiang"], "title": "$\\mathtt{M^3VIR}$: A Large-Scale Multi-Modality Multi-View Synthesized Benchmark Dataset for Image Restoration and Content Creation", "comment": null, "summary": "The gaming and entertainment industry is rapidly evolving, driven by\nimmersive experiences and the integration of generative AI (GAI) technologies.\nTraining such models effectively requires large-scale datasets that capture the\ndiversity and context of gaming environments. However, existing datasets are\noften limited to specific domains or rely on artificial degradations, which do\nnot accurately capture the unique characteristics of gaming content. Moreover,\nbenchmarks for controllable video generation remain absent.\n  To address these limitations, we introduce $\\mathtt{M^3VIR}$, a large-scale,\nmulti-modal, multi-view dataset specifically designed to overcome the\nshortcomings of current resources. Unlike existing datasets, $\\mathtt{M^3VIR}$\nprovides diverse, high-fidelity gaming content rendered with Unreal Engine 5,\noffering authentic ground-truth LR-HR paired and multi-view frames across 80\nscenes in 8 categories. It includes $\\mathtt{M^3VIR\\_MR}$ for super-resolution\n(SR), novel view synthesis (NVS), and combined NVS+SR tasks, and\n$\\mathtt{M^3VIR\\_{MS}}$, the first multi-style, object-level ground-truth set\nenabling research on controlled video generation. Additionally, we benchmark\nseveral state-of-the-art SR and NVS methods to establish performance baselines.\nWhile no existing approaches directly handle controlled video generation,\n$\\mathtt{M^3VIR}$ provides a benchmark for advancing this area. By releasing\nthe dataset, we aim to facilitate research in AI-powered restoration,\ncompression, and controllable content generation for next-generation cloud\ngaming and entertainment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86M\u00b3VIR\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u4e3a\u6e38\u620f\u548c\u5a31\u4e50\u884c\u4e1a\u8bbe\u8ba1\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u591a\u89c6\u56fe\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u6355\u6349\u6e38\u620f\u5185\u5bb9\u72ec\u7279\u7279\u5f81\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u53ef\u63a7\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u6216\u4f9d\u8d56\u4eba\u5de5\u964d\u8d28\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u6e38\u620f\u5185\u5bb9\u7684\u72ec\u7279\u7279\u5f81\uff0c\u4e14\u7f3a\u4e4f\u53ef\u63a7\u89c6\u9891\u751f\u6210\u7684\u57fa\u51c6\u3002", "method": "\u4f7f\u7528Unreal Engine 5\u6e32\u67d380\u4e2a\u573a\u666f\u76848\u4e2a\u7c7b\u522b\uff0c\u63d0\u4f9b\u9ad8\u4fdd\u771f\u5ea6\u7684LR-HR\u914d\u5bf9\u548c\u591a\u89c6\u56fe\u5e27\uff0c\u5305\u62ecM\u00b3VIR_MR\u7528\u4e8e\u8d85\u5206\u8fa8\u7387\u3001\u65b0\u89c6\u56fe\u5408\u6210\u53ca\u7ec4\u5408\u4efb\u52a1\uff0c\u4ee5\u53caM\u00b3VIR_MS\u7528\u4e8e\u53ef\u63a7\u89c6\u9891\u751f\u6210\u7814\u7a76\u3002", "result": "\u5efa\u7acb\u4e86\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u8d85\u5206\u8fa8\u7387\u548c\u65b0\u89c6\u56fe\u5408\u6210\u65b9\u6cd5\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u5e76\u4e3a\u53ef\u63a7\u89c6\u9891\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u9996\u4e2a\u591a\u98ce\u683c\u3001\u5bf9\u8c61\u7ea7\u771f\u5b9e\u6570\u636e\u96c6\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03M\u00b3VIR\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u4e0b\u4e00\u4ee3\u4e91\u6e38\u620f\u548c\u5a31\u4e50\u4e2dAI\u9a71\u52a8\u7684\u6062\u590d\u3001\u538b\u7f29\u548c\u53ef\u63a7\u5185\u5bb9\u751f\u6210\u7684\u7814\u7a76\u3002"}}
{"id": "2509.16886", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16886", "abs": "https://arxiv.org/abs/2509.16886", "authors": ["Yingzhen Hu", "Yiheng Zhong", "Ruobing Li", "Yingxue Su", "Jiabao An", "Feilong Tang", "Jionglong Su", "Imran Razzak"], "title": "SAM-DCE: Addressing Token Uniformity and Semantic Over-Smoothing in Medical Segmentation", "comment": null, "summary": "The Segment Anything Model (SAM) demonstrates impressive zero-shot\nsegmentation ability on natural images but encounters difficulties in medical\nimaging due to domain shifts, anatomical variability, and its reliance on\nuser-provided prompts. Recent prompt-free adaptations alleviate the need for\nexpert intervention, yet still suffer from limited robustness and adaptability,\noften overlooking the issues of semantic over-smoothing and token uniformity.\nWe propose SAM-DCE, which balances local discrimination and global semantics\nwhile mitigating token uniformity, enhancing inter-class separability, and\nenriching mask decoding with fine-grained, consistent representations.\nExtensive experiments on diverse medical benchmarks validate its effectiveness.", "AI": {"tldr": "SAM-DCE\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u5c40\u90e8\u5224\u522b\u6027\u548c\u5168\u5c40\u8bed\u4e49\uff0c\u89e3\u51b3SAM\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u9047\u5230\u7684\u9886\u57df\u504f\u79fb\u3001\u89e3\u5256\u53d8\u5f02\u6027\u548c\u8bed\u4e49\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "motivation": "SAM\u6a21\u578b\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u5206\u5272\u80fd\u529b\uff0c\u4f46\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u7531\u4e8e\u9886\u57df\u504f\u79fb\u3001\u89e3\u5256\u53d8\u5f02\u6027\u548c\u5bf9\u7528\u6237\u63d0\u793a\u7684\u4f9d\u8d56\u800c\u9047\u5230\u56f0\u96be\u3002\u73b0\u6709\u7684\u65e0\u63d0\u793a\u9002\u5e94\u65b9\u6cd5\u867d\u7136\u51cf\u5c11\u4e86\u4e13\u5bb6\u5e72\u9884\uff0c\u4f46\u4ecd\u5b58\u5728\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSAM-DCE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u5c40\u90e8\u5224\u522b\u6027\u548c\u5168\u5c40\u8bed\u4e49\uff0c\u7f13\u89e3\u6807\u8bb0\u5747\u5300\u6027\u95ee\u9898\uff0c\u589e\u5f3a\u7c7b\u95f4\u53ef\u5206\u6027\uff0c\u5e76\u901a\u8fc7\u7ec6\u7c92\u5ea6\u4e00\u81f4\u7684\u8868\u793a\u4e30\u5bcc\u63a9\u7801\u89e3\u7801\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SAM-DCE\u80fd\u591f\u6709\u6548\u63d0\u5347SAM\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u8bed\u4e49\u8fc7\u5e73\u6ed1\u548c\u6807\u8bb0\u5747\u5300\u6027\u95ee\u9898\u3002"}}
{"id": "2509.16888", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16888", "abs": "https://arxiv.org/abs/2509.16888", "authors": ["Youwei Pang", "Xiaoqi Zhao", "Lihe Zhang", "Huchuan Lu", "Georges El Fakhri", "Xiaofeng Liu", "Shijian Lu"], "title": "Rethinking Evaluation of Infrared Small Target Detection", "comment": "NeurIPS 2025; Evaluation Toolkit:\n  https://github.com/lartpang/PyIRSTDMetrics", "summary": "As an essential vision task, infrared small target detection (IRSTD) has seen\nsignificant advancements through deep learning. However, critical limitations\nin current evaluation protocols impede further progress. First, existing\nmethods rely on fragmented pixel- and target-level specific metrics, which\nfails to provide a comprehensive view of model capabilities. Second, an\nexcessive emphasis on overall performance scores obscures crucial error\nanalysis, which is vital for identifying failure modes and improving real-world\nsystem performance. Third, the field predominantly adopts dataset-specific\ntraining-testing paradigms, hindering the understanding of model robustness and\ngeneralization across diverse infrared scenarios. This paper addresses these\nissues by introducing a hybrid-level metric incorporating pixel- and\ntarget-level performance, proposing a systematic error analysis method, and\nemphasizing the importance of cross-dataset evaluation. These aim to offer a\nmore thorough and rational hierarchical analysis framework, ultimately\nfostering the development of more effective and robust IRSTD models. An\nopen-source toolkit has be released to facilitate standardized benchmarking.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u6df7\u5408\u7ea7\u5ea6\u91cf\u3001\u7cfb\u7edf\u8bef\u5dee\u5206\u6790\u548c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u7684\u65b0\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f7f\u7528\u788e\u7247\u5316\u7684\u50cf\u7d20\u7ea7\u548c\u76ee\u6807\u7ea7\u6307\u6807\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\uff1b2\uff09\u8fc7\u5ea6\u5173\u6ce8\u6574\u4f53\u6027\u80fd\u5206\u6570\uff0c\u5ffd\u89c6\u5173\u952e\u8bef\u5dee\u5206\u6790\uff1b3\uff09\u91c7\u7528\u6570\u636e\u96c6\u7279\u5b9a\u7684\u8bad\u7ec3\u6d4b\u8bd5\u8303\u5f0f\uff0c\u963b\u788d\u4e86\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u7ea7\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7ed3\u5408\u50cf\u7d20\u7ea7\u548c\u76ee\u6807\u7ea7\u6027\u80fd\uff1b\u5f00\u53d1\u7cfb\u7edf\u8bef\u5dee\u5206\u6790\u65b9\u6cd5\uff1b\u5f3a\u8c03\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u3001\u5408\u7406\u7684\u5206\u5c42\u5206\u6790\u6846\u67b6\uff0c\u5e76\u53d1\u5e03\u4e86\u5f00\u6e90\u5de5\u5177\u5305\u4ee5\u4fc3\u8fdb\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u63a8\u52a8\u5f00\u53d1\u66f4\u6709\u6548\u3001\u66f4\u9c81\u68d2\u7684\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u4e3a\u9886\u57df\u63d0\u4f9b\u66f4\u79d1\u5b66\u7684\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2509.16892", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16892", "abs": "https://arxiv.org/abs/2509.16892", "authors": ["Jiahe Qian", "Yaoyu Fang", "Ziqiao Weng", "Xinkun Wang", "Lee A. Cooper", "Bo Zhou"], "title": "Learning from Gene Names, Expression Values and Images: Contrastive Masked Text-Image Pretraining for Spatial Transcriptomics Representation Learning", "comment": "9 pages, 3 figures", "summary": "Spatial transcriptomics aims to connect high-resolution histology images with\nspatially resolved gene expression. To achieve better performance on downstream\ntasks such as gene expression prediction, large-scale pre-training is required\nto obtain generalisable representations that can bridge histology and\ntranscriptomics across tissues, protocols, and laboratories. Existing\ncross-modal pre-training approaches for spatial transcriptomics rely on either\ngene names or expression values in isolation, which strips the gene branch of\nessential semantics and breaks the association between each gene and its\nquantitative magnitude. In addition, by restricting supervision to image-text\nalignment, these methods ignore intrinsic visual cues that are critical for\nlearning robust image features. We present CoMTIP, the first Contrastive Masked\nText-Image Pretraining framework that jointly learns from images, gene names,\nand expression values while capturing fine-grained visual context for spatial\ntranscriptomics. The vision branch uses Masked Feature Modeling to reconstruct\noccluded patches and learn context-aware image embeddings. The text branch\napplies a scalable Gene-Text Encoder that processes all gene sentences in\nparallel, enriches each gene and its numerical value with dedicated embeddings,\nand employs Pair-aware Adversarial Training (PAAT) to preserve correct\ngene-value associations. Image and text representations are aligned in a shared\nInfoNCE-optimised space. Experiments on public spatial transcriptomics datasets\nshow that CoMTIP not only surpasses previous methods on diverse downstream\ntasks but also achieves zero-shot gene expression prediction, a capability that\nexisting approaches do not provide.", "AI": {"tldr": "CoMTIP\u662f\u4e00\u4e2a\u7528\u4e8e\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u7684\u5bf9\u6bd4\u63a9\u7801\u6587\u672c-\u56fe\u50cf\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u56fe\u50cf\u3001\u57fa\u56e0\u540d\u79f0\u548c\u8868\u8fbe\u503c\u6765\u83b7\u5f97\u66f4\u597d\u7684\u8de8\u6a21\u6001\u8868\u793a\uff0c\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u5e76\u5b9e\u73b0\u96f6\u6837\u672c\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f7f\u7528\u57fa\u56e0\u540d\u79f0\u6216\u8868\u8fbe\u503c\uff0c\u7f3a\u4e4f\u57fa\u56e0\u8bed\u4e49\u4fe1\u606f\u548c\u6570\u503c\u5173\u8054\uff0c\u4e14\u4ec5\u4f9d\u8d56\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u76d1\u7763\uff0c\u5ffd\u7565\u4e86\u91cd\u8981\u7684\u89c6\u89c9\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u63d0\u51faCoMTIP\u6846\u67b6\uff1a\u89c6\u89c9\u5206\u652f\u4f7f\u7528\u63a9\u7801\u7279\u5f81\u5efa\u6a21\u91cd\u5efa\u906e\u6321\u56fe\u50cf\u5757\uff1b\u6587\u672c\u5206\u652f\u4f7f\u7528\u53ef\u6269\u5c55\u7684\u57fa\u56e0-\u6587\u672c\u7f16\u7801\u5668\u5e76\u884c\u5904\u7406\u6240\u6709\u57fa\u56e0\u53e5\u5b50\uff0c\u91c7\u7528\u914d\u5bf9\u611f\u77e5\u5bf9\u6297\u8bad\u7ec3\u4fdd\u6301\u57fa\u56e0-\u503c\u5173\u8054\uff1b\u5728\u5171\u4eab\u7684InfoNCE\u4f18\u5316\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u56fe\u50cf\u548c\u6587\u672c\u8868\u793a\u3002", "result": "\u5728\u516c\u5171\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCoMTIP\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u73b0\u6709\u65b9\u6cd5\u4e0d\u5177\u5907\u7684\u96f6\u6837\u672c\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "CoMTIP\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u56fe\u50cf\u3001\u57fa\u56e0\u540d\u79f0\u548c\u8868\u8fbe\u503c\uff0c\u540c\u65f6\u6355\u83b7\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4e0a\u4e0b\u6587\uff0c\u4e3a\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u6846\u67b6\u3002"}}
{"id": "2509.16897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16897", "abs": "https://arxiv.org/abs/2509.16897", "authors": ["Xuewan He", "Jielei Wang", "Zihan Cheng", "Yuchen Su", "Shiyue Huang", "Guoming Lu"], "title": "PRISM: Precision-Recall Informed Data-Free Knowledge Distillation via Generative Diffusion", "comment": null, "summary": "Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to\na student without access to the real in-distribution (ID) data. While existing\nmethods perform well on small-scale images, they suffer from mode collapse when\nsynthesizing large-scale images, resulting in limited knowledge transfer.\nRecently, leveraging advanced generative models to synthesize photorealistic\nimages has emerged as a promising alternative. Nevertheless, directly using\noff-the-shelf diffusion to generate datasets faces the precision-recall\nchallenges: 1) ensuring synthetic data aligns with the real distribution, and\n2) ensuring coverage of the real ID manifold. In response, we propose PRISM, a\nprecision-recall informed synthesis method. Specifically, we introduce\nEnergy-guided Distribution Alignment to avoid the generation of\nout-of-distribution samples, and design the Diversified Prompt Engineering to\nenhance coverage of the real ID manifold. Extensive experiments on various\nlarge-scale image datasets demonstrate the superiority of PRISM. Moreover, we\ndemonstrate that models trained with PRISM exhibit strong domain\ngeneralization.", "AI": {"tldr": "PRISM\u662f\u4e00\u79cd\u57fa\u4e8e\u7cbe\u5ea6-\u53ec\u56de\u7387\u7684\u6570\u636e\u81ea\u7531\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u80fd\u91cf\u5f15\u5bfc\u5206\u5e03\u5bf9\u9f50\u548c\u591a\u6837\u5316\u63d0\u793a\u5de5\u7a0b\u89e3\u51b3\u5927\u89c4\u6a21\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898", "motivation": "\u73b0\u6709\u6570\u636e\u81ea\u7531\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u56fe\u50cf\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u5408\u6210\u65f6\u4f1a\u51fa\u73b0\u6a21\u5f0f\u5d29\u6e83\uff0c\u5bfc\u81f4\u77e5\u8bc6\u8f6c\u79fb\u53d7\u9650\u3002\u76f4\u63a5\u4f7f\u7528\u73b0\u6210\u6269\u6563\u6a21\u578b\u751f\u6210\u6570\u636e\u96c6\u9762\u4e34\u7cbe\u5ea6-\u53ec\u56de\u7387\u6311\u6218", "method": "\u63d0\u51faPRISM\u65b9\u6cd5\uff1a1\uff09\u80fd\u91cf\u5f15\u5bfc\u5206\u5e03\u5bf9\u9f50\u907f\u514d\u751f\u6210\u5206\u5e03\u5916\u6837\u672c\uff1b2\uff09\u591a\u6837\u5316\u63d0\u793a\u5de5\u7a0b\u589e\u5f3a\u5bf9\u771f\u5b9e\u5206\u5e03\u6d41\u5f62\u7684\u8986\u76d6", "result": "\u5728\u591a\u4e2a\u5927\u89c4\u6a21\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86PRISM\u7684\u4f18\u8d8a\u6027\uff0c\u4f7f\u7528PRISM\u8bad\u7ec3\u7684\u6a21\u578b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9886\u57df\u6cdb\u5316\u80fd\u529b", "conclusion": "PRISM\u901a\u8fc7\u89e3\u51b3\u7cbe\u5ea6-\u53ec\u56de\u7387\u6311\u6218\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6570\u636e\u81ea\u7531\u77e5\u8bc6\u84b8\u998f\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u4e0a\u7684\u6027\u80fd"}}
{"id": "2509.16900", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16900", "abs": "https://arxiv.org/abs/2509.16900", "authors": ["Chengsheng Zhang", "Linhao Qu", "Xiaoyu Liu", "Zhijian Song"], "title": "ME-Mamba: Multi-Expert Mamba with Efficient Knowledge Capture and Fusion for Multimodal Survival Analysis", "comment": null, "summary": "Survival analysis using whole-slide images (WSIs) is crucial in cancer\nresearch. Despite significant successes, pathology images typically only\nprovide slide-level labels, which hinders the learning of discriminative\nrepresentations from gigapixel WSIs. With the rapid advancement of\nhigh-throughput sequencing technologies, multimodal survival analysis\nintegrating pathology images and genomics data has emerged as a promising\napproach. We propose a Multi-Expert Mamba (ME-Mamba) system that captures\ndiscriminative pathological and genomic features while enabling efficient\nintegration of both modalities. This approach achieves complementary\ninformation fusion without losing critical information from individual\nmodalities, thereby facilitating accurate cancer survival analysis.\nSpecifically, we first introduce a Pathology Expert and a Genomics Expert to\nprocess unimodal data separately. Both experts are designed with Mamba\narchitectures that incorporate conventional scanning and attention-based\nscanning mechanisms, allowing them to extract discriminative features from long\ninstance sequences containing substantial redundant or irrelevant information.\nSecond, we design a Synergistic Expert responsible for modality fusion. It\nexplicitly learns token-level local correspondences between the two modalities\nvia Optimal Transport, and implicitly enhances distribution consistency through\na global cross-modal fusion loss based on Maximum Mean Discrepancy. The fused\nfeature representations are then passed to a mamba backbone for further\nintegration. Through the collaboration of the Pathology Expert, Genomics\nExpert, and Synergistic Expert, our method achieves stable and accurate\nsurvival analysis with relatively low computational complexity. Extensive\nexperimental results on five datasets in The Cancer Genome Atlas (TCGA)\ndemonstrate our state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86ME-Mamba\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u4e13\u5bb6\u67b6\u6784\u6574\u5408\u75c5\u7406\u56fe\u50cf\u548c\u57fa\u56e0\u7ec4\u6570\u636e\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u764c\u75c7\u751f\u5b58\u5206\u6790\uff0c\u5728TCGA\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u75c5\u7406\u56fe\u50cf\u901a\u5e38\u53ea\u6709\u5e7b\u706f\u7247\u7ea7\u6807\u7b7e\uff0c\u963b\u788d\u4e86\u4ece\u5343\u5146\u50cf\u7d20WSI\u4e2d\u5b66\u4e60\u5224\u522b\u6027\u8868\u5f81\u3002\u591a\u6a21\u6001\u751f\u5b58\u5206\u6790\u6574\u5408\u75c5\u7406\u56fe\u50cf\u548c\u57fa\u56e0\u7ec4\u6570\u636e\u6210\u4e3a\u6709\u524d\u666f\u7684\u65b9\u6cd5", "method": "\u8bbe\u8ba1\u4e09\u4e2a\u4e13\u5bb6\u6a21\u5757\uff1a\u75c5\u7406\u4e13\u5bb6\u548c\u57fa\u56e0\u7ec4\u4e13\u5bb6\u5206\u522b\u5904\u7406\u5355\u6a21\u6001\u6570\u636e\uff0c\u91c7\u7528Mamba\u67b6\u6784\uff1b\u534f\u540c\u4e13\u5bb6\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5b66\u4e60token\u7ea7\u5c40\u90e8\u5bf9\u5e94\u5173\u7cfb\uff0c\u4f7f\u7528\u6700\u5927\u5747\u503c\u5dee\u5f02\u8fdb\u884c\u5168\u5c40\u8de8\u6a21\u6001\u878d\u5408", "result": "\u5728TCGA\u7684\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u5148\u8fdb\u6027\u80fd", "conclusion": "\u901a\u8fc7\u75c5\u7406\u4e13\u5bb6\u3001\u57fa\u56e0\u7ec4\u4e13\u5bb6\u548c\u534f\u540c\u4e13\u5bb6\u7684\u534f\u4f5c\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7a33\u5b9a\u51c6\u786e\u7684\u751f\u5b58\u5206\u6790\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u76f8\u5bf9\u8f83\u4f4e"}}
{"id": "2509.16909", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16909", "abs": "https://arxiv.org/abs/2509.16909", "authors": ["Yijun Yuan", "Zhuoguang Chen", "Kenan Li", "Weibang Wang", "Hang Zhao"], "title": "SLAM-Former: Putting SLAM into One Transformer", "comment": "Project Page:https://tsinghua-mars-lab.github.io/SLAM-Former", "summary": "We present SLAM-Former, a novel neural approach that integrates full SLAM\ncapabilities into a single transformer. Similar to traditional SLAM systems,\nSLAM-Former comprises both a frontend and a backend that operate in tandem. The\nfrontend processes sequential monocular images in real-time for incremental\nmapping and tracking, while the backend performs global refinement to ensure a\ngeometrically consistent result. This alternating execution allows the frontend\nand backend to mutually promote one another, enhancing overall system\nperformance. Comprehensive experimental results demonstrate that SLAM-Former\nachieves superior or highly competitive performance compared to\nstate-of-the-art dense SLAM methods.", "AI": {"tldr": "SLAM-Former\u662f\u4e00\u4e2a\u57fa\u4e8etransformer\u7684\u795e\u7ecfSLAM\u65b9\u6cd5\uff0c\u5c06\u5b8c\u6574\u7684SLAM\u529f\u80fd\u96c6\u6210\u5230\u5355\u4e00transformer\u4e2d\uff0c\u5305\u542b\u524d\u7aef\u5b9e\u65f6\u5904\u7406\u548c\u540e\u53f0\u5168\u5c40\u4f18\u5316\uff0c\u5728\u5bc6\u96c6SLAM\u4efb\u52a1\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfSLAM\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u795e\u7ecf\u65b9\u6cd5\uff0c\u5c06SLAM\u7684\u524d\u7aef\u548c\u540e\u53f0\u529f\u80fd\u96c6\u6210\u5230\u5355\u4e00transformer\u67b6\u6784\u4e2d\uff0c\u7b80\u5316\u7cfb\u7edf\u8bbe\u8ba1\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528transformer\u67b6\u6784\uff0c\u5305\u542b\u524d\u7aef\u548c\u540e\u53f0\u4e24\u4e2a\u90e8\u5206\uff1a\u524d\u7aef\u5b9e\u65f6\u5904\u7406\u5355\u76ee\u56fe\u50cf\u5e8f\u5217\u8fdb\u884c\u589e\u91cf\u5efa\u56fe\u548c\u8ddf\u8e2a\uff0c\u540e\u53f0\u8fdb\u884c\u5168\u5c40\u4f18\u5316\u786e\u4fdd\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u4e24\u8005\u4ea4\u66ff\u6267\u884c\u76f8\u4e92\u4fc3\u8fdb\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSLAM-Former\u5728\u5bc6\u96c6SLAM\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4f18\u4e8e\u6216\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u7ade\u4e89\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "SLAM-Former\u6210\u529f\u5c55\u793a\u4e86\u5c06\u5b8c\u6574SLAM\u529f\u80fd\u96c6\u6210\u5230\u5355\u4e00transformer\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u795e\u7ecfSLAM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u601d\u8def\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5efa\u56fe\u548c\u8ddf\u8e2a\u6548\u679c\u3002"}}
{"id": "2509.16935", "categories": ["cs.CV", "68T07", "I.2.10; I.4.9; I.5.4"], "pdf": "https://arxiv.org/pdf/2509.16935", "abs": "https://arxiv.org/abs/2509.16935", "authors": ["Lavish Ramchandani", "Gunjan Deotale", "Dev Kumar Das"], "title": "Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification", "comment": "MIDOG'25", "summary": "Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated\nwith tumor aggressiveness and poor prognosis. Their detection remains a\nsignificant challenge due to subtle morphological cues, class imbalance, and\ninter-observer variability among pathologists. The MIDOG 2025 challenge\nintroduced a dedicated track for atypical mitosis classification, enabling\nsystematic evaluation of deep learning methods. In this study, we investigated\nthe use of large vision foundation models, including Virchow, Virchow2, and\nUNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We\nconducted extensive experiments with different LoRA ranks, as well as random\nand group-based data splits, to analyze robustness under varied conditions. Our\nbest approach, Virchow with LoRA rank 8 and ensemble of three-fold\ncross-validation, achieved a balanced accuracy of 88.37% on the preliminary\ntest set, ranking joint 9th in the challenge leaderboard. These results\nhighlight the promise of foundation models with efficient adaptation strategies\nfor the classification of atypical mitosis, while underscoring the need for\nimprovements in specificity and domain generalization.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4f7f\u7528\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08Virchow\u3001Virchow2\u548cUNI\uff09\u7ed3\u5408LoRA\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\uff08AMFs\uff09\u5206\u7c7b\uff0c\u5728MIDOG 2025\u6311\u6218\u4e2d\u53d6\u5f9788.37%\u7684\u5e73\u8861\u51c6\u786e\u7387\u3002", "motivation": "\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\uff08AMFs\uff09\u7684\u68c0\u6d4b\u5bf9\u80bf\u7624\u4fb5\u88ad\u6027\u548c\u9884\u540e\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5f62\u6001\u5b66\u7279\u5f81\u7ec6\u5fae\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u75c5\u7406\u5b66\u5bb6\u95f4\u89c2\u5bdf\u5dee\u5f02\u7b49\u56e0\u7d20\uff0c\u5176\u68c0\u6d4b\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u91c7\u7528\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08Virchow\u3001Virchow2\u3001UNI\uff09\u7ed3\u5408\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u901a\u8fc7\u4e0d\u540cLoRA\u79e9\u548c\u968f\u673a/\u5206\u7ec4\u6570\u636e\u5206\u5272\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5206\u6790\u6a21\u578b\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u6700\u4f73\u65b9\u6cd5\uff08Virchow\u6a21\u578b+LoRA\u79e98+\u4e09\u6298\u4ea4\u53c9\u9a8c\u8bc1\u96c6\u6210\uff09\u5728\u521d\u6b65\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523088.37%\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u5728\u6311\u6218\u6392\u884c\u699c\u4e2d\u5e76\u5217\u7b2c9\u540d\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u9ad8\u6548\u9002\u5e94\u7b56\u7565\u5728\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u5728\u7279\u5f02\u6027\u548c\u9886\u57df\u6cdb\u5316\u65b9\u9762\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2509.16942", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16942", "abs": "https://arxiv.org/abs/2509.16942", "authors": ["Bin Wang", "Fei Deng", "Zeyu Chen", "Zhicheng Yu", "Yiguang Liu"], "title": "Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation", "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) enables domain adaptation for semantic\nsegmentation of Remote Sensing Images (RSIs) using only a well-trained source\nmodel and unlabeled target domain data. However, the lack of ground-truth\nlabels in the target domain often leads to the generation of noisy\npseudo-labels. Such noise impedes the effective mitigation of domain shift\n(DS). To address this challenge, we propose ProSFDA, a prototype-guided SFDA\nframework. It employs prototype-weighted pseudo-labels to facilitate reliable\nself-training (ST) under pseudo-labels noise. We, in addition, introduce a\nprototype-contrast strategy that encourages the aggregation of features\nbelonging to the same class, enabling the model to learn discriminative target\ndomain representations without relying on ground-truth supervision. Extensive\nexperiments show that our approach substantially outperforms existing methods.", "AI": {"tldr": "ProSFDA\u662f\u4e00\u4e2a\u539f\u578b\u5f15\u5bfc\u7684\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\uff0c\u901a\u8fc7\u539f\u578b\u52a0\u6743\u4f2a\u6807\u7b7e\u548c\u539f\u578b\u5bf9\u6bd4\u7b56\u7565\u89e3\u51b3\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u4e2d\u7531\u4e8e\u76ee\u6807\u57df\u7f3a\u4e4f\u771f\u5b9e\u6807\u7b7e\u5bfc\u81f4\u7684\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u8fd9\u79cd\u566a\u58f0\u963b\u788d\u4e86\u6709\u6548\u7f13\u89e3\u57df\u504f\u79fb\u3002", "method": "\u91c7\u7528\u539f\u578b\u52a0\u6743\u4f2a\u6807\u7b7e\u8fdb\u884c\u53ef\u9760\u7684\u81ea\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165\u539f\u578b\u5bf9\u6bd4\u7b56\u7565\u4fc3\u8fdb\u540c\u7c7b\u7279\u5f81\u7684\u805a\u5408\uff0c\u4ece\u800c\u5728\u6ca1\u6709\u771f\u5b9e\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u5224\u522b\u6027\u76ee\u6807\u57df\u8868\u793a\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ProSFDA\u6846\u67b6\u901a\u8fc7\u539f\u578b\u5f15\u5bfc\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86SFDA\u4e2d\u7684\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u57df\u81ea\u9002\u5e94\u6027\u80fd\u3002"}}
{"id": "2509.16944", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16944", "abs": "https://arxiv.org/abs/2509.16944", "authors": ["Yuheng Shi", "Xiaohuan Pei", "Minjing Dong", "Chang Xu"], "title": "Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception", "comment": "19 pages, 5 figures", "summary": "Multimodal Large Language Models (MLLMs) require high-resolution visual\ninformation to perform fine-grained perception, yet processing entire\nhigh-resolution images is computationally prohibitive. While recent methods\nleverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they\ntypically present a difficult trade-off: training-based approaches depend on\nlarge-scale annotated datasets, while training-free methods that utilize the\nmodel's internal attention are computationally inefficient and less accurate,\nrequiring either multi-pass prefill stages or reliance on the slow\nauto-regressive decoding process. In this paper, we propose an efficient,\nannotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves\nthis trade-off. The SD-RPN is built around a pipeline that transforms the noisy\nattention maps from the MLLM's middle layers into high-quality pseudo-RoI\nlabels by explicitly denoising the signal and resolving ambiguity. We use these\nlabels to train a lightweight Region Proposal Network (RPN) that learns a more\nprecise localization. This RPN is also highly efficient, predicting the RoI in\na single forward pass using features from the MLLM's middle layers, decoupling\nRoI identification from the auto-regressive generation and avoiding costly\nmulti-pass operations.To validate our approach, we integrate the framework into\nthe LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K)\nquestion-answer pairs, our method demonstrates exceptional data efficiency and\ngeneralization, achieving over a 10% absolute accuracy improvement on unseen\nbenchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a\npractical and scalable solution for enhancing the fine-grained perception of\nMLLMs without requiring costly supervision or full model fine-tuning. Code is\navailable at https://github.com/YuHengsss/SD-RPN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u84b8\u998f\u533a\u57df\u5efa\u8bae\u7f51\u7edc\uff08SD-RPN\uff09\uff0c\u901a\u8fc7\u5c06MLLM\u4e2d\u95f4\u5c42\u7684\u566a\u58f0\u6ce8\u610f\u529b\u56fe\u8f6c\u5316\u4e3a\u9ad8\u8d28\u91cf\u4f2aRoI\u6807\u7b7e\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7RPN\u6765\u5b9e\u73b0\u9ad8\u6548\u3001\u65e0\u9700\u6807\u6ce8\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u4fe1\u606f\u8fdb\u884c\u7ec6\u7c92\u5ea6\u611f\u77e5\uff0c\u4f46\u5904\u7406\u5168\u5206\u8fa8\u7387\u56fe\u50cf\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u73b0\u6709RoI\u65b9\u6cd5\u9762\u4e34\u4e24\u96be\u9009\u62e9\uff1a\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\uff0c\u800c\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\u4e14\u51c6\u786e\u6027\u5dee\u3002", "method": "\u6784\u5efaSD-RPN\u7ba1\u9053\uff0c\u5c06MLLM\u4e2d\u95f4\u5c42\u7684\u566a\u58f0\u6ce8\u610f\u529b\u56fe\u53bb\u566a\u5e76\u89e3\u51b3\u6b67\u4e49\uff0c\u8f6c\u5316\u4e3a\u9ad8\u8d28\u91cf\u4f2aRoI\u6807\u7b7e\uff0c\u7528\u4e8e\u8bad\u7ec3\u8f7b\u91cf\u7ea7RPN\u3002RPN\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u9884\u6d4bRoI\uff0c\u89e3\u8026RoI\u8bc6\u522b\u4e0e\u81ea\u56de\u5f52\u751f\u6210\u3002", "result": "\u5728LLaVA-1.5\u67b6\u6784\u4e0a\u96c6\u6210SD-RPN\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\uff08\u598210K\uff09\u95ee\u7b54\u5bf9\u8bad\u7ec3\uff0c\u5728TextVQA\u3001DocVQA\u548cV-Star\u7b49\u672a\u89c1\u57fa\u51c6\u4e0a\u5b9e\u73b0\u8d85\u8fc710%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "SD-RPN\u4e3a\u589e\u5f3aMLLM\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u6602\u8d35\u7684\u76d1\u7763\u6216\u5168\u6a21\u578b\u5fae\u8c03\u3002"}}
{"id": "2509.16949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16949", "abs": "https://arxiv.org/abs/2509.16949", "authors": ["Ruicong Liu", "Takehiko Ohkawa", "Tze Ho Elden Tse", "Mingfang Zhang", "Angela Yao", "Yoichi Sato"], "title": "Leveraging RGB Images for Pre-Training of Event-Based Hand Pose Estimation", "comment": null, "summary": "This paper presents RPEP, the first pre-training method for event-based 3D\nhand pose estimation using labeled RGB images and unpaired, unlabeled event\ndata. Event data offer significant benefits such as high temporal resolution\nand low latency, but their application to hand pose estimation is still limited\nby the scarcity of labeled training data. To address this, we repurpose real\nRGB datasets to train event-based estimators. This is done by constructing\npseudo-event-RGB pairs, where event data is generated and aligned with the\nground-truth poses of RGB images. Unfortunately, existing pseudo-event\ngeneration techniques assume stationary objects, thus struggling to handle\nnon-stationary, dynamically moving hands. To overcome this, RPEP introduces a\nnovel generation strategy that decomposes hand movements into smaller,\nstep-by-step motions. This decomposition allows our method to capture temporal\nchanges in articulation, constructing more realistic event data for a moving\nhand. Additionally, RPEP imposes a motion reversal constraint, regularizing\nevent generation using reversed motion. Extensive experiments show that our\npre-trained model significantly outperforms state-of-the-art methods on real\nevent data, achieving up to 24% improvement on EvRealHands. Moreover, it\ndelivers strong performance with minimal labeled samples for fine-tuning,\nmaking it well-suited for practical deployment.", "AI": {"tldr": "RPEP\u662f\u9996\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u76843D\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u6807\u8bb0\u7684RGB\u56fe\u50cf\u548c\u672a\u914d\u5bf9\u7684\u672a\u6807\u8bb0\u4e8b\u4ef6\u6570\u636e\uff0c\u901a\u8fc7\u5206\u89e3\u624b\u90e8\u8fd0\u52a8\u4e3a\u9010\u6b65\u52a8\u4f5c\u548c\u8fd0\u52a8\u53cd\u8f6c\u7ea6\u675f\u6765\u751f\u6210\u66f4\u771f\u5b9e\u7684\u4e8b\u4ef6\u6570\u636e\u3002", "motivation": "\u4e8b\u4ef6\u6570\u636e\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u4f4e\u5ef6\u8fdf\u7684\u4f18\u52bf\uff0c\u4f46\u5728\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u3002\u73b0\u6709\u4f2a\u4e8b\u4ef6\u751f\u6210\u6280\u672f\u5047\u8bbe\u7269\u4f53\u9759\u6b62\uff0c\u96be\u4ee5\u5904\u7406\u52a8\u6001\u79fb\u52a8\u7684\u624b\u90e8\u3002", "method": "RPEP\u901a\u8fc7\u6784\u5efa\u4f2a\u4e8b\u4ef6-RGB\u5bf9\uff0c\u5c06\u624b\u90e8\u8fd0\u52a8\u5206\u89e3\u4e3a\u8f83\u5c0f\u7684\u9010\u6b65\u52a8\u4f5c\u6765\u6355\u6349\u5173\u8282\u7684\u65f6\u95f4\u53d8\u5316\uff0c\u5e76\u5f15\u5165\u8fd0\u52a8\u53cd\u8f6c\u7ea6\u675f\u6765\u6b63\u5219\u5316\u4e8b\u4ef6\u751f\u6210\u3002", "result": "\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728EvRealHands\u4e0a\u5b9e\u73b0\u9ad8\u8fbe24%\u7684\u6539\u8fdb\uff0c\u4e14\u53ea\u9700\u5c11\u91cf\u6807\u8bb0\u6837\u672c\u8fdb\u884c\u5fae\u8c03\u5373\u53ef\u83b7\u5f97\u5f3a\u6027\u80fd\u3002", "conclusion": "RPEP\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u6570\u636e\u6807\u8bb0\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4e3a\u57fa\u4e8e\u4e8b\u4ef6\u7684\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u9884\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16956", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16956", "abs": "https://arxiv.org/abs/2509.16956", "authors": ["Luca Zanchetta", "Lorenzo Papa", "Luca Maiano", "Irene Amerini"], "title": "VidCLearn: A Continual Learning Approach for Text-to-Video Generation", "comment": null, "summary": "Text-to-video generation is an emerging field in generative AI, enabling the\ncreation of realistic, semantically accurate videos from text prompts. While\ncurrent models achieve impressive visual quality and alignment with input text,\nthey typically rely on static knowledge, making it difficult to incorporate new\ndata without retraining from scratch. To address this limitation, we propose\nVidCLearn, a continual learning framework for diffusion-based text-to-video\ngeneration. VidCLearn features a student-teacher architecture where the student\nmodel is incrementally updated with new text-video pairs, and the teacher model\nhelps preserve previously learned knowledge through generative replay.\nAdditionally, we introduce a novel temporal consistency loss to enhance motion\nsmoothness and a video retrieval module to provide structural guidance at\ninference. Our architecture is also designed to be more computationally\nefficient than existing models while retaining satisfactory generation\nperformance. Experimental results show VidCLearn's superiority over baseline\nmethods in terms of visual quality, semantic alignment, and temporal coherence.", "AI": {"tldr": "VidCLearn\u662f\u4e00\u4e2a\u7528\u4e8e\u6269\u6563\u5f0f\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5e08\u751f\u67b6\u6784\u548c\u751f\u6210\u91cd\u653e\u6765\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u878d\u5165\u65b0\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u77e5\u8bc6\uff0c\u96be\u4ee5\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u878d\u5165\u65b0\u6570\u636e\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u5e08\u751f\u67b6\u6784\uff0c\u5b66\u751f\u6a21\u578b\u901a\u8fc7\u65b0\u6587\u672c-\u89c6\u9891\u5bf9\u589e\u91cf\u66f4\u65b0\uff0c\u6559\u5e08\u6a21\u578b\u901a\u8fc7\u751f\u6210\u91cd\u653e\u4fdd\u7559\u5df2\u5b66\u77e5\u8bc6\uff1b\u5f15\u5165\u65f6\u95f4\u4e00\u81f4\u6027\u635f\u5931\u589e\u5f3a\u8fd0\u52a8\u5e73\u6ed1\u6027\uff0c\u89c6\u9891\u68c0\u7d22\u6a21\u5757\u5728\u63a8\u7406\u65f6\u63d0\u4f9b\u7ed3\u6784\u6307\u5bfc\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eVidCLearn\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VidCLearn\u6846\u67b6\u5728\u4fdd\u6301\u6ee1\u610f\u751f\u6210\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6bd4\u73b0\u6709\u6a21\u578b\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u4e3a\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16957", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16957", "abs": "https://arxiv.org/abs/2509.16957", "authors": ["Leiyu Wang", "Biao Jin", "Feng Huang", "Liqiong Chen", "Zhengyong Wang", "Xiaohai He", "Honggang Chen"], "title": "MO R-CNN: Multispectral Oriented R-CNN for Object Detection in Remote Sensing Image", "comment": null, "summary": "Oriented object detection for multi-spectral imagery faces significant\nchallenges due to differences both within and between modalities. Although\nexisting methods have improved detection accuracy through complex network\narchitectures, their high computational complexity and memory consumption\nseverely restrict their performance. Motivated by the success of large kernel\nconvolutions in remote sensing, we propose MO R-CNN, a lightweight framework\nfor multi-spectral oriented detection featuring heterogeneous feature\nextraction network (HFEN), single modality supervision (SMS), and\ncondition-based multimodal label fusion (CMLF). HFEN leverages inter-modal\ndifferences to adaptively align, merge, and enhance multi-modal features. SMS\nconstrains multi-scale features and enables the model to learn from multiple\nmodalities. CMLF fuses multimodal labels based on specific rules, providing the\nmodel with a more robust and consistent supervisory signal. Experiments on the\nDroneVehicle, VEDAI and OGSOD datasets prove the superiority of our method. The\nsource code is available at:https://github.com/Iwill-github/MORCNN.", "AI": {"tldr": "\u63d0\u51faMO R-CNN\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u5149\u8c31\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u5b9a\u5411\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u5f02\u6784\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u3001\u5355\u6a21\u6001\u76d1\u7763\u548c\u57fa\u4e8e\u6761\u4ef6\u7684\u591a\u6a21\u6001\u6807\u7b7e\u878d\u5408\u6765\u89e3\u51b3\u6a21\u6001\u95f4\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "\u591a\u5149\u8c31\u56fe\u50cf\u4e2d\u7684\u5b9a\u5411\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u5dee\u5f02\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u6d88\u8017\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u6027\u80fd\u3002", "method": "\u63d0\u51faMO R-CNN\u6846\u67b6\uff0c\u5305\u542b\u5f02\u6784\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff08HFEN\uff09\u81ea\u9002\u5e94\u5bf9\u9f50\u548c\u589e\u5f3a\u591a\u6a21\u6001\u7279\u5f81\uff0c\u5355\u6a21\u6001\u76d1\u7763\uff08SMS\uff09\u7ea6\u675f\u591a\u5c3a\u5ea6\u7279\u5f81\u5b66\u4e60\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6761\u4ef6\u7684\u591a\u6a21\u6001\u6807\u7b7e\u878d\u5408\uff08CMLF\uff09\u63d0\u4f9b\u66f4\u9c81\u68d2\u7684\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u5728DroneVehicle\u3001VEDAI\u548cOGSOD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u9ad8\u6027\u80fd\u7684\u591a\u5149\u8c31\u5b9a\u5411\u76ee\u6807\u68c0\u6d4b\u3002", "conclusion": "MO R-CNN\u901a\u8fc7\u521b\u65b0\u7684\u7279\u5f81\u63d0\u53d6\u3001\u76d1\u7763\u5b66\u4e60\u548c\u6807\u7b7e\u878d\u5408\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5149\u8c31\u9065\u611f\u56fe\u50cf\u4e2d\u5b9a\u5411\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u8f7b\u91cf\u5316\u7684\u540c\u65f6\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.16968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16968", "abs": "https://arxiv.org/abs/2509.16968", "authors": ["Haoyang Xu", "Tianhao Zhao", "Sibei Yang", "Yutian Li"], "title": "Penalizing Boundary Activation for Object Completeness in Diffusion Models", "comment": null, "summary": "Diffusion models have emerged as a powerful technique for text-to-image (T2I)\ngeneration, creating high-quality, diverse images across various domains.\nHowever, a common limitation in these models is the incomplete display of\nobjects, where fragments or missing parts undermine the model's performance in\ndownstream applications. In this study, we conduct an in-depth analysis of the\nincompleteness issue and reveal that the primary factor behind incomplete\nobject generation is the usage of RandomCrop during model training. This widely\nused data augmentation method, though enhances model generalization ability,\ndisrupts object continuity during training. To address this, we propose a\ntraining-free solution that penalizes activation values at image boundaries\nduring the early denoising steps. Our method is easily applicable to\npre-trained Stable Diffusion models with minimal modifications and negligible\ncomputational overhead. Extensive experiments demonstrate the effectiveness of\nour method, showing substantial improvements in object integrity and image\nquality.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u7269\u4f53\u4e0d\u5b8c\u6574\u663e\u793a\u7684\u95ee\u9898\uff0c\u53d1\u73b0RandomCrop\u6570\u636e\u589e\u5f3a\u662f\u4e3b\u8981\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u6539\u5584\u7269\u4f53\u5b8c\u6574\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u666e\u904d\u5b58\u5728\u7269\u4f53\u663e\u793a\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u65e9\u671f\u53bb\u566a\u6b65\u9aa4\u4e2d\u5bf9\u56fe\u50cf\u8fb9\u754c\u7684\u6fc0\u6d3b\u503c\u8fdb\u884c\u60e9\u7f5a\uff0c\u8be5\u65b9\u6cd5\u53ef\u8f7b\u677e\u5e94\u7528\u4e8e\u9884\u8bad\u7ec3\u7684Stable Diffusion\u6a21\u578b\uff0c\u53ea\u9700\u6700\u5c0f\u4fee\u6539\u548c\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u5728\u7269\u4f53\u5b8c\u6574\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790RandomCrop\u5bf9\u7269\u4f53\u8fde\u7eed\u6027\u7684\u7834\u574f\uff0c\u63d0\u51fa\u7684\u8fb9\u754c\u6fc0\u6d3b\u60e9\u7f5a\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u7269\u4f53\u4e0d\u5b8c\u6574\u95ee\u9898\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2509.16970", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16970", "abs": "https://arxiv.org/abs/2509.16970", "authors": ["Wei Liao", "Chunyan Xu", "Chenxu Wang", "Zhen Cui"], "title": "LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing Object Detection", "comment": null, "summary": "Sparse annotation in remote sensing object detection poses significant\nchallenges due to dense object distributions and category imbalances. Although\nexisting Dense Pseudo-Label methods have demonstrated substantial potential in\npseudo-labeling tasks, they remain constrained by selection ambiguities and\ninconsistencies in confidence estimation.In this paper, we introduce an\nLLM-assisted semantic guidance framework tailored for sparsely annotated remote\nsensing object detection, exploiting the advanced semantic reasoning\ncapabilities of large language models (LLMs) to distill high-confidence\npseudo-labels.By integrating LLM-generated semantic priors, we propose a\nClass-Aware Dense Pseudo-Label Assignment mechanism that adaptively assigns\npseudo-labels for both unlabeled and sparsely labeled data, ensuring robust\nsupervision across varying data distributions. Additionally, we develop an\nAdaptive Hard-Negative Reweighting Module to stabilize the supervised learning\nbranch by mitigating the influence of confounding background information.\nExtensive experiments on DOTA and HRSC2016 demonstrate that the proposed method\noutperforms existing single-stage detector-based frameworks, significantly\nimproving detection performance under sparse annotations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u8bed\u4e49\u5f15\u5bfc\u7684\u7a00\u758f\u6807\u6ce8\u9065\u611f\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5bc6\u96c6\u4f2a\u6807\u7b7e\u65b9\u6cd5\u7684\u9009\u62e9\u6a21\u7cca\u6027\u548c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u7a00\u758f\u6807\u6ce8\u5728\u9065\u611f\u76ee\u6807\u68c0\u6d4b\u4e2d\u9762\u4e34\u5bc6\u96c6\u76ee\u6807\u5206\u5e03\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6311\u6218\uff0c\u73b0\u6709\u5bc6\u96c6\u4f2a\u6807\u7b7e\u65b9\u6cd5\u5b58\u5728\u9009\u62e9\u6a21\u7cca\u6027\u548c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u4e0d\u4e00\u81f4\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faLLM\u8f85\u52a9\u8bed\u4e49\u5f15\u5bfc\u6846\u67b6\uff0c\u96c6\u6210LLM\u751f\u6210\u7684\u8bed\u4e49\u5148\u9a8c\uff0c\u8bbe\u8ba1\u7c7b\u611f\u77e5\u5bc6\u96c6\u4f2a\u6807\u7b7e\u5206\u914d\u673a\u5236\u548c\u81ea\u9002\u5e94\u786c\u8d1f\u6837\u672c\u91cd\u52a0\u6743\u6a21\u5757\uff0c\u4e3a\u672a\u6807\u6ce8\u548c\u7a00\u758f\u6807\u6ce8\u6570\u636e\u81ea\u9002\u5e94\u5206\u914d\u4f2a\u6807\u7b7e\u3002", "result": "\u5728DOTA\u548cHRSC2016\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u6807\u6ce8\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "LLM\u8f85\u52a9\u7684\u8bed\u4e49\u5f15\u5bfc\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u6807\u6ce8\u9065\u611f\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u4e3a\u4f2a\u6807\u7b7e\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2509.16972", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16972", "abs": "https://arxiv.org/abs/2509.16972", "authors": ["Quanzhu Niu", "Dengxian Gong", "Shihao Chen", "Tao Zhang", "Yikang Zhou", "Haobo Yuan", "Lu Qi", "Xiangtai Li", "Shunping Ji"], "title": "The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA", "comment": "1st place report of 7th LSVOS RVOS track in ICCV 2025. The code is\n  released in Sa2VA repository: https://github.com/magic-research/Sa2VA", "summary": "Referring video object segmentation (RVOS) requires segmenting and tracking\nobjects in videos conditioned on natural-language expressions, demanding\nfine-grained understanding of both appearance and motion. Building on Sa2VA,\nwhich couples a Multi-modal Large Language Model (MLLM) with the video\nsegmentation model SAM2, we identify two key bottlenecks that limit\nsegmentation performance: sparse frame sampling and reliance on a single [SEG]\ntoken for an entire video. We propose Segmentation Augmented and Selective\nAveraged Sa2VA SaSaSa2VA to address these issues. On the 7th LSVOS Challenge\n(RVOS track), SaSaSa2VA achieves a $J\\&F$ of 67.45, ranking first and\nsurpassing the runner-up by 2.80 points. This result and ablation studies\ndemonstrate that efficient segmentation augmentation and test-time ensembling\nsubstantially enhance grounded MLLMs for RVOS. The code is released in Sa2VA\nrepository: https://github.com/magic-research/Sa2VA.", "AI": {"tldr": "SaSaSa2VA\u901a\u8fc7\u5728Sa2VA\u57fa\u7840\u4e0a\u6539\u8fdb\u7a00\u758f\u5e27\u91c7\u6837\u548c\u5355\u4e00[SEG]token\u95ee\u9898\uff0c\u5728RVOS\u4efb\u52a1\u4e2d\u53d6\u5f97SOTA\u6548\u679c", "motivation": "\u89e3\u51b3RVOS\u4efb\u52a1\u4e2d\u7a00\u758f\u5e27\u91c7\u6837\u548c\u4f9d\u8d56\u5355\u4e00[SEG]token\u9650\u5236\u5206\u5272\u6027\u80fd\u7684\u95ee\u9898", "method": "\u63d0\u51faSegmentation Augmented and Selective Averaged Sa2VA\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u5206\u5272\u589e\u5f3a\u548c\u6d4b\u8bd5\u65f6\u96c6\u6210\u6280\u672f\u6539\u8fdbgrounded MLLMs", "result": "\u5728LSVOS\u6311\u6218\u8d5bRVOS\u8d5b\u9053\u83b7\u5f97J&F 67.45\uff0c\u6392\u540d\u7b2c\u4e00\uff0c\u6bd4\u7b2c\u4e8c\u540d\u9ad8\u51fa2.80\u5206", "conclusion": "\u5206\u5272\u589e\u5f3a\u548c\u6d4b\u8bd5\u65f6\u96c6\u6210\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347grounded MLLMs\u5728RVOS\u4efb\u52a1\u4e2d\u7684\u6027\u80fd"}}
{"id": "2509.16977", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16977", "abs": "https://arxiv.org/abs/2509.16977", "authors": ["Petros Georgoulas Wraight", "Giorgos Sfikas", "Ioannis Kordonis", "Petros Maragos", "George Retsinas"], "title": "Optimal Transport for Handwritten Text Recognition in a Low-Resource Regime", "comment": null, "summary": "Handwritten Text Recognition (HTR) is a task of central importance in the\nfield of document image understanding. State-of-the-art methods for HTR require\nthe use of extensive annotated sets for training, making them impractical for\nlow-resource domains like historical archives or limited-size modern\ncollections. This paper introduces a novel framework that, unlike the standard\nHTR model paradigm, can leverage mild prior knowledge of lexical\ncharacteristics; this is ideal for scenarios where labeled data are scarce. We\npropose an iterative bootstrapping approach that aligns visual features\nextracted from unlabeled images with semantic word representations using\nOptimal Transport (OT). Starting with a minimal set of labeled examples, the\nframework iteratively matches word images to text labels, generates\npseudo-labels for high-confidence alignments, and retrains the recognizer on\nthe growing dataset. Numerical experiments demonstrate that our iterative\nvisual-semantic alignment scheme significantly improves recognition accuracy on\nlow-resource HTR benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u8fed\u4ee3\u89c6\u89c9\u8bed\u4e49\u5bf9\u9f50\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4f4e\u8d44\u6e90\u624b\u5199\u6587\u672c\u8bc6\u522b\u95ee\u9898\uff0c\u80fd\u591f\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u5229\u7528\u8bcd\u6c47\u7279\u5f81\u77e5\u8bc6\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u7387", "motivation": "\u4f20\u7edfHTR\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4e0d\u9002\u7528\u4e8e\u5386\u53f2\u6863\u6848\u7b49\u4f4e\u8d44\u6e90\u9886\u57df\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u5229\u7528\u8bcd\u6c47\u7279\u5f81\u77e5\u8bc6\u7684\u6846\u67b6", "method": "\u91c7\u7528\u8fed\u4ee3\u81ea\u4e3e\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5c06\u672a\u6807\u6ce8\u56fe\u50cf\u4e2d\u7684\u89c6\u89c9\u7279\u5f81\u4e0e\u8bed\u4e49\u8bcd\u6c47\u8868\u793a\u5bf9\u9f50\uff0c\u4ece\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u5f00\u59cb\uff0c\u8fed\u4ee3\u5339\u914d\u8bcd\u56fe\u50cf\u4e0e\u6587\u672c\u6807\u7b7e\uff0c\u751f\u6210\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\uff0c\u5e76\u5728\u589e\u957f\u7684\u6570\u636e\u96c6\u4e0a\u91cd\u65b0\u8bad\u7ec3\u8bc6\u522b\u5668", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u8fed\u4ee3\u89c6\u89c9\u8bed\u4e49\u5bf9\u9f50\u65b9\u6848\u5728\u4f4e\u8d44\u6e90HTR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u51c6\u786e\u7387", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4f4e\u8d44\u6e90HTR\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u826f\u597d\u7684\u8bc6\u522b\u6027\u80fd"}}
{"id": "2509.16986", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16986", "abs": "https://arxiv.org/abs/2509.16986", "authors": ["Feng Han", "Chao Gong", "Zhipeng Wei", "Jingjing Chen", "Yu-Gang Jiang"], "title": "VCE: Safe Autoregressive Image Generation via Visual Contrast Exploitation", "comment": null, "summary": "Recently, autoregressive image generation models have wowed audiences with\ntheir remarkable capability in creating surprisingly realistic images. Models\nsuch as GPT-4o and LlamaGen can not only produce images that faithfully mimic\nrenowned artistic styles like Ghibli, Van Gogh, or Picasso, but also\npotentially generate Not-Safe-For-Work (NSFW) content, raising significant\nconcerns regarding copyright infringement and ethical use. Despite these\nconcerns, methods to safeguard autoregressive text-to-image models remain\nunderexplored. Previous concept erasure methods, primarily designed for\ndiffusion models that operate in denoising latent space, are not directly\napplicable to autoregressive models that generate images token by token. To\naddress this critical gap, we propose Visual Contrast Exploitation (VCE), a\nnovel framework comprising: (1) an innovative contrastive image pair\nconstruction paradigm that precisely decouples unsafe concepts from their\nassociated content semantics, and (2) a sophisticated DPO-based training\napproach that enhances the model's ability to identify and leverage visual\ncontrastive features from image pairs, enabling precise concept erasure. Our\ncomprehensive experiments across three challenging tasks-artist style erasure,\nexplicit content erasure, and object removal-demonstrate that our method\neffectively secures the model, achieving state-of-the-art results while erasing\nunsafe concepts and maintaining the integrity of unrelated safe concepts. The\ncode and models are available at https://github.com/Maplebb/VCE.", "AI": {"tldr": "\u63d0\u51fa\u4e86Visual Contrast Exploitation (VCE)\u6846\u67b6\uff0c\u7528\u4e8e\u4fdd\u62a4\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u514d\u53d7\u4e0d\u5b89\u5168\u6982\u5ff5\uff08\u5982NSFW\u5185\u5bb9\u3001\u4fb5\u6743\u98ce\u683c\uff09\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5bf9\u6bd4\u56fe\u50cf\u5bf9\u6784\u5efa\u548cDPO\u8bad\u7ec3\u5b9e\u73b0\u7cbe\u786e\u6982\u5ff5\u64e6\u9664\u3002", "motivation": "\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08\u5982GPT-4o\u3001LlamaGen\uff09\u80fd\u591f\u751f\u6210\u903c\u771f\u56fe\u50cf\uff0c\u4f46\u4e5f\u53ef\u80fd\u4ea7\u751fNSFW\u5185\u5bb9\u548c\u4fb5\u6743\u98ce\u683c\uff0c\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6269\u6563\u6a21\u578b\uff0c\u4e0d\u9002\u7528\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u3002", "method": "VCE\u6846\u67b6\u5305\u542b\uff1a(1)\u521b\u65b0\u7684\u5bf9\u6bd4\u56fe\u50cf\u5bf9\u6784\u5efa\u8303\u5f0f\uff0c\u7cbe\u786e\u89e3\u8026\u4e0d\u5b89\u5168\u6982\u5ff5\u4e0e\u5185\u5bb9\u8bed\u4e49\uff1b(2)\u57fa\u4e8eDPO\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u589e\u5f3a\u6a21\u578b\u8bc6\u522b\u548c\u5229\u7528\u89c6\u89c9\u5bf9\u6bd4\u7279\u5f81\u7684\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u6311\u6218\u6027\u4efb\u52a1\uff08\u827a\u672f\u5bb6\u98ce\u683c\u64e6\u9664\u3001\u663e\u5f0f\u5185\u5bb9\u64e6\u9664\u3001\u5bf9\u8c61\u79fb\u9664\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u62a4\u6a21\u578b\uff0c\u5728\u64e6\u9664\u4e0d\u5b89\u5168\u6982\u5ff5\u7684\u540c\u65f6\u4fdd\u6301\u65e0\u5173\u5b89\u5168\u6982\u5ff5\u7684\u5b8c\u6574\u6027\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "VCE\u662f\u9996\u4e2a\u9488\u5bf9\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.16988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16988", "abs": "https://arxiv.org/abs/2509.16988", "authors": ["Mingshuai Sheng", "Bhatti Uzair Aslam", "Junfeng Zhang", "Siling Feng", "Yonis Gulzar"], "title": "A Cross-Hierarchical Multi-Feature Fusion Network Based on Multiscale Encoder-Decoder for Hyperspectral Change Detection", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Hyperspectral change detection (HCD) aims to accurately identify land-cover\nchanges in hyperspectral images of the same area acquired at different times,\nwith key applications in environmental monitoring and disaster assessment. To\naddress limitations of existing methods, such as insufficient use of multiscale\nfeatures and low efficiency in differential feature fusion, this paper proposes\na cross-hierarchical multi-feature fusion network (CHMFFN) based on a\nmultiscale encoder-decoder architecture. The front-end adopts a multiscale\nfeature extraction subnetwork, built on an encoder-decoder backbone with\nresidual connections and a dual-core channel-spatial attention (DCCSA) module\nto extract spectral-spatial-temporal features (SSTF). The encoder captures\nmultiscale features from shallow details to deep semantics via residual blocks\nand convolutional kernels with varying receptive fields. The decoder restores\nspatial resolution and suppresses noise information through skip connections\nintegrating encoder features. Additionally, a spectral-temporal change feature\nlearning (STCFL) module learns cross-temporal change features at different\nlevels, strengthening inter-temporal difference capture. An adaptive fusion of\nadvanced features (AFAF) module dynamically balances hierarchical differential\nfeatures via adaptive weights, enhancing representation of complex changes.\nExperiments on four public hyperspectral datasets show CHMFFN outperforms\nstate-of-the-art methods, verifying its effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5c3a\u5ea6\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u7684\u8de8\u5c42\u6b21\u591a\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff08CHMFFN\uff09\uff0c\u7528\u4e8e\u9ad8\u5149\u8c31\u53d8\u5316\u68c0\u6d4b\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3001\u53cc\u6838\u901a\u9053-\u7a7a\u95f4\u6ce8\u610f\u529b\u6a21\u5757\u548c\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u53d8\u5316\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u9ad8\u5149\u8c31\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u5728\u591a\u5c3a\u5ea6\u7279\u5f81\u5229\u7528\u4e0d\u8db3\u548c\u5dee\u5f02\u7279\u5f81\u878d\u5408\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u73af\u5883\u76d1\u6d4b\u548c\u707e\u5bb3\u8bc4\u4f30\u7b49\u5e94\u7528\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u524d\u7aef\u4f7f\u7528\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u5b50\u7f51\u7edc\uff0c\u5305\u542b\u53cc\u6838\u901a\u9053-\u7a7a\u95f4\u6ce8\u610f\u529b\u6a21\u5757\u63d0\u53d6\u5149\u8c31-\u7a7a\u95f4-\u65f6\u95f4\u7279\u5f81\uff1b\u7f16\u7801\u5668\u901a\u8fc7\u6b8b\u5dee\u5757\u548c\u4e0d\u540c\u611f\u53d7\u91ce\u7684\u5377\u79ef\u6838\u6355\u83b7\u591a\u5c3a\u5ea6\u7279\u5f81\uff1b\u89e3\u7801\u5668\u901a\u8fc7\u8df3\u8dc3\u8fde\u63a5\u6062\u590d\u7a7a\u95f4\u5206\u8fa8\u7387\uff1b\u5149\u8c31-\u65f6\u95f4\u53d8\u5316\u7279\u5f81\u5b66\u4e60\u6a21\u5757\u5b66\u4e60\u8de8\u65f6\u95f4\u53d8\u5316\u7279\u5f81\uff1b\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\u52a8\u6001\u5e73\u8861\u5c42\u6b21\u5dee\u5f02\u7279\u5f81\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCHMFFN\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684CHMFFN\u7f51\u7edc\u901a\u8fc7\u6709\u6548\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\uff0c\u5728\u9ad8\u5149\u8c31\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.17012", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.17012", "abs": "https://arxiv.org/abs/2509.17012", "authors": ["Zhichao Ma", "Fan Huang", "Lu Zhao", "Fengjun Guo", "Guangtao Zhai", "Xiongkuo Min"], "title": "DocIQ: A Benchmark Dataset and Feature Fusion Network for Document Image Quality Assessment", "comment": null, "summary": "Document image quality assessment (DIQA) is an important component for\nvarious applications, including optical character recognition (OCR), document\nrestoration, and the evaluation of document image processing systems. In this\npaper, we introduce a subjective DIQA dataset DIQA-5000. The DIQA-5000 dataset\ncomprises 5,000 document images, generated by applying multiple document\nenhancement techniques to 500 real-world images with diverse distortions. Each\nenhanced image was rated by 15 subjects across three rating dimensions: overall\nquality, sharpness, and color fidelity. Furthermore, we propose a specialized\nno-reference DIQA model that exploits document layout features to maintain\nquality perception at reduced resolutions to lower computational cost.\nRecognizing that image quality is influenced by both low-level and high-level\nvisual features, we designed a feature fusion module to extract and integrate\nmulti-level features from document images. To generate multi-dimensional\nscores, our model employs independent quality heads for each dimension to\npredict score distributions, allowing it to learn distinct aspects of document\nimage quality. Experimental results demonstrate that our method outperforms\ncurrent state-of-the-art general-purpose IQA models on both DIQA-5000 and an\nadditional document image dataset focused on OCR accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DIQA-5000\u4e3b\u89c2\u6587\u6863\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u65e0\u53c2\u8003DIQA\u6a21\u578b\uff0c\u5229\u7528\u6587\u6863\u5e03\u5c40\u7279\u5f81\u548c\u591a\u7ea7\u7279\u5f81\u878d\u5408\u6765\u8bc4\u4f30\u6587\u6863\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u6587\u6863\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u5728OCR\u3001\u6587\u6863\u4fee\u590d\u7b49\u5e94\u7528\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002\u73b0\u6709\u901a\u7528IQA\u6a21\u578b\u5728\u6587\u6863\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u6784\u5efaDIQA-5000\u6570\u636e\u96c6\uff085000\u5f20\u56fe\u50cf\uff0c15\u4eba\u6807\u6ce8\uff09\uff0c\u63d0\u51fa\u65e0\u53c2\u8003DIQA\u6a21\u578b\uff0c\u4f7f\u7528\u6587\u6863\u5e03\u5c40\u7279\u5f81\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u8bbe\u8ba1\u7279\u5f81\u878d\u5408\u6a21\u5757\u6574\u5408\u591a\u7ea7\u7279\u5f81\uff0c\u4e3a\u6bcf\u4e2a\u8d28\u91cf\u7ef4\u5ea6\u4f7f\u7528\u72ec\u7acb\u7684\u8d28\u91cf\u5934\u9884\u6d4b\u5206\u6570\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728DIQA-5000\u548c\u53e6\u4e00\u4e2aOCR\u7cbe\u5ea6\u76f8\u5173\u7684\u6587\u6863\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u901a\u7528IQA\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e13\u95e8DIQA\u6a21\u578b\u80fd\u6709\u6548\u8bc4\u4f30\u6587\u6863\u56fe\u50cf\u8d28\u91cf\uff0c\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u4f18\u4e8e\u901a\u7528\u65b9\u6cd5\uff0c\u4e3a\u6587\u6863\u56fe\u50cf\u5904\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2509.17024", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17024", "abs": "https://arxiv.org/abs/2509.17024", "authors": ["Wenxuan Fang", "Jili Fan", "Chao Wang", "Xiantao Hu", "Jiangwei Weng", "Ying Tai", "Jian Yang", "Jun Li"], "title": "When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration", "comment": null, "summary": "Adverse Weather Image Restoration (AWIR) is a highly challenging task due to\nthe unpredictable and dynamic nature of weather-related degradations.\nTraditional task-specific methods often fail to generalize to unseen or complex\ndegradation types, while recent prompt-learning approaches depend heavily on\nthe degradation estimation capabilities of vision-language models, resulting in\ninconsistent restorations. In this paper, we propose \\textbf{LCDiff}, a novel\nframework comprising two key components: \\textit{Lumina-Chroma Decomposition\nNetwork} (LCDN) and \\textit{Lumina-Guided Diffusion Model} (LGDM). LCDN\nprocesses degraded images in the YCbCr color space, separately handling\ndegradation-related luminance and degradation-invariant chrominance components.\nThis decomposition effectively mitigates weather-induced degradation while\npreserving color fidelity. To further enhance restoration quality, LGDM\nleverages degradation-related luminance information as a guiding condition,\neliminating the need for explicit degradation prompts. Additionally, LGDM\nincorporates a \\textit{Dynamic Time Step Loss} to optimize the denoising\nnetwork, ensuring a balanced recovery of both low- and high-frequency features\nin the image. Finally, we present DriveWeather, a comprehensive all-weather\ndriving dataset designed to enable robust evaluation. Extensive experiments\ndemonstrate that our approach surpasses state-of-the-art methods, setting a new\nbenchmark in AWIR. The dataset and code are available at:\nhttps://github.com/fiwy0527/LCDiff.", "AI": {"tldr": "LCDiff\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u4eae\u5ea6-\u8272\u5ea6\u5206\u89e3\u7f51\u7edc\u548c\u4eae\u5ea6\u5f15\u5bfc\u6269\u6563\u6a21\u578b\uff0c\u5728YCbCr\u8272\u5f69\u7a7a\u95f4\u4e2d\u5206\u522b\u5904\u7406\u9000\u5316\u76f8\u5173\u7684\u4eae\u5ea6\u548c\u9000\u5316\u4e0d\u53d8\u7684\u8272\u5ea6\uff0c\u65e0\u9700\u663e\u5f0f\u9000\u5316\u63d0\u793a\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u6062\u590d\u3002", "motivation": "\u4f20\u7edf\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u89c1\u6216\u590d\u6742\u9000\u5316\u7c7b\u578b\uff0c\u800c\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9000\u5316\u4f30\u8ba1\u80fd\u529b\uff0c\u5bfc\u81f4\u6062\u590d\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faLCDiff\u6846\u67b6\uff0c\u5305\u542b\u4eae\u5ea6-\u8272\u5ea6\u5206\u89e3\u7f51\u7edc\uff08LCDN\uff09\u548c\u4eae\u5ea6\u5f15\u5bfc\u6269\u6563\u6a21\u578b\uff08LGDM\uff09\u3002LCDN\u5728YCbCr\u8272\u5f69\u7a7a\u95f4\u5206\u522b\u5904\u7406\u4eae\u5ea6\u548c\u8272\u5ea6\uff0cLGDM\u5229\u7528\u9000\u5316\u76f8\u5173\u4eae\u5ea6\u4fe1\u606f\u4f5c\u4e3a\u5f15\u5bfc\u6761\u4ef6\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u65f6\u95f4\u6b65\u635f\u5931\u4f18\u5316\u53bb\u566a\u7f51\u7edc\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "LCDiff\u901a\u8fc7\u4eae\u5ea6-\u8272\u5ea6\u5206\u89e3\u548c\u4eae\u5ea6\u5f15\u5bfc\u6269\u6563\u6a21\u578b\uff0c\u6709\u6548\u7f13\u89e3\u5929\u6c14\u5f15\u8d77\u7684\u9000\u5316\u5e76\u4fdd\u6301\u8272\u5f69\u4fdd\u771f\u5ea6\uff0c\u4e3a\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17027", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17027", "abs": "https://arxiv.org/abs/2509.17027", "authors": ["Zhenya Yang"], "title": "Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic Views", "comment": "Workshop Paper of AECAI@MICCAI 2025", "summary": "Surgical simulation is essential for medical training, enabling practitioners\nto develop crucial skills in a risk-free environment while improving patient\nsafety and surgical outcomes. However, conventional methods for building\nsimulation environments are cumbersome, time-consuming, and difficult to scale,\noften resulting in poor details and unrealistic simulations. In this paper, we\npropose a Gaussian Splatting-based framework to directly reconstruct\ninteractive surgical scenes from endoscopic data while ensuring efficiency,\nrendering quality, and realism. A key challenge in this data-driven simulation\nparadigm is the restricted movement of endoscopic cameras, which limits\nviewpoint diversity. As a result, the Gaussian Splatting representation\noverfits specific perspectives, leading to reduced geometric accuracy. To\naddress this issue, we introduce a novel virtual camera-based regularization\nmethod that adaptively samples virtual viewpoints around the scene and\nincorporates them into the optimization process to mitigate overfitting. An\neffective depth-based regularization is applied to both real and virtual views\nto further refine the scene geometry. To enable fast deformation simulation, we\npropose a sparse control node-based Material Point Method, which integrates\nphysical properties into the reconstructed scene while significantly reducing\ncomputational costs. Experimental results on representative surgical data\ndemonstrate that our method can efficiently reconstruct and simulate surgical\nscenes from sparse endoscopic views. Notably, our method takes only a few\nminutes to reconstruct the surgical scene and is able to produce physically\nplausible deformations in real-time with user-defined interactions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u6846\u67b6\uff0c\u4ece\u5185\u7aa5\u955c\u6570\u636e\u76f4\u63a5\u91cd\u5efa\u4ea4\u4e92\u5f0f\u624b\u672f\u573a\u666f\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u91cd\u5efa\u6548\u7387\u4f4e\u3001\u7ec6\u8282\u5dee\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u865a\u62df\u76f8\u673a\u6b63\u5219\u5316\u548c\u7269\u7406\u6a21\u62df\u5b9e\u73b0\u5b9e\u65f6\u53d8\u5f62\u3002", "motivation": "\u4f20\u7edf\u624b\u672f\u6a21\u62df\u73af\u5883\u6784\u5efa\u65b9\u6cd5\u7e41\u7410\u8017\u65f6\u3001\u96be\u4ee5\u6269\u5c55\uff0c\u5bfc\u81f4\u7ec6\u8282\u4e0d\u8db3\u548c\u6a21\u62df\u4e0d\u771f\u5b9e\u3002\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6765\u91cd\u5efa\u771f\u5b9e\u7684\u624b\u672f\u573a\u666f\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u6cd5\u91cd\u5efa\u624b\u672f\u573a\u666f\uff0c\u5f15\u5165\u865a\u62df\u76f8\u673a\u6b63\u5219\u5316\u65b9\u6cd5\u7f13\u89e3\u89c6\u89d2\u53d7\u9650\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u7ed3\u5408\u6df1\u5ea6\u6b63\u5219\u5316\u4f18\u5316\u51e0\u4f55\u7cbe\u5ea6\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u7a00\u758f\u63a7\u5236\u8282\u70b9\u7684\u6750\u6599\u70b9\u65b9\u6cd5\u5b9e\u73b0\u5feb\u901f\u7269\u7406\u53d8\u5f62\u6a21\u62df\u3002", "result": "\u5728\u4ee3\u8868\u6027\u624b\u672f\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u51e0\u5206\u949f\u5185\u91cd\u5efa\u624b\u672f\u573a\u666f\uff0c\u5e76\u5b9e\u65f6\u751f\u6210\u7269\u7406\u4e0a\u5408\u7406\u7684\u53d8\u5f62\u6548\u679c\uff0c\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u624b\u672f\u573a\u666f\u91cd\u5efa\u7684\u6548\u7387\u548c\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u533b\u7597\u57f9\u8bad\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u3001\u4ea4\u4e92\u6027\u66f4\u5f3a\u7684\u6a21\u62df\u73af\u5883\u3002"}}
{"id": "2509.17040", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17040", "abs": "https://arxiv.org/abs/2509.17040", "authors": ["Hang Du", "Jiayang Zhang", "Guoshun Nan", "Wendi Deng", "Zhenyan Chen", "Chenyang Zhang", "Wang Xiao", "Shan Huang", "Yuqi Pan", "Tao Qi", "Sicong Leng"], "title": "From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning", "comment": null, "summary": "Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language\nModels (MLLMs) ability to jointly comprehend and reason across multiple images\nand their associated textual contexts, introducing unique challenges beyond\nsingle-image or non-interleaved multi-image tasks. While current multi-image\nbenchmarks overlook interleaved textual contexts and neglect distinct\nrelationships between individual images and their associated texts, enabling\nmodels to reason over multi-image interleaved data may significantly enhance\ntheir comprehension of complex scenes and better capture cross-modal\ncorrelations. To bridge this gap, we introduce a novel benchmark MIR, requiring\njoint reasoning over multiple images accompanied by interleaved textual\ncontexts to accurately associate image regions with corresponding texts and\nlogically connect information across images. To enhance MLLMs ability to\ncomprehend multi-image interleaved data, we introduce reasoning steps for each\ninstance within the benchmark and propose a stage-wise curriculum learning\nstrategy. This strategy follows an \"easy to hard\" approach, progressively\nguiding models from simple to complex scenarios, thereby enhancing their\nability to handle challenging tasks. Extensive experiments benchmarking\nmultiple MLLMs demonstrate that our method significantly enhances models\nreasoning performance on MIR and other established benchmarks. We believe that\nMIR will encourage further research into multi-image interleaved reasoning,\nfacilitating advancements in MLLMs capability to handle complex inter-modal\ntasks.Our code and dataset are available at\nhttps://github.com/Shelly-coder239/MIRBench.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMIR\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u4ea4\u9519\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u5ffd\u7565\u4e86\u4ea4\u9519\u6587\u672c\u4e0a\u4e0b\u6587\u548c\u56fe\u50cf\u4e0e\u6587\u672c\u4e4b\u95f4\u7684\u7279\u5b9a\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u590d\u6742\u573a\u666f\u7684\u7406\u89e3\u548c\u8de8\u6a21\u6001\u76f8\u5173\u6027\u7684\u6355\u6349\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86MIR\u57fa\u51c6\uff0c\u8981\u6c42\u6a21\u578b\u5728\u591a\u4e2a\u56fe\u50cf\u548c\u4ea4\u9519\u6587\u672c\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u8054\u5408\u63a8\u7406\uff0c\u5e76\u91c7\u7528\u5206\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u4ece\u7b80\u5355\u5230\u590d\u6742\u9010\u6b65\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728MIR\u57fa\u51c6\u548c\u5176\u4ed6\u73b0\u6709\u57fa\u51c6\u4e0a\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "MIR\u57fa\u51c6\u5c06\u4fc3\u8fdb\u591a\u56fe\u50cf\u4ea4\u9519\u63a8\u7406\u7814\u7a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u8de8\u6a21\u6001\u4efb\u52a1\u7684\u80fd\u529b\u3002"}}
{"id": "2509.17041", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17041", "abs": "https://arxiv.org/abs/2509.17041", "authors": ["Samia Mohinta", "Daniel Franco-Barranco", "Shi Yan Lee", "Albert Cardona"], "title": "Towards Generalized Synapse Detection Across Invertebrate Species", "comment": null, "summary": "Behavioural differences across organisms, whether healthy or pathological,\nare closely tied to the structure of their neural circuits. Yet, the fine-scale\nsynaptic changes that give rise to these variations remain poorly understood,\nin part due to persistent challenges in detecting synapses reliably and at\nscale. Volume electron microscopy (EM) offers the resolution required to\ncapture synaptic architecture, but automated detection remains difficult due to\nsparse annotations, morphological variability, and cross-dataset domain shifts.\nTo address this, we make three key contributions. First, we curate a diverse EM\nbenchmark spanning four datasets across two invertebrate species: adult and\nlarval Drosophila melanogaster, and Megaphragma viggianii (micro-WASP). Second,\nwe propose SimpSyn, a single-stage Residual U-Net trained to predict\ndual-channel spherical masks around pre- and post-synaptic sites, designed to\nprioritize training and inference speeds and annotation efficiency over\narchitectural complexity. Third, we benchmark SimpSyn against Buhmann et al.'s\nSynful [1], a state-of-the-art multi-task model that jointly infers synaptic\npairs. Despite its simplicity, SimpSyn consistently outperforms Synful in\nF1-score across all volumes for synaptic site detection. While generalization\nacross datasets remains limited, SimpSyn achieves competitive performance when\ntrained on the combined cohort. Finally, ablations reveal that simple\npost-processing strategies - such as local peak detection and distance-based\nfiltering - yield strong performance without complex test-time heuristics.\nTaken together, our results suggest that lightweight models, when aligned with\ntask structure, offer a practical and scalable solution for synapse detection\nin large-scale connectomic pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SimpSyn\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5355\u9636\u6bb5\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7684\u7a81\u89e6\u68c0\u6d4b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u7b80\u5355\u6a21\u578b\u5728\u5927\u89c4\u6a21\u8fde\u63a5\u7ec4\u5b66\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7a81\u89e6\u81ea\u52a8\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5305\u62ec\u7a00\u758f\u6807\u6ce8\u3001\u5f62\u6001\u53d8\u5f02\u6027\u548c\u8de8\u6570\u636e\u96c6\u57df\u504f\u79fb\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u795e\u7ecf\u73af\u8def\u7814\u7a76\u63d0\u4f9b\u53ef\u9760\u5de5\u5177\u3002", "method": "\u63d0\u51faSimpSyn\u6a21\u578b\uff1a\u57fa\u4e8e\u5355\u9636\u6bb5\u6b8b\u5deeU-Net\uff0c\u9884\u6d4b\u7a81\u89e6\u524d\u540e\u4f4d\u70b9\u7684\u53cc\u901a\u9053\u7403\u5f62\u63a9\u7801\uff1b\u4f7f\u7528\u56db\u4e2a\u4e0d\u540c\u7269\u79cd\u7684EM\u6570\u636e\u96c6\u6784\u5efa\u57fa\u51c6\uff1b\u91c7\u7528\u5c40\u90e8\u5cf0\u503c\u68c0\u6d4b\u548c\u8ddd\u79bb\u8fc7\u6ee4\u7b49\u7b80\u5355\u540e\u5904\u7406\u7b56\u7565\u3002", "result": "SimpSyn\u5728\u6240\u6709\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u7a81\u89e6\u4f4d\u70b9\u68c0\u6d4bF1\u5206\u6570\u4e0a\u5747\u4f18\u4e8eSynful\u65b9\u6cd5\uff1b\u5728\u7ec4\u5408\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u65f6\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\uff1b\u7b80\u5355\u540e\u5904\u7406\u7b56\u7565\u5373\u53ef\u83b7\u5f97\u5f3a\u6027\u80fd\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e0e\u4efb\u52a1\u7ed3\u6784\u5bf9\u9f50\u65f6\uff0c\u53ef\u4ee5\u4e3a\u5927\u89c4\u6a21\u8fde\u63a5\u7ec4\u5b66\u7ba1\u9053\u4e2d\u7684\u7a81\u89e6\u68c0\u6d4b\u63d0\u4f9b\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u590d\u6742\u7684\u6d4b\u8bd5\u65f6\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2509.17044", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17044", "abs": "https://arxiv.org/abs/2509.17044", "authors": ["Mingqing Zhang", "Zhuoning Xu", "Peijie Wang", "Rongji Li", "Liang Wang", "Qiang Liu", "Jian Xu", "Xuyao Zhang", "Shu Wu", "Liang Wang"], "title": "AgriDoctor: A Multimodal Intelligent Assistant for Agriculture", "comment": null, "summary": "Accurate crop disease diagnosis is essential for sustainable agriculture and\nglobal food security. Existing methods, which primarily rely on unimodal models\nsuch as image-based classifiers and object detectors, are limited in their\nability to incorporate domain-specific agricultural knowledge and lack support\nfor interactive, language-based understanding. Recent advances in large\nlanguage models (LLMs) and large vision-language models (LVLMs) have opened new\navenues for multimodal reasoning. However, their performance in agricultural\ncontexts remains limited due to the absence of specialized datasets and\ninsufficient domain adaptation. In this work, we propose AgriDoctor, a modular\nand extensible multimodal framework designed for intelligent crop disease\ndiagnosis and agricultural knowledge interaction. As a pioneering effort to\nintroduce agent-based multimodal reasoning into the agricultural domain,\nAgriDoctor offers a novel paradigm for building interactive and domain-adaptive\ncrop health solutions. It integrates five core components: a router,\nclassifier, detector, knowledge retriever and LLMs. To facilitate effective\ntraining and evaluation, we construct AgriMM, a comprehensive benchmark\ncomprising 400000 annotated disease images, 831 expert-curated knowledge\nentries, and 300000 bilingual prompts for intent-driven tool selection.\nExtensive experiments demonstrate that AgriDoctor, trained on AgriMM,\nsignificantly outperforms state-of-the-art LVLMs on fine-grained agricultural\ntasks, establishing a new paradigm for intelligent and sustainable farming\napplications.", "AI": {"tldr": "AgriDoctor\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u4f5c\u7269\u75c5\u5bb3\u8bca\u65ad\u548c\u519c\u4e1a\u77e5\u8bc6\u4ea4\u4e92\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u4f5c\u7269\u75c5\u5bb3\u8bca\u65ad\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5355\u6a21\u6001\u6a21\u578b\uff0c\u65e0\u6cd5\u6574\u5408\u9886\u57df\u77e5\u8bc6\u548c\u652f\u6301\u8bed\u8a00\u4ea4\u4e92\uff0c\u800c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u519c\u4e1a\u9886\u57df\u8868\u73b0\u6709\u9650\u3002", "method": "\u63d0\u51faAgriDoctor\u6846\u67b6\uff0c\u5305\u542b\u8def\u7531\u5668\u3001\u5206\u7c7b\u5668\u3001\u68c0\u6d4b\u5668\u3001\u77e5\u8bc6\u68c0\u7d22\u5668\u548cLLMs\u4e94\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5e76\u6784\u5efaAgriMM\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAgriDoctor\u5728\u7cbe\u7ec6\u519c\u4e1a\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "AgriDoctor\u4e3a\u667a\u80fd\u53ef\u6301\u7eed\u519c\u4e1a\u5e94\u7528\u5efa\u7acb\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u4ee3\u7406\u7684\u591a\u6a21\u6001\u63a8\u7406\u5728\u519c\u4e1a\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.17049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17049", "abs": "https://arxiv.org/abs/2509.17049", "authors": ["Peng Wang", "Yong Li", "Lin Zhao", "Xiu-Shen Wei"], "title": "Learning Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query Optimization", "comment": null, "summary": "Fine-grained hashing has become a powerful solution for rapid and efficient\nimage retrieval, particularly in scenarios requiring high discrimination\nbetween visually similar categories. To enable each hash bit to correspond to\nspecific visual attributes, we propoe a novel method that harnesses learnable\nqueries for attribute-aware hash codes learning. This method deploys a tailored\nset of queries to capture and represent nuanced attribute-level information\nwithin the hashing process, thereby enhancing both the interpretability and\nrelevance of each hash bit. Building on this query-based optimization\nframework, we incorporate an auxiliary branch to help alleviate the challenges\nof complex landscape optimization often encountered with low-bit hash codes.\nThis auxiliary branch models high-order attribute interactions, reinforcing the\nrobustness and specificity of the generated hash codes. Experimental results on\nbenchmark datasets demonstrate that our method generates attribute-aware hash\ncodes and consistently outperforms state-of-the-art techniques in retrieval\naccuracy and robustness, especially for low-bit hash codes, underscoring its\npotential in fine-grained image hashing tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5b66\u4e60\u67e5\u8be2\u7684\u5c5e\u6027\u611f\u77e5\u54c8\u5e0c\u7801\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u5236\u67e5\u8be2\u96c6\u6355\u83b7\u7ec6\u7c92\u5ea6\u5c5e\u6027\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u8f85\u52a9\u5206\u652f\u5efa\u6a21\u9ad8\u9636\u5c5e\u6027\u4ea4\u4e92\uff0c\u63d0\u5347\u4f4e\u6bd4\u7279\u54c8\u5e0c\u7801\u7684\u68c0\u7d22\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u4e2d\u54c8\u5e0c\u7801\u4e0e\u5177\u4f53\u89c6\u89c9\u5c5e\u6027\u5bf9\u5e94\u7684\u95ee\u9898\uff0c\u589e\u5f3a\u54c8\u5e0c\u7801\u7684\u53ef\u89e3\u91ca\u6027\u548c\u76f8\u5173\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4f4e\u6bd4\u7279\u54c8\u5e0c\u7801\u7684\u590d\u6742\u4f18\u5316\u6311\u6218\u3002", "method": "\u4f7f\u7528\u53ef\u5b66\u4e60\u67e5\u8be2\u96c6\u8fdb\u884c\u5c5e\u6027\u611f\u77e5\u54c8\u5e0c\u7801\u5b66\u4e60\uff0c\u90e8\u7f72\u8f85\u52a9\u5206\u652f\u5efa\u6a21\u9ad8\u9636\u5c5e\u6027\u4ea4\u4e92\u4ee5\u4f18\u5316\u4f4e\u6bd4\u7279\u54c8\u5e0c\u7801\u7684\u590d\u6742\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u5c5e\u6027\u611f\u77e5\u54c8\u5e0c\u7801\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6bd4\u7279\u54c8\u5e0c\u7801\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u54c8\u5e0c\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u80fd\u591f\u751f\u6210\u66f4\u5177\u89e3\u91ca\u6027\u548c\u76f8\u5173\u6027\u7684\u54c8\u5e0c\u7801\uff0c\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2509.17050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17050", "abs": "https://arxiv.org/abs/2509.17050", "authors": ["Junhao Jia", "Yunyou Liu", "Yifei Sun", "Huangwei Chen", "Feiwei Qin", "Changmiao Wang", "Yong Peng"], "title": "Geodesic Prototype Matching via Diffusion Maps for Interpretable Fine-Grained Recognition", "comment": null, "summary": "Nonlinear manifolds are widespread in deep visual features, where Euclidean\ndistances often fail to capture true similarity. This limitation becomes\nparticularly severe in prototype-based interpretable fine-grained recognition,\nwhere subtle semantic distinctions are essential. To address this challenge, we\npropose a novel paradigm for prototype-based recognition that anchors\nsimilarity within the intrinsic geometry of deep features. Specifically, we\ndistill the latent manifold structure of each class into a diffusion space and\nintroduce a differentiable Nystr\\\"om interpolation, making the geometry\naccessible to both unseen samples and learnable prototypes. To ensure\nefficiency, we employ compact per-class landmark sets with periodic updates.\nThis design keeps the embedding aligned with the evolving backbone, enabling\nfast and scalable inference. Extensive experiments on the CUB-200-2011 and\nStanford Cars datasets show that our GeoProto framework produces prototypes\nfocusing on semantically aligned parts, significantly outperforming Euclidean\nprototype networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGeoProto\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u7a7a\u95f4\u548cNystr\u00f6m\u63d2\u503c\u5728\u6df1\u5ea6\u7279\u5f81\u7684\u5185\u5728\u51e0\u4f55\u4e2d\u951a\u5b9a\u76f8\u4f3c\u6027\uff0c\u89e3\u51b3\u539f\u578b\u53ef\u89e3\u91ca\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4e2d\u6b27\u6c0f\u8ddd\u79bb\u5931\u6548\u7684\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u89c6\u89c9\u7279\u5f81\u666e\u904d\u5b58\u5728\u975e\u7ebf\u6027\u6d41\u5f62\u7ed3\u6784\uff0c\u6b27\u6c0f\u8ddd\u79bb\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u76f8\u4f3c\u6027\uff0c\u8fd9\u5728\u9700\u8981\u6355\u6349\u7ec6\u5fae\u8bed\u4e49\u5dee\u5f02\u7684\u539f\u578b\u53ef\u89e3\u91ca\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4e2d\u5c24\u4e3a\u4e25\u91cd\u3002", "method": "\u5c06\u6bcf\u4e2a\u7c7b\u7684\u6f5c\u5728\u6d41\u5f62\u7ed3\u6784\u84b8\u998f\u5230\u6269\u6563\u7a7a\u95f4\uff0c\u5f15\u5165\u53ef\u5fae\u5206\u7684Nystr\u00f6m\u63d2\u503c\u4f7f\u51e0\u4f55\u7ed3\u6784\u5bf9\u672a\u89c1\u6837\u672c\u548c\u53ef\u5b66\u4e60\u539f\u578b\u90fd\u53ef\u7528\uff0c\u4f7f\u7528\u7d27\u51d1\u7684\u6bcf\u7c7b\u5730\u6807\u96c6\u5e76\u5b9a\u671f\u66f4\u65b0\u4ee5\u4fdd\u8bc1\u6548\u7387\u3002", "result": "\u5728CUB-200-2011\u548cStanford Cars\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGeoProto\u6846\u67b6\u4ea7\u751f\u7684\u539f\u578b\u805a\u7126\u4e8e\u8bed\u4e49\u5bf9\u9f50\u7684\u90e8\u5206\uff0c\u663e\u8457\u4f18\u4e8e\u6b27\u6c0f\u539f\u578b\u7f51\u7edc\u3002", "conclusion": "GeoProto\u901a\u8fc7\u5229\u7528\u6df1\u5ea6\u7279\u5f81\u7684\u5185\u5728\u51e0\u4f55\u7ed3\u6784\uff0c\u6709\u6548\u63d0\u5347\u4e86\u539f\u578b\u53ef\u89e3\u91ca\u7ec6\u7c92\u5ea6\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5728\u975e\u7ebf\u6027\u6d41\u5f62\u4e2d\u951a\u5b9a\u76f8\u4f3c\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.17065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17065", "abs": "https://arxiv.org/abs/2509.17065", "authors": ["Yao Du", "Jiarong Guo", "Xiaomeng Li"], "title": "CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a Few-shot Manner", "comment": "Accepted by MICCAI 2025", "summary": "Echocardiography is a vital non-invasive modality for cardiac assessment,\nwith left ventricular ejection fraction (LVEF) serving as a key indicator of\nheart function. Existing LVEF estimation methods depend on large-scale\nannotated video datasets, which are costly and limit adaptability across\nvarious clinical settings. Recent vision-language models for echocardiography,\nsuch as EchoCLIP, apply image-to-text pretraining but fail to capture crucial\ntemporal dynamics and localized cardiac structures essential for accurate\ndiagnosis. To address these challenges, we propose CardiacCLIP, a video-based\nframework that enhances LVEF prediction through attention-based frame\naggregation and multi-resolution input scaling. Specifically, we introduce MFL\n(Multi Frame Learning), a novel attention-based mechanism for selectively\nfusing informative frames, and EchoZoom, a multi-scale feature extraction\nstrategy that refines spatial representations of cardiac structures. As a novel\nadaptation of CLIP models for few-shot echocardiogram video analysis, our\napproach significantly improves diagnostic accuracy, reducing MAE by 2.07 on\nthe EchoNet-Dynamic dataset under 1-shot setting. The code is available at\nhttps://github.com/xmed-lab/CardiacCLIP.", "AI": {"tldr": "CardiacCLIP\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u5206\u8fa8\u7387\u8f93\u5165\u589e\u5f3aLVEF\u9884\u6d4b\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u8d85\u58f0\u5fc3\u52a8\u56fe\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709LVEF\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u89c6\u9891\u6570\u636e\u96c6\uff0c\u6210\u672c\u9ad8\u4e14\u4e34\u5e8a\u9002\u5e94\u6027\u6709\u9650\u3002\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5982EchoCLIP\u65e0\u6cd5\u6355\u6349\u5173\u952e\u7684\u65f6\u95f4\u52a8\u6001\u548c\u5c40\u90e8\u5fc3\u810f\u7ed3\u6784\u3002", "method": "\u63d0\u51faMFL\uff08\u591a\u5e27\u5b66\u4e60\uff09\u6ce8\u610f\u529b\u673a\u5236\u9009\u62e9\u6027\u878d\u5408\u4fe1\u606f\u5e27\uff0c\u4ee5\u53caEchoZoom\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u7b56\u7565\u7ec6\u5316\u5fc3\u810f\u7ed3\u6784\u7a7a\u95f4\u8868\u793a\u3002", "result": "\u5728EchoNet-Dynamic\u6570\u636e\u96c6\u4e0a\uff0c1-shot\u8bbe\u7f6e\u4e0bMAE\u964d\u4f4e\u4e862.07\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3002", "conclusion": "CardiacCLIP\u4f5c\u4e3aCLIP\u6a21\u578b\u5728\u5c11\u6837\u672c\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\u5206\u6790\u4e2d\u7684\u65b0\u9002\u5e94\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.17074", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17074", "abs": "https://arxiv.org/abs/2509.17074", "authors": ["Qian Zhang", "Lin Zhang", "Xing Fang", "Mingxin Zhang", "Zhiyuan Wei", "Ran Song", "Wei Zhang"], "title": "Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models", "comment": "Submitted to the IEEE International Conference on Robotics and\n  Automation (ICRA) 2026", "summary": "Visual affordance learning is crucial for robots to understand and interact\neffectively with the physical world. Recent advances in this field attempt to\nleverage pre-trained knowledge of vision-language foundation models to learn\naffordance properties with limited training data, providing a novel paradigm\nfor visual affordance learning. However, these methods overlook the\nsignificance of maintaining feature alignment between visual images and\nlanguage descriptions for identifying affordance areas with textual guidance,\nand thus may lead to suboptimal results. In this paper, we present an\ninformative framework for text-guided affordance learning, which involves\ninformation-based constraints to achieve text-image alignment at feature level.\nSpecifically, we design an affordance mutual information constraint that helps\nlearn appropriate textual prompts and task-oriented visual features\nsimultaneously by maximizing the mutual information between the features of the\naffordance areas in the input images and the corresponding textual prompts. In\naddition, we propose an object-level information constraint that maximizes the\nmutual information between the visual features of a given object and the text\nfeatures of the category it belongs to. This enables the model to capture\nhigh-quality representations for the object, providing more reliable semantic\npriors for identifying affordance regions. Experimental results on the AGD20K\ndataset show that the proposed method outperforms existing approaches and\nachieves the new state-of-the-art in one-shot affordance learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u7ea6\u675f\u7684\u6587\u672c\u5f15\u5bfc\u89c6\u89c9\u53ef\u4f9b\u6027\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u53ef\u4f9b\u6027\u533a\u57df\u7279\u5f81\u4e0e\u6587\u672c\u63d0\u793a\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u4ee5\u53ca\u5bf9\u8c61\u7ea7\u89c6\u89c9\u7279\u5f81\u4e0e\u7c7b\u522b\u6587\u672c\u7279\u5f81\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u5b9e\u73b0\u7279\u5f81\u5c42\u9762\u7684\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\uff0c\u5728\u5355\u6837\u672c\u53ef\u4f9b\u6027\u5b66\u4e60\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u53ef\u4f9b\u6027\u5b66\u4e60\u65f6\uff0c\u5ffd\u89c6\u4e86\u4fdd\u6301\u89c6\u89c9\u56fe\u50cf\u4e0e\u8bed\u8a00\u63cf\u8ff0\u4e4b\u95f4\u7279\u5f81\u5bf9\u9f50\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u6587\u672c\u5f15\u5bfc\u8bc6\u522b\u53ef\u4f9b\u6027\u533a\u57df\u7684\u6548\u679c\u4e0d\u7406\u60f3\u3002", "method": "\u8bbe\u8ba1\u53ef\u4f9b\u6027\u4e92\u4fe1\u606f\u7ea6\u675f\u6765\u540c\u65f6\u5b66\u4e60\u5408\u9002\u7684\u6587\u672c\u63d0\u793a\u548c\u4efb\u52a1\u5bfc\u5411\u7684\u89c6\u89c9\u7279\u5f81\uff1b\u63d0\u51fa\u5bf9\u8c61\u7ea7\u4fe1\u606f\u7ea6\u675f\u6765\u6355\u83b7\u9ad8\u8d28\u91cf\u7684\u5bf9\u8c61\u8868\u793a\uff0c\u4e3a\u8bc6\u522b\u53ef\u4f9b\u6027\u533a\u57df\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8bed\u4e49\u5148\u9a8c\u3002", "result": "\u5728AGD20K\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5355\u6837\u672c\u53ef\u4f9b\u6027\u5b66\u4e60\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4fe1\u606f\u7ea6\u675f\u5b9e\u73b0\u6587\u672c-\u56fe\u50cf\u7279\u5f81\u5bf9\u9f50\u662f\u63d0\u5347\u6587\u672c\u5f15\u5bfc\u53ef\u4f9b\u6027\u5b66\u4e60\u6548\u679c\u7684\u6709\u6548\u9014\u5f84\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u89c6\u89c9\u53ef\u4f9b\u6027\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17078", "abs": "https://arxiv.org/abs/2509.17078", "authors": ["Kihyun Kim", "Michalis Lazarou", "Tania Stathaki"], "title": "Enhanced Detection of Tiny Objects in Aerial Images", "comment": null, "summary": "While one-stage detectors like YOLOv8 offer fast training speed, they often\nunder-perform on detecting small objects as a trade-off. This becomes even more\ncritical when detecting tiny objects in aerial imagery due to low-resolution\ntargets and cluttered backgrounds. To address this, we introduce three\nenhancement strategies -- input image resolution adjustment, data augmentation,\nand attention mechanisms -- that can be easily implemented on YOLOv8. We\ndemonstrate that image size enlargement and the proper use of augmentation can\nlead to enhancement. Additionally, we designed a Mixture of Orthogonal\nNeural-modules Network (MoonNet) pipeline which consists of attention-augmented\nCNNs. Two well-known attention modules, the Squeeze-and-Excitation Block (SE\nBlock) and the Convolutional Block Attention Module (CBAM), were integrated\ninto the backbone of YOLOv8 with an increased number of channels, and the\nMoonNet backbone obtained improved detection accuracy compared to the original\nYOLOv8. MoonNet further proved its adaptability and potential by achieving\nstate-of-the-art performance on a tiny-object benchmark when integrated with\nthe YOLC model. Our codes are available at: https://github.com/Kihyun11/MoonNet", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9YOLOv8\u5728\u68c0\u6d4b\u5c0f\u7269\u4f53\u65f6\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u822a\u62cd\u56fe\u50cf\u4e2d\u68c0\u6d4b\u5fae\u5c0f\u7269\u4f53\u65f6\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u589e\u5f3a\u7b56\u7565\uff1a\u8f93\u5165\u56fe\u50cf\u5206\u8fa8\u7387\u8c03\u6574\u3001\u6570\u636e\u589e\u5f3a\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u8bbe\u8ba1\u4e86MoonNet\u7ba1\u9053\u6765\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4e00\u9636\u6bb5\u68c0\u6d4b\u5668\u5982YOLOv8\u867d\u7136\u8bad\u7ec3\u901f\u5ea6\u5feb\uff0c\u4f46\u5728\u68c0\u6d4b\u5c0f\u7269\u4f53\u65f6\u6027\u80fd\u8f83\u5dee\uff0c\u5c24\u5176\u662f\u5728\u822a\u62cd\u56fe\u50cf\u4e2d\u7531\u4e8e\u76ee\u6807\u5206\u8fa8\u7387\u4f4e\u548c\u80cc\u666f\u6742\u4e71\uff0c\u8fd9\u4e00\u95ee\u9898\u66f4\u52a0\u4e25\u91cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u589e\u5f3a\u7b56\u7565\uff1a\u8c03\u6574\u8f93\u5165\u56fe\u50cf\u5206\u8fa8\u7387\u3001\u6570\u636e\u589e\u5f3a\u548c\u6ce8\u610f\u529b\u673a\u5236\u3002\u8bbe\u8ba1\u4e86MoonNet\u7ba1\u9053\uff0c\u5c06SE Block\u548cCBAM\u6ce8\u610f\u529b\u6a21\u5757\u96c6\u6210\u5230YOLOv8\u7684\u4e3b\u5e72\u7f51\u7edc\u4e2d\uff0c\u5e76\u589e\u52a0\u4e86\u901a\u9053\u6570\u3002", "result": "\u56fe\u50cf\u5c3a\u5bf8\u653e\u5927\u548c\u9002\u5f53\u7684\u6570\u636e\u589e\u5f3a\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\u3002MoonNet\u4e3b\u5e72\u7f51\u7edc\u76f8\u6bd4\u539f\u59cbYOLOv8\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u5728\u4e0eYOLC\u6a21\u578b\u96c6\u6210\u65f6\u5728\u5fae\u5c0f\u7269\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MoonNet\u8bc1\u660e\u4e86\u5176\u9002\u5e94\u6027\u548c\u6f5c\u529b\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u589e\u5f3a\u7b56\u7565\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u96c6\u6210\uff0c\u6709\u6548\u63d0\u5347\u4e86YOLOv8\u5728\u5fae\u5c0f\u7269\u4f53\u68c0\u6d4b\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.17079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17079", "abs": "https://arxiv.org/abs/2509.17079", "authors": ["Yuhong Feng", "Hongtao Chen", "Qi Zhang", "Jie Chen", "Zhaoxi He", "Mingzhe Liu", "Jianghai Liao"], "title": "A Dual-Modulation Framework for RGB-T Crowd Counting via Spatially Modulated Attention and Adaptive Fusion", "comment": "Submitted to ICASSP 2026", "summary": "Accurate RGB-Thermal (RGB-T) crowd counting is crucial for public safety in\nchallenging conditions. While recent Transformer-based methods excel at\ncapturing global context, their inherent lack of spatial inductive bias causes\nattention to spread to irrelevant background regions, compromising crowd\nlocalization precision. Furthermore, effectively bridging the gap between these\ndistinct modalities remains a major hurdle. To tackle this, we propose the Dual\nModulation Framework, comprising two modules: Spatially Modulated Attention\n(SMA), which improves crowd localization by using a learnable Spatial Decay\nMask to penalize attention between distant tokens and prevent focus from\nspreading to the background; and Adaptive Fusion Modulation (AFM), which\nimplements a dynamic gating mechanism to prioritize the most reliable modality\nfor adaptive cross-modal fusion. Extensive experiments on RGB-T crowd counting\ndatasets demonstrate the superior performance of our method compared to\nprevious works. Code available at\nhttps://github.com/Cht2924/RGBT-Crowd-Counting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8c03\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u8c03\u5236\u6ce8\u610f\u529b\uff08SMA\uff09\u548c\u81ea\u9002\u5e94\u878d\u5408\u8c03\u5236\uff08AFM\uff09\u6765\u89e3\u51b3RGB-\u70ed\u6210\u50cf\u4eba\u7fa4\u8ba1\u6570\u4e2d\u7684\u6ce8\u610f\u529b\u5206\u6563\u548c\u6a21\u6001\u878d\u5408\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u7fa4\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u5728RGB-\u70ed\u6210\u50cf\u4eba\u7fa4\u8ba1\u6570\u4e2d\u867d\u7136\u80fd\u6355\u6349\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u4f46\u7f3a\u4e4f\u7a7a\u95f4\u5f52\u7eb3\u504f\u7f6e\u5bfc\u81f4\u6ce8\u610f\u529b\u5206\u6563\u5230\u65e0\u5173\u80cc\u666f\u533a\u57df\uff0c\u5f71\u54cd\u4eba\u7fa4\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4e14\u4e0d\u540c\u6a21\u6001\u95f4\u7684\u6709\u6548\u878d\u5408\u4ecd\u5177\u6311\u6218\u3002", "method": "\u63d0\u51fa\u53cc\u8c03\u5236\u6846\u67b6\uff1a1\uff09\u7a7a\u95f4\u8c03\u5236\u6ce8\u610f\u529b\uff08SMA\uff09\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u8870\u51cf\u63a9\u7801\u60e9\u7f5a\u8fdc\u8ddd\u79bbtoken\u95f4\u7684\u6ce8\u610f\u529b\uff0c\u9632\u6b62\u6ce8\u610f\u529b\u6269\u6563\u5230\u80cc\u666f\uff1b2\uff09\u81ea\u9002\u5e94\u878d\u5408\u8c03\u5236\uff08AFM\uff09\u901a\u8fc7\u52a8\u6001\u95e8\u63a7\u673a\u5236\u4f18\u5148\u9009\u62e9\u6700\u53ef\u9760\u7684\u6a21\u6001\u8fdb\u884c\u81ea\u9002\u5e94\u8de8\u6a21\u6001\u878d\u5408\u3002", "result": "\u5728RGB-\u70ed\u6210\u50cf\u4eba\u7fa4\u8ba1\u6570\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4e4b\u524d\u7684\u5de5\u4f5c\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cc\u8c03\u5236\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86RGB-\u70ed\u6210\u50cf\u4eba\u7fa4\u8ba1\u6570\u4e2d\u7684\u6ce8\u610f\u529b\u5206\u6563\u548c\u6a21\u6001\u878d\u5408\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4eba\u7fa4\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8ba1\u6570\u6027\u80fd\u3002"}}
{"id": "2509.17083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17083", "abs": "https://arxiv.org/abs/2509.17083", "authors": ["Zipeng Wang", "Dan Xu"], "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis", "comment": null, "summary": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative\nto NeRF-based approaches, enabling real-time, high-quality novel view synthesis\nthrough explicit, optimizable 3D Gaussians. However, 3DGS suffers from\nsignificant memory overhead due to its reliance on per-Gaussian parameters to\nmodel view-dependent effects and anisotropic shapes. While recent works propose\ncompressing 3DGS with neural fields, these methods struggle to capture\nhigh-frequency spatial variations in Gaussian properties, leading to degraded\nreconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a\nnovel scene representation that combines the strengths of explicit Gaussians\nand neural fields. HyRF decomposes the scene into (1) a compact set of explicit\nGaussians storing only critical high-frequency parameters and (2) grid-based\nneural fields that predict remaining properties. To enhance representational\ncapacity, we introduce a decoupled neural field architecture, separately\nmodeling geometry (scale, opacity, rotation) and view-dependent color.\nAdditionally, we propose a hybrid rendering scheme that composites Gaussian\nsplatting with a neural field-predicted background, addressing limitations in\ndistant scene representation. Experiments demonstrate that HyRF achieves\nstate-of-the-art rendering quality while reducing model size by over 20 times\ncompared to 3DGS and maintaining real-time performance. Our project page is\navailable at https://wzpscott.github.io/hyrf/.", "AI": {"tldr": "HyRF\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u663e\u5f0f\u9ad8\u65af\u548c\u795e\u7ecf\u573a\u7684\u6df7\u5408\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u573a\u666f\u4e3a\u7d27\u51d1\u7684\u9ad8\u65af\u96c6\u5408\u548c\u7f51\u683c\u795e\u7ecf\u573a\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u5360\u7528\u5e76\u4fdd\u6301\u5b9e\u65f6\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u867d\u7136\u80fd\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u8d28\u91cf\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u4f46\u5b58\u5728\u663e\u8457\u5185\u5b58\u5f00\u9500\u95ee\u9898\u3002\u73b0\u6709\u795e\u7ecf\u573a\u538b\u7f29\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u9ad8\u65af\u5c5e\u6027\u7684\u9ad8\u9891\u7a7a\u95f4\u53d8\u5316\uff0c\u5bfc\u81f4\u7ec6\u8282\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u3002", "method": "HyRF\u5c06\u573a\u666f\u5206\u89e3\u4e3a\uff1a(1)\u5b58\u50a8\u5173\u952e\u9ad8\u9891\u53c2\u6570\u7684\u7d27\u51d1\u663e\u5f0f\u9ad8\u65af\u96c6\u5408\uff1b(2)\u9884\u6d4b\u5269\u4f59\u5c5e\u6027\u7684\u7f51\u683c\u795e\u7ecf\u573a\u3002\u91c7\u7528\u89e3\u8026\u795e\u7ecf\u573a\u67b6\u6784\u5206\u522b\u5efa\u6a21\u51e0\u4f55\u548c\u989c\u8272\uff0c\u5e76\u63d0\u51fa\u6df7\u5408\u6e32\u67d3\u65b9\u6848\u7ed3\u5408\u9ad8\u65af\u6cfc\u6e85\u548c\u795e\u7ecf\u573a\u9884\u6d4b\u80cc\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHyRF\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\uff0c\u76f8\u6bd43DGS\u5c06\u6a21\u578b\u5927\u5c0f\u51cf\u5c1120\u500d\u4ee5\u4e0a\uff0c\u5e76\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "HyRF\u6210\u529f\u7ed3\u5408\u4e86\u663e\u5f0f\u9ad8\u65af\u548c\u795e\u7ecf\u573a\u7684\u4f18\u52bf\uff0c\u5728\u663e\u8457\u538b\u7f29\u6a21\u578b\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f6\u6e32\u67d3\u80fd\u529b\uff0c\u4e3a3D\u573a\u666f\u8868\u793a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17084", "abs": "https://arxiv.org/abs/2509.17084", "authors": ["Binhua Huang", "Nan Wang", "Arjun Parakash", "Soumyabrata Dev"], "title": "MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors", "comment": "6 pages, 3 figures", "summary": "Video action recognition is a fundamental task in computer vision, but\nstate-of-the-art models are often computationally expensive and rely on\nextensive video pre-training. In parallel, large-scale vision-language models\nlike Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shot\ncapabilities on static images, while motion vectors (MV) provide highly\nefficient temporal information directly from compressed video streams. To\nsynergize the strengths of these paradigms, we propose MoCLIP-Lite, a simple\nyet powerful two-stream late fusion framework for efficient video recognition.\nOur approach combines features from a frozen CLIP image encoder with features\nfrom a lightweight, supervised network trained on raw MV. During fusion, both\nbackbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head is\ntrained, ensuring extreme efficiency. Through comprehensive experiments on the\nUCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy,\nsignificantly outperforming strong zero-shot (65.0%) and MV-only (66.5%)\nbaselines. Our work provides a new, highly efficient baseline for video\nunderstanding that effectively bridges the gap between large static models and\ndynamic, low-cost motion cues. Our code and models are available at\nhttps://github.com/microa/MoCLIP-Lite.", "AI": {"tldr": "MoCLIP-Lite\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u53cc\u6d41\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u51bb\u7ed3\u7684CLIP\u56fe\u50cf\u7f16\u7801\u5668\u548c\u8f7b\u91cf\u7ea7\u8fd0\u52a8\u5411\u91cf\u7f51\u7edc\uff0c\u4ec5\u8bad\u7ec3\u5c0f\u578bMLP\u5934\uff0c\u5728UCF101\u6570\u636e\u96c6\u4e0a\u8fbe\u523089.2%\u7684Top-1\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4f9d\u8d56\u5927\u91cf\u89c6\u9891\u9884\u8bad\u7ec3\uff0c\u800cCLIP\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u80fd\u529b\u4f46\u7f3a\u4e4f\u65f6\u95f4\u4fe1\u606f\uff0c\u8fd0\u52a8\u5411\u91cf\u80fd\u9ad8\u6548\u63d0\u4f9b\u65f6\u95f4\u4fe1\u606f\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u53cc\u6d41\u540e\u671f\u878d\u5408\u6846\u67b6\uff1a\u4e00\u4e2a\u6d41\u4f7f\u7528\u51bb\u7ed3\u7684CLIP\u56fe\u50cf\u7f16\u7801\u5668\u63d0\u53d6\u9759\u6001\u7279\u5f81\uff0c\u53e6\u4e00\u4e2a\u6d41\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7f51\u7edc\u5904\u7406\u539f\u59cb\u8fd0\u52a8\u5411\u91cf\u63d0\u53d6\u52a8\u6001\u7279\u5f81\uff0c\u4ec5\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7684MLP\u878d\u5408\u5934\u3002", "result": "\u5728UCF101\u6570\u636e\u96c6\u4e0a\u8fbe\u523089.2%\u7684Top-1\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u57fa\u7ebf(65.0%)\u548c\u4ec5\u4f7f\u7528\u8fd0\u52a8\u5411\u91cf\u7684\u57fa\u7ebf(66.5%)\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u65b0\u57fa\u51c6\uff0c\u6709\u6548\u5f25\u5408\u4e86\u5927\u578b\u9759\u6001\u6a21\u578b\u4e0e\u4f4e\u6210\u672c\u52a8\u6001\u8fd0\u52a8\u7ebf\u7d22\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.17086", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17086", "abs": "https://arxiv.org/abs/2509.17086", "authors": ["Jie Chen", "Yuhong Feng", "Tao Dai", "Mingzhe Liu", "Hongtao Chen", "Zhaoxi He", "Jiancong Bai"], "title": "SFN-YOLO: Towards Free-Range Poultry Detection via Scale-aware Fusion Networks", "comment": "Submitted to ICASSP 2026", "summary": "Detecting and localizing poultry is essential for advancing smart poultry\nfarming. Despite the progress of detection-centric methods, challenges persist\nin free-range settings due to multiscale targets, obstructions, and complex or\ndynamic backgrounds. To tackle these challenges, we introduce an innovative\npoultry detection approach named SFN-YOLO that utilizes scale-aware fusion.\nThis approach combines detailed local features with broader global context to\nimprove detection in intricate environments. Furthermore, we have developed a\nnew expansive dataset (M-SCOPE) tailored for varied free-range conditions.\nComprehensive experiments demonstrate our model achieves an mAP of 80.7% with\njust 7.2M parameters, which is 35.1% fewer than the benchmark, while retaining\nstrong generalization capability across different domains. The efficient and\nreal-time detection capabilities of SFN-YOLO support automated smart poultry\nfarming. The code and dataset can be accessed at\nhttps://github.com/chenjessiee/SFN-YOLO.", "AI": {"tldr": "SFN-YOLO\u662f\u4e00\u79cd\u521b\u65b0\u7684\u5bb6\u79bd\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c3a\u5ea6\u611f\u77e5\u878d\u5408\u6280\u672f\u7ed3\u5408\u5c40\u90e8\u7ec6\u8282\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u5728\u590d\u6742\u81ea\u7531\u653e\u517b\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u5728M-SCOPE\u6570\u636e\u96c6\u4e0a\u8fbe\u523080.7%\u7684mAP\uff0c\u53c2\u6570\u4ec57.2M\uff0c\u6bd4\u57fa\u51c6\u6a21\u578b\u51cf\u5c1135.1%\u3002", "motivation": "\u81ea\u7531\u653e\u517b\u73af\u5883\u4e2d\u7684\u5bb6\u79bd\u68c0\u6d4b\u9762\u4e34\u591a\u5c3a\u5ea6\u76ee\u6807\u3001\u906e\u6321\u548c\u590d\u6742\u52a8\u6001\u80cc\u666f\u7b49\u6311\u6218\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u8fd9\u4e9b\u573a\u666f\u4e0b\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faSFN-YOLO\u65b9\u6cd5\uff0c\u91c7\u7528\u5c3a\u5ea6\u611f\u77e5\u878d\u5408\u6280\u672f\uff0c\u7ed3\u5408\u5c40\u90e8\u7279\u5f81\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4e13\u95e8\u9488\u5bf9\u81ea\u7531\u653e\u517b\u6761\u4ef6\u7684M-SCOPE\u6570\u636e\u96c6\u3002", "result": "\u6a21\u578b\u5728mAP\u6307\u6807\u4e0a\u8fbe\u523080.7%\uff0c\u53c2\u6570\u6570\u91cf\u4ec57.2M\uff0c\u6bd4\u57fa\u51c6\u6a21\u578b\u51cf\u5c1135.1%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SFN-YOLO\u5177\u6709\u9ad8\u6548\u5b9e\u65f6\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u652f\u6301\u81ea\u52a8\u5316\u667a\u80fd\u5bb6\u79bd\u517b\u6b96\u5e94\u7528\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.17088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17088", "abs": "https://arxiv.org/abs/2509.17088", "authors": ["Jiexuan Zhang", "Yiheng Du", "Qian Wang", "Weiqi Li", "Yu Gu", "Jian Zhang"], "title": "AlignedGen: Aligning Style Across Generated Images", "comment": null, "summary": "Despite their generative power, diffusion models struggle to maintain style\nconsistency across images conditioned on the same style prompt, hindering their\npractical deployment in creative workflows. While several training-free methods\nattempt to solve this, they are constrained to the U-Net architecture, which\nnot only leads to low-quality results and artifacts like object repetition but\nalso renders them incompatible with superior Diffusion Transformer (DiT). To\naddress these issues, we introduce AlignedGen, a novel training-free framework\nthat enhances style consistency across images generated by DiT models. Our work\nfirst reveals a critical insight: naive attention sharing fails in DiT due to\nconflicting positional signals from improper position embeddings. We introduce\nShifted Position Embedding (ShiftPE), an effective solution that resolves this\nconflict by allocating a non-overlapping set of positional indices to each\nimage. Building on this foundation, we develop Advanced Attention Sharing\n(AAS), a suite of three techniques meticulously designed to fully unleash the\npotential of attention sharing within the DiT. Furthermore, to broaden the\napplicability of our method, we present an efficient query, key, and value\nfeature extraction algorithm, enabling our method to seamlessly incorporate\nexternal images as style references. Extensive experimental results validate\nthat our method effectively enhances style consistency across generated images\nwhile maintaining precise text-to-image alignment.", "AI": {"tldr": "AlignedGen\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7Shifted Position Embedding\u548cAdvanced Attention Sharing\u6280\u672f\u89e3\u51b3DiT\u6a21\u578b\u4e2d\u98ce\u683c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u652f\u6301\u5916\u90e8\u56fe\u50cf\u4f5c\u4e3a\u98ce\u683c\u53c2\u8003\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u76f8\u540c\u98ce\u683c\u63d0\u793a\u4e0b\u96be\u4ee5\u4fdd\u6301\u56fe\u50cf\u95f4\u7684\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u963b\u788d\u4e86\u5728\u521b\u610f\u5de5\u4f5c\u6d41\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8eU-Net\u67b6\u6784\uff0c\u5b58\u5728\u8d28\u91cf\u4f4e\u3001\u4f2a\u5f71\u7b49\u95ee\u9898\uff0c\u4e14\u4e0d\u517c\u5bb9\u66f4\u4f18\u7684DiT\u6a21\u578b\u3002", "method": "1. \u63d0\u51faShifted Position Embedding\u89e3\u51b3\u4f4d\u7f6e\u5d4c\u5165\u51b2\u7a81\u95ee\u9898\uff1b2. \u5f00\u53d1Advanced Attention Sharing\u6280\u672f\u5957\u4ef6\uff1b3. \u8bbe\u8ba1\u9ad8\u6548\u7684QKV\u7279\u5f81\u63d0\u53d6\u7b97\u6cd5\u652f\u6301\u5916\u90e8\u98ce\u683c\u53c2\u8003\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3a\u751f\u6210\u56fe\u50cf\u95f4\u7684\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7cbe\u786e\u7684\u6587\u56fe\u5bf9\u9f50\u3002", "conclusion": "AlignedGen\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86DiT\u6a21\u578b\u4e2d\u7684\u98ce\u683c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u5728\u521b\u610f\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17098", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17098", "abs": "https://arxiv.org/abs/2509.17098", "authors": ["Yuzhu Li", "An Sui", "Fuping Wu", "Xiahai Zhuang"], "title": "Uncertainty-Supervised Interpretable and Robust Evidential Segmentation", "comment": null, "summary": "Uncertainty estimation has been widely studied in medical image segmentation\nas a tool to provide reliability, particularly in deep learning approaches.\nHowever, previous methods generally lack effective supervision in uncertainty\nestimation, leading to low interpretability and robustness of the predictions.\nIn this work, we propose a self-supervised approach to guide the learning of\nuncertainty. Specifically, we introduce three principles about the\nrelationships between the uncertainty and the image gradients around boundaries\nand noise. Based on these principles, two uncertainty supervision losses are\ndesigned. These losses enhance the alignment between model predictions and\nhuman interpretation. Accordingly, we introduce novel quantitative metrics for\nevaluating the interpretability and robustness of uncertainty. Experimental\nresults demonstrate that compared to state-of-the-art approaches, the proposed\nmethod can achieve competitive segmentation performance and superior results in\nout-of-distribution (OOD) scenarios while significantly improving the\ninterpretability and robustness of uncertainty estimation. Code is available\nvia https://github.com/suiannaius/SURE.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u5173\u4e8e\u4e0d\u786e\u5b9a\u6027\u4e0e\u56fe\u50cf\u68af\u5ea6\u5173\u7cfb\u7684\u539f\u5219\u8bbe\u8ba1\u76d1\u7763\u635f\u5931\uff0c\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u4e0d\u786e\u5b9a\u6027\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u76d1\u7763\uff0c\u5bfc\u81f4\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u4e0e\u8fb9\u754c\u68af\u5ea6\u3001\u566a\u58f0\u5173\u7cfb\u7684\u4e09\u4e2a\u539f\u5219\uff0c\u8bbe\u8ba1\u4e24\u79cd\u4e0d\u786e\u5b9a\u6027\u76d1\u7763\u635f\u5931\uff0c\u5b9e\u73b0\u81ea\u76d1\u7763\u7684\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\u3002", "result": "\u5728OOD\u573a\u666f\u4e0b\u53d6\u5f97\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u5206\u5272\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u5f0f\u6709\u6548\u6307\u5bfc\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5de5\u5177\u3002"}}
{"id": "2509.17100", "categories": ["cs.CV", "68T07", "I.2.10; J.3"], "pdf": "https://arxiv.org/pdf/2509.17100", "abs": "https://arxiv.org/abs/2509.17100", "authors": ["Deepak Alapatt", "Jennifer Eckhoff", "Zhiliang Lyu", "Yutong Ban", "Jean-Paul Mazellier", "Sarah Choksi", "Kunyi Yang", "2024 CVS Challenge Consortium", "Quanzheng Li", "Filippo Filicori", "Xiang Li", "Pietro Mascagni", "Daniel A. Hashimoto", "Guy Rosman", "Ozanan Meireles", "Nicolas Padoy"], "title": "The SAGES Critical View of Safety Challenge: A Global Benchmark for AI-Assisted Surgical Quality Assessment", "comment": "18 pages, 10 figures", "summary": "Advances in artificial intelligence (AI) for surgical quality assessment\npromise to democratize access to expertise, with applications in training,\nguidance, and accreditation. This study presents the SAGES Critical View of\nSafety (CVS) Challenge, the first AI competition organized by a surgical\nsociety, using the CVS in laparoscopic cholecystectomy, a universally\nrecommended yet inconsistently performed safety step, as an exemplar of\nsurgical quality assessment. A global collaboration across 54 institutions in\n24 countries engaged hundreds of clinicians and engineers to curate 1,000\nvideos annotated by 20 surgical experts according to a consensus-validated\nprotocol. The challenge addressed key barriers to real-world deployment in\nsurgery, including achieving high performance, capturing uncertainty in\nsubjective assessment, and ensuring robustness to clinical variability. To\nenable this scale of effort, we developed EndoGlacier, a framework for managing\nlarge, heterogeneous surgical video and multi-annotator workflows. Thirteen\ninternational teams participated, achieving up to a 17\\% relative gain in\nassessment performance, over 80\\% reduction in calibration error, and a 17\\%\nrelative improvement in robustness over the state-of-the-art. Analysis of\nresults highlighted methodological trends linked to model performance,\nproviding guidance for future research toward robust, clinically deployable AI\nfor surgical quality assessment.", "AI": {"tldr": "SAGES CVS Challenge\u662f\u9996\u4e2a\u7531\u5916\u79d1\u5b66\u4f1a\u7ec4\u7ec7\u7684AI\u7ade\u8d5b\uff0c\u65e8\u5728\u901a\u8fc7\u8179\u8154\u955c\u80c6\u56ca\u5207\u9664\u672f\u4e2d\u7684\u5173\u952e\u5b89\u5168\u89c6\u56fe\u8bc4\u4f30\u624b\u672f\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86AI\u5728\u624b\u672f\u8d28\u91cf\u8bc4\u4f30\u4e2d\u90e8\u7f72\u7684\u5173\u952e\u969c\u788d\u3002", "motivation": "\u901a\u8fc7AI\u6280\u672f\u6c11\u4e3b\u5316\u624b\u672f\u4e13\u4e1a\u77e5\u8bc6\u83b7\u53d6\uff0c\u5e94\u7528\u4e8e\u57f9\u8bad\u3001\u6307\u5bfc\u548c\u8ba4\u8bc1\uff0c\u89e3\u51b3\u624b\u672f\u8d28\u91cf\u8bc4\u4f30\u4e2d\u6027\u80fd\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u4e34\u5e8a\u53d8\u5f02\u6027\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86EndoGlacier\u6846\u67b6\u7ba1\u7406\u5927\u89c4\u6a21\u5f02\u8d28\u624b\u672f\u89c6\u9891\u548c\u591a\u6807\u6ce8\u8005\u5de5\u4f5c\u6d41\uff0c\u7ec4\u7ec7\u5168\u740354\u4e2a\u673a\u6784\u7684\u56fd\u9645\u5408\u4f5c\uff0c\u4f7f\u75281000\u4e2a\u4e13\u5bb6\u6807\u6ce8\u89c6\u9891\u8fdb\u884cAI\u6a21\u578b\u8bc4\u4f30\u3002", "result": "13\u4e2a\u56fd\u9645\u56e2\u961f\u53c2\u4e0e\uff0c\u5b9e\u73b0\u4e8617%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\uff0c80%\u4ee5\u4e0a\u7684\u6821\u51c6\u8bef\u5dee\u51cf\u5c11\uff0c\u4ee5\u53ca17%\u7684\u76f8\u5bf9\u9c81\u68d2\u6027\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u4e3a\u672a\u6765\u5f00\u53d1\u7a33\u5065\u3001\u53ef\u4e34\u5e8a\u90e8\u7f72\u7684\u624b\u672f\u8d28\u91cf\u8bc4\u4f30AI\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u6307\u5bfc\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2509.17107", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.17107", "abs": "https://arxiv.org/abs/2509.17107", "authors": ["Lingzhao Kong", "Jiacheng Lin", "Siyu Li", "Kai Luo", "Zhiyong Li", "Kailun Yang"], "title": "CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception", "comment": "The source code will be made publicly available at\n  https://github.com/godk0509/CoBEVMoE", "summary": "Collaborative perception aims to extend sensing coverage and improve\nperception accuracy by sharing information among multiple agents. However, due\nto differences in viewpoints and spatial positions, agents often acquire\nheterogeneous observations. Existing intermediate fusion methods primarily\nfocus on aligning similar features, often overlooking the perceptual diversity\namong agents. To address this limitation, we propose CoBEVMoE, a novel\ncollaborative perception framework that operates in the Bird's Eye View (BEV)\nspace and incorporates a Dynamic Mixture-of-Experts (DMoE) architecture. In\nDMoE, each expert is dynamically generated based on the input features of a\nspecific agent, enabling it to extract distinctive and reliable cues while\nattending to shared semantics. This design allows the fusion process to\nexplicitly model both feature similarity and heterogeneity across agents.\nFurthermore, we introduce a Dynamic Expert Metric Loss (DEML) to enhance\ninter-expert diversity and improve the discriminability of the fused\nrepresentation. Extensive experiments on the OPV2V and DAIR-V2X-C datasets\ndemonstrate that CoBEVMoE achieves state-of-the-art performance. Specifically,\nit improves the IoU for Camera-based BEV segmentation by +1.5% on OPV2V and the\nAP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C, verifying the\neffectiveness of expert-based heterogeneous feature modeling in multi-agent\ncollaborative perception. The source code will be made publicly available at\nhttps://github.com/godk0509/CoBEVMoE.", "AI": {"tldr": "CoBEVMoE\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u534f\u4f5c\u611f\u77e5\u6846\u67b6\uff0c\u5728BEV\u7a7a\u95f4\u4e2d\u4f7f\u7528\u52a8\u6001\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u4e13\u5bb6\u6765\u5efa\u6a21\u5f02\u6784\u7279\u5f81\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e2d\u95f4\u878d\u5408\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5bf9\u9f50\u76f8\u4f3c\u7279\u5f81\uff0c\u4f46\u5ffd\u7565\u4e86\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u611f\u77e5\u591a\u6837\u6027\u3002\u7531\u4e8e\u89c6\u89d2\u548c\u7a7a\u95f4\u4f4d\u7f6e\u5dee\u5f02\uff0c\u667a\u80fd\u4f53\u5f80\u5f80\u83b7\u5f97\u5f02\u6784\u89c2\u6d4b\u6570\u636e\u3002", "method": "\u63d0\u51faCoBEVMoE\u6846\u67b6\uff0c\u5728BEV\u7a7a\u95f4\u4e2d\u91c7\u7528\u52a8\u6001\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u6839\u636e\u7279\u5b9a\u667a\u80fd\u4f53\u7684\u8f93\u5165\u7279\u5f81\u52a8\u6001\u751f\u6210\uff0c\u80fd\u591f\u63d0\u53d6\u72ec\u7279\u53ef\u9760\u7ebf\u7d22\u5e76\u5173\u6ce8\u5171\u4eab\u8bed\u4e49\u3002\u8fd8\u5f15\u5165\u52a8\u6001\u4e13\u5bb6\u5ea6\u91cf\u635f\u5931\u6765\u589e\u5f3a\u4e13\u5bb6\u95f4\u591a\u6837\u6027\u3002", "result": "\u5728OPV2V\u548cDAIR-V2X-C\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCoBEVMoE\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1a\u76f8\u673aBEV\u5206\u5272IoU\u63d0\u5347+1.5%\uff0c\u6fc0\u5149\u96f7\u8fbe3D\u68c0\u6d4bAP@50\u63d0\u5347+3.0%\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u4e13\u5bb6\u7684\u5f02\u6784\u7279\u5f81\u5efa\u6a21\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u611f\u77e5\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2509.17120", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17120", "abs": "https://arxiv.org/abs/2509.17120", "authors": ["Gordon Chen", "Ziqi Huang", "Cheston Tan", "Ziwei Liu"], "title": "Stencil: Subject-Driven Generation with Context Guidance", "comment": "Accepted as Spotlight at ICIP 2025", "summary": "Recent text-to-image diffusion models can generate striking visuals from text\nprompts, but they often fail to maintain subject consistency across generations\nand contexts. One major limitation of current fine-tuning approaches is the\ninherent trade-off between quality and efficiency. Fine-tuning large models\nimproves fidelity but is computationally expensive, while fine-tuning\nlightweight models improves efficiency but compromises image fidelity.\nMoreover, fine-tuning pre-trained models on a small set of images of the\nsubject can damage the existing priors, resulting in suboptimal results. To\nthis end, we present Stencil, a novel framework that jointly employs two\ndiffusion models during inference. Stencil efficiently fine-tunes a lightweight\nmodel on images of the subject, while a large frozen pre-trained model provides\ncontextual guidance during inference, injecting rich priors to enhance\ngeneration with minimal overhead. Stencil excels at generating high-fidelity,\nnovel renditions of the subject in less than a minute, delivering\nstate-of-the-art performance and setting a new benchmark in subject-driven\ngeneration.", "AI": {"tldr": "Stencil\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f7f\u7528\u4e24\u4e2a\u6269\u6563\u6a21\u578b\u6765\u89e3\u51b3\u4e3b\u9898\u4e00\u81f4\u6027\u751f\u6210\u4e2d\u7684\u8d28\u91cf\u4e0e\u6548\u7387\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u8de8\u751f\u6210\u548c\u4e0a\u4e0b\u6587\u4e2d\u96be\u4ee5\u4fdd\u6301\u4e3b\u9898\u4e00\u81f4\u6027\uff0c\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u8d28\u91cf\u4e0e\u6548\u7387\u7684\u56fa\u6709\u6743\u8861\uff1a\u5fae\u8c03\u5927\u6a21\u578b\u8d28\u91cf\u9ad8\u4f46\u8ba1\u7b97\u6602\u8d35\uff0c\u5fae\u8c03\u8f7b\u91cf\u7ea7\u6a21\u578b\u6548\u7387\u9ad8\u4f46\u56fe\u50cf\u4fdd\u771f\u5ea6\u5dee\uff0c\u4e14\u5728\u5c0f\u6837\u672c\u5fae\u8c03\u65f6\u4f1a\u635f\u5bb3\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "method": "Stencil\u6846\u67b6\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8054\u5408\u4f7f\u7528\u4e24\u4e2a\u6269\u6563\u6a21\u578b\uff1a\u9ad8\u6548\u5fae\u8c03\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u578b\u6765\u5904\u7406\u4e3b\u9898\u56fe\u50cf\uff0c\u540c\u65f6\u4f7f\u7528\u4e00\u4e2a\u5927\u578b\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u4e0a\u4e0b\u6587\u6307\u5bfc\uff0c\u4ee5\u6700\u5c0f\u7684\u5f00\u9500\u6ce8\u5165\u4e30\u5bcc\u7684\u5148\u9a8c\u77e5\u8bc6\u6765\u589e\u5f3a\u751f\u6210\u6548\u679c\u3002", "result": "Stencil\u80fd\u591f\u5728\u4e0d\u5230\u4e00\u5206\u949f\u5185\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u65b0\u9896\u7684\u4e3b\u9898\u6e32\u67d3\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u4e3b\u9898\u9a71\u52a8\u751f\u6210\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "Stencil\u901a\u8fc7\u53cc\u6a21\u578b\u534f\u540c\u5de5\u4f5c\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4e3b\u9898\u4e00\u81f4\u6027\u751f\u6210\u4e2d\u7684\u8d28\u91cf-\u6548\u7387\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17136", "abs": "https://arxiv.org/abs/2509.17136", "authors": ["Yuhao Tian", "Zheming Yang"], "title": "SAEC: Scene-Aware Enhanced Edge-Cloud Collaborative Industrial Vision Inspection with Multimodal LLM", "comment": "5 pages, 5 figures", "summary": "Industrial vision inspection requires high accuracy under stringent resource\nconstraints, yet existing approaches face a fundamental trade-off. Multimodal\nLLMs (MLLMs) deliver strong reasoning capabilities but incur prohibitive\ncomputational costs, while lightweight edge models often fail on complex cases.\nIn this paper, we present SAEC, a scene-aware enhanced edge-cloud collaborative\nindustrial vision inspection framework with MLLM. The framework is composed of\nthree synergistic components: (1) Efficient MLLM Fine-Tuning for Complex Defect\nInspection, (2) Lightweight Multiscale Scene-Complexity Estimation, and (3)\nAdaptive Edge-Cloud Scheduler. Together, these modules enable robust defect\ndetection by tailoring multimodal reasoning to scene complexity and dynamically\nbalancing computation between edge and cloud resources. Experimental results on\nMVTec AD and KSDD2 datasets demonstrate that SAEC attains 85.11% and 82.72%\naccuracy, surpassing Qwen by 22.1% and 20.8%, and LLaVA by 33.3% and 31.6%. It\nalso reduces runtime by up to 22.4% and cuts energy per correct decision by\n40%-74%. The code is available at https://github.com/YuHao-Tian/SAEC.", "AI": {"tldr": "SAEC\u662f\u4e00\u4e2a\u9762\u5411\u5de5\u4e1a\u89c6\u89c9\u68c0\u6d4b\u7684\u573a\u666f\u611f\u77e5\u8fb9\u7f18-\u4e91\u534f\u540c\u6846\u67b6\uff0c\u901a\u8fc7MLLM\u5b9e\u73b0\u9ad8\u6548\u7f3a\u9677\u68c0\u6d4b\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u80fd\u8017", "motivation": "\u89e3\u51b3\u5de5\u4e1a\u89c6\u89c9\u68c0\u6d4b\u4e2dMLLM\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u4e0e\u8f7b\u91cf\u8fb9\u7f18\u6a21\u578b\u590d\u6742\u573a\u666f\u68c0\u6d4b\u80fd\u529b\u4e0d\u8db3\u7684\u77db\u76fe\uff0c\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u5e73\u8861", "method": "\u5305\u542b\u4e09\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1a\u9ad8\u6548MLLM\u5fae\u8c03\u7528\u4e8e\u590d\u6742\u7f3a\u9677\u68c0\u6d4b\u3001\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u573a\u666f\u590d\u6742\u5ea6\u4f30\u8ba1\u3001\u81ea\u9002\u5e94\u8fb9\u7f18-\u4e91\u8c03\u5ea6\u5668\uff0c\u6839\u636e\u573a\u666f\u590d\u6742\u5ea6\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90", "result": "\u5728MVTec AD\u548cKSDD2\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523085.11%\u548c82.72%\u51c6\u786e\u7387\uff0c\u6bd4Qwen\u63d0\u534722.1%\u548c20.8%\uff0c\u6bd4LLaVA\u63d0\u534733.3%\u548c31.6%\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c1122.4%\uff0c\u6bcf\u6b63\u786e\u51b3\u7b56\u80fd\u8017\u964d\u4f4e40%-74%", "conclusion": "SAEC\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5de5\u4e1a\u89c6\u89c9\u68c0\u6d4b\u4e2d\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9ad8\u8d28\u91cf\u7f3a\u9677\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2509.17172", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17172", "abs": "https://arxiv.org/abs/2509.17172", "authors": ["Djamel Eddine Boukhari"], "title": "SynergyNet: Fusing Generative Priors and State-Space Models for Facial Beauty Prediction", "comment": null, "summary": "The automated prediction of facial beauty is a benchmark task in affective\ncomputing that requires a sophisticated understanding of both local aesthetic\ndetails (e.g., skin texture) and global facial harmony (e.g., symmetry,\nproportions). Existing models, based on either Convolutional Neural Networks\n(CNNs) or Vision Transformers (ViTs), exhibit inherent architectural biases\nthat limit their performance; CNNs excel at local feature extraction but\nstruggle with long-range dependencies, while ViTs model global relationships at\na significant computational cost. This paper introduces the\n\\textbf{Mamba-Diffusion Network (MD-Net)}, a novel dual-stream architecture\nthat resolves this trade-off by delegating specialized roles to\nstate-of-the-art models. The first stream leverages a frozen U-Net encoder from\na pre-trained latent diffusion model, providing a powerful generative prior for\nfine-grained aesthetic qualities. The second stream employs a Vision Mamba\n(Vim), a modern state-space model, to efficiently capture global facial\nstructure with linear-time complexity. By synergistically integrating these\ncomplementary representations through a cross-attention mechanism, MD-Net\ncreates a holistic and nuanced feature space for prediction. Evaluated on the\nSCUT-FBP5500 benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson\nCorrelation of \\textbf{0.9235} and demonstrating the significant potential of\nhybrid architectures that fuse generative and sequential modeling paradigms for\ncomplex visual assessment tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMD-Net\uff0c\u4e00\u79cd\u53cc\u6d41\u67b6\u6784\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684U-Net\u7f16\u7801\u5668\u548cVision Mamba\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u5c40\u90e8\u7f8e\u5b66\u7ec6\u8282\u548c\u5168\u5c40\u9762\u90e8\u7ed3\u6784\uff0c\u5728\u4eba\u8138\u7f8e\u989c\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCNN\u548cViT\u7684\u6a21\u578b\u5b58\u5728\u67b6\u6784\u504f\u5dee\uff1aCNN\u64c5\u957f\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u4f46\u96be\u4ee5\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\uff0cViT\u80fd\u5efa\u6a21\u5168\u5c40\u5173\u7cfb\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5904\u7406\u5c40\u90e8\u7f8e\u5b66\u7ec6\u8282\u548c\u5168\u5c40\u9762\u90e8\u548c\u8c10\u6027\u7684\u65b9\u6cd5\u3002", "method": "MD-Net\u91c7\u7528\u53cc\u6d41\u67b6\u6784\uff1a\u7b2c\u4e00\u6d41\u4f7f\u7528\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u51bb\u7ed3U-Net\u7f16\u7801\u5668\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7f8e\u5b66\u7279\u5f81\uff1b\u7b2c\u4e8c\u6d41\u4f7f\u7528Vision Mamba\u9ad8\u6548\u6355\u83b7\u5168\u5c40\u9762\u90e8\u7ed3\u6784\u3002\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u4e24\u79cd\u4e92\u8865\u8868\u793a\u3002", "result": "\u5728SCUT-FBP5500\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMD-Net\u8fbe\u5230Pearson\u76f8\u5173\u7cfb\u65700.9235\uff0c\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "MD-Net\u8bc1\u660e\u4e86\u878d\u5408\u751f\u6210\u5f0f\u548c\u5e8f\u5217\u5efa\u6a21\u8303\u5f0f\u7684\u6df7\u5408\u67b6\u6784\u5728\u590d\u6742\u89c6\u89c9\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u63d0\u53d6\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2509.17187", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17187", "abs": "https://arxiv.org/abs/2509.17187", "authors": ["Lalith Bharadwaj Baru", "Kamalaker Dadi", "Tapabrata Chakraborti", "Raju S. Bapi"], "title": "Ambiguous Medical Image Segmentation Using Diffusion Schr\u00f6dinger Bridge", "comment": "MICCAI 2025 (11 pages, 2 figures, 1 table, and 26 references)", "summary": "Accurate segmentation of medical images is challenging due to unclear lesion\nboundaries and mask variability. We introduce \\emph{Segmentation Sch\\\"{o}dinger\nBridge (SSB)}, the first application of Sch\\\"{o}dinger Bridge for ambiguous\nmedical image segmentation, modelling joint image-mask dynamics to enhance\nperformance. SSB preserves structural integrity, delineates unclear boundaries\nwithout additional guidance, and maintains diversity using a novel loss\nfunction. We further propose the \\emph{Diversity Divergence Index} ($D_{DDI}$)\nto quantify inter-rater variability, capturing both diversity and consensus.\nSSB achieves state-of-the-art performance on LIDC-IDRI, COCA, and RACER\n(in-house) datasets.", "AI": {"tldr": "SSB\u662f\u9996\u4e2a\u5c06Schr\u00f6dinger Bridge\u5e94\u7528\u4e8e\u6a21\u7cca\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u56fe\u50cf-\u63a9\u7801\u8054\u5408\u52a8\u6001\u6765\u63d0\u5347\u5206\u5272\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6548\u679c", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9762\u4e34\u75c5\u53d8\u8fb9\u754c\u6a21\u7cca\u548c\u63a9\u7801\u53d8\u5f02\u6027\u5927\u7684\u6311\u6218\uff0c\u9700\u8981\u80fd\u591f\u5904\u7406\u6a21\u7cca\u8fb9\u754c\u5e76\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\u7684\u65b9\u6cd5", "method": "\u63d0\u51faSegmentation Schr\u00f6dinger Bridge (SSB)\u65b9\u6cd5\uff0c\u5efa\u6a21\u56fe\u50cf-\u63a9\u7801\u8054\u5408\u52a8\u6001\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\u4fdd\u6301\u591a\u6837\u6027\uff0c\u5e76\u63d0\u51faDiversity Divergence Index (DDI)\u91cf\u5316\u8bc4\u4f30\u8005\u95f4\u53d8\u5f02\u6027", "result": "SSB\u5728LIDC-IDRI\u3001COCA\u548cRACER\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u80fd\u591f\u51c6\u786e\u5206\u5272\u6a21\u7cca\u8fb9\u754c\u5e76\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027", "conclusion": "SSB\u4e3a\u6a21\u7cca\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u8fb9\u754c\u4e0d\u6e05\u6670\u7684\u60c5\u51b5\u5e76\u91cf\u5316\u5206\u5272\u7684\u591a\u6837\u6027"}}
{"id": "2509.17190", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17190", "abs": "https://arxiv.org/abs/2509.17190", "authors": ["Kabir Hamzah Muhammad", "Marawan Elbatel", "Yi Qin", "Xiaomeng Li"], "title": "Echo-Path: Pathology-Conditioned Echo Video Generation", "comment": "10 pages, 3 figures, MICCAI-AMAI2025 Workshop", "summary": "Cardiovascular diseases (CVDs) remain the leading cause of mortality\nglobally, and echocardiography is critical for diagnosis of both common and\ncongenital cardiac conditions. However, echocardiographic data for certain\npathologies are scarce, hindering the development of robust automated diagnosis\nmodels. In this work, we propose Echo-Path, a novel generative framework to\nproduce echocardiogram videos conditioned on specific cardiac pathologies.\nEcho-Path can synthesize realistic ultrasound video sequences that exhibit\ntargeted abnormalities, focusing here on atrial septal defect (ASD) and\npulmonary arterial hypertension (PAH). Our approach introduces a\npathology-conditioning mechanism into a state-of-the-art echo video generator,\nallowing the model to learn and control disease-specific structural and motion\npatterns in the heart. Quantitative evaluation demonstrates that the synthetic\nvideos achieve low distribution distances, indicating high visual fidelity.\nClinically, the generated echoes exhibit plausible pathology markers.\nFurthermore, classifiers trained on our synthetic data generalize well to real\ndata and, when used to augment real training sets, it improves downstream\ndiagnosis of ASD and PAH by 7\\% and 8\\% respectively. Code, weights and dataset\nare available here https://github.com/Marshall-mk/EchoPathv1", "AI": {"tldr": "\u63d0\u51faEcho-Path\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u75c5\u7406\u6761\u4ef6\u673a\u5236\u5408\u6210\u7279\u5b9a\u5fc3\u810f\u75c5\u53d8\u7684\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\uff0c\u7528\u4e8e\u89e3\u51b3\u7f55\u89c1\u75c5\u7406\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u81ea\u52a8\u8bca\u65ad\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\uff0c\u8d85\u58f0\u5fc3\u52a8\u56fe\u662f\u91cd\u8981\u8bca\u65ad\u5de5\u5177\uff0c\u4f46\u67d0\u4e9b\u75c5\u7406\u6570\u636e\u7a00\u7f3a\u9650\u5236\u4e86\u81ea\u52a8\u8bca\u65ad\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u5728\u5148\u8fdb\u56de\u58f0\u89c6\u9891\u751f\u6210\u5668\u4e2d\u5f15\u5165\u75c5\u7406\u6761\u4ef6\u673a\u5236\uff0c\u5b66\u4e60\u5e76\u63a7\u5236\u75be\u75c5\u7279\u5f02\u6027\u7ed3\u6784\u548c\u8fd0\u52a8\u6a21\u5f0f\uff0c\u751f\u6210\u9488\u5bf9\u623f\u95f4\u9694\u7f3a\u635f\u548c\u80ba\u52a8\u8109\u9ad8\u538b\u7684\u8d85\u58f0\u89c6\u9891\u3002", "result": "\u5408\u6210\u89c6\u9891\u89c6\u89c9\u4fdd\u771f\u5ea6\u9ad8\uff0c\u5206\u5e03\u8ddd\u79bb\u4f4e\uff1b\u751f\u6210\u7684\u56de\u58f0\u663e\u793a\u5408\u7406\u7684\u75c5\u7406\u6807\u5fd7\uff1b\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u6cdb\u5316\u826f\u597d\uff0c\u5c06ASD\u548cPAH\u8bca\u65ad\u51c6\u786e\u7387\u5206\u522b\u63d0\u53477%\u548c8%\u3002", "conclusion": "Echo-Path\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u75c5\u7406\u7279\u5f02\u6027\u8d85\u58f0\u89c6\u9891\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u8bca\u65ad\u6027\u80fd\uff0c\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.17191", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17191", "abs": "https://arxiv.org/abs/2509.17191", "authors": ["Jinchao Ge", "Tengfei Cheng", "Biao Wu", "Zeyu Zhang", "Shiya Huang", "Judith Bishop", "Gillian Shepherd", "Meng Fang", "Ling Chen", "Yang Zhao"], "title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery", "comment": null, "summary": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general\nmodels lack domain expertise, and SFT often overfits superficial patterns,\nyielding brittle reasoning for authentication and historical attribution. This\nraises the question of how to equip MLLMs with robust, expert-level reasoning\nfor ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns\nevaluation into supervision: we construct a taxonomy of question types, probe\nthe SFT model to localize type-specific performance gaps, and optimize with\ntype-conditioned, compositionality-oriented rewards targeting those gaps. We\nalso release VaseVQA, a comprehensive benchmark of 31,773 images designed to\nprobe deep understanding. Experiments show state-of-the-art results on style\nclassification and historical attribution with marked gains in compositional\nrobustness over SFT-only baselines, validating diagnosis-guided,\ntaxonomy-conditioned reward engineering and providing a reusable resource for\nfuture research. Code and dataset will be available at\nhttps://github.com/AIGeeksGroup/VaseVQA.", "AI": {"tldr": "VaseVL\u662f\u4e00\u4e2a\u9488\u5bf9\u53e4\u5e0c\u814a\u9676\u5668\u5206\u6790\u7684MLLM\u7cfb\u7edf\uff0c\u901a\u8fc7SFT-then-RL\u65b9\u6cd5\u5c06\u8bc4\u4f30\u8f6c\u5316\u4e3a\u76d1\u7763\uff0c\u7ed3\u5408\u95ee\u9898\u7c7b\u578b\u5206\u7c7b\u548c\u9488\u5bf9\u6027\u5956\u52b1\u4f18\u5316\uff0c\u5728\u98ce\u683c\u5206\u7c7b\u548c\u5386\u53f2\u5f52\u5c5e\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MLLM\u5728\u6587\u5316\u9057\u4ea7\u5206\u6790\u9886\u57df\u5b58\u5728\u5c40\u9650\u6027\uff1a\u901a\u7528\u6a21\u578b\u7f3a\u4e4f\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800cSFT\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u8868\u9762\u6a21\u5f0f\uff0c\u5bfc\u81f4\u8ba4\u8bc1\u548c\u5386\u53f2\u5f52\u5c5e\u63a8\u7406\u8106\u5f31\u3002\u9700\u8981\u4e3a\u53e4\u5e0c\u814a\u9676\u5668\u5206\u6790\u5f00\u53d1\u5177\u6709\u4e13\u5bb6\u7ea7\u63a8\u7406\u80fd\u529b\u7684\u7a33\u5065\u6a21\u578b\u3002", "method": "\u6784\u5efa\u95ee\u9898\u7c7b\u578b\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u63a2\u6d4bSFT\u6a21\u578b\u5b9a\u4f4d\u7c7b\u578b\u7279\u5b9a\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4f7f\u7528\u7c7b\u578b\u6761\u4ef6\u5316\u3001\u9762\u5411\u7ec4\u5408\u6027\u7684\u5956\u52b1\u9488\u5bf9\u8fd9\u4e9b\u5dee\u8ddd\u8fdb\u884c\u4f18\u5316\u3002\u540c\u65f6\u53d1\u5e03\u4e86\u5305\u542b31,773\u5f20\u56fe\u50cf\u7684VaseVQA\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u5728\u98ce\u683c\u5206\u7c7b\u548c\u5386\u53f2\u5f52\u5c5e\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528SFT\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u7ec4\u5408\u9c81\u68d2\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u8bca\u65ad\u5f15\u5bfc\u3001\u5206\u7c7b\u6cd5\u6761\u4ef6\u5316\u7684\u5956\u52b1\u5de5\u7a0b\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u7528\u7684\u8d44\u6e90\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5728GitHub\u4e0a\u516c\u5f00\u3002"}}
{"id": "2509.17206", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17206", "abs": "https://arxiv.org/abs/2509.17206", "authors": ["Gunner Stone", "Sushmita Sarker", "Alireza Tavakkoli"], "title": "Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation", "comment": null, "summary": "Generating realistic 3D point clouds is a fundamental problem in computer\nvision with applications in remote sensing, robotics, and digital object\nmodeling. Existing generative approaches primarily capture geometry, and when\nsemantics are considered, they are typically imposed post hoc through external\nsegmentation or clustering rather than integrated into the generative process\nitself. We propose a diffusion-based framework that embeds per-point semantic\nconditioning directly within generation. Each point is associated with a\nconditional variable corresponding to its semantic label, which guides the\ndiffusion dynamics and enables the joint synthesis of geometry and semantics.\nThis design produces point clouds that are both structurally coherent and\nsegmentation-aware, with object parts explicitly represented during synthesis.\nThrough a comparative analysis of guided and unguided diffusion processes, we\ndemonstrate the significant impact of conditional variables on diffusion\ndynamics and generation quality. Extensive experiments validate the efficacy of\nour approach, producing detailed and accurate 3D point clouds tailored to\nspecific parts and features.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u5c06\u9010\u70b9\u8bed\u4e49\u6761\u4ef6\u5d4c\u5165\u5230\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u8054\u5408\u5408\u6210\u51e0\u4f55\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u751f\u6210\u7ed3\u6784\u8fde\u8d2f\u4e14\u5206\u5272\u611f\u77e5\u76843D\u70b9\u4e91\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u6355\u83b7\u51e0\u4f55\u4fe1\u606f\uff0c\u8bed\u4e49\u4fe1\u606f\u901a\u5e38\u901a\u8fc7\u5916\u90e8\u5206\u5272\u6216\u805a\u7c7b\u540e\u5904\u7406\u65bd\u52a0\uff0c\u800c\u4e0d\u662f\u96c6\u6210\u5230\u751f\u6210\u8fc7\u7a0b\u672c\u8eab\u3002", "method": "\u6269\u6563\u6846\u67b6\u5c06\u6bcf\u4e2a\u70b9\u4e0e\u5176\u8bed\u4e49\u6807\u7b7e\u5bf9\u5e94\u7684\u6761\u4ef6\u53d8\u91cf\u5173\u8054\uff0c\u6307\u5bfc\u6269\u6563\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u51e0\u4f55\u548c\u8bed\u4e49\u7684\u8054\u5408\u5408\u6210\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u5f15\u5bfc\u548c\u975e\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u8bc1\u660e\u4e86\u6761\u4ef6\u53d8\u91cf\u5bf9\u6269\u6563\u52a8\u529b\u5b66\u548c\u751f\u6210\u8d28\u91cf\u7684\u663e\u8457\u5f71\u54cd\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9488\u5bf9\u7279\u5b9a\u90e8\u4ef6\u548c\u7279\u5f81\u7684\u8be6\u7ec6\u51c6\u786e3D\u70b9\u4e91\uff0c\u5728\u51e0\u4f55\u548c\u8bed\u4e49\u8054\u5408\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.17207", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17207", "abs": "https://arxiv.org/abs/2509.17207", "authors": ["Gunner Stone", "Youngsook Choi", "Alireza Tavakkoli", "Ankita Shukla"], "title": "Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds", "comment": null, "summary": "Pre-training strategies play a critical role in advancing the performance of\ntransformer-based models for 3D point cloud tasks. In this paper, we introduce\nPoint-RTD (Replaced Token Denoising), a novel pretraining strategy designed to\nimprove token robustness through a corruption-reconstruction framework. Unlike\ntraditional mask-based reconstruction tasks that hide data segments for later\nprediction, Point-RTD corrupts point cloud tokens and leverages a\ndiscriminator-generator architecture for denoising. This shift enables more\neffective learning of structural priors and significantly enhances model\nperformance and efficiency. On the ShapeNet dataset, Point-RTD reduces\nreconstruction error by over 93% compared to PointMAE, and achieves more than\n14x lower Chamfer Distance on the test set. Our method also converges faster\nand yields higher classification accuracy on ShapeNet, ModelNet10, and\nModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework\nin every case.", "AI": {"tldr": "Point-RTD\u662f\u4e00\u79cd\u65b0\u9896\u76843D\u70b9\u4e91\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u66ff\u6362\u4ee4\u724c\u53bb\u566a\u7684\u7834\u574f-\u91cd\u5efa\u6846\u67b6\u63d0\u5347token\u9c81\u68d2\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u63a9\u7801\u91cd\u5efa\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u63a9\u7801\u7684\u91cd\u5efa\u4efb\u52a1\u901a\u8fc7\u9690\u85cf\u6570\u636e\u7247\u6bb5\u8fdb\u884c\u9884\u6d4b\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u57283D\u70b9\u4e91\u4efb\u52a1\u4e2d\u53ef\u80fd\u4e0d\u591f\u6709\u6548\u3002Point-RTD\u65e8\u5728\u901a\u8fc7\u7834\u574f-\u91cd\u5efa\u6846\u67b6\u66f4\u6709\u6548\u5730\u5b66\u4e60\u7ed3\u6784\u5148\u9a8c\u77e5\u8bc6\u3002", "method": "Point-RTD\u91c7\u7528\u5224\u522b\u5668-\u751f\u6210\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u7834\u574f\u70b9\u4e91token\u7136\u540e\u8fdb\u884c\u53bb\u566a\u91cd\u5efa\uff0c\u800c\u975e\u4f20\u7edf\u7684\u63a9\u7801\u9884\u6d4b\u65b9\u6cd5\u3002\u8fd9\u79cd\u7834\u574f-\u91cd\u5efa\u6846\u67b6\u80fd\u591f\u66f4\u597d\u5730\u63d0\u5347token\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728ShapeNet\u6570\u636e\u96c6\u4e0a\uff0cPoint-RTD\u76f8\u6bd4PointMAE\u91cd\u5efa\u8bef\u5dee\u964d\u4f4e93%\u4ee5\u4e0a\uff0c\u6d4b\u8bd5\u96c6\u4e0a\u7684Chamfer\u8ddd\u79bb\u964d\u4f4e14\u500d\u4ee5\u4e0a\u3002\u65b9\u6cd5\u6536\u655b\u66f4\u5feb\uff0c\u5728ShapeNet\u3001ModelNet10\u548cModelNet40\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u7c7b\u51c6\u786e\u7387\u66f4\u9ad8\u3002", "conclusion": "Point-RTD\u57283D\u70b9\u4e91\u9884\u8bad\u7ec3\u4efb\u52a1\u4e2d\u660e\u663e\u4f18\u4e8e\u57fa\u7ebfPoint-MAE\u6846\u67b6\uff0c\u5728\u91cd\u5efa\u7cbe\u5ea6\u3001\u6536\u655b\u901f\u5ea6\u548c\u5206\u7c7b\u6027\u80fd\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2509.17220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17220", "abs": "https://arxiv.org/abs/2509.17220", "authors": ["Mingchen Xu", "Yukun Lai", "Ze Ji", "Jing Wu"], "title": "MirrorSAM2: Segment Mirror in Videos with Depth Perception", "comment": "8 pages", "summary": "This paper presents MirrorSAM2, the first framework that adapts Segment\nAnything Model 2 (SAM2) to the task of RGB-D video mirror segmentation.\nMirrorSAM2 addresses key challenges in mirror detection, such as reflection\nambiguity and texture confusion, by introducing four tailored modules: a Depth\nWarping Module for RGB and depth alignment, a Depth-guided Multi-Scale Point\nPrompt Generator for automatic prompt generation, a Frequency Detail Attention\nFusion Module to enhance structural boundaries, and a Mirror Mask Decoder with\na learnable mirror token for refined segmentation. By fully leveraging the\ncomplementarity between RGB and depth, MirrorSAM2 extends SAM2's capabilities\nto the prompt-free setting. To our knowledge, this is the first work to enable\nSAM2 for automatic video mirror segmentation. Experiments on the VMD and DVMD\nbenchmark demonstrate that MirrorSAM2 achieves SOTA performance, even under\nchallenging conditions such as small mirrors, weak boundaries, and strong\nreflections.", "AI": {"tldr": "MirrorSAM2\u662f\u9996\u4e2a\u5c06Segment Anything Model 2\uff08SAM2\uff09\u9002\u914d\u5230RGB-D\u89c6\u9891\u955c\u9762\u5206\u5272\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u5b9a\u5236\u6a21\u5757\u89e3\u51b3\u955c\u9762\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728VMD\u548cDVMD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u955c\u9762\u68c0\u6d4b\u4e2d\u7684\u53cd\u5c04\u6a21\u7cca\u548c\u7eb9\u7406\u6df7\u6dc6\u7b49\u5173\u952e\u6311\u6218\uff0c\u5c06SAM2\u7684\u80fd\u529b\u6269\u5c55\u5230\u65e0\u9700\u63d0\u793a\u7684\u81ea\u52a8\u89c6\u9891\u955c\u9762\u5206\u5272\u573a\u666f\u3002", "method": "\u63d0\u51fa\u56db\u4e2a\u5b9a\u5236\u6a21\u5757\uff1a\u6df1\u5ea6\u626d\u66f2\u6a21\u5757\uff08RGB\u548c\u6df1\u5ea6\u5bf9\u9f50\uff09\u3001\u6df1\u5ea6\u5f15\u5bfc\u591a\u5c3a\u5ea6\u70b9\u63d0\u793a\u751f\u6210\u5668\uff08\u81ea\u52a8\u63d0\u793a\u751f\u6210\uff09\u3001\u9891\u7387\u7ec6\u8282\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\uff08\u589e\u5f3a\u7ed3\u6784\u8fb9\u754c\uff09\u3001\u955c\u9762\u63a9\u7801\u89e3\u7801\u5668\uff08\u53ef\u5b66\u4e60\u955c\u9762\u4ee4\u724c\u8fdb\u884c\u7cbe\u7ec6\u5206\u5272\uff09\u3002", "result": "\u5728VMD\u548cDVMD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u5c0f\u578b\u955c\u9762\u3001\u5f31\u8fb9\u754c\u548c\u5f3a\u53cd\u5c04\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MirrorSAM2\u6210\u529f\u5c06SAM2\u6269\u5c55\u5230\u65e0\u63d0\u793a\u8bbe\u7f6e\u7684\u81ea\u52a8\u89c6\u9891\u955c\u9762\u5206\u5272\u4efb\u52a1\uff0c\u662f\u9996\u4e2a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u5de5\u4f5c\uff0c\u8bc1\u660e\u4e86RGB\u548c\u6df1\u5ea6\u4fe1\u606f\u4e92\u8865\u6027\u7684\u6709\u6548\u5229\u7528\u3002"}}
{"id": "2509.17232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17232", "abs": "https://arxiv.org/abs/2509.17232", "authors": ["Bo Liu", "Runlong Li", "Li Zhou", "Yan Zhou"], "title": "DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction", "comment": "15 pages", "summary": "This paper proposes a Diffusion Model-Optimized Neural Radiance Field\n(DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency\nin 3D scene reconstruction. By combining diffusion models with Transformers,\nDT-NeRF effectively restores details under sparse viewpoints and maintains high\naccuracy in complex geometric scenes. Experimental results demonstrate that\nDT-NeRF significantly outperforms traditional NeRF and other state-of-the-art\nmethods on the Matterport3D and ShapeNet datasets, particularly in metrics such\nas PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further\nconfirm the critical role of the diffusion and Transformer modules in the\nmodel's performance, with the removal of either module leading to a decline in\nperformance. The design of DT-NeRF showcases the synergistic effect between\nmodules, providing an efficient and accurate solution for 3D scene\nreconstruction. Future research may focus on further optimizing the model,\nexploring more advanced generative models and network architectures to enhance\nits performance in large-scale dynamic scenes.", "AI": {"tldr": "DT-NeRF\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548cTransformer\uff0c\u63d0\u5347\u4e863D\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u7ec6\u8282\u6062\u590d\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u5728\u7a00\u758f\u89c6\u89d2\u548c\u590d\u6742\u51e0\u4f55\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edfNeRF\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u89d2\u4e0b\u7ec6\u8282\u6062\u590d\u4e0d\u8db3\uff0c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u8f83\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u6062\u590d\u7ec6\u8282\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u76843D\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDT-NeRF\u65b9\u6cd5\uff0c\u5c06\u6269\u6563\u6a21\u578b\u4e0eTransformer\u7ed3\u5408\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u4f18\u5316\u795e\u7ecf\u8f90\u5c04\u573a\uff0c\u589e\u5f3a\u7ec6\u8282\u6062\u590d\u80fd\u529b\u5e76\u4fdd\u6301\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "result": "\u5728Matterport3D\u548cShapeNet\u6570\u636e\u96c6\u4e0a\uff0cDT-NeRF\u5728PSNR\u3001SSIM\u3001Chamfer Distance\u548cFidelity\u7b49\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edfNeRF\u548c\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u6269\u6563\u548cTransformer\u6a21\u5757\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "DT-NeRF\u5c55\u793a\u4e86\u6a21\u5757\u95f4\u7684\u534f\u540c\u6548\u5e94\uff0c\u4e3a3D\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\uff0c\u63a2\u7d22\u66f4\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u548c\u7f51\u7edc\u67b6\u6784\u4ee5\u63d0\u5347\u5728\u5927\u89c4\u6a21\u52a8\u6001\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.17246", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17246", "abs": "https://arxiv.org/abs/2509.17246", "authors": ["Ranran Huang", "Krystian Mikolajczyk"], "title": "SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views", "comment": null, "summary": "We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian\nsplatting from sparse multi-view images, requiring no ground-truth poses during\ntraining and inference. It employs a shared feature extraction backbone,\nenabling simultaneous prediction of 3D Gaussian primitives and camera poses in\na canonical space from unposed inputs. A masked attention mechanism is\nintroduced to efficiently estimate target poses during training, while a\nreprojection loss enforces pixel-aligned Gaussian primitives, providing\nstronger geometric constraints. We further demonstrate the compatibility of our\ntraining framework with different reconstruction architectures, resulting in\ntwo model variants. Remarkably, despite the absence of pose supervision, our\nmethod achieves state-of-the-art performance in both in-domain and\nout-of-domain novel view synthesis, even under extreme viewpoint changes and\nlimited image overlap, and surpasses recent methods that rely on geometric\nsupervision for relative pose estimation. By eliminating dependence on\nground-truth poses, our method offers the scalability to leverage larger and\nmore diverse datasets. Code and pretrained models will be available on our\nproject page: https://ranrhuang.github.io/spfsplatv2/.", "AI": {"tldr": "SPFSplatV2\u662f\u4e00\u4e2a\u65e0\u9700\u5730\u9762\u771f\u5b9e\u76f8\u673a\u4f4d\u59ff\u7684\u9ad8\u65483D\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u53ef\u4ece\u7a00\u758f\u591a\u89c6\u89d2\u56fe\u50cf\u5b9e\u73b0\u9ad8\u8d28\u91cf\u65b0\u89c6\u89d2\u5408\u6210", "motivation": "\u6d88\u9664\u5bf9\u5730\u9762\u771f\u5b9e\u76f8\u673a\u4f4d\u59ff\u7684\u4f9d\u8d56\uff0c\u4f7f3D\u91cd\u5efa\u65b9\u6cd5\u80fd\u591f\u5229\u7528\u66f4\u5927\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u6781\u7aef\u89c6\u89d2\u53d8\u5316\u548c\u6709\u9650\u56fe\u50cf\u91cd\u53e0\u7684\u6311\u6218", "method": "\u4f7f\u7528\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u9aa8\u5e72\u7f51\u7edc\uff0c\u5728\u89c4\u8303\u7a7a\u95f4\u4e2d\u540c\u65f6\u9884\u6d4b3D\u9ad8\u65af\u57fa\u5143\u548c\u76f8\u673a\u4f4d\u59ff\uff1b\u5f15\u5165\u63a9\u7801\u6ce8\u610f\u529b\u673a\u5236\u4f30\u8ba1\u76ee\u6807\u4f4d\u59ff\uff0c\u4f7f\u7528\u91cd\u6295\u5f71\u635f\u5931\u5f3a\u5236\u50cf\u7d20\u5bf9\u9f50\u7684\u9ad8\u65af\u57fa\u5143", "result": "\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u7684\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u6781\u7aef\u89c6\u89d2\u53d8\u5316\u548c\u6709\u9650\u56fe\u50cf\u91cd\u53e0\u60c5\u51b5\u4e0b\u4e5f\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u4f9d\u8d56\u51e0\u4f55\u76d1\u7763\u7684\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6d88\u9664\u5bf9\u5730\u9762\u771f\u5b9e\u4f4d\u59ff\u7684\u4f9d\u8d56\uff0c\u63d0\u4f9b\u4e86\u6269\u5c55\u5230\u66f4\u5927\u66f4\u591a\u6837\u5316\u6570\u636e\u96c6\u7684\u6f5c\u529b\uff0c\u5c55\u793a\u4e86\u5728\u65e0\u4f4d\u59ff\u76d1\u7763\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u91cd\u5efa\u7684\u53ef\u80fd\u6027"}}
{"id": "2509.17262", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.17262", "abs": "https://arxiv.org/abs/2509.17262", "authors": ["Xiumei Li", "Marc Windsheimer", "Misha Sadeghi", "Bj\u00f6rn Eskofier", "Andr\u00e9 Kaup"], "title": "Optimized Learned Image Compression for Facial Expression Recognition", "comment": "Accepted at ICIP 2025", "summary": "Efficient data compression is crucial for the storage and transmission of\nvisual data. However, in facial expression recognition (FER) tasks, lossy\ncompression often leads to feature degradation and reduced accuracy. To address\nthese challenges, this study proposes an end-to-end model designed to preserve\ncritical features and enhance both compression and recognition performance. A\ncustom loss function is introduced to optimize the model, tailored to balance\ncompression and recognition performance effectively. This study also examines\nthe influence of varying loss term weights on this balance. Experimental\nresults indicate that fine-tuning the compression model alone improves\nclassification accuracy by 0.71% and compression efficiency by 49.32%, while\njoint optimization achieves significant gains of 4.04% in accuracy and 89.12%\nin efficiency. Moreover, the findings demonstrate that the jointly optimized\nclassification model maintains high accuracy on both compressed and\nuncompressed data, while the compression model reliably preserves image\ndetails, even at high compression rates.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u901a\u8fc7\u5b9a\u5236\u635f\u5931\u51fd\u6570\u5e73\u8861\u538b\u7f29\u548c\u8bc6\u522b\u6027\u80fd\uff0c\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4efb\u52a1\u4e2d\u540c\u65f6\u63d0\u5347\u538b\u7f29\u6548\u7387\u548c\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u6709\u635f\u538b\u7f29\u5bfc\u81f4\u7279\u5f81\u9000\u5316\u548c\u51c6\u786e\u7387\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u9700\u8981\u5728\u538b\u7f29\u6548\u7387\u548c\u8bc6\u522b\u6027\u80fd\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u6a21\u578b\u8bbe\u8ba1\uff0c\u5f15\u5165\u5b9a\u5236\u635f\u5931\u51fd\u6570\u6765\u4f18\u5316\u6a21\u578b\uff0c\u5e73\u8861\u538b\u7f29\u548c\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u635f\u5931\u9879\u6743\u91cd\u5bf9\u5e73\u8861\u7684\u5f71\u54cd\u3002", "result": "\u4ec5\u5fae\u8c03\u538b\u7f29\u6a21\u578b\u4f7f\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u53470.71%\uff0c\u538b\u7f29\u6548\u7387\u63d0\u534749.32%\uff1b\u8054\u5408\u4f18\u5316\u540e\u51c6\u786e\u7387\u63d0\u53474.04%\uff0c\u6548\u7387\u63d0\u534789.12%\u3002\u5206\u7c7b\u6a21\u578b\u5728\u538b\u7f29\u548c\u672a\u538b\u7f29\u6570\u636e\u4e0a\u5747\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\uff0c\u538b\u7f29\u6a21\u578b\u5728\u9ad8\u538b\u7f29\u7387\u4e0b\u4ecd\u80fd\u53ef\u9760\u4fdd\u7559\u56fe\u50cf\u7ec6\u8282\u3002", "conclusion": "\u63d0\u51fa\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u538b\u7f29\u4e0e\u8bc6\u522b\u6027\u80fd\u7684\u6743\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6548\u7387\u3002"}}
{"id": "2509.17282", "categories": ["cs.CV", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.17282", "abs": "https://arxiv.org/abs/2509.17282", "authors": ["Xiangmin Xu", "Zhen Meng", "Kan Chen", "Jiaming Yang", "Emma Li", "Philip G. Zhao", "David Flynn"], "title": "Task-Oriented Communications for 3D Scene Representation: Balancing Timeliness and Fidelity", "comment": "Submitted to IEEE Transactions on Mobile Computing", "summary": "Real-time Three-dimensional (3D) scene representation is a foundational\nelement that supports a broad spectrum of cutting-edge applications, including\ndigital manufacturing, Virtual, Augmented, and Mixed Reality (VR/AR/MR), and\nthe emerging metaverse. Despite advancements in real-time communication and\ncomputing, achieving a balance between timeliness and fidelity in 3D scene\nrepresentation remains a challenge. This work investigates a wireless network\nwhere multiple homogeneous mobile robots, equipped with cameras, capture an\nenvironment and transmit images to an edge server over channels for 3D\nrepresentation. We propose a contextual-bandit Proximal Policy Optimization\n(PPO) framework incorporating both Age of Information (AoI) and semantic\ninformation to optimize image selection for representation, balancing data\nfreshness and representation quality. Two policies -- the $\\omega$-threshold\nand $\\omega$-wait policies -- together with two benchmark methods are\nevaluated, timeliness embedding and weighted sum, on standard datasets and\nbaseline 3D scene representation models. Experimental results demonstrate\nimproved representation fidelity while maintaining low latency, offering\ninsight into the model's decision-making process. This work advances real-time\n3D scene representation by optimizing the trade-off between timeliness and\nfidelity in dynamic environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587bandit PPO\u6846\u67b6\u7684\u65e0\u7ebf\u7f51\u7edc\u56fe\u50cf\u9009\u62e9\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u6570\u636e\u65b0\u9c9c\u5ea6\u548c\u8bed\u4e49\u4fe1\u606f\u6765\u63d0\u5347\u5b9e\u65f63D\u573a\u666f\u8868\u793a\u7684\u8d28\u91cf\u548c\u5ef6\u8fdf\u6027\u80fd", "motivation": "\u5b9e\u65f63D\u573a\u666f\u8868\u793a\u5728\u6570\u5b57\u5236\u9020\u3001VR/AR/MR\u548c\u5143\u5b87\u5b99\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5982\u4f55\u5728\u65f6\u95f4\u6027\u548c\u4fdd\u771f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u4ecd\u662f\u4e00\u4e2a\u6311\u6218", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587bandit\u8fd1\u7aef\u7b56\u7565\u4f18\u5316(PPO)\u6846\u67b6\uff0c\u7ed3\u5408\u4fe1\u606f\u5e74\u9f84(AoI)\u548c\u8bed\u4e49\u4fe1\u606f\u6765\u4f18\u5316\u56fe\u50cf\u9009\u62e9\uff0c\u63d0\u51fa\u4e86\u03c9-threshold\u548c\u03c9-wait\u4e24\u79cd\u7b56\u7565", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u548c\u57fa\u7ebf3D\u573a\u666f\u8868\u793a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8868\u793a\u4fdd\u771f\u5ea6", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u4f18\u5316\u52a8\u6001\u73af\u5883\u4e2d\u65f6\u95f4\u6027\u548c\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u63a8\u52a8\u4e86\u5b9e\u65f63D\u573a\u666f\u8868\u793a\u6280\u672f\u7684\u53d1\u5c55"}}
{"id": "2509.17283", "categories": ["cs.CV", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.17283", "abs": "https://arxiv.org/abs/2509.17283", "authors": ["Licheng Zhan", "Bach Le", "Naveed Akhtar", "Tuan Ngo"], "title": "Automated Facility Enumeration for Building Compliance Checking using Door Detection and Large Language Models", "comment": null, "summary": "Building compliance checking (BCC) is a critical process for ensuring that\nconstructed facilities meet regulatory standards. A core component of BCC is\nthe accurate enumeration of facility types and their spatial distribution.\nDespite its importance, this problem has been largely overlooked in the\nliterature, posing a significant challenge for BCC and leaving a critical gap\nin existing workflows. Performing this task manually is time-consuming and\nlabor-intensive. Recent advances in large language models (LLMs) offer new\nopportunities to enhance automation by combining visual recognition with\nreasoning capabilities. In this paper, we introduce a new task for BCC:\nautomated facility enumeration, which involves validating the quantity of each\nfacility type against statutory requirements. To address it, we propose a novel\nmethod that integrates door detection with LLM-based reasoning. We are the\nfirst to apply LLMs to this task and further enhance their performance through\na Chain-of-Thought (CoT) pipeline. Our approach generalizes well across diverse\ndatasets and facility types. Experiments on both real-world and synthetic floor\nplan data demonstrate the effectiveness and robustness of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5efa\u7b51\u5408\u89c4\u68c0\u67e5\u4efb\u52a1\uff1a\u81ea\u52a8\u5316\u8bbe\u65bd\u679a\u4e3e\uff0c\u901a\u8fc7\u7ed3\u5408\u95e8\u68c0\u6d4b\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6765\u9a8c\u8bc1\u8bbe\u65bd\u6570\u91cf\u662f\u5426\u7b26\u5408\u6cd5\u89c4\u8981\u6c42\u3002", "motivation": "\u5efa\u7b51\u5408\u89c4\u68c0\u67e5\u4e2d\u8bbe\u65bd\u7c7b\u578b\u548c\u7a7a\u95f4\u5206\u5e03\u7684\u51c6\u786e\u679a\u4e3e\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\uff0c\u624b\u52a8\u6267\u884c\u8017\u65f6\u8d39\u529b\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u4e3a\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\uff0c\u5c06\u95e8\u68c0\u6d4b\u4e0e\u57fa\u4e8eLLM\u7684\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u9996\u6b21\u5c06LLMs\u5e94\u7528\u4e8e\u6b64\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u601d\u7ef4\u94fe\uff08CoT\uff09\u7ba1\u9053\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u7684\u5e73\u9762\u56fe\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u6570\u636e\u96c6\u548c\u8bbe\u65bd\u7c7b\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5efa\u7b51\u5408\u89c4\u68c0\u67e5\u7684\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u4f5c\u6d41\u4e2d\u7684\u5173\u952e\u7a7a\u767d\u3002"}}
{"id": "2509.17323", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.17323", "abs": "https://arxiv.org/abs/2509.17323", "authors": ["Buyin Deng", "Lingxin Huang", "Kai Luo", "Fei Teng", "Kailun Yang"], "title": "DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking", "comment": "The source code will be made publicly available at\n  https://github.com/warriordby/DepTR-MOT", "summary": "Visual Multi-Object Tracking (MOT) is a crucial component of robotic\nperception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D\ncues, such as bounding boxes and motion modeling, which struggle under\nocclusions and close-proximity interactions. Trackers relying on these 2D cues\nare particularly unreliable in robotic environments, where dense targets and\nfrequent occlusions are common. While depth information has the potential to\nalleviate these issues, most existing MOT datasets lack depth annotations,\nleading to its underexploited role in the domain. To unveil the potential of\ndepth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based\ndetector enhanced with instance-level depth information. Specifically, we\npropose two key innovations: (i) foundation model-based instance-level soft\ndepth label supervision, which refines depth prediction, and (ii) the\ndistillation of dense depth maps to maintain global depth consistency. These\nstrategies enable DepTR-MOT to output instance-level depth during inference,\nwithout requiring foundation models and without additional computational cost.\nBy incorporating depth cues, our method enhances the robustness of the TBD\nparadigm, effectively resolving occlusion and close-proximity challenges.\nExperiments on both the QuadTrack and DanceTrack datasets demonstrate the\neffectiveness of our approach, achieving HOTA scores of 27.59 and 44.47,\nrespectively. In particular, results on QuadTrack, a robotic platform MOT\ndataset, highlight the advantages of our method in handling occlusion and\nclose-proximity challenges in robotic tracking. The source code will be made\npublicly available at https://github.com/warriordby/DepTR-MOT.", "AI": {"tldr": "DepTR-MOT\u662f\u4e00\u4e2a\u57fa\u4e8eDETR\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5b9e\u4f8b\u7ea7\u6df1\u5ea6\u4fe1\u606f\u6765\u89e3\u51b3\u906e\u6321\u548c\u8fd1\u8ddd\u79bb\u4ea4\u4e92\u95ee\u9898\uff0c\u5728\u673a\u5668\u4eba\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d562D\u7ebf\u7d22\uff08\u5982\u8fb9\u754c\u6846\u548c\u8fd0\u52a8\u5efa\u6a21\uff09\uff0c\u5728\u906e\u6321\u548c\u8fd1\u8ddd\u79bb\u4ea4\u4e92\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u6df1\u5ea6\u4fe1\u606f\u6709\u6f5c\u529b\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u5927\u591a\u6570MOT\u6570\u636e\u96c6\u7f3a\u4e4f\u6df1\u5ea6\u6807\u6ce8\uff0c\u5bfc\u81f4\u6df1\u5ea6\u4fe1\u606f\u5728\u8be5\u9886\u57df\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51faDepTR-MOT\uff0c\u4e00\u4e2a\u57fa\u4e8eDETR\u7684\u68c0\u6d4b\u5668\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a(1)\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u5b9e\u4f8b\u7ea7\u8f6f\u6df1\u5ea6\u6807\u7b7e\u76d1\u7763\uff0c\u7528\u4e8e\u7ec6\u5316\u6df1\u5ea6\u9884\u6d4b\uff1b(2)\u5bc6\u96c6\u6df1\u5ea6\u56fe\u7684\u84b8\u998f\uff0c\u4ee5\u4fdd\u6301\u5168\u5c40\u6df1\u5ea6\u4e00\u81f4\u6027\u3002\u8fd9\u4e9b\u7b56\u7565\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u63a8\u7406\u65f6\u8f93\u51fa\u5b9e\u4f8b\u7ea7\u6df1\u5ea6\uff0c\u65e0\u9700\u57fa\u7840\u6a21\u578b\u4e14\u4e0d\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728QuadTrack\u548cDanceTrack\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5206\u522b\u8fbe\u5230\u4e8627.59\u548c44.47\u7684HOTA\u5206\u6570\u3002\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u5e73\u53f0\u6570\u636e\u96c6QuadTrack\u4e0a\uff0c\u7ed3\u679c\u7a81\u51fa\u4e86\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u906e\u6321\u548c\u8fd1\u8ddd\u79bb\u4ea4\u4e92\u6311\u6218\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u6df1\u5ea6\u7ebf\u7d22\uff0cDepTR-MOT\u589e\u5f3a\u4e86TBD\u8303\u5f0f\u7684\u9c81\u68d2\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u906e\u6321\u548c\u8fd1\u8ddd\u79bb\u4ea4\u4e92\u95ee\u9898\uff0c\u5728\u673a\u5668\u4eba\u8ddf\u8e2a\u573a\u666f\u4e2d\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2509.17328", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.17328", "abs": "https://arxiv.org/abs/2509.17328", "authors": ["Hongxin Li", "Jingran Su", "Jingfan Chen", "Zheng Ju", "Yuntao Chen", "Qing Li", "Zhaoxiang Zhang"], "title": "UIPro: Unleashing Superior Interaction Capability For GUI Agents", "comment": "Accepted to ICCV 2025", "summary": "Building autonomous agents that perceive and operate graphical user\ninterfaces (GUIs) like humans has long been a vision in the field of artificial\nintelligence. Central to these agents is the capability for GUI interaction,\nwhich involves GUI understanding and planning capabilities. Existing methods\nhave tried developing GUI agents based on the multi-modal comprehension ability\nof vision-language models (VLMs). However, the limited scenario, insufficient\nsize, and heterogeneous action spaces hinder the progress of building\ngeneralist GUI agents. To resolve these issues, this paper proposes\n\\textbf{UIPro}, a novel generalist GUI agent trained with extensive\nmulti-platform and multi-task GUI interaction data, coupled with a unified\naction space. We first curate a comprehensive dataset encompassing 20.6 million\nGUI understanding tasks to pre-train UIPro, granting it a strong GUI grounding\ncapability, which is key to downstream GUI agent tasks. Subsequently, we\nestablish a unified action space to harmonize heterogeneous GUI agent task\ndatasets and produce a merged dataset to foster the action prediction ability\nof UIPro via continued fine-tuning. Experimental results demonstrate UIPro's\nsuperior performance across multiple GUI task benchmarks on various platforms,\nhighlighting the effectiveness of our approach.", "AI": {"tldr": "UIPro\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u7684\u901a\u7528GUI\u4ee3\u7406\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u5e73\u53f0GUI\u4ea4\u4e92\u6570\u636e\u548c\u7edf\u4e00\u52a8\u4f5c\u7a7a\u95f4\u8bad\u7ec3\uff0c\u5728\u591a\u4e2aGUI\u4efb\u52a1\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u65b9\u6cd5\u53d7\u9650\u4e8e\u573a\u666f\u6709\u9650\u3001\u6570\u636e\u89c4\u6a21\u4e0d\u8db3\u548c\u5f02\u6784\u52a8\u4f5c\u7a7a\u95f4\uff0c\u963b\u788d\u4e86\u901a\u7528GUI\u4ee3\u7406\u7684\u53d1\u5c55", "method": "\u9996\u5148\u6784\u5efa\u5305\u542b2060\u4e07GUI\u7406\u89e3\u4efb\u52a1\u7684\u7efc\u5408\u6570\u636e\u96c6\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5efa\u7acb\u7edf\u4e00\u52a8\u4f5c\u7a7a\u95f4\u6574\u5408\u5f02\u6784GUI\u4efb\u52a1\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6301\u7eed\u5fae\u8c03\u63d0\u5347\u52a8\u4f5c\u9884\u6d4b\u80fd\u529b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eUIPro\u5728\u591a\u4e2a\u5e73\u53f0\u7684\u5404\u79cdGUI\u4efb\u52a1\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd", "conclusion": "UIPro\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u5e73\u53f0\u6570\u636e\u548c\u7edf\u4e00\u52a8\u4f5c\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u901a\u7528GUI\u4ee3\u7406\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u95ee\u9898"}}
{"id": "2509.17329", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17329", "abs": "https://arxiv.org/abs/2509.17329", "authors": ["Neham Jain", "Andrew Jong", "Sebastian Scherer", "Ioannis Gkioulekas"], "title": "SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction", "comment": "Project website: https://imaging.cs.cmu.edu/smokeseer", "summary": "Smoke in real-world scenes can severely degrade the quality of images and\nhamper visibility. Recent methods for image restoration either rely on\ndata-driven priors that are susceptible to hallucinations, or are limited to\nstatic low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D\nscene reconstruction and smoke removal from a video capturing multiple views of\na scene. Our method uses thermal and RGB images, leveraging the fact that the\nreduced scattering in thermal images enables us to see through the smoke. We\nbuild upon 3D Gaussian splatting to fuse information from the two image\nmodalities, and decompose the scene explicitly into smoke and non-smoke\ncomponents. Unlike prior approaches, SmokeSeer handles a broad range of smoke\ndensities and can adapt to temporally varying smoke. We validate our approach\non synthetic data and introduce a real-world multi-view smoke dataset with RGB\nand thermal images. We provide open-source code and data at the project\nwebsite.", "AI": {"tldr": "SmokeSeer\u662f\u4e00\u79cd\u4ece\u89c6\u9891\u4e2d\u540c\u65f6\u8fdb\u884c3D\u573a\u666f\u91cd\u5efa\u548c\u70df\u96fe\u53bb\u9664\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u70ed\u6210\u50cf\u548cRGB\u56fe\u50cf\uff0c\u901a\u8fc73D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5206\u89e3\u70df\u96fe\u548c\u975e\u70df\u96fe\u6210\u5206\uff0c\u80fd\u591f\u5904\u7406\u5404\u79cd\u5bc6\u5ea6\u548c\u52a8\u6001\u53d8\u5316\u7684\u70df\u96fe\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u70df\u96fe\u4f1a\u4e25\u91cd\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\u5e76\u963b\u788d\u53ef\u89c1\u6027\u3002\u73b0\u6709\u7684\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u7684\u6570\u636e\u9a71\u52a8\u5148\u9a8c\uff0c\u8981\u4e48\u4ec5\u9650\u4e8e\u9759\u6001\u4f4e\u5bc6\u5ea6\u70df\u96fe\u3002", "method": "\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u878d\u5408\u70ed\u6210\u50cf\u548cRGB\u56fe\u50cf\u4fe1\u606f\uff0c\u5229\u7528\u70ed\u6210\u50cf\u4e2d\u6563\u5c04\u51cf\u5c11\u7684\u7279\u6027\u900f\u8fc7\u70df\u96fe\u89c2\u5bdf\u573a\u666f\uff0c\u5c06\u573a\u666f\u660e\u786e\u5206\u89e3\u4e3a\u70df\u96fe\u548c\u975e\u70df\u96fe\u6210\u5206\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u591a\u89c6\u89d2\u70df\u96fe\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5904\u7406\u5e7f\u6cdb\u7684\u70df\u96fe\u5bc6\u5ea6\u8303\u56f4\u5e76\u9002\u5e94\u65f6\u95f4\u53d8\u5316\u7684\u70df\u96fe\u3002", "conclusion": "SmokeSeer\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u70df\u96fe\u53bb\u9664\u548c3D\u573a\u666f\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\uff0c\u9879\u76ee\u7f51\u7ad9\u63d0\u4f9b\u4e86\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u3002"}}
{"id": "2509.17365", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17365", "abs": "https://arxiv.org/abs/2509.17365", "authors": ["Amanuel Tafese Dufera"], "title": "Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model", "comment": null, "summary": "Automatic image captioning, a multifaceted task bridging computer vision and\nnatural lan- guage processing, aims to generate descriptive textual content\nfrom visual input. While Convolutional Neural Networks (CNNs) and Long\nShort-Term Memory (LSTM) networks have achieved significant advancements, they\npresent limitations. The inherent sequential nature of RNNs leads to sluggish\ntraining and inference times. LSTMs further struggle with retaining information\nfrom earlier sequence elements when dealing with very long se- quences. This\nproject presents a comprehensive guide to constructing and comprehending\ntransformer models for image captioning. Transformers employ self-attention\nmechanisms, capturing both short- and long-range dependencies within the data.\nThis facilitates efficient parallelization during both training and inference\nphases. We leverage the well-established Transformer architecture, recognized\nfor its effectiveness in managing sequential data, and present a meticulous\nmethodology. Utilizing the Flickr30k dataset, we conduct data pre- processing,\nconstruct a model architecture that integrates an EfficientNetB0 CNN for fea-\nture extraction, and train the model with attention mechanisms incorporated.\nOur approach exemplifies the utilization of parallelization for efficient\ntraining and inference. You can find the project on GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4f7f\u7528Transformer\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edfCNN-LSTM\u65b9\u6cd5\u5728\u8bad\u7ec3\u901f\u5ea6\u548c\u4fe1\u606f\u4fdd\u7559\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u63cf\u8ff0\u65b9\u6cd5\uff08\u5982CNN-LSTM\uff09\u5b58\u5728\u8bad\u7ec3\u901f\u5ea6\u6162\u3001\u5904\u7406\u957f\u5e8f\u5217\u65f6\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\u3002Transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u6570\u636e\u4e2d\u7684\u77ed\u7a0b\u548c\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u652f\u6301\u9ad8\u6548\u7684\u5e76\u884c\u5316\u5904\u7406\u3002", "method": "\u91c7\u7528Transformer\u67b6\u6784\uff0c\u7ed3\u5408EfficientNetB0 CNN\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5728Flickr30k\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u52a0\u5165\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5e76\u884c\u5316\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "Transformer\u6a21\u578b\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u957f\u5e8f\u5217\u4f9d\u8d56\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2509.17374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17374", "abs": "https://arxiv.org/abs/2509.17374", "authors": ["Ankit Yadav", "Ta Duc Huy", "Lingqiao Liu"], "title": "Revisiting Vision Language Foundations for No-Reference Image Quality Assessment", "comment": "23 pages, 16 figures", "summary": "Large-scale vision language pre-training has recently shown promise for\nno-reference image-quality assessment (NR-IQA), yet the relative merits of\nmodern Vision Transformer foundations remain poorly understood. In this work,\nwe present the first systematic evaluation of six prominent pretrained\nbackbones, CLIP, SigLIP2, DINOv2, DINOv3, Perception, and ResNet, for the task\nof No-Reference Image Quality Assessment (NR-IQA), each finetuned using an\nidentical lightweight MLP head. Our study uncovers two previously overlooked\nfactors: (1) SigLIP2 consistently achieves strong performance; and (2) the\nchoice of activation function plays a surprisingly crucial role, particularly\nfor enhancing the generalization ability of image quality assessment models.\nNotably, we find that simple sigmoid activations outperform commonly used ReLU\nand GELU on several benchmarks. Motivated by this finding, we introduce a\nlearnable activation selection mechanism that adaptively determines the\nnonlinearity for each channel, eliminating the need for manual activation\ndesign, and achieving new state-of-the-art SRCC on CLIVE, KADID10K, and\nAGIQA3K. Extensive ablations confirm the benefits across architectures and\nregimes, establishing strong, resource-efficient NR-IQA baselines.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u516d\u79cd\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\uff08CLIP\u3001SigLIP2\u3001DINOv2\u3001DINOv3\u3001Perception\u3001ResNet\uff09\u5728\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0SigLIP2\u8868\u73b0\u4f18\u5f02\u4e14\u6fc0\u6d3b\u51fd\u6570\u9009\u62e9\u5bf9\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u53ef\u5b66\u4e60\u7684\u6fc0\u6d3b\u9009\u62e9\u673a\u5236\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u5728\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u73b0\u4ee3Vision Transformer\u57fa\u7840\u7684\u76f8\u5bf9\u4f18\u52bf\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u9884\u8bad\u7ec3\u9aa8\u5e72\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u76f8\u540c\u7684\u8f7b\u91cf\u7ea7MLP\u5934\u5bf9\u516d\u79cd\u9884\u8bad\u7ec3\u9aa8\u5e72\u8fdb\u884c\u5fae\u8c03\uff0c\u53d1\u73b0sigmoid\u6fc0\u6d3b\u51fd\u6570\u4f18\u4e8eReLU\u548cGELU\uff0c\u8fdb\u800c\u63d0\u51fa\u53ef\u5b66\u4e60\u7684\u6fc0\u6d3b\u9009\u62e9\u673a\u5236\uff0c\u81ea\u9002\u5e94\u786e\u5b9a\u6bcf\u4e2a\u901a\u9053\u7684\u975e\u7ebf\u6027\u51fd\u6570\u3002", "result": "SigLIP2\u8868\u73b0\u4e00\u81f4\u5f3a\u52b2\uff0c\u53ef\u5b66\u4e60\u6fc0\u6d3b\u9009\u62e9\u673a\u5236\u5728CLIVE\u3001KADID10K\u548cAGIQA3K\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684SOTA SRCC\u6027\u80fd\uff0c\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u5efa\u7acb\u4e86\u5f3a\u5927\u4e14\u8d44\u6e90\u9ad8\u6548\u7684NR-IQA\u57fa\u7ebf\uff0c\u63ed\u793a\u4e86\u6fc0\u6d3b\u51fd\u6570\u9009\u62e9\u5bf9\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u4f5c\u7528\uff0c\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u673a\u5236\u6d88\u9664\u4e86\u624b\u52a8\u8bbe\u8ba1\u6fc0\u6d3b\u51fd\u6570\u7684\u9700\u6c42\u3002"}}
{"id": "2509.17397", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.17397", "abs": "https://arxiv.org/abs/2509.17397", "authors": ["Jiaqi Zhu", "Shouyi Lu", "Ziyao Li", "Guirong Zhuo", "Lu Xiong"], "title": "Diff-GNSS: Diffusion-based Pseudorange Error Estimation", "comment": null, "summary": "Global Navigation Satellite Systems (GNSS) are vital for reliable urban\npositioning. However, multipath and non-line-of-sight reception often introduce\nlarge measurement errors that degrade accuracy. Learning-based methods for\npredicting and compensating pseudorange errors have gained traction, but their\nperformance is limited by complex error distributions. To address this\nchallenge, we propose Diff-GNSS, a coarse-to-fine GNSS measurement\n(pseudorange) error estimation framework that leverages a conditional diffusion\nmodel to capture such complex distributions. Firstly, a Mamba-based module\nperforms coarse estimation to provide an initial prediction with appropriate\nscale and trend. Then, a conditional denoising diffusion layer refines the\nestimate, enabling fine-grained modeling of pseudorange errors. To suppress\nuncontrolled generative diversity and achieve controllable synthesis, three key\nfeatures related to GNSS measurement quality are used as conditions to\nprecisely guide the reverse denoising process. We further incorporate\nper-satellite uncertainty modeling within the diffusion stage to assess the\nreliability of the predicted errors. We have collected and publicly released a\nreal-world dataset covering various scenes. Experiments on public and\nself-collected datasets show that DiffGNSS consistently outperforms\nstate-of-the-art baselines across multiple metrics. To the best of our\nknowledge, this is the first application of diffusion models to pseudorange\nerror estimation. The proposed diffusion-based refinement module is\nplug-and-play and can be readily integrated into existing networks to markedly\nimprove estimation accuracy.", "AI": {"tldr": "Diff-GNSS\u662f\u4e00\u4e2a\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684GNSS\u4f2a\u8ddd\u8bef\u5dee\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u89e3\u51b3\u590d\u6742\u8bef\u5dee\u5206\u5e03\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u57ce\u5e02\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "GNSS\u5728\u57ce\u5e02\u5b9a\u4f4d\u4e2d\u9762\u4e34\u591a\u5f84\u548c\u975e\u89c6\u8ddd\u4f20\u64ad\u5e26\u6765\u7684\u5927\u6d4b\u91cf\u8bef\u5dee\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u590d\u6742\u8bef\u5dee\u5206\u5e03\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bef\u5dee\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u57fa\u4e8eMamba\u6a21\u5757\u7684\u7c97\u4f30\u8ba1\u63d0\u4f9b\u521d\u59cb\u9884\u6d4b\uff1b2\uff09\u6761\u4ef6\u53bb\u566a\u6269\u6563\u5c42\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bef\u5dee\u5efa\u6a21\uff0c\u4f7f\u7528\u4e09\u4e2aGNSS\u6d4b\u91cf\u8d28\u91cf\u7279\u5f81\u4f5c\u4e3a\u6761\u4ef6\u6307\u5bfc\u53cd\u5411\u53bb\u566a\u8fc7\u7a0b\uff0c\u5e76\u52a0\u5165\u9010\u536b\u661f\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3002", "result": "\u5728\u516c\u5f00\u548c\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDiff-GNSS\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u662f\u6269\u6563\u6a21\u578b\u5728\u4f2a\u8ddd\u8bef\u5dee\u4f30\u8ba1\u4e2d\u7684\u9996\u6b21\u5e94\u7528\uff0c\u63d0\u51fa\u7684\u6269\u6563\u7cbe\u70bc\u6a21\u5757\u662f\u5373\u63d2\u5373\u7528\u7684\uff0c\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u7f51\u7edc\u4e2d\u663e\u8457\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002"}}
{"id": "2509.17401", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17401", "abs": "https://arxiv.org/abs/2509.17401", "authors": ["Jinyeong Kim", "Junhyeok Kim", "Yumin Shim", "Joohyeok Kim", "Sunyoung Jung", "Seong Jae Hwang"], "title": "Interpreting vision transformers via residual replacement model", "comment": null, "summary": "How do vision transformers (ViTs) represent and process the world? This paper\naddresses this long-standing question through the first systematic analysis of\n6.6K features across all layers, extracted via sparse autoencoders, and by\nintroducing the residual replacement model, which replaces ViT computations\nwith interpretable features in the residual stream. Our analysis reveals not\nonly a feature evolution from low-level patterns to high-level semantics, but\nalso how ViTs encode curves and spatial positions through specialized feature\ntypes. The residual replacement model scalably produces a faithful yet\nparsimonious circuit for human-scale interpretability by significantly\nsimplifying the original computations. As a result, this framework enables\nintuitive understanding of ViT mechanisms. Finally, we demonstrate the utility\nof our framework in debiasing spurious correlations.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790ViT\u76846.6K\u7279\u5f81\uff0c\u63d0\u51fa\u6b8b\u5dee\u66ff\u6362\u6a21\u578b\u6765\u7b80\u5316ViT\u8ba1\u7b97\uff0c\u63ed\u793a\u7279\u5f81\u4ece\u4f4e\u5c42\u6a21\u5f0f\u5230\u9ad8\u5c42\u8bed\u4e49\u7684\u6f14\u53d8\uff0c\u4ee5\u53ca\u66f2\u7ebf\u548c\u7a7a\u95f4\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684ViT\u673a\u5236\u7406\u89e3\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9Transformer\uff08ViT\uff09\u5982\u4f55\u8868\u793a\u548c\u5904\u7406\u4e16\u754c\uff0c\u89e3\u51b3\u957f\u671f\u4ee5\u6765\u5bf9ViT\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u6240\u6709\u5c42\u76846.6K\u7279\u5f81\uff0c\u5f15\u5165\u6b8b\u5dee\u66ff\u6362\u6a21\u578b\u5c06ViT\u8ba1\u7b97\u66ff\u6362\u4e3a\u53ef\u89e3\u91ca\u7279\u5f81\uff0c\u7b80\u5316\u539f\u59cb\u8ba1\u7b97\u8fc7\u7a0b\u3002", "result": "\u63ed\u793a\u4e86ViT\u7279\u5f81\u4ece\u4f4e\u5c42\u6a21\u5f0f\u5230\u9ad8\u5c42\u8bed\u4e49\u7684\u6f14\u53d8\u8fc7\u7a0b\uff0c\u53d1\u73b0\u4e86\u4e13\u95e8\u7f16\u7801\u66f2\u7ebf\u548c\u7a7a\u95f4\u4f4d\u7f6e\u7684\u7279\u5f81\u7c7b\u578b\uff0c\u6b8b\u5dee\u66ff\u6362\u6a21\u578b\u80fd\u5fe0\u5b9e\u4e14\u7b80\u6d01\u5730\u518d\u73b0ViT\u8ba1\u7b97\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u76f4\u89c2\u7406\u89e3ViT\u673a\u5236\u7684\u9014\u5f84\uff0c\u5e76\u5728\u6d88\u9664\u865a\u5047\u76f8\u5173\u6027\u65b9\u9762\u5c55\u793a\u4e86\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3aViT\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.17406", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17406", "abs": "https://arxiv.org/abs/2509.17406", "authors": ["Jonathan Wuntu", "Muhamad Dwisnanto Putro", "Rendy Syahputra"], "title": "Real-Time Fish Detection in Indonesian Marine Ecosystems Using Lightweight YOLOv10-nano Architecture", "comment": null, "summary": "Indonesia's marine ecosystems, part of the globally recognized Coral\nTriangle, are among the richest in biodiversity, requiring efficient monitoring\ntools to support conservation. Traditional fish detection methods are\ntime-consuming and demand expert knowledge, prompting the need for automated\nsolutions. This study explores the implementation of YOLOv10-nano, a\nstate-of-the-art deep learning model, for real-time marine fish detection in\nIndonesian waters, using test data from Bunaken National Marine Park. YOLOv10's\narchitecture, featuring improvements like the CSPNet backbone, PAN for feature\nfusion, and Pyramid Spatial Attention Block, enables efficient and accurate\nobject detection even in complex environments. The model was evaluated on the\nDeepFish and OpenImages V7-Fish datasets. Results show that YOLOv10-nano\nachieves a high detection accuracy with mAP50 of 0.966 and mAP50:95 of 0.606\nwhile maintaining low computational demand (2.7M parameters, 8.4 GFLOPs). It\nalso delivered an average inference speed of 29.29 FPS on the CPU, making it\nsuitable for real-time deployment. Although OpenImages V7-Fish alone provided\nlower accuracy, it complemented DeepFish in enhancing model robustness.\nOverall, this study demonstrates YOLOv10-nano's potential for efficient,\nscalable marine fish monitoring and conservation applications in data-limited\nenvironments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86YOLOv10-nano\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5370\u5c3c\u6c34\u57df\u5b9e\u65f6\u6d77\u6d0b\u9c7c\u7c7b\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u9700\u6c42\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u9002\u5408\u5b9e\u65f6\u90e8\u7f72\u3002", "motivation": "\u5370\u5c3c\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u662f\u5168\u7403\u516c\u8ba4\u7684\u73ca\u745a\u4e09\u89d2\u533a\uff0c\u751f\u7269\u591a\u6837\u6027\u4e30\u5bcc\uff0c\u9700\u8981\u9ad8\u6548\u76d1\u6d4b\u5de5\u5177\u652f\u6301\u4fdd\u62a4\u3002\u4f20\u7edf\u9c7c\u7c7b\u68c0\u6d4b\u65b9\u6cd5\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528YOLOv10-nano\u6a21\u578b\uff0c\u5176\u67b6\u6784\u5305\u542bCSPNet\u9aa8\u5e72\u7f51\u7edc\u3001PAN\u7279\u5f81\u878d\u5408\u548c\u91d1\u5b57\u5854\u7a7a\u95f4\u6ce8\u610f\u529b\u5757\uff0c\u5728Bunaken\u56fd\u5bb6\u6d77\u6d0b\u516c\u56ed\u7684\u6d4b\u8bd5\u6570\u636e\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5728DeepFish\u548cOpenImages V7-Fish\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "YOLOv10-nano\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff08mAP50\u4e3a0.966\uff0cmAP50:95\u4e3a0.606\uff09\uff0c\u8ba1\u7b97\u9700\u6c42\u4f4e\uff08270\u4e07\u53c2\u6570\uff0c8.4 GFLOPs\uff09\uff0c\u5728CPU\u4e0a\u7684\u5e73\u5747\u63a8\u7406\u901f\u5ea6\u4e3a29.29 FPS\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86YOLOv10-nano\u5728\u6570\u636e\u6709\u9650\u73af\u5883\u4e2d\u8fdb\u884c\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6d77\u6d0b\u9c7c\u7c7b\u76d1\u6d4b\u548c\u4fdd\u62a4\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.17427", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17427", "abs": "https://arxiv.org/abs/2509.17427", "authors": ["Hodaka Kawachi", "Jose Reinaldo Cunha Santos A. V. Silva Neto", "Yasushi Yagi", "Hajime Nagahara", "Tomoya Nakamura"], "title": "Single-Image Depth from Defocus with Coded Aperture and Diffusion Posterior Sampling", "comment": null, "summary": "We propose a single-snapshot depth-from-defocus (DFD) reconstruction method\nfor coded-aperture imaging that replaces hand-crafted priors with a learned\ndiffusion prior used purely as regularization. Our optimization framework\nenforces measurement consistency via a differentiable forward model while\nguiding solutions with the diffusion prior in the denoised image domain,\nyielding higher accuracy and stability than clas- sical optimization. Unlike\nU-Net-style regressors, our approach requires no paired defocus-RGBD training\ndata and does not tie training to a specific camera configuration. Experiments\non comprehensive simulations and a prototype camera demonstrate consistently\nstrong RGBD reconstructions across noise levels, outperforming both U-Net\nbaselines and a classical coded- aperture DFD method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u6269\u6563\u5148\u9a8c\u7684\u5355\u6b21\u5feb\u7167\u79bb\u7126\u6df1\u5ea6\u91cd\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u7f16\u7801\u5b54\u5f84\u6210\u50cf\uff0c\u901a\u8fc7\u4f18\u5316\u6846\u67b6\u7ed3\u5408\u53ef\u5fae\u5206\u524d\u5411\u6a21\u578b\u548c\u6269\u6563\u5148\u9a8c\uff0c\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u4e14\u4e0d\u4f9d\u8d56\u7279\u5b9a\u76f8\u673a\u914d\u7f6e\u3002", "motivation": "\u4f20\u7edf\u79bb\u7126\u6df1\u5ea6\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u5148\u9a8c\uff0c\u800c\u57fa\u4e8eU-Net\u7684\u56de\u5f52\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u4e14\u53d7\u9650\u4e8e\u7279\u5b9a\u76f8\u673a\u914d\u7f6e\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u914d\u5bf9\u6570\u636e\u3001\u914d\u7f6e\u65e0\u5173\u7684\u6df1\u5ea6\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5b66\u4e60\u6269\u6563\u5148\u9a8c\u4f5c\u4e3a\u6b63\u5219\u5316\uff0c\u901a\u8fc7\u4f18\u5316\u6846\u67b6\u5f3a\u5236\u6d4b\u91cf\u4e00\u81f4\u6027\uff08\u53ef\u5fae\u5206\u524d\u5411\u6a21\u578b\uff09\uff0c\u5728\u53bb\u566a\u56fe\u50cf\u57df\u4e2d\u5229\u7528\u6269\u6563\u5148\u9a8c\u6307\u5bfc\u89e3\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u7efc\u5408\u4eff\u771f\u548c\u539f\u578b\u76f8\u673a\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4e0b\u5747\u80fd\u5b9e\u73b0\u7a33\u5b9a\u7684RGBD\u91cd\u5efa\uff0c\u6027\u80fd\u4f18\u4e8eU-Net\u57fa\u7ebf\u548c\u4f20\u7edf\u7f16\u7801\u5b54\u5f84\u79bb\u7126\u6df1\u5ea6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u5b66\u4e60\u6269\u6563\u5148\u9a8c\u5e94\u7528\u4e8e\u79bb\u7126\u6df1\u5ea6\u91cd\u5efa\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u3001\u76f8\u673a\u914d\u7f6e\u65e0\u5173\u7684\u9ad8\u8d28\u91cf\u6df1\u5ea6\u91cd\u5efa\uff0c\u4e3a\u5355\u6b21\u5feb\u7167\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.17429", "categories": ["cs.CV", "68T45", "I.2.10"], "pdf": "https://arxiv.org/pdf/2509.17429", "abs": "https://arxiv.org/abs/2509.17429", "authors": ["Zhitao Zeng", "Guojian Yuan", "Junyuan Mao", "Yuxuan Wang", "Xiaoshuang Jia", "Yueming Jin"], "title": "Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration", "comment": "20 pages, 6 figures", "summary": "Accurate temporal prediction is the bridge between comprehensive scene\nunderstanding and embodied artificial intelligence. However, predicting\nmultiple fine-grained states of a scene at multiple temporal scales is\ndifficult for vision-language models. We formalize the Multi-Scale Temporal\nPrediction (MSTP) task in general and surgical scenes by decomposing\nmulti-scale into two orthogonal dimensions: the temporal scale, forecasting\nstates of humans and surgery at varying look-ahead intervals, and the state\nscale, modeling a hierarchy of states in general and surgical scenes. For\nexample, in general scenes, states of contact relationships are finer-grained\nthan states of spatial relationships. In surgical scenes, medium-level steps\nare finer-grained than high-level phases yet remain constrained by their\nencompassing phase. To support this unified task, we introduce the first MSTP\nBenchmark, featuring synchronized annotations across multiple state scales and\ntemporal scales. We further propose a method, Incremental Generation and\nMulti-agent Collaboration (IG-MC), which integrates two key innovations. First,\nwe present a plug-and-play incremental generation module that continuously\nsynthesizes up-to-date visual previews at expanding temporal scales to inform\nmultiple decision-making agents, keeping decisions and generated visuals\nsynchronized and preventing performance degradation as look-ahead intervals\nlengthen. Second, we present a decision-driven multi-agent collaboration\nframework for multi-state prediction, comprising generation, initiation, and\nmulti-state assessment agents that dynamically trigger and evaluate prediction\ncycles to balance global coherence and local fidelity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u5c3a\u5ea6\u65f6\u95f4\u9884\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u589e\u91cf\u751f\u6210\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u6cd5\uff0c\u5728\u901a\u7528\u548c\u624b\u672f\u573a\u666f\u4e2d\u9884\u6d4b\u591a\u4e2a\u65f6\u95f4\u5c3a\u5ea6\u548c\u72b6\u6001\u5c42\u6b21\u4e0a\u7684\u7ec6\u7c92\u5ea6\u72b6\u6001\u3002", "motivation": "\u51c6\u786e\u7684\u65f6\u95f4\u9884\u6d4b\u662f\u5168\u9762\u573a\u666f\u7406\u89e3\u4e0e\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4f46\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5728\u591a\u4e2a\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u9884\u6d4b\u591a\u4e2a\u7ec6\u7c92\u5ea6\u72b6\u6001\u3002", "method": "\u63d0\u51faIG-MC\u65b9\u6cd5\uff0c\u5305\u542b\u589e\u91cf\u751f\u6210\u6a21\u5757\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u3002\u589e\u91cf\u751f\u6210\u6a21\u5757\u6301\u7eed\u5408\u6210\u6700\u65b0\u89c6\u89c9\u9884\u89c8\uff0c\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5305\u542b\u751f\u6210\u3001\u542f\u52a8\u548c\u591a\u72b6\u6001\u8bc4\u4f30\u667a\u80fd\u4f53\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2aMSTP\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u8de8\u591a\u4e2a\u72b6\u6001\u5c3a\u5ea6\u548c\u65f6\u95f4\u5c3a\u5ea6\u7684\u540c\u6b65\u6807\u6ce8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4fdd\u6301\u51b3\u7b56\u4e0e\u751f\u6210\u89c6\u89c9\u7684\u540c\u6b65\uff0c\u9632\u6b62\u968f\u7740\u9884\u6d4b\u95f4\u9694\u5ef6\u957f\u800c\u6027\u80fd\u4e0b\u964d\uff0c\u5e73\u8861\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2509.17430", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17430", "abs": "https://arxiv.org/abs/2509.17430", "authors": ["Gunjan Chhablani", "Xiaomeng Ye", "Muhammad Zubair Irshad", "Zsolt Kira"], "title": "EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device", "comment": "16 pages, 18 figures, paper accepted at ICCV, 2025", "summary": "The field of Embodied AI predominantly relies on simulation for training and\nevaluation, often using either fully synthetic environments that lack\nphotorealism or high-fidelity real-world reconstructions captured with\nexpensive hardware. As a result, sim-to-real transfer remains a major\nchallenge. In this paper, we introduce EmbodiedSplat, a novel approach that\npersonalizes policy training by efficiently capturing the deployment\nenvironment and fine-tuning policies within the reconstructed scenes. Our\nmethod leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to\nbridge the gap between realistic scene capture and effective training\nenvironments. Using iPhone-captured deployment scenes, we reconstruct meshes\nvia GS, enabling training in settings that closely approximate real-world\nconditions. We conduct a comprehensive analysis of training strategies,\npre-training datasets, and mesh reconstruction techniques, evaluating their\nimpact on sim-to-real predictivity in real-world scenarios. Experimental\nresults demonstrate that agents fine-tuned with EmbodiedSplat outperform both\nzero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and\nsynthetically generated datasets (HSSD), achieving absolute success rate\nimprovements of 20\\% and 40\\% on real-world Image Navigation task. Moreover,\nour approach yields a high sim-vs-real correlation (0.87--0.97) for the\nreconstructed meshes, underscoring its effectiveness in adapting policies to\ndiverse environments with minimal effort. Project page:\nhttps://gchhablani.github.io/embodied-splat", "AI": {"tldr": "EmbodiedSplat\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc73D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u91cd\u5efa\u90e8\u7f72\u73af\u5883\u6765\u4e2a\u6027\u5316\u7b56\u7565\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86sim-to-real\u7684\u8fc1\u79fb\u6548\u679c\u3002", "motivation": "\u5f53\u524dEmbodied AI\u4e3b\u8981\u4f9d\u8d56\u4eff\u771f\u8bad\u7ec3\uff0c\u4f46\u5408\u6210\u73af\u5883\u7f3a\u4e4f\u771f\u5b9e\u611f\uff0c\u800c\u9ad8\u4fdd\u771f\u5b9e\u4e16\u754c\u91cd\u5efa\u6210\u672c\u9ad8\u6602\uff0c\u5bfc\u81f4sim-to-real\u8fc1\u79fb\u6210\u4e3a\u4e3b\u8981\u6311\u6218\u3002", "method": "\u5229\u7528iPhone\u6355\u83b7\u90e8\u7f72\u573a\u666f\uff0c\u901a\u8fc73D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u91cd\u5efa\u7f51\u683c\uff0c\u5728Habitat-Sim\u6a21\u62df\u5668\u4e2d\u8fdb\u884c\u7b56\u7565\u5fae\u8c03\uff0c\u5b9e\u73b0\u771f\u5b9e\u573a\u666f\u7684\u8fd1\u4f3c\u8bad\u7ec3\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEmbodiedSplat\u5fae\u8c03\u7684\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u5bfc\u822a\u4efb\u52a1\u4e0a\u6bd4HM3D\u548cHSSD\u9884\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\u5206\u522b\u63d0\u534720%\u548c40%\u7684\u6210\u529f\u7387\uff0csim-vs-real\u76f8\u5173\u6027\u8fbe\u52300.87-0.97\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u6700\u5c0f\u6210\u672c\u6709\u6548\u9002\u5e94\u591a\u6837\u5316\u73af\u5883\uff0c\u4e3asim-to-real\u8fc1\u79fb\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17431", "abs": "https://arxiv.org/abs/2509.17431", "authors": ["Keyu Du", "Jingyu Hu", "Haipeng Li", "Hao Xu", "Haibing Huang", "Chi-Wing Fu", "Shuaicheng Liu"], "title": "Emergent 3D Correspondence from Neural Shape Representation", "comment": "This paper is accepted by Siggraph Asia 2025 conference track", "summary": "This paper presents a new approach to estimate accurate and robust 3D\nsemantic correspondence with the hierarchical neural semantic representation.\nOur work has three key contributions. First, we design the hierarchical neural\nsemantic representation (HNSR), which consists of a global semantic feature to\ncapture high-level structure and multi-resolution local geometric features to\npreserve fine details, by carefully harnessing 3D priors from pre-trained 3D\ngenerative models. Second, we design a progressive global-to-local matching\nstrategy, which establishes coarse semantic correspondence using the global\nsemantic feature, then iteratively refines it with local geometric features,\nyielding accurate and semantically-consistent mappings. Third, our framework is\ntraining-free and broadly compatible with various pre-trained 3D generative\nbackbones, demonstrating strong generalization across diverse shape categories.\nOur method also supports various applications, such as shape co-segmentation,\nkeypoint matching, and texture transfer, and generalizes well to structurally\ndiverse shapes, with promising results even in cross-category scenarios. Both\nqualitative and quantitative evaluations show that our method outperforms\nprevious state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u795e\u7ecf\u8bed\u4e49\u8868\u793a\u76843D\u8bed\u4e49\u5bf9\u5e94\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u8bed\u4e49\u7279\u5f81\u548c\u5c40\u90e8\u51e0\u4f55\u7279\u5f81\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u51c6\u786e\u9c81\u68d2\u76843D\u8bed\u4e49\u5bf9\u5e94\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b33D\u8bed\u4e49\u5bf9\u5e94\u4f30\u8ba1\u4e2d\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5904\u7406\u7ed3\u6784\u591a\u6837\u5316\u5f62\u72b6\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5206\u5c42\u795e\u7ecf\u8bed\u4e49\u8868\u793a\uff08HNSR\uff09\uff0c\u5305\u542b\u5168\u5c40\u8bed\u4e49\u7279\u5f81\u548c\u591a\u5206\u8fa8\u7387\u5c40\u90e8\u51e0\u4f55\u7279\u5f81\uff1b\u91c7\u7528\u6e10\u8fdb\u5f0f\u5168\u5c40\u5230\u5c40\u90e8\u5339\u914d\u7b56\u7565\uff1b\u6846\u67b6\u65e0\u9700\u8bad\u7ec3\uff0c\u517c\u5bb9\u591a\u79cd\u9884\u8bad\u7ec33D\u751f\u6210\u6a21\u578b\u3002", "result": "\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\uff0c\u652f\u6301\u5f62\u72b6\u5171\u5206\u5272\u3001\u5173\u952e\u70b9\u5339\u914d\u548c\u7eb9\u7406\u8fc1\u79fb\u7b49\u5e94\u7528\uff0c\u5728\u8de8\u7c7b\u522b\u573a\u666f\u4e0b\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u5c42\u8868\u793a\u548c\u6e10\u8fdb\u5339\u914d\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u3001\u9c81\u68d2\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u76843D\u8bed\u4e49\u5bf9\u5e94\u4f30\u8ba1\uff0c\u4e3a\u591a\u79cd3D\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17452", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17452", "abs": "https://arxiv.org/abs/2509.17452", "authors": ["Dujin Lee", "Sojung An", "Jungmyung Wi", "Kuniaki Saito", "Donghyun Kim"], "title": "Training-Free Label Space Alignment for Universal Domain Adaptation", "comment": "22 pages, 12 figures", "summary": "Universal domain adaptation (UniDA) transfers knowledge from a labeled source\ndomain to an unlabeled target domain, where label spaces may differ and the\ntarget domain may contain private classes. Previous UniDA methods primarily\nfocused on visual space alignment but often struggled with visual ambiguities\ndue to content differences, which limited their robustness and\ngeneralizability. To overcome this, we introduce a novel approach that\nleverages the strong \\textit{zero-shot capabilities} of recent vision-language\nfoundation models (VLMs) like CLIP, concentrating solely on label space\nalignment to enhance adaptation stability. CLIP can generate task-specific\nclassifiers based only on label names. However, adapting CLIP to UniDA is\nchallenging because the label space is not fully known in advance. In this\nstudy, we first utilize generative vision-language models to identify unknown\ncategories in the target domain. Noise and semantic ambiguities in the\ndiscovered labels -- such as those similar to source labels (e.g., synonyms,\nhypernyms, hyponyms) -- complicate label alignment. To address this, we propose\na training-free label-space alignment method for UniDA (\\ours). Our method\naligns label spaces instead of visual spaces by filtering and refining noisy\nlabels between the domains. We then construct a \\textit{universal classifier}\nthat integrates both shared knowledge and target-private class information,\nthereby improving generalizability under domain shifts. The results reveal that\nthe proposed method considerably outperforms existing UniDA techniques across\nkey DomainBed benchmarks, delivering an average improvement of\n\\textcolor{blue}{+7.9\\%}in H-score and \\textcolor{blue}{+6.1\\%} in H$^3$-score.\nFurthermore, incorporating self-training further enhances performance and\nachieves an additional (\\textcolor{blue}{+1.6\\%}) increment in both H- and\nH$^3$-scores.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u901a\u7528\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u6807\u7b7e\u7a7a\u95f4\u5bf9\u9f50\u800c\u975e\u89c6\u89c9\u7a7a\u95f4\u5bf9\u9f50\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u76ee\u6807\u57df\u4e2d\u7684\u672a\u77e5\u7c7b\u522b\uff0c\u5e76\u6784\u5efa\u901a\u7528\u5206\u7c7b\u5668\u6765\u63d0\u9ad8\u57df\u9002\u5e94\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u901a\u7528\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4f46\u7531\u4e8e\u5185\u5bb9\u5dee\u5f02\u5bfc\u81f4\u7684\u89c6\u89c9\u6a21\u7cca\u6027\u9650\u5236\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u901a\u8fc7\u6807\u7b7e\u7a7a\u95f4\u5bf9\u9f50\u6765\u63d0\u5347\u57df\u9002\u5e94\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u9996\u5148\u4f7f\u7528\u751f\u6210\u5f0f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u76ee\u6807\u57df\u4e2d\u7684\u672a\u77e5\u7c7b\u522b\uff1b\u7136\u540e\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u6807\u7b7e\u7a7a\u95f4\u5bf9\u9f50\u65b9\u6cd5\uff0c\u8fc7\u6ee4\u548c\u7cbe\u70bc\u57df\u95f4\u566a\u58f0\u6807\u7b7e\uff1b\u6700\u540e\u6784\u5efa\u96c6\u6210\u5171\u4eab\u77e5\u8bc6\u548c\u76ee\u6807\u79c1\u6709\u7c7b\u522b\u4fe1\u606f\u7684\u901a\u7528\u5206\u7c7b\u5668\u3002", "result": "\u5728DomainBed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u901a\u7528\u57df\u81ea\u9002\u5e94\u6280\u672f\uff0cH-score\u5e73\u5747\u63d0\u53477.9%\uff0cH\u00b3-score\u5e73\u5747\u63d0\u53476.1%\u3002\u7ed3\u5408\u81ea\u8bad\u7ec3\u540e\uff0cH-score\u548cH\u00b3-score\u8fdb\u4e00\u6b65\u63d0\u5347\u4e861.6%\u3002", "conclusion": "\u901a\u8fc7\u4e13\u6ce8\u4e8e\u6807\u7b7e\u7a7a\u95f4\u5bf9\u9f50\u800c\u975e\u89c6\u89c9\u7a7a\u95f4\u5bf9\u9f50\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u8be5\u65b9\u6cd5\u5728\u901a\u7528\u57df\u81ea\u9002\u5e94\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5c55\u793a\u4e86\u6807\u7b7e\u7a7a\u95f4\u5bf9\u9f50\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.17457", "categories": ["cs.CV", "cs.AI", "68T10", "I.2.10; I.4.m"], "pdf": "https://arxiv.org/pdf/2509.17457", "abs": "https://arxiv.org/abs/2509.17457", "authors": ["Pawe\u0142 Jakub Borsukiewicz", "Jordan Samhi", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Explainable AI for Analyzing Person-Specific Patterns in Facial Recognition Tasks", "comment": "22 pages; 24 tables; 11 figures", "summary": "The proliferation of facial recognition systems presents major privacy risks,\ndriving the need for effective countermeasures. Current adversarial techniques\napply generalized methods rather than adapting to individual facial\ncharacteristics, limiting their effectiveness and inconspicuousness. In this\nwork, we introduce Layer Embedding Activation Mapping (LEAM), a novel technique\nthat identifies which facial areas contribute most to recognition at an\nindividual level. Unlike adversarial attack methods that aim to fool\nrecognition systems, LEAM is an explainability technique designed to understand\nhow these systems work, providing insights that could inform future privacy\nprotection research. We integrate LEAM with a face parser to analyze data from\n1000 individuals across 9 pre-trained facial recognition models.\n  Our analysis reveals that while different layers within facial recognition\nmodels vary significantly in their focus areas, these models generally\nprioritize similar facial regions across architectures when considering their\noverall activation patterns, which show significantly higher similarity between\nimages of the same individual (Bhattacharyya Coefficient: 0.32-0.57) vs.\ndifferent individuals (0.04-0.13), validating the existence of person-specific\nrecognition patterns. Our results show that facial recognition models\nprioritize the central region of face images (with nose areas accounting for\n18.9-29.7% of critical recognition regions), while still distributing attention\nacross multiple facial fragments. Proper selection of relevant facial areas was\nconfirmed using validation occlusions, based on just 1% of the most relevant,\nLEAM-identified, image pixels, which proved to be transferable across different\nmodels. Our findings establish the foundation for future individually tailored\nprivacy protection systems centered around LEAM's choice of areas to be\nperturbed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLEAM\u6280\u672f\uff0c\u901a\u8fc7\u5206\u6790\u9762\u90e8\u8bc6\u522b\u6a21\u578b\u5728\u4e2a\u4f53\u5c42\u9762\u7684\u5173\u952e\u8bc6\u522b\u533a\u57df\uff0c\u4e3a\u4e2a\u6027\u5316\u9690\u79c1\u4fdd\u62a4\u7cfb\u7edf\u63d0\u4f9b\u57fa\u7840\u3002\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u6574\u4f53\u6fc0\u6d3b\u6a21\u5f0f\u4e0a\u5177\u6709\u76f8\u4f3c\u6027\uff0c\u4e14\u9f3b\u90e8\u533a\u57df\u662f\u91cd\u8981\u7684\u8bc6\u522b\u7279\u5f81\u3002", "motivation": "\u5f53\u524d\u5bf9\u6297\u6027\u6280\u672f\u91c7\u7528\u901a\u7528\u65b9\u6cd5\u800c\u975e\u9488\u5bf9\u4e2a\u4f53\u9762\u90e8\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u548c\u9690\u853d\u6027\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u9762\u90e8\u8bc6\u522b\u7cfb\u7edf\u5de5\u4f5c\u539f\u7406\u7684\u6280\u672f\u6765\u6307\u5bfc\u9690\u79c1\u4fdd\u62a4\u7814\u7a76\u3002", "method": "\u5f15\u5165Layer Embedding Activation Mapping (LEAM)\u6280\u672f\uff0c\u7ed3\u5408\u9762\u90e8\u89e3\u6790\u5668\uff0c\u5206\u67901000\u4e2a\u4e2a\u4f53\u57289\u4e2a\u9884\u8bad\u7ec3\u9762\u90e8\u8bc6\u522b\u6a21\u578b\u4e2d\u7684\u6570\u636e\uff0c\u8bc6\u522b\u5bf9\u8bc6\u522b\u8d21\u732e\u6700\u5927\u7684\u9762\u90e8\u533a\u57df\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9762\u90e8\u8bc6\u522b\u6a21\u578b\u4f18\u5148\u5173\u6ce8\u9762\u90e8\u4e2d\u592e\u533a\u57df\uff08\u9f3b\u90e8\u5360\u5173\u952e\u8bc6\u522b\u533a\u57df\u768418.9-29.7%\uff09\uff0c\u4e14\u540c\u4e00\u4eba\u7684\u56fe\u50cf\u6fc0\u6d3b\u6a21\u5f0f\u76f8\u4f3c\u5ea6\u663e\u8457\u9ad8\u4e8e\u4e0d\u540c\u4eba\u3002\u4ec5\u4f7f\u75281%\u6700\u76f8\u5173\u50cf\u7d20\u7684\u9a8c\u8bc1\u906e\u6321\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "LEAM\u6280\u672f\u4e3a\u672a\u6765\u57fa\u4e8e\u4e2a\u4f53\u5316\u9762\u90e8\u7279\u5f81\u9009\u62e9\u7684\u9690\u79c1\u4fdd\u62a4\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5176\u8bc6\u522b\u7684\u91cd\u8981\u533a\u57df\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u5177\u6709\u53ef\u8fc1\u79fb\u6027\u3002"}}
{"id": "2509.17458", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17458", "abs": "https://arxiv.org/abs/2509.17458", "authors": ["Seyed Amir Kasaei", "Ali Aghayari", "Arash Marioriyad", "Niki Sepasian", "Shayan Baghayi Nejad", "MohammadAmin Fazli", "Mahdieh Soleymani Baghshah", "Mohammad Hossein Rohban"], "title": "CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration", "comment": null, "summary": "Text-to-image diffusion models, such as Stable Diffusion, can produce\nhigh-quality and diverse images but often fail to achieve compositional\nalignment, particularly when prompts describe complex object relationships,\nattributes, or spatial arrangements. Recent inference-time approaches address\nthis by optimizing or exploring the initial noise under the guidance of reward\nfunctions that score text-image alignment without requiring model fine-tuning.\nWhile promising, each strategy has intrinsic limitations when used alone:\noptimization can stall due to poor initialization or unfavorable search\ntrajectories, whereas exploration may require a prohibitively large number of\nsamples to locate a satisfactory output. Our analysis further shows that\nneither single reward metrics nor ad-hoc combinations reliably capture all\naspects of compositionality, leading to weak or inconsistent guidance. To\novercome these challenges, we present Category-Aware Reward-based Initial Noise\nOptimization and Exploration (CARINOX), a unified framework that combines noise\noptimization and exploration with a principled reward selection procedure\ngrounded in correlation with human judgments. Evaluations on two complementary\nbenchmarks covering diverse compositional challenges show that CARINOX raises\naverage alignment scores by +16% on T2I-CompBench++ and +11% on the HRS\nbenchmark, consistently outperforming state-of-the-art optimization and\nexploration-based methods across all major categories, while preserving image\nquality and diversity. The project page is available at\nhttps://amirkasaei.com/carinox/{this URL}.", "AI": {"tldr": "CARINOX\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u566a\u58f0\u4f18\u5316\u548c\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u7684\u5956\u52b1\u9009\u62e9\u7a0b\u5e8f\uff0c\u63d0\u9ad8\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u7ec4\u5408\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u5728\u5904\u7406\u590d\u6742\u5bf9\u8c61\u5173\u7cfb\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u6392\u5217\u65f6\u5f80\u5f80\u65e0\u6cd5\u5b9e\u73b0\u7ec4\u5408\u5bf9\u9f50\u3002\u73b0\u6709\u7684\u63a8\u7406\u65f6\u65b9\u6cd5\u5404\u6709\u5c40\u9650\u6027\uff1a\u4f18\u5316\u65b9\u6cd5\u53ef\u80fd\u56e0\u521d\u59cb\u5316\u4e0d\u826f\u800c\u505c\u6ede\uff0c\u63a2\u7d22\u65b9\u6cd5\u5219\u9700\u8981\u5927\u91cf\u6837\u672c\u3002", "method": "CARINOX\u6846\u67b6\u7ed3\u5408\u4e86\u566a\u58f0\u4f18\u5316\u548c\u63a2\u7d22\u7b56\u7565\uff0c\u91c7\u7528\u57fa\u4e8e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u7684\u539f\u5219\u6027\u5956\u52b1\u9009\u62e9\u7a0b\u5e8f\uff0c\u89e3\u51b3\u4e86\u5355\u4e00\u5956\u52b1\u6307\u6807\u6216\u4e34\u65f6\u7ec4\u5408\u65e0\u6cd5\u53ef\u9760\u6355\u6349\u6240\u6709\u7ec4\u5408\u6027\u65b9\u9762\u7684\u95ee\u9898\u3002", "result": "\u5728\u4e24\u4e2a\u4e92\u8865\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCARINOX\u5c06\u5e73\u5747\u5bf9\u9f50\u5206\u6570\u63d0\u9ad8\u4e86T2I-CompBench++\u4e0a\u7684+16%\u548cHRS\u57fa\u51c6\u4e0a\u7684+11%\uff0c\u5728\u6240\u6709\u4e3b\u8981\u7c7b\u522b\u4e2d\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4f18\u5316\u548c\u63a2\u7d22\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "conclusion": "CARINOX\u901a\u8fc7\u7edf\u4e00\u4f18\u5316\u548c\u63a2\u7d22\u7b56\u7565\uff0c\u7ed3\u5408\u539f\u5219\u6027\u5956\u52b1\u9009\u62e9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u7ec4\u5408\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.17461", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17461", "abs": "https://arxiv.org/abs/2509.17461", "authors": ["Yuhao Zhang", "Chengjun Zhang", "Di Wu", "Jie Yang", "Mohamad Sawan"], "title": "CSDformer: A Conversion Method for Fully Spike-Driven Transformer", "comment": null, "summary": "Spike-based transformer is a novel architecture aiming to enhance the\nperformance of spiking neural networks while mitigating the energy overhead\ninherent to transformers. However, methods for generating these models suffer\nfrom critical limitations: excessive training costs introduced by direct\ntraining methods, or unavoidably hardware-unfriendly operations in existing\nconversion methods. In this paper, we propose CSDformer, a novel conversion\nmethod for fully spike-driven transformers. We tailor a conversion-oriented\ntransformer-based architecture and propose a new function NReLU to replace\nsoftmax in self-attention. Subsequently, this model is quantized and trained,\nand converted into a fully spike-driven model with temporal decomposition\ntechnique. Also, we propose delayed Integrate-andFire neurons to reduce\nconversion errors and improve the performance of spiking models. We evaluate\nCSDformer on ImageNet, CIFAR-10 and CIFAR-100 datasets and achieve 76.36% top-1\naccuracy under 7 time-steps on ImageNet, demonstrating superiority over\nstate-of-the-art models. Furthermore, CSDformer eliminates the need for\ntraining SNNs, thereby reducing training costs (reducing computational resource\nby 75% and accelerating training speed by 2-3$\\times$). To the best of our\nknowledge, this is the first fully spike-driven transformer-based model\ndeveloped via conversion method, achieving high performance under ultra-low\nlatency, while dramatically reducing both computational complexity and training\noverhead.", "AI": {"tldr": "CSDformer\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5b8c\u5168\u8109\u51b2\u9a71\u52a8\u53d8\u538b\u5668\u8f6c\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7NReLU\u51fd\u6570\u66ff\u6362softmax\u3001\u65f6\u95f4\u5206\u89e3\u6280\u672f\u548c\u5ef6\u8fdf\u79ef\u5206\u70b9\u706b\u795e\u7ecf\u5143\uff0c\u5728\u8d85\u4f4e\u5ef6\u8fdf\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u8109\u51b2\u53d8\u538b\u5668\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u6210\u672c\u8fc7\u9ad8\u6216\u786c\u4ef6\u4e0d\u53cb\u597d\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u53c8\u80fd\u964d\u4f4e\u80fd\u8017\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faCSDformer\u8f6c\u6362\u65b9\u6cd5\uff1a\u8bbe\u8ba1\u8f6c\u6362\u5bfc\u5411\u7684\u53d8\u538b\u5668\u67b6\u6784\uff0c\u7528NReLU\u66ff\u6362softmax\uff0c\u91cf\u5316\u8bad\u7ec3\u540e\u901a\u8fc7\u65f6\u95f4\u5206\u89e3\u6280\u672f\u8f6c\u6362\u4e3a\u5b8c\u5168\u8109\u51b2\u9a71\u52a8\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u5ef6\u8fdf\u79ef\u5206\u70b9\u706b\u795e\u7ecf\u5143\u51cf\u5c11\u8f6c\u6362\u8bef\u5dee\u3002", "result": "\u5728ImageNet\u4e0a\u8fbe\u523076.36% top-1\u51c6\u786e\u7387\uff087\u4e2a\u65f6\u95f4\u6b65\uff09\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u51cf\u5c1175%\u8ba1\u7b97\u8d44\u6e90\u548c2-3\u500d\u8bad\u7ec3\u52a0\u901f\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u901a\u8fc7\u8f6c\u6362\u65b9\u6cd5\u5f00\u53d1\u7684\u5b8c\u5168\u8109\u51b2\u9a71\u52a8\u53d8\u538b\u5668\u6a21\u578b\uff0c\u5728\u8d85\u4f4e\u5ef6\u8fdf\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8bad\u7ec3\u5f00\u9500\u3002"}}
{"id": "2509.17462", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17462", "abs": "https://arxiv.org/abs/2509.17462", "authors": ["Changwon Kang", "Jisong Kim", "Hongjae Shin", "Junseo Park", "Jun Won Choi"], "title": "MAESTRO: Task-Relevant Optimization via Adaptive Feature Enhancement and Suppression for Multi-task 3D Perception", "comment": "Accepted to ICCV 2025", "summary": "The goal of multi-task learning is to learn to conduct multiple tasks\nsimultaneously based on a shared data representation. While this approach can\nimprove learning efficiency, it may also cause performance degradation due to\ntask conflicts that arise when optimizing the model for different objectives.\nTo address this challenge, we introduce MAESTRO, a structured framework\ndesigned to generate task-specific features and mitigate feature interference\nin multi-task 3D perception, including 3D object detection, bird's-eye view\n(BEV) map segmentation, and 3D occupancy prediction. MAESTRO comprises three\ncomponents: the Class-wise Prototype Generator (CPG), the Task-Specific Feature\nGenerator (TSFG), and the Scene Prototype Aggregator (SPA). CPG groups class\ncategories into foreground and background groups and generates group-wise\nprototypes. The foreground and background prototypes are assigned to the 3D\nobject detection task and the map segmentation task, respectively, while both\nare assigned to the 3D occupancy prediction task. TSFG leverages these\nprototype groups to retain task-relevant features while suppressing irrelevant\nfeatures, thereby enhancing the performance for each task. SPA enhances the\nprototype groups assigned for 3D occupancy prediction by utilizing the\ninformation produced by the 3D object detection head and the map segmentation\nhead. Extensive experiments on the nuScenes and Occ3D benchmarks demonstrate\nthat MAESTRO consistently outperforms existing methods across 3D object\ndetection, BEV map segmentation, and 3D occupancy prediction tasks.", "AI": {"tldr": "MAESTRO\u662f\u4e00\u4e2a\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u4efb\u52a13D\u611f\u77e5\u4e2d\u7684\u4efb\u52a1\u51b2\u7a81\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\u548c\u51cf\u8f7b\u7279\u5f81\u5e72\u6270\u6765\u63d0\u9ad83D\u76ee\u6807\u68c0\u6d4b\u3001BEV\u5730\u56fe\u5206\u5272\u548c3D\u5360\u636e\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u4efb\u52a1\u5b66\u4e60\u867d\u7136\u80fd\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\uff0c\u4f46\u53ef\u80fd\u56e0\u4e0d\u540c\u4efb\u52a1\u76ee\u6807\u4e4b\u95f4\u7684\u51b2\u7a81\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u7279\u522b\u662f\u57283D\u611f\u77e5\u4efb\u52a1\u4e2d\uff0c\u4efb\u52a1\u95f4\u7684\u7279\u5f81\u5e72\u6270\u95ee\u9898\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "MAESTRO\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u7c7b\u539f\u578b\u751f\u6210\u5668\uff08CPG\uff09\u5c06\u7c7b\u522b\u5206\u4e3a\u524d\u666f\u548c\u80cc\u666f\u7ec4\u5e76\u751f\u6210\u7ec4\u539f\u578b\uff1b\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\u751f\u6210\u5668\uff08TSFG\uff09\u5229\u7528\u539f\u578b\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff1b\u573a\u666f\u539f\u578b\u805a\u5408\u5668\uff08SPA\uff09\u901a\u8fc7\u5176\u4ed6\u4efb\u52a1\u5934\u7684\u4fe1\u606f\u589e\u5f3a3D\u5360\u636e\u9884\u6d4b\u7684\u539f\u578b\u3002", "result": "\u5728nuScenes\u548cOcc3D\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMAESTRO\u57283D\u76ee\u6807\u68c0\u6d4b\u3001BEV\u5730\u56fe\u5206\u5272\u548c3D\u5360\u636e\u9884\u6d4b\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MAESTRO\u901a\u8fc7\u7ed3\u6784\u5316\u7279\u5f81\u751f\u6210\u548c\u539f\u578b\u7ba1\u7406\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u591a\u4efb\u52a13D\u611f\u77e5\u4e2d\u7684\u7279\u5f81\u5e72\u6270\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5404\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.17476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17476", "abs": "https://arxiv.org/abs/2509.17476", "authors": ["Mallikarjun B. R.", "Fei Yin", "Vikram Voleti", "Nikita Drobyshev", "Maksim Lapin", "Aaryaman Vasishta", "Varun Jampani"], "title": "Stable Video-Driven Portraits", "comment": "https://stable-video-driven-portraits.github.io/", "summary": "Portrait animation aims to generate photo-realistic videos from a single\nsource image by reenacting the expression and pose from a driving video. While\nearly methods relied on 3D morphable models or feature warping techniques, they\noften suffered from limited expressivity, temporal inconsistency, and poor\ngeneralization to unseen identities or large pose variations. Recent advances\nusing diffusion models have demonstrated improved quality but remain\nconstrained by weak control signals and architectural limitations. In this\nwork, we propose a novel diffusion based framework that leverages masked facial\nregions specifically the eyes, nose, and mouth from the driving video as strong\nmotion control cues. To enable robust training without appearance leakage, we\nadopt cross identity supervision. To leverage the strong prior from the\npretrained diffusion model, our novel architecture introduces minimal new\nparameters that converge faster and help in better generalization. We introduce\nspatial temporal attention mechanisms that allow inter frame and intra frame\ninteractions, effectively capturing subtle motions and reducing temporal\nartifacts. Our model uses history frames to ensure continuity across segments.\nAt inference, we propose a novel signal fusion strategy that balances motion\nfidelity with identity preservation. Our approach achieves superior temporal\nconsistency and accurate expression control, enabling high-quality,\ncontrollable portrait animation suitable for real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4eba\u50cf\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u9a71\u52a8\u89c6\u9891\u4e2d\u63a9\u7801\u9762\u90e8\u533a\u57df\uff08\u773c\u775b\u3001\u9f3b\u5b50\u3001\u5634\u5df4\uff09\u4f5c\u4e3a\u5f3a\u8fd0\u52a8\u63a7\u5236\u4fe1\u53f7\uff0c\u7ed3\u5408\u8de8\u8eab\u4efd\u76d1\u7763\u548c\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u4eba\u50cf\u52a8\u753b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8868\u60c5\u8868\u8fbe\u80fd\u529b\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u5bf9\u672a\u89c1\u8eab\u4efd\u6216\u5927\u59ff\u6001\u53d8\u5316\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u6269\u6563\u6a21\u578b\u5728\u5f31\u63a7\u5236\u4fe1\u53f7\u548c\u67b6\u6784\u9650\u5236\u4e0b\u7684\u7ea6\u675f\u3002", "method": "\u91c7\u7528\u63a9\u7801\u9762\u90e8\u533a\u57df\u4f5c\u4e3a\u8fd0\u52a8\u63a7\u5236\u7ebf\u7d22\uff0c\u4f7f\u7528\u8de8\u8eab\u4efd\u76d1\u7763\u9632\u6b62\u5916\u89c2\u6cc4\u9732\uff0c\u5f15\u5165\u6700\u5c0f\u65b0\u53c2\u6570\u7684\u67b6\u6784\u4ee5\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5148\u9a8c\uff0c\u8bbe\u8ba1\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u6355\u83b7\u7ec6\u5fae\u8fd0\u52a8\u5e76\u51cf\u5c11\u65f6\u95f4\u4f2a\u5f71\uff0c\u4f7f\u7528\u5386\u53f2\u5e27\u786e\u4fdd\u8fde\u7eed\u6027\uff0c\u63d0\u51fa\u4fe1\u53f7\u878d\u5408\u7b56\u7565\u5e73\u8861\u8fd0\u52a8\u4fdd\u771f\u5ea6\u548c\u8eab\u4efd\u4fdd\u6301\u3002", "result": "\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u51c6\u786e\u7684\u8868\u60c5\u63a7\u5236\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u4eba\u50cf\u52a8\u753b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5f3a\u8fd0\u52a8\u63a7\u5236\u4fe1\u53f7\u3001\u9ad8\u6548\u67b6\u6784\u8bbe\u8ba1\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u50cf\u52a8\u753b\u7684\u8d28\u91cf\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17481", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17481", "abs": "https://arxiv.org/abs/2509.17481", "authors": ["Xingqi Wang", "Yiming Cui", "Xin Yao", "Shijin Wang", "Guoping Hu", "Xiaoyu Qin"], "title": "ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have recently demonstrated remarkable\nprogress, yet hallucination remains a critical barrier, particularly in chart\nunderstanding, which requires sophisticated perceptual and cognitive abilities\nas well as rigorous factual accuracy. While prior work has investigated\nhallucinations and chart comprehension independently, their intersection\nremains largely unexplored. To address this gap, we present ChartHal, a\nbenchmark that features a fine-grained taxonomy of hallucination scenarios in\nchart understanding, along with a human-validated dataset of 1,062 samples. Our\nevaluation shows that state-of-the-art LVLMs suffer from severe hallucinations\non ChartHal, including proprietary models such as GPT-5 and o4-mini, which\nachieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals\nthat questions involving information absent from or contradictory to charts are\nespecially likely to trigger hallucinations, underscoring the urgent need for\nmore robust mitigation strategies. Code and data are available at\nhttps://github.com/ymcui/ChartHal .", "AI": {"tldr": "ChartHal\u662f\u4e00\u4e2a\u9488\u5bf9\u56fe\u8868\u7406\u89e3\u4e2d\u5e7b\u89c9\u95ee\u9898\u7684\u7ec6\u7c92\u5ea6\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,062\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u6837\u672c\uff0c\u8bc4\u4f30\u663e\u793a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4e2d\u5b58\u5728\u4e25\u91cd\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5e7b\u89c9\u95ee\u9898\u4ecd\u7136\u662f\u5173\u952e\u969c\u788d\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u590d\u6742\u611f\u77e5\u8ba4\u77e5\u80fd\u529b\u548c\u4e25\u683c\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u56fe\u8868\u7406\u89e3\u9886\u57df\u3002\u73b0\u6709\u7814\u7a76\u72ec\u7acb\u63a2\u8ba8\u4e86\u5e7b\u89c9\u548c\u56fe\u8868\u7406\u89e3\uff0c\u4f46\u4e24\u8005\u7684\u4ea4\u53c9\u9886\u57df\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86ChartHal\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u7ec6\u7c92\u5ea6\u7684\u56fe\u8868\u7406\u89e3\u5e7b\u89c9\u573a\u666f\u5206\u7c7b\u548c\u4eba\u5de5\u9a8c\u8bc1\u7684\u6570\u636e\u96c6\uff0c\u5bf9\u5305\u62ecGPT-5\u548co4-mini\u5728\u5185\u7684\u5148\u8fdbLVLMs\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u6700\u5148\u8fdb\u7684LVLMs\u5728ChartHal\u4e0a\u5b58\u5728\u4e25\u91cd\u5e7b\u89c9\uff0cGPT-5\u548co4-mini\u7684\u51c6\u786e\u7387\u5206\u522b\u4ec5\u4e3a34.46%\u548c22.79%\u3002\u6d89\u53ca\u56fe\u8868\u4e2d\u7f3a\u5931\u6216\u77db\u76fe\u4fe1\u606f\u7684\u95ee\u9898\u7279\u522b\u5bb9\u6613\u5f15\u53d1\u5e7b\u89c9\u3002", "conclusion": "\u56fe\u8868\u7406\u89e3\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u4e25\u91cd\uff0c\u8feb\u5207\u9700\u8981\u66f4\u9c81\u68d2\u7684\u7f13\u89e3\u7b56\u7565\u3002ChartHal\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u7814\u7a76\u548c\u6539\u8fdbLVLMs\u5728\u56fe\u8868\u7406\u89e3\u4e2d\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2509.17492", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17492", "abs": "https://arxiv.org/abs/2509.17492", "authors": ["Qinghua Lin", "Guang-Hai Liu", "Zuoyong Li", "Yang Li", "Yuting Jiang", "Xiang Wu"], "title": "Multimodal Medical Image Classification via Synergistic Learning Pre-training", "comment": null, "summary": "Multimodal pathological images are usually in clinical diagnosis, but\ncomputer vision-based multimodal image-assisted diagnosis faces challenges with\nmodality fusion, especially in the absence of expert-annotated data. To achieve\nthe modality fusion in multimodal images with label scarcity, we propose a\nnovel ``pretraining + fine-tuning\" framework for multimodal semi-supervised\nmedical image classification. Specifically, we propose a synergistic learning\npretraining framework of consistency, reconstructive, and aligned learning. By\ntreating one modality as an augmented sample of another modality, we implement\na self-supervised learning pre-train, enhancing the baseline model's feature\nrepresentation capability. Then, we design a fine-tuning method for multimodal\nfusion. During the fine-tuning stage, we set different encoders to extract\nfeatures from the original modalities and provide a multimodal fusion encoder\nfor fusion modality. In addition, we propose a distribution shift method for\nmultimodal fusion features, which alleviates the prediction uncertainty and\noverfitting risks caused by the lack of labeled samples. We conduct extensive\nexperiments on the publicly available gastroscopy image datasets Kvasir and\nKvasirv2. Quantitative and qualitative results demonstrate that the proposed\nmethod outperforms the current state-of-the-art classification methods. The\ncode will be released at: https://github.com/LQH89757/MICS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u9884\u8bad\u7ec3+\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u3001\u91cd\u6784\u548c\u5bf9\u9f50\u5b66\u4e60\u5b9e\u73b0\u6a21\u6001\u878d\u5408\uff0c\u5728\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u75c5\u7406\u56fe\u50cf\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u5e38\u89c1\uff0c\u4f46\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u591a\u6a21\u6001\u56fe\u50cf\u8f85\u52a9\u8bca\u65ad\u9762\u4e34\u6a21\u6001\u878d\u5408\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7f3a\u4e4f\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u534f\u540c\u5b66\u4e60\u9884\u8bad\u7ec3\u6846\u67b6\uff08\u4e00\u81f4\u6027\u3001\u91cd\u6784\u3001\u5bf9\u9f50\u5b66\u4e60\uff09\uff0c\u5c06\u4e00\u79cd\u6a21\u6001\u89c6\u4e3a\u53e6\u4e00\u79cd\u6a21\u6001\u7684\u589e\u5f3a\u6837\u672c\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff1b\u8bbe\u8ba1\u591a\u6a21\u6001\u878d\u5408\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e0d\u540c\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u5206\u5e03\u504f\u79fb\u65b9\u6cd5\u7f13\u89e3\u8fc7\u62df\u5408\u98ce\u9669\u3002", "result": "\u5728\u516c\u5f00\u80c3\u955c\u56fe\u50cf\u6570\u636e\u96c6Kvasir\u548cKvasirv2\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u7ed3\u679c\u4e0a\u90fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5728\u6807\u7b7e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u878d\u5408\u95ee\u9898\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u8f85\u52a9\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17498", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.17498", "abs": "https://arxiv.org/abs/2509.17498", "authors": ["Dilshara Herath", "Chinthaka Abeyrathne", "Prabhani Jayaweera"], "title": "Vision-Based Driver Drowsiness Monitoring: Comparative Analysis of YOLOv5-v11 Models", "comment": "Drowsiness Detection using state of the art YOLO algorithms", "summary": "Driver drowsiness remains a critical factor in road accidents, accounting for\nthousands of fatalities and injuries each year. This paper presents a\ncomprehensive evaluation of real-time, non-intrusive drowsiness detection\nmethods, focusing on computer vision based YOLO (You Look Only Once)\nalgorithms. A publicly available dataset namely, UTA-RLDD was used, containing\nboth awake and drowsy conditions, ensuring variability in gender, eyewear,\nillumination, and skin tone. Seven YOLO variants (v5s, v9c, v9t, v10n, v10l,\nv11n, v11l) are fine-tuned, with performance measured in terms of Precision,\nRecall, mAP0.5, and mAP 0.5-0.95. Among these, YOLOv9c achieved the highest\naccuracy (0.986 mAP 0.5, 0.978 Recall) while YOLOv11n strikes the optimal\nbalance between precision (0.954) and inference efficiency, making it highly\nsuitable for embedded deployment. Additionally, we implement an Eye Aspect\nRatio (EAR) approach using Dlib's facial landmarks, which despite its low\ncomputational footprint exhibits reduced robustness under pose variation and\nocclusions. Our findings illustrate clear trade offs between accuracy, latency,\nand resource requirements, and offer practical guidelines for selecting or\ncombining detection methods in autonomous driving and industrial safety\napplications.", "AI": {"tldr": "\u672c\u6587\u5bf9\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684YOLO\u7b97\u6cd5\u8fdb\u884c\u5b9e\u65f6\u975e\u4fb5\u5165\u5f0f\u9a7e\u9a76\u5458\u75b2\u52b3\u68c0\u6d4b\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e867\u79cdYOLO\u53d8\u4f53\u7684\u6027\u80fd\uff0c\u53d1\u73b0YOLOv9c\u51c6\u786e\u7387\u6700\u9ad8\uff0cYOLOv11n\u5728\u7cbe\u5ea6\u548c\u63a8\u7406\u6548\u7387\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u9a7e\u9a76\u5458\u75b2\u52b3\u662f\u5bfc\u81f4\u9053\u8def\u4e8b\u6545\u7684\u5173\u952e\u56e0\u7d20\uff0c\u6bcf\u5e74\u9020\u6210\u6570\u5343\u4eba\u6b7b\u4ea1\u548c\u4f24\u5bb3\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u5b9e\u65f6\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528UTA-RLDD\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5bf97\u79cdYOLO\u53d8\u4f53\uff08v5s\u3001v9c\u3001v9t\u3001v10n\u3001v10l\u3001v11n\u3001v11l\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u540c\u65f6\u5b9e\u73b0\u57fa\u4e8eDlib\u9762\u90e8\u5173\u952e\u70b9\u7684\u773c\u52a8\u6bd4\uff08EAR\uff09\u65b9\u6cd5\u3002", "result": "YOLOv9c\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u7387\uff08mAP0.5\u4e3a0.986\uff0c\u53ec\u56de\u7387\u4e3a0.978\uff09\uff0cYOLOv11n\u5728\u7cbe\u5ea6\uff080.954\uff09\u548c\u63a8\u7406\u6548\u7387\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002EAR\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5c0f\u4f46\u9c81\u68d2\u6027\u8f83\u5dee\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u8d44\u6e90\u9700\u6c42\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u548c\u5de5\u4e1a\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u68c0\u6d4b\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2509.17500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17500", "abs": "https://arxiv.org/abs/2509.17500", "authors": ["Yujie Xie", "Hongyang Zhang", "Zhihui Liu", "Shihai Ruan"], "title": "SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge", "comment": null, "summary": "Large-scale Video Object Segmentation (LSVOS) addresses the challenge of\naccurately tracking and segmenting objects in long video sequences, where\ndifficulties stem from object reappearance, small-scale targets, heavy\nocclusions, and crowded scenes. Existing approaches predominantly adopt\nSAM2-based frameworks with various memory mechanisms for complex video mask\ngeneration. In this report, we proposed Segment Anything with Memory\nStrengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE\ntrack of ICCV 2025, which integrates the strengths of stateof-the-art VOS\nmodels into an effective paradigm. To handle visually similar instances and\nlong-term object disappearance in MOSE, we incorporate a long-term memorymodule\nfor reliable object re-identification. Additionly, we adopt SAM2Long as a\npost-processing strategy to reduce error accumulation and enhance segmentation\nstability in long video sequences. Our method achieved a final performance of\n0.8427 in terms of J &F in the test-set leaderboard.", "AI": {"tldr": "SAMSON\u662fICCV 2025 MOSE\u8d5b\u9053\u7b2c\u4e09\u540d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u96c6\u6210\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\u548cSAM2Long\u540e\u5904\u7406\u7b56\u7565\uff0c\u6709\u6548\u5904\u7406\u957f\u89c6\u9891\u5e8f\u5217\u4e2d\u7684\u76ee\u6807\u8ddf\u8e2a\u548c\u5206\u5272\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u957f\u89c6\u9891\u76ee\u6807\u5206\u5272\u4e2d\u7684\u76ee\u6807\u91cd\u73b0\u3001\u5c0f\u76ee\u6807\u3001\u4e25\u91cd\u906e\u6321\u548c\u62e5\u6324\u573a\u666f\u7b49\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8eSAM2\u6846\u67b6\u4f46\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faSegment Anything with Memory Strengthened Object Navigation (SAMSON)\uff0c\u96c6\u6210\u6700\u5148\u8fdbVOS\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5305\u542b\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\u7528\u4e8e\u76ee\u6807\u91cd\u8bc6\u522b\uff0c\u91c7\u7528SAM2Long\u4f5c\u4e3a\u540e\u5904\u7406\u7b56\u7565\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "\u5728\u6d4b\u8bd5\u96c6\u6392\u884c\u699c\u4e0a\u53d6\u5f97\u4e860.8427\u7684J&F\u5206\u6570\u3002", "conclusion": "SAMSON\u901a\u8fc7\u8bb0\u5fc6\u589e\u5f3a\u7684\u76ee\u6807\u5bfc\u822a\u673a\u5236\uff0c\u5728\u590d\u6742\u7684\u957f\u89c6\u9891\u76ee\u6807\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.17506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17506", "abs": "https://arxiv.org/abs/2509.17506", "authors": ["Houqiang Zhong", "Zihan Zheng", "Qiang Hu", "Yuan Tian", "Ning Cao", "Lan Xu", "Xiaoyun Zhang", "Zhengxue Cheng", "Li Song", "Wenjun Zhang"], "title": "4D-MoDe: Towards Editable and Scalable Volumetric Streaming via Motion-Decoupled 4D Gaussian Compression", "comment": null, "summary": "Volumetric video has emerged as a key medium for immersive telepresence and\naugmented/virtual reality, enabling six-degrees-of-freedom (6DoF) navigation\nand realistic spatial interactions. However, delivering high-quality dynamic\nvolumetric content at scale remains challenging due to massive data volume,\ncomplex motion, and limited editability of existing representations. In this\npaper, we present 4D-MoDe, a motion-decoupled 4D Gaussian compression framework\ndesigned for scalable and editable volumetric video streaming. Our method\nintroduces a layered representation that explicitly separates static\nbackgrounds from dynamic foregrounds using a lookahead-based motion\ndecomposition strategy, significantly reducing temporal redundancy and enabling\nselective background/foreground streaming. To capture continuous motion\ntrajectories, we employ a multi-resolution motion estimation grid and a\nlightweight shared MLP, complemented by a dynamic Gaussian compensation\nmechanism to model emergent content. An adaptive grouping scheme dynamically\ninserts background keyframes to balance temporal consistency and compression\nefficiency. Furthermore, an entropy-aware training pipeline jointly optimizes\nthe motion fields and Gaussian parameters under a rate-distortion (RD)\nobjective, while employing range-based and KD-tree compression to minimize\nstorage overhead. Extensive experiments on multiple datasets demonstrate that\n4D-MoDe consistently achieves competitive reconstruction quality with an order\nof magnitude lower storage cost (e.g., as low as \\textbf{11.4} KB/frame)\ncompared to state-of-the-art methods, while supporting practical applications\nsuch as background replacement and foreground-only streaming.", "AI": {"tldr": "4D-MoDe\u662f\u4e00\u79cd\u8fd0\u52a8\u89e3\u8026\u76844D\u9ad8\u65af\u538b\u7f29\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u6269\u5c55\u548c\u53ef\u7f16\u8f91\u7684\u4f53\u79ef\u89c6\u9891\u6d41\u4f20\u8f93\uff0c\u901a\u8fc7\u5206\u5c42\u8868\u793a\u548c\u524d\u77bb\u6027\u8fd0\u52a8\u5206\u89e3\u663e\u8457\u964d\u4f4e\u5b58\u50a8\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u4f53\u79ef\u89c6\u9891\u57286DoF\u5bfc\u822a\u548c\u7a7a\u95f4\u4ea4\u4e92\u4e2d\u7684\u5927\u89c4\u6a21\u4f20\u8f93\u6311\u6218\uff0c\u73b0\u6709\u8868\u793a\u65b9\u6cd5\u6570\u636e\u91cf\u5927\u3001\u8fd0\u52a8\u590d\u6742\u4e14\u7f16\u8f91\u6027\u6709\u9650\u3002", "method": "\u91c7\u7528\u5206\u5c42\u8868\u793a\u5206\u79bb\u9759\u6001\u80cc\u666f\u548c\u52a8\u6001\u524d\u666f\uff0c\u4f7f\u7528\u591a\u5206\u8fa8\u7387\u8fd0\u52a8\u4f30\u8ba1\u7f51\u683c\u548c\u8f7b\u91cf\u7ea7\u5171\u4eabMLP\u6355\u6349\u8fde\u7eed\u8fd0\u52a8\u8f68\u8ff9\uff0c\u7ed3\u5408\u52a8\u6001\u9ad8\u65af\u8865\u507f\u673a\u5236\u548c\u81ea\u9002\u5e94\u5206\u7ec4\u65b9\u6848\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u7ade\u4e89\u6027\u91cd\u5efa\u8d28\u91cf\uff0c\u5b58\u50a8\u6210\u672c\u6bd4\u73b0\u6709\u65b9\u6cd5\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\uff08\u6700\u4f4e11.4KB/\u5e27\uff09\uff0c\u652f\u6301\u80cc\u666f\u66ff\u6362\u548c\u4ec5\u524d\u666f\u6d41\u4f20\u8f93\u7b49\u5e94\u7528\u3002", "conclusion": "4D-MoDe\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5b58\u50a8\u9700\u6c42\uff0c\u4e3a\u53ef\u6269\u5c55\u548c\u53ef\u7f16\u8f91\u7684\u4f53\u79ef\u89c6\u9891\u6d41\u4f20\u8f93\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17513", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17513", "abs": "https://arxiv.org/abs/2509.17513", "authors": ["Zihan Zheng", "Zhenlong Wu", "Houqiang Zhong", "Yuan Tian", "Ning Cao", "Lan Xu", "Jiangchao Yao", "Xiaoyun Zhang", "Qiang Hu", "Wenjun Zhang"], "title": "4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming", "comment": "NeurIPS 2025", "summary": "Achieving seamless viewing of high-fidelity volumetric video, comparable to\n2D video experiences, remains an open challenge. Existing volumetric video\ncompression methods either lack the flexibility to adjust quality and bitrate\nwithin a single model for efficient streaming across diverse networks and\ndevices, or struggle with real-time decoding and rendering on lightweight\nmobile platforms. To address these challenges, we introduce 4DGCPro, a novel\nhierarchical 4D Gaussian compression framework that facilitates real-time\nmobile decoding and high-quality rendering via progressive volumetric video\nstreaming in a single bitstream. Specifically, we propose a\nperceptually-weighted and compression-friendly hierarchical 4D Gaussian\nrepresentation with motion-aware adaptive grouping to reduce temporal\nredundancy, preserve coherence, and enable scalable multi-level detail\nstreaming. Furthermore, we present an end-to-end entropy-optimized training\nscheme, which incorporates layer-wise rate-distortion (RD) supervision and\nattribute-specific entropy modeling for efficient bitstream generation.\nExtensive experiments show that 4DGCPro enables flexible quality and multiple\nbitrate within a single model, achieving real-time decoding and rendering on\nmobile devices while outperforming existing methods in RD performance across\nmultiple datasets. Project Page: https://mediax-sjtu.github.io/4DGCPro", "AI": {"tldr": "4DGCPro\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5206\u5c424D\u9ad8\u65af\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u4f53\u79ef\u89c6\u9891\u6d41\u5b9e\u73b0\u5b9e\u65f6\u79fb\u52a8\u89e3\u7801\u548c\u9ad8\u8d28\u91cf\u6e32\u67d3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\u3001\u65e0\u6cd5\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u8c03\u6574\u8d28\u91cf\u548c\u6bd4\u7279\u7387\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4f53\u79ef\u89c6\u9891\u538b\u7f29\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u8c03\u6574\u8d28\u91cf\u548c\u6bd4\u7279\u7387\u7684\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u7f51\u7edc\u548c\u8bbe\u5907\u7684\u9ad8\u6548\u6d41\u5a92\u4f53\u4f20\u8f93\uff0c\u8981\u4e48\u5728\u8f7b\u91cf\u7ea7\u79fb\u52a8\u5e73\u53f0\u4e0a\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u89e3\u7801\u548c\u6e32\u67d3\u3002", "method": "\u63d0\u51fa\u4e86\u611f\u77e5\u52a0\u6743\u548c\u538b\u7f29\u53cb\u597d\u7684\u5206\u5c424D\u9ad8\u65af\u8868\u793a\uff0c\u91c7\u7528\u8fd0\u52a8\u611f\u77e5\u81ea\u9002\u5e94\u5206\u7ec4\u51cf\u5c11\u65f6\u95f4\u5197\u4f59\u3001\u4fdd\u6301\u8fde\u8d2f\u6027\uff0c\u5e76\u652f\u6301\u53ef\u6269\u5c55\u7684\u591a\u7ea7\u7ec6\u8282\u6d41\u4f20\u8f93\uff1b\u540c\u65f6\u8bbe\u8ba1\u4e86\u7aef\u5230\u7aef\u71b5\u4f18\u5316\u8bad\u7ec3\u65b9\u6848\uff0c\u5305\u542b\u5206\u5c42\u7387\u5931\u771f\u76d1\u7763\u548c\u5c5e\u6027\u7279\u5b9a\u71b5\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660e4DGCPro\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u8d28\u91cf\u548c\u591a\u79cd\u6bd4\u7279\u7387\u8c03\u6574\uff0c\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u89e3\u7801\u548c\u6e32\u67d3\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u7387\u5931\u771f\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "4DGCPro\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4f53\u79ef\u89c6\u9891\u538b\u7f29\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u9ad8\u8d28\u91cf\u4f53\u79ef\u89c6\u9891\u7684\u5b9e\u65f6\u6d41\u5a92\u4f53\u4f20\u8f93\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17520", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17520", "abs": "https://arxiv.org/abs/2509.17520", "authors": ["Mingda Zhang", "Yuyang Zheng", "Ruixiang Tang", "Jingru Qiu", "Haiyan Ding"], "title": "Unified Multimodal Coherent Field: Synchronous Semantic-Spatial-Vision Fusion for Brain Tumor Segmentation", "comment": "8 pages, 3 figures", "summary": "Brain tumor segmentation requires accurate identification of hierarchical\nregions including whole tumor (WT), tumor core (TC), and enhancing tumor (ET)\nfrom multi-sequence magnetic resonance imaging (MRI) images. Due to tumor\ntissue heterogeneity, ambiguous boundaries, and contrast variations across MRI\nsequences, methods relying solely on visual information or post-hoc loss\nconstraints show unstable performance in boundary delineation and hierarchy\npreservation. To address this challenge, we propose the Unified Multimodal\nCoherent Field (UMCF) method. This method achieves synchronous interactive\nfusion of visual, semantic, and spatial information within a unified 3D latent\nspace, adaptively adjusting modal contributions through parameter-free\nuncertainty gating, with medical prior knowledge directly participating in\nattention computation, avoiding the traditional \"process-then-concatenate\"\nseparated architecture. On Brain Tumor Segmentation (BraTS) 2020 and 2021\ndatasets, UMCF+nnU-Net achieves average Dice coefficients of 0.8579 and 0.8977\nrespectively, with an average 4.18% improvement across mainstream\narchitectures. By deeply integrating clinical knowledge with imaging features,\nUMCF provides a new technical pathway for multimodal information fusion in\nprecision medicine.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u591a\u6a21\u6001\u76f8\u5e72\u573a\uff08UMCF\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u8111\u80bf\u7624\u5206\u5272\uff0c\u901a\u8fc7\u540c\u6b65\u878d\u5408\u89c6\u89c9\u3001\u8bed\u4e49\u548c\u7a7a\u95f4\u4fe1\u606f\u6765\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u8fb9\u754c\u63cf\u7ed8\u548c\u5c42\u6b21\u7ed3\u6784\u4fdd\u6301\u65b9\u9762\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u8111\u80bf\u7624\u5206\u5272\u9700\u8981\u51c6\u786e\u8bc6\u522b\u5c42\u6b21\u533a\u57df\uff08\u5168\u80bf\u7624\u3001\u80bf\u7624\u6838\u5fc3\u3001\u589e\u5f3a\u80bf\u7624\uff09\uff0c\u4f46\u7531\u4e8e\u80bf\u7624\u7ec4\u7ec7\u5f02\u8d28\u6027\u3001\u8fb9\u754c\u6a21\u7cca\u548cMRI\u5e8f\u5217\u5bf9\u6bd4\u5ea6\u53d8\u5316\uff0c\u4ec5\u4f9d\u8d56\u89c6\u89c9\u4fe1\u606f\u6216\u540e\u5904\u7406\u635f\u5931\u7ea6\u675f\u7684\u65b9\u6cd5\u5728\u8fb9\u754c\u63cf\u7ed8\u548c\u5c42\u6b21\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002", "method": "UMCF\u65b9\u6cd5\u5728\u7edf\u4e00\u76843D\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u89c6\u89c9\u3001\u8bed\u4e49\u548c\u7a7a\u95f4\u4fe1\u606f\u7684\u540c\u6b65\u4ea4\u4e92\u878d\u5408\uff0c\u901a\u8fc7\u65e0\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u95e8\u63a7\u81ea\u9002\u5e94\u8c03\u6574\u6a21\u6001\u8d21\u732e\uff0c\u533b\u5b66\u5148\u9a8c\u77e5\u8bc6\u76f4\u63a5\u53c2\u4e0e\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u7684\"\u5148\u5904\u7406\u518d\u62fc\u63a5\"\u5206\u79bb\u67b6\u6784\u3002", "result": "\u5728BraTS 2020\u548c2021\u6570\u636e\u96c6\u4e0a\uff0cUMCF+nnU-Net\u5206\u522b\u5b9e\u73b0\u4e860.8579\u548c0.8977\u7684\u5e73\u5747Dice\u7cfb\u6570\uff0c\u76f8\u6bd4\u4e3b\u6d41\u67b6\u6784\u5e73\u5747\u63d0\u5347\u4e864.18%\u3002", "conclusion": "\u901a\u8fc7\u6df1\u5ea6\u6574\u5408\u4e34\u5e8a\u77e5\u8bc6\u4e0e\u5f71\u50cf\u7279\u5f81\uff0cUMCF\u4e3a\u7cbe\u51c6\u533b\u5b66\u4e2d\u7684\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2509.17522", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17522", "abs": "https://arxiv.org/abs/2509.17522", "authors": ["Hangzhou He", "Lei Zhu", "Kaiwen Li", "Xinliang Zhang", "Jiakui Hu", "Ourui Fu", "Zhengjian Yao", "Yanye Lu"], "title": "Chat-CBM: Towards Interactive Concept Bottleneck Models with Frozen Large Language Models", "comment": null, "summary": "Concept Bottleneck Models (CBMs) provide inherent interpretability by first\npredicting a set of human-understandable concepts and then mapping them to\nlabels through a simple classifier. While users can intervene in the concept\nspace to improve predictions, traditional CBMs typically employ a fixed linear\nclassifier over concept scores, which restricts interventions to manual value\nadjustments and prevents the incorporation of new concepts or domain knowledge\nat test time. These limitations are particularly severe in unsupervised CBMs,\nwhere concept activations are often noisy and densely activated, making user\ninterventions ineffective. We introduce Chat-CBM, which replaces score-based\nclassifiers with a language-based classifier that reasons directly over concept\nsemantics. By grounding prediction in the semantic space of concepts, Chat-CBM\npreserves the interpretability of CBMs while enabling richer and more intuitive\ninterventions, such as concept correction, addition or removal of concepts,\nincorporation of external knowledge, and high-level reasoning guidance.\nLeveraging the language understanding and few-shot capabilities of frozen large\nlanguage models, Chat-CBM extends the intervention interface of CBMs beyond\nnumerical editing and remains effective even in unsupervised settings.\nExperiments on nine datasets demonstrate that Chat-CBM achieves higher\npredictive performance and substantially improves user interactivity while\nmaintaining the concept-based interpretability of CBMs.", "AI": {"tldr": "Chat-CBM\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u66ff\u4ee3\u4f20\u7edf\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u4e2d\u7684\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u5728\u6982\u5ff5\u8bed\u4e49\u7a7a\u95f4\u8fdb\u884c\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u66f4\u4e30\u5bcc\u76f4\u89c2\u7684\u7528\u6237\u5e72\u9884\u65b9\u5f0f\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u4f7f\u7528\u56fa\u5b9a\u7684\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u9650\u5236\u4e86\u7528\u6237\u5e72\u9884\u53ea\u80fd\u624b\u52a8\u8c03\u6574\u6570\u503c\uff0c\u65e0\u6cd5\u5728\u6d4b\u8bd5\u65f6\u5f15\u5165\u65b0\u6982\u5ff5\u6216\u9886\u57df\u77e5\u8bc6\uff0c\u7279\u522b\u662f\u5728\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\u6982\u5ff5\u6fc0\u6d3b\u566a\u58f0\u5927\u3001\u5e72\u9884\u6548\u679c\u5dee\u3002", "method": "\u7528\u57fa\u4e8e\u8bed\u8a00\u7684\u5206\u7c7b\u5668\u66ff\u4ee3\u57fa\u4e8e\u5206\u6570\u7684\u5206\u7c7b\u5668\uff0c\u76f4\u63a5\u5728\u6982\u5ff5\u8bed\u4e49\u7a7a\u95f4\u8fdb\u884c\u63a8\u7406\uff0c\u5229\u7528\u51bb\u7ed3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u7406\u89e3\u548c\u5c11\u6837\u672c\u80fd\u529b\u3002", "result": "\u5728\u4e5d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cChat-CBM\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u663e\u8457\u6539\u5584\u4e86\u7528\u6237\u4ea4\u4e92\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "Chat-CBM\u901a\u8fc7\u5c06\u9884\u6d4b\u5efa\u7acb\u5728\u6982\u5ff5\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff0c\u6269\u5c55\u4e86\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u7684\u5e72\u9884\u754c\u9762\uff0c\u4f7f\u5176\u8d85\u8d8a\u6570\u503c\u7f16\u8f91\uff0c\u5728\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\u4ecd\u4fdd\u6301\u6709\u6548\u6027\u3002"}}
{"id": "2509.17537", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17537", "abs": "https://arxiv.org/abs/2509.17537", "authors": ["Dian Jin", "Yanghao Zhou", "Jinxing Zhou", "Jiaqi Ma", "Ruohao Guo", "Dan Guo"], "title": "SimToken: A Simple Baseline for Referring Audio-Visual Segmentation", "comment": null, "summary": "Referring Audio-Visual Segmentation (Ref-AVS) aims to segment specific\nobjects in videos based on natural language expressions involving audio,\nvision, and text information. This task poses significant challenges in\ncross-modal reasoning and fine-grained object localization. In this paper, we\npropose a simple framework, SimToken, that integrates a multimodal large\nlanguage model (MLLM) with the Segment Anything Model (SAM). The MLLM is guided\nto generate a special semantic token representing the referred object. This\ncompact token, enriched with contextual information from all modalities, acts\nas a prompt to guide SAM to segment objectsacross video frames. To further\nimprove semantic learning, we introduce a novel target-consistent semantic\nalignment loss that aligns token embeddings from different expressions but\nreferring to the same object. Experiments on the Ref-AVS benchmark demonstrate\nthat our approach achieves superior performance compared to existing\nmethods.Code will be available at https://github.com/DianJin-HFUT/SimToken", "AI": {"tldr": "SimToken\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548cSegment Anything Model\u6765\u89e3\u51b3\u97f3\u9891-\u89c6\u89c9\u5206\u5272\u4efb\u52a1\uff0c\u5229\u7528\u7279\u6b8a\u8bed\u4e49token\u6307\u5bfc\u89c6\u9891\u5bf9\u8c61\u5206\u5272", "motivation": "\u89e3\u51b3Referring Audio-Visual Segmentation\u4efb\u52a1\u4e2d\u7684\u8de8\u6a21\u6001\u63a8\u7406\u548c\u7ec6\u7c92\u5ea6\u5bf9\u8c61\u5b9a\u4f4d\u6311\u6218", "method": "\u4f7f\u7528MLLM\u751f\u6210\u4ee3\u8868\u76ee\u6807\u5bf9\u8c61\u7684\u7279\u6b8a\u8bed\u4e49token\uff0c\u4f5c\u4e3aSAM\u7684\u63d0\u793a\u6765\u5206\u5272\u89c6\u9891\u5e27\u4e2d\u7684\u5bf9\u8c61\uff0c\u5e76\u5f15\u5165\u76ee\u6807\u4e00\u81f4\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\u6765\u6539\u8fdb\u8bed\u4e49\u5b66\u4e60", "result": "\u5728Ref-AVS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd", "conclusion": "SimToken\u6846\u67b6\u901a\u8fc7\u7b80\u5355\u7684token\u96c6\u6210\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u7684\u8de8\u6a21\u6001\u5206\u5272\u95ee\u9898"}}
{"id": "2509.17561", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17561", "abs": "https://arxiv.org/abs/2509.17561", "authors": ["Edwine Nabahirwa", "Wei Song", "Minghua Zhang", "Shufan Chen"], "title": "An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection", "comment": "28 Pages, 12 Figures", "summary": "Underwater object detection (UOD) remains a critical challenge in computer\nvision due to underwater distortions which degrade low-level features and\ncompromise the reliability of even state-of-the-art detectors. While YOLO\nmodels have become the backbone of real-time object detection, little work has\nsystematically examined their robustness under these uniquely challenging\nconditions. This raises a critical question: Are YOLO models genuinely robust\nwhen operating under the chaotic and unpredictable conditions of underwater\nenvironments? In this study, we present one of the first comprehensive\nevaluations of recent YOLO variants (YOLOv8-YOLOv12) across six simulated\nunderwater environments. Using a unified dataset of 10,000 annotated images\nfrom DUO and Roboflow100, we not only benchmark model robustness but also\nanalyze how distortions affect key low-level features such as texture, edges,\nand color. Our findings show that (1) YOLOv12 delivers the strongest overall\nperformance but is highly vulnerable to noise, and (2) noise disrupts edge and\ntexture features, explaining the poor detection performance in noisy images.\nClass imbalance is a persistent challenge in UOD. Experiments revealed that (3)\nimage counts and instance frequency primarily drive detection performance,\nwhile object appearance exerts only a secondary influence. Finally, we\nevaluated lightweight training-aware strategies: noise-aware sample injection,\nwhich improves robustness in both noisy and real-world conditions, and\nfine-tuning with advanced enhancement, which boosts accuracy in enhanced\ndomains but slightly lowers performance in original data, demonstrating strong\npotential for domain adaptation, respectively. Together, these insights provide\npractical guidance for building resilient and cost-efficient UOD systems.", "AI": {"tldr": "\u672c\u6587\u5bf9YOLO\u7cfb\u5217\u6a21\u578b\u5728\u6c34\u4e0b\u7269\u4f53\u68c0\u6d4b\u4e2d\u7684\u9c81\u68d2\u6027\u8fdb\u884c\u4e86\u9996\u6b21\u5168\u9762\u8bc4\u4f30\uff0c\u53d1\u73b0\u5728\u6c34\u4e0b\u5931\u771f\u73af\u5883\u4e0bYOLOv12\u8868\u73b0\u6700\u4f73\u4f46\u6613\u53d7\u566a\u58f0\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u6c34\u4e0b\u7269\u4f53\u68c0\u6d4b\u9762\u4e34\u4e25\u91cd\u6311\u6218\uff0c\u56e0\u4e3a\u6c34\u4e0b\u5931\u771f\u4f1a\u964d\u4f4e\u4f4e\u7ea7\u7279\u5f81\u8d28\u91cf\uff0c\u5f71\u54cd\u6700\u5148\u8fdb\u68c0\u6d4b\u5668\u7684\u53ef\u9760\u6027\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9YOLO\u6a21\u578b\u5728\u8fd9\u79cd\u72ec\u7279\u6311\u6218\u6761\u4ef6\u4e0b\u9c81\u68d2\u6027\u7684\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u7edf\u4e00\u6570\u636e\u96c6\uff0810,000\u5f20\u6807\u6ce8\u56fe\u50cf\uff09\u5728\u516d\u4e2a\u6a21\u62df\u6c34\u4e0b\u73af\u5883\u4e2d\u8bc4\u4f30YOLOv8-YOLOv12\u53d8\u4f53\uff0c\u5206\u6790\u5931\u771f\u5bf9\u7eb9\u7406\u3001\u8fb9\u7f18\u548c\u989c\u8272\u7b49\u5173\u952e\u4f4e\u7ea7\u7279\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u566a\u58f0\u611f\u77e5\u6837\u672c\u6ce8\u5165\u548c\u9ad8\u7ea7\u589e\u5f3a\u5fae\u8c03\u7b49\u8f7b\u91cf\u7ea7\u8bad\u7ec3\u7b56\u7565\u3002", "result": "YOLOv12\u6574\u4f53\u6027\u80fd\u6700\u5f3a\u4f46\u5bf9\u566a\u58f0\u9ad8\u5ea6\u654f\u611f\uff1b\u566a\u58f0\u4f1a\u7834\u574f\u8fb9\u7f18\u548c\u7eb9\u7406\u7279\u5f81\uff1b\u56fe\u50cf\u6570\u91cf\u548c\u5b9e\u4f8b\u9891\u7387\u662f\u68c0\u6d4b\u6027\u80fd\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff1b\u566a\u58f0\u611f\u77e5\u6837\u672c\u6ce8\u5165\u80fd\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u9ad8\u7ea7\u589e\u5f3a\u5fae\u8c03\u5728\u589e\u5f3a\u57df\u4e2d\u63d0\u5347\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u5f39\u6027\u548c\u6210\u672c\u6548\u76ca\u9ad8\u7684\u6c34\u4e0b\u7269\u4f53\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u8bad\u7ec3\u7b56\u7565\u5728\u9886\u57df\u9002\u5e94\u65b9\u9762\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.17562", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17562", "abs": "https://arxiv.org/abs/2509.17562", "authors": ["Yuxuan Li", "Yicheng Zhang", "Wenhao Tang", "Yimian Dai", "Ming-Ming Cheng", "Xiang Li", "Jian Yang"], "title": "Visual Instruction Pretraining for Domain-Specific Foundation Models", "comment": null, "summary": "Modern computer vision is converging on a closed loop in which perception,\nreasoning and generation mutually reinforce each other. However, this loop\nremains incomplete: the top-down influence of high-level reasoning on the\nfoundational learning of low-level perceptual features is not yet\nunderexplored. This paper addresses this gap by proposing a new paradigm for\npretraining foundation models in downstream domains. We introduce Visual\ninsTruction Pretraining (ViTP), a novel approach that directly leverages\nreasoning to enhance perception. ViTP embeds a Vision Transformer (ViT)\nbackbone within a Vision-Language Model and pretrains it end-to-end using a\nrich corpus of visual instruction data curated from target downstream domains.\nViTP is powered by our proposed Visual Robustness Learning (VRL), which compels\nthe ViT to learn robust and domain-relevant features from a sparse set of\nvisual tokens. Extensive experiments on 16 challenging remote sensing and\nmedical imaging benchmarks demonstrate that ViTP establishes new\nstate-of-the-art performance across a diverse range of downstream tasks. The\ncode is available at github.com/zcablii/ViTP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faViTP\uff08Visual insTruction Pretraining\uff09\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5d4c\u5165ViT\u9aa8\u5e72\u7f51\u7edc\uff0c\u5229\u7528\u89c6\u89c9\u6307\u4ee4\u6570\u636e\u7aef\u5230\u7aef\u9884\u8bad\u7ec3\uff0c\u589e\u5f3a\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3\u8ba1\u7b97\u673a\u89c6\u89c9\u5f62\u6210\u4e86\u611f\u77e5\u3001\u63a8\u7406\u548c\u751f\u6210\u7684\u95ed\u73af\uff0c\u4f46\u9ad8\u5c42\u63a8\u7406\u5bf9\u5e95\u5c42\u611f\u77e5\u7279\u5f81\u5b66\u4e60\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "ViTP\u5c06Vision Transformer\u5d4c\u5165\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u76ee\u6807\u4e0b\u6e38\u9886\u57df\u7684\u89c6\u89c9\u6307\u4ee4\u6570\u636e\u8fdb\u884c\u7aef\u5230\u7aef\u9884\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528Visual Robustness Learning\uff08VRL\uff09\u65b9\u6cd5\u4ece\u7a00\u758f\u89c6\u89c9\u6807\u8bb0\u4e2d\u5b66\u4e60\u9c81\u68d2\u7279\u5f81\u3002", "result": "\u572816\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u9065\u611f\u5f71\u50cf\u548c\u533b\u5b66\u5f71\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cViTP\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ViTP\u901a\u8fc7\u5c06\u63a8\u7406\u76f4\u63a5\u878d\u5165\u611f\u77e5\u9884\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4e3a\u4e0b\u6e38\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.17566", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17566", "abs": "https://arxiv.org/abs/2509.17566", "authors": ["Ding Shaodong", "Liu Ziyang", "Zhou Yijun", "Liu Tao"], "title": "MRN: Harnessing 2D Vision Foundation Models for Diagnosing Parkinson's Disease with Limited 3D MR Data", "comment": "First-place solution of the classification track for MICCAI'2025\n  PDCADxFoundation Challenge", "summary": "The automatic diagnosis of Parkinson's disease is in high clinical demand due\nto its prevalence and the importance of targeted treatment. Current clinical\npractice often relies on diagnostic biomarkers in QSM and NM-MRI images.\nHowever, the lack of large, high-quality datasets makes training diagnostic\nmodels from scratch prone to overfitting. Adapting pre-trained 3D medical\nmodels is also challenging, as the diversity of medical imaging leads to\nmismatches in voxel spacing and modality between pre-training and fine-tuning\ndata. In this paper, we address these challenges by leveraging 2D vision\nfoundation models (VFMs). Specifically, we crop multiple key ROIs from NM and\nQSM images, process each ROI through separate branches to compress the ROI into\na token, and then combine these tokens into a unified patient representation\nfor classification. Within each branch, we use 2D VFMs to encode axial slices\nof the 3D ROI volume and fuse them into the ROI token, guided by an auxiliary\nsegmentation head that steers the feature extraction toward specific brain\nnuclei. Additionally, we introduce multi-ROI supervised contrastive learning,\nwhich improves diagnostic performance by pulling together representations of\npatients from the same class while pushing away those from different classes.\nOur approach achieved first place in the MICCAI 2025 PDCADxFoundation\nchallenge, with an accuracy of 86.0% trained on a dataset of only 300 labeled\nQSM and NM-MRI scans, outperforming the second-place method by 5.5%.These\nresults highlight the potential of 2D VFMs for clinical analysis of 3D MR\nimages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u75282D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5e15\u91d1\u68ee\u75c5\u81ea\u52a8\u8bca\u65ad\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591aROI\u5904\u7406\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8686.0%\u7684\u51c6\u786e\u7387\uff0c\u83b7\u5f97MICCAI 2025\u6311\u6218\u8d5b\u7b2c\u4e00\u540d\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\u7684\u81ea\u52a8\u8bca\u65ad\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u91cf\u5c0f\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u4ee5\u53ca3D\u533b\u5b66\u6a21\u578b\u9884\u8bad\u7ec3\u4e0e\u5fae\u8c03\u65f6\u7684\u6a21\u6001\u4e0d\u5339\u914d\u6311\u6218\u3002", "method": "\u4eceNM\u548cQSM\u56fe\u50cf\u4e2d\u88c1\u526a\u5173\u952eROI\uff0c\u4f7f\u75282D VFMs\u7f16\u78013D ROI\u7684\u8f74\u5411\u5207\u7247\uff0c\u901a\u8fc7\u8f85\u52a9\u5206\u5272\u5934\u5f15\u5bfc\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5f15\u5165\u591aROI\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6765\u63d0\u5347\u8bca\u65ad\u6027\u80fd\u3002", "result": "\u5728\u4ec5300\u4e2a\u6807\u6ce8QSM\u548cNM-MRI\u626b\u63cf\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u8fbe\u5230\u4e8686.0%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u7b2c\u4e8c\u540d\u65b9\u6cd5\u9ad8\u51fa5.5%\uff0c\u83b7\u5f97MICCAI 2025\u6311\u6218\u8d5b\u7b2c\u4e00\u540d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e862D VFMs\u57283D MR\u56fe\u50cf\u4e34\u5e8a\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5c0f\u6837\u672c\u533b\u5b66\u56fe\u50cf\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17581", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17581", "abs": "https://arxiv.org/abs/2509.17581", "authors": ["Florinel Alin Croitoru", "Vlad Hondru", "Radu Tudor Ionescu"], "title": "PRNU-Bench: A Novel Benchmark and Model for PRNU-Based Camera Identification", "comment": null, "summary": "We propose a novel benchmark for camera identification via Photo Response\nNon-Uniformity (PRNU) estimation. The benchmark comprises 13K photos taken with\n120+ cameras, where the training and test photos are taken in different\nscenarios, enabling ``in-the-wild'' evaluation. In addition, we propose a novel\nPRNU-based camera identification model that employs a hybrid architecture,\ncomprising a denoising autoencoder to estimate the PRNU signal and a\nconvolutional network that can perform 1:N verification of camera devices.\nInstead of using a conventional approach based on contrastive learning, our\nmethod takes the Hadamard product between reference and query PRNU signals as\ninput. This novel design leads to significantly better results compared with\nstate-of-the-art models based on denoising autoencoders and contrastive\nlearning. We release our dataset and code at:\nhttps://github.com/CroitoruAlin/PRNU-Bench.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u76f8\u673a\u8bc6\u522b\u7684PRNU\u4f30\u8ba1\u65b0\u57fa\u51c6\uff0c\u5305\u542b13K\u5f20\u7167\u7247\u548c120+\u76f8\u673a\uff0c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8fdb\u884c\uff0c\u5b9e\u73b0\"\u91ce\u5916\"\u8bc4\u4f30\u3002\u540c\u65f6\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8ePRNU\u7684\u6df7\u5408\u67b6\u6784\u76f8\u673a\u8bc6\u522b\u6a21\u578b\uff0c\u4f7f\u7528\u53bb\u566a\u81ea\u7f16\u7801\u5668\u4f30\u8ba1PRNU\u4fe1\u53f7\uff0c\u5377\u79ef\u7f51\u7edc\u8fdb\u884c1:N\u9a8c\u8bc1\uff0c\u91c7\u7528Hadamard\u4e58\u79ef\u65b9\u6cd5\u66ff\u4ee3\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u76f8\u673a\u8bc6\u522b\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8bc4\u4f30\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u4e0d\u540c\u62cd\u6444\u6761\u4ef6\u4e0b\u8fdb\u884c\u51c6\u786e\u76f8\u673a\u8bc6\u522b\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5305\u542b13K\u7167\u7247\u3001120+\u76f8\u673a\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u6df7\u5408\u67b6\u6784\u6a21\u578b\uff1a\u4f7f\u7528\u53bb\u566a\u81ea\u7f16\u7801\u5668\u4f30\u8ba1PRNU\u4fe1\u53f7\uff0c\u5377\u79ef\u7f51\u7edc\u8fdb\u884c\u8bbe\u5907\u9a8c\u8bc1\uff0c\u91c7\u7528Hadamard\u4e58\u79ef\u65b9\u6cd5\u5904\u7406\u53c2\u8003\u548c\u67e5\u8be2PRNU\u4fe1\u53f7\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\"\u91ce\u5916\"\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u53bb\u566a\u81ea\u7f16\u7801\u5668\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u76f8\u673a\u8bc6\u522b\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u6df7\u5408\u67b6\u6784\u6a21\u578b\u4e3a\u76f8\u673a\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u548c\u5de5\u5177\u3002"}}
{"id": "2509.17588", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17588", "abs": "https://arxiv.org/abs/2509.17588", "authors": ["Jinyeong Kim", "Seil Kang", "Jiwoo Park", "Junhyeok Kim", "Seong Jae Hwang"], "title": "Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) answer visual questions by transferring\ninformation from images to text through a series of attention heads. While this\nimage-to-text information flow is central to visual question answering, its\nunderlying mechanism remains difficult to interpret due to the simultaneous\noperation of numerous attention heads. To address this challenge, we propose\nhead attribution, a technique inspired by component attribution methods, to\nidentify consistent patterns among attention heads that play a key role in\ninformation transfer. Using head attribution, we investigate how LVLMs rely on\nspecific attention heads to identify and answer questions about the main object\nin an image. Our analysis reveals that a distinct subset of attention heads\nfacilitates the image-to-text information flow. Remarkably, we find that the\nselection of these heads is governed by the semantic content of the input image\nrather than its visual appearance. We further examine the flow of information\nat the token level and discover that (1) text information first propagates to\nrole-related tokens and the final token before receiving image information, and\n(2) image information is embedded in both object-related and background tokens.\nOur work provides evidence that image-to-text information flow follows a\nstructured process, and that analysis at the attention-head level offers a\npromising direction toward understanding the mechanisms of LVLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ahead attribution\u7684\u6280\u672f\u6765\u5206\u6790\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u56fe\u50cf\u5230\u6587\u672c\u4fe1\u606f\u6d41\u7684\u673a\u5236\uff0c\u53d1\u73b0\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u5728\u4fe1\u606f\u4f20\u9012\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4e14\u8fd9\u4e9b\u5934\u7684\u9009\u62e9\u7531\u56fe\u50cf\u8bed\u4e49\u5185\u5bb9\u800c\u975e\u89c6\u89c9\u5916\u89c2\u51b3\u5b9a\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u6ce8\u610f\u529b\u5934\u5b9e\u73b0\u56fe\u50cf\u5230\u6587\u672c\u7684\u4fe1\u606f\u4f20\u9012\uff0c\u4f46\u7531\u4e8e\u4f17\u591a\u6ce8\u610f\u529b\u5934\u540c\u65f6\u8fd0\u4f5c\uff0c\u5176\u5e95\u5c42\u673a\u5236\u96be\u4ee5\u89e3\u91ca\uff0c\u9700\u8981\u5f00\u53d1\u65b9\u6cd5\u6765\u7406\u89e3\u4fe1\u606f\u6d41\u7684\u7ed3\u6784\u5316\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fahead attribution\u6280\u672f\uff0c\u57fa\u4e8e\u7ec4\u4ef6\u5f52\u56e0\u65b9\u6cd5\u8bc6\u522b\u5728\u4fe1\u606f\u4f20\u9012\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u7684\u6ce8\u610f\u529b\u5934\u6a21\u5f0f\uff0c\u5206\u6790LVLMs\u5982\u4f55\u4f9d\u8d56\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u8bc6\u522b\u548c\u56de\u7b54\u5173\u4e8e\u56fe\u50cf\u4e3b\u8981\u5bf9\u8c61\u7684\u95ee\u9898\u3002", "result": "\u53d1\u73b0\u4e00\u4e2a\u72ec\u7279\u7684\u6ce8\u610f\u529b\u5934\u5b50\u96c6\u4fc3\u8fdb\u56fe\u50cf\u5230\u6587\u672c\u4fe1\u606f\u6d41\uff0c\u8fd9\u4e9b\u5934\u7684\u9009\u62e9\u7531\u8f93\u5165\u56fe\u50cf\u7684\u8bed\u4e49\u5185\u5bb9\u51b3\u5b9a\uff1b\u6587\u672c\u4fe1\u606f\u5148\u4f20\u64ad\u5230\u89d2\u8272\u76f8\u5173\u6807\u8bb0\u548c\u6700\u7ec8\u6807\u8bb0\uff0c\u7136\u540e\u63a5\u6536\u56fe\u50cf\u4fe1\u606f\uff1b\u56fe\u50cf\u4fe1\u606f\u5d4c\u5165\u5728\u5bf9\u8c61\u76f8\u5173\u548c\u80cc\u666f\u6807\u8bb0\u4e2d\u3002", "conclusion": "\u56fe\u50cf\u5230\u6587\u672c\u4fe1\u606f\u6d41\u9075\u5faa\u7ed3\u6784\u5316\u8fc7\u7a0b\uff0c\u5728\u6ce8\u610f\u529b\u5934\u5c42\u9762\u7684\u5206\u6790\u4e3a\u7406\u89e3LVLMs\u673a\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2509.17593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17593", "abs": "https://arxiv.org/abs/2509.17593", "authors": ["Samet Hicsonmez", "Abd El Rahman Shabayek", "Arunkumar Rathinam", "Djamila Aouada"], "title": "Domain Adaptive Object Detection for Space Applications with Real-Time Constraints", "comment": "Advanced Space Technologies in Robotics and Automation (ASTRA) 2025", "summary": "Object detection is essential in space applications targeting Space Domain\nAwareness and also applications involving relative navigation scenarios.\nCurrent deep learning models for Object Detection in space applications are\noften trained on synthetic data from simulators, however, the model performance\ndrops significantly on real-world data due to the domain gap. However, domain\nadaptive object detection is an overlooked problem in the community. In this\nwork, we first show the importance of domain adaptation and then explore\nSupervised Domain Adaptation (SDA) to reduce this gap using minimal labeled\nreal data. We build on a recent semi-supervised adaptation method and tailor it\nfor object detection. Our approach combines domain-invariant feature learning\nwith a CNN-based domain discriminator and invariant risk minimization using a\ndomain-independent regression head. To meet real-time deployment needs, we test\nour method on a lightweight Single Shot Multibox Detector (SSD) with MobileNet\nbackbone and on the more advanced Fully Convolutional One-Stage object detector\n(FCOS) with ResNet-50 backbone. We evaluated on two space datasets, SPEED+ and\nSPARK. The results show up to 20-point improvements in average precision (AP)\nwith just 250 labeled real images.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7a7a\u95f4\u5e94\u7528\u4e2d\u76ee\u6807\u68c0\u6d4b\u7684\u9886\u57df\u81ea\u9002\u5e94\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9886\u57df\u4e0d\u53d8\u7279\u5f81\u5b66\u4e60\u548c\u4e0d\u53d8\u98ce\u9669\u6700\u5c0f\u5316\u7684\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5728\u8f7b\u91cf\u7ea7\u548c\u5148\u8fdb\u68c0\u6d4b\u5668\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u7a7a\u95f4\u5e94\u7528\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u901a\u5e38\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u4f46\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u9886\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u5728\u793e\u533a\u4e2d\u88ab\u5ffd\u89c6\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u9886\u57df\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u534a\u76d1\u7763\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u7ed3\u5408CNN\u9886\u57df\u5224\u522b\u5668\u7684\u9886\u57df\u4e0d\u53d8\u7279\u5f81\u5b66\u4e60\u548c\u4f7f\u7528\u9886\u57df\u65e0\u5173\u56de\u5f52\u5934\u7684\u4e0d\u53d8\u98ce\u9669\u6700\u5c0f\u5316\u3002\u5728SSD+MobileNet\u548cFCOS+ResNet-50\u67b6\u6784\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5728SPEED+\u548cSPARK\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4ec5\u4f7f\u7528250\u5f20\u6807\u6ce8\u771f\u5b9e\u56fe\u50cf\uff0c\u5e73\u5747\u7cbe\u5ea6(AP)\u63d0\u5347\u4e86\u9ad8\u8fbe20\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7f29\u5c0f\u4e86\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u8bc1\u660e\u4e86\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u5728\u7a7a\u95f4\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5b9e\u65f6\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17598", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17598", "abs": "https://arxiv.org/abs/2509.17598", "authors": ["Aiming Zhang", "Tianyuan Yu", "Liang Bai", "Jun Tang", "Yanming Guo", "Yirun Ruan", "Yun Zhou", "Zhihe Lu"], "title": "COLA: Context-aware Language-driven Test-time Adaptation", "comment": null, "summary": "Test-time adaptation (TTA) has gained increasing popularity due to its\nefficacy in addressing ``distribution shift'' issue while simultaneously\nprotecting data privacy.\n  However, most prior methods assume that a paired source domain model and\ntarget domain sharing the same label space coexist, heavily limiting their\napplicability.\n  In this paper, we investigate a more general source model capable of\nadaptation to multiple target domains without needing shared labels.\n  This is achieved by using a pre-trained vision-language model (VLM), \\egno,\nCLIP, that can recognize images through matching with class descriptions.\n  While the zero-shot performance of VLMs is impressive, they struggle to\neffectively capture the distinctive attributes of a target domain.\n  To that end, we propose a novel method -- Context-aware Language-driven TTA\n(COLA).\n  The proposed method incorporates a lightweight context-aware module that\nconsists of three key components: a task-aware adapter, a context-aware unit,\nand a residual connection unit for exploring task-specific knowledge,\ndomain-specific knowledge from the VLM and prior knowledge of the VLM,\nrespectively.\n  It is worth noting that the context-aware module can be seamlessly integrated\ninto a frozen VLM, ensuring both minimal effort and parameter efficiency.\n  Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy\nto mitigate the adverse effects caused by class imbalance.\n  We demonstrate the effectiveness of our method not only in TTA scenarios but\nalso in class generalisation tasks.\n  The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5COLA\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5b9e\u73b0\u65e0\u9700\u5171\u4eab\u6807\u7b7e\u7a7a\u95f4\u7684\u591a\u76ee\u6807\u57df\u81ea\u9002\u5e94\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5757\u548c\u7c7b\u5e73\u8861\u4f2a\u6807\u7b7e\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6e90\u57df\u6a21\u578b\u548c\u76ee\u6807\u57df\u5171\u4eab\u76f8\u540c\u7684\u6807\u7b7e\u7a7a\u95f4\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u66f4\u901a\u7528\u7684\u573a\u666f\uff0c\u5373\u6e90\u6a21\u578b\u80fd\u591f\u9002\u5e94\u591a\u4e2a\u76ee\u6807\u57df\u800c\u65e0\u9700\u5171\u4eab\u6807\u7b7e\u3002", "method": "\u63d0\u51faCOLA\u65b9\u6cd5\uff0c\u5305\u542b\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5757\uff08\u4efb\u52a1\u611f\u77e5\u9002\u914d\u5668\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u5355\u5143\u548c\u6b8b\u5dee\u8fde\u63a5\u5355\u5143\uff09\u548c\u7c7b\u5e73\u8861\u4f2a\u6807\u7b7e\u7b56\u7565\uff08CBPL\uff09\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u51bb\u7ed3\u7684VLM\u4e2d\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5728\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u573a\u666f\u4e2d\u6709\u6548\uff0c\u5728\u7c7b\u522b\u6cdb\u5316\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "COLA\u65b9\u6cd5\u901a\u8fc7\u5229\u7528VLM\u7684\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.17602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17602", "abs": "https://arxiv.org/abs/2509.17602", "authors": ["Giulio Martellucci", "Herve Goeau", "Pierre Bonnet", "Fabrice Vinatier", "Alexis Joly"], "title": "Overview of PlantCLEF 2025: Multi-Species Plant Identification in Vegetation Quadrat Images", "comment": "13 pages, 4 figures, CLEF 2025 Conference and Labs of the Evaluation\n  Forum, September 09 to 12, 2024, Madrid, Spain", "summary": "Quadrat images are essential for ecological studies, as they enable\nstandardized sampling, the assessment of plant biodiversity, long-term\nmonitoring, and large-scale field campaigns. These images typically cover an\narea of fifty centimetres or one square meter, and botanists carefully identify\nall the species present. Integrating AI could help specialists accelerate their\ninventories and expand the spatial coverage of ecological studies. To assess\nprogress in this area, the PlantCLEF 2025 challenge relies on a new test set of\n2,105 high-resolution multi-label images annotated by experts and covering\naround 400 species. It also provides a large training set of 1.4 million\nindividual plant images, along with vision transformer models pre-trained on\nthis data. The task is formulated as a (weakly labelled) multi-label\nclassification problem, where the goal is to predict all species present in a\nquadrat image using single-label training data. This paper provides a detailed\ndescription of the data, the evaluation methodology, the methods and models\nused by participants, and the results achieved.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86PlantCLEF 2025\u6311\u6218\u8d5b\uff0c\u8be5\u6311\u6218\u8d5b\u65e8\u5728\u5229\u7528AI\u6280\u672f\u52a0\u901f\u690d\u7269\u751f\u6001\u5b66\u7814\u7a76\u4e2d\u7684\u7269\u79cd\u8bc6\u522b\uff0c\u901a\u8fc7\u591a\u6807\u7b7e\u5206\u7c7b\u65b9\u6cd5\u9884\u6d4b\u6837\u65b9\u56fe\u50cf\u4e2d\u7684\u6240\u6709\u7269\u79cd\u3002", "motivation": "\u6837\u65b9\u56fe\u50cf\u5728\u751f\u6001\u5b66\u7814\u7a76\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4eba\u5de5\u8bc6\u522b\u7269\u79cd\u8017\u65f6\u8d39\u529b\u3002\u96c6\u6210AI\u6280\u672f\u53ef\u4ee5\u5e2e\u52a9\u4e13\u5bb6\u52a0\u901f\u7269\u79cd\u6e05\u5355\u7f16\u5236\u5e76\u6269\u5927\u751f\u6001\u7814\u7a76\u7684\u7a7a\u95f4\u8986\u76d6\u8303\u56f4\u3002", "method": "\u5c06\u4efb\u52a1\u5236\u5b9a\u4e3a\uff08\u5f31\u6807\u8bb0\u7684\uff09\u591a\u6807\u7b7e\u5206\u7c7b\u95ee\u9898\uff0c\u4f7f\u7528\u5305\u542b140\u4e07\u5f20\u5355\u6807\u7b7e\u690d\u7269\u56fe\u50cf\u7684\u5927\u578b\u8bad\u7ec3\u96c6\u548c\u9884\u8bad\u7ec3\u7684\u89c6\u89c9Transformer\u6a21\u578b\uff0c\u9884\u6d4b\u6837\u65b9\u56fe\u50cf\u4e2d\u7684\u6240\u6709\u7269\u79cd\u3002", "result": "\u63d0\u4f9b\u4e86\u5305\u542b2,105\u5f20\u9ad8\u5206\u8fa8\u7387\u591a\u6807\u7b7e\u56fe\u50cf\u7684\u6d4b\u8bd5\u96c6\uff0c\u8986\u76d6\u7ea6400\u4e2a\u7269\u79cd\uff0c\u5e76\u8be6\u7ec6\u63cf\u8ff0\u4e86\u53c2\u4e0e\u8005\u4f7f\u7528\u7684\u65b9\u6cd5\u548c\u6a21\u578b\u4ee5\u53ca\u53d6\u5f97\u7684\u6210\u679c\u3002", "conclusion": "PlantCLEF 2025\u6311\u6218\u8d5b\u4e3a\u8bc4\u4f30AI\u5728\u690d\u7269\u751f\u6001\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u8fdb\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u5e73\u53f0\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2509.17615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17615", "abs": "https://arxiv.org/abs/2509.17615", "authors": ["Lars Heckler-Kram", "Ashwin Vaidya", "Jan-Hendrik Neudeck", "Ulla Scheler", "Dick Ameln", "Samet Akcay", "Paula Ramos"], "title": "From Benchmarks to Reality: Advancing Visual Anomaly Detection by the VAND 3.0 Challenge", "comment": null, "summary": "Visual anomaly detection is a strongly application-driven field of research.\nConsequently, the connection between academia and industry is of paramount\nimportance. In this regard, we present the VAND 3.0 Challenge to showcase\ncurrent progress in anomaly detection across different practical settings\nwhilst addressing critical issues in the field. The challenge hosted two\ntracks, fostering the development of anomaly detection methods robust against\nreal-world distribution shifts (Category 1) and exploring the capabilities of\nVision Language Models within the few-shot regime (Category 2), respectively.\nThe participants' solutions reached significant improvements over previous\nbaselines by combining or adapting existing approaches and fusing them with\nnovel pipelines. While for both tracks the progress in large pre-trained vision\n(language) backbones played a pivotal role for the performance increase,\nscaling up anomaly detection methods more efficiently needs to be addressed by\nfuture research to meet real-time and computational constraints on-site.", "AI": {"tldr": "VAND 3.0\u6311\u6218\u8d5b\u5c55\u793a\u4e86\u5f02\u5e38\u68c0\u6d4b\u5728\u4e0d\u540c\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u5bf9\u73b0\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u80fd\u529b\u3002", "motivation": "\u52a0\u5f3a\u5b66\u672f\u754c\u4e0e\u5de5\u4e1a\u754c\u7684\u8054\u7cfb\uff0c\u89e3\u51b3\u5f02\u5e38\u68c0\u6d4b\u9886\u57df\u7684\u5173\u952e\u95ee\u9898\uff0c\u5c55\u793a\u5f53\u524d\u5728\u4e0d\u540c\u5b9e\u8df5\u73af\u5883\u4e0b\u7684\u5f02\u5e38\u68c0\u6d4b\u8fdb\u5c55\u3002", "method": "\u6311\u6218\u8d5b\u8bbe\u7f6e\u4e24\u4e2a\u8d5b\u9053\uff1a\u4e00\u662f\u5f00\u53d1\u5bf9\u73b0\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\u5177\u6709\u9c81\u68d2\u6027\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e8c\u662f\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u80fd\u529b\u3002\u53c2\u4e0e\u8005\u901a\u8fc7\u7ed3\u5408\u6216\u8c03\u6574\u73b0\u6709\u65b9\u6cd5\u5e76\u878d\u5408\u65b0\u9896\u6d41\u7a0b\u6765\u6539\u8fdb\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u53c2\u4e0e\u8005\u7684\u89e3\u51b3\u65b9\u6848\u76f8\u6bd4\u4e4b\u524d\u57fa\u7ebf\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5927\u578b\u9884\u8bad\u7ec3\u89c6\u89c9\uff08\u8bed\u8a00\uff09\u9aa8\u5e72\u7f51\u7edc\u5728\u6027\u80fd\u63d0\u5347\u4e2d\u53d1\u6325\u4e86\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u867d\u7136\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u672a\u6765\u7814\u7a76\u9700\u8981\u66f4\u6709\u6548\u5730\u6269\u5c55\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u6ee1\u8db3\u73b0\u573a\u5b9e\u65f6\u6027\u548c\u8ba1\u7b97\u7ea6\u675f\u8981\u6c42\u3002"}}
{"id": "2509.17620", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17620", "abs": "https://arxiv.org/abs/2509.17620", "authors": ["Gregory Schroeder", "Mohamed Sabry", "Cristina Olaverri-Monreal"], "title": "Tensor-Based Self-Calibration of Cameras via the TrifocalCalib Method", "comment": null, "summary": "Estimating camera intrinsic parameters without prior scene knowledge is a\nfundamental challenge in computer vision. This capability is particularly\nimportant for applications such as autonomous driving and vehicle platooning,\nwhere precalibrated setups are impractical and real-time adaptability is\nnecessary. To advance the state-of-the-art, we present a set of equations based\non the calibrated trifocal tensor, enabling projective camera self-calibration\nfrom minimal image data. Our method, termed TrifocalCalib, significantly\nimproves accuracy and robustness compared to both recent learning-based and\nclassical approaches. Unlike many existing techniques, our approach requires no\ncalibration target, imposes no constraints on camera motion, and simultaneously\nestimates both focal length and principal point. Evaluations in both\nprocedurally generated synthetic environments and structured dataset-based\nscenarios demonstrate the effectiveness of our approach. To support\nreproducibility, we make the code publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6821\u51c6\u4e09\u7126\u5f20\u91cf\u7684\u76f8\u673a\u81ea\u6807\u5b9a\u65b9\u6cd5TrifocalCalib\uff0c\u65e0\u9700\u5148\u9a8c\u573a\u666f\u77e5\u8bc6\u6216\u6807\u5b9a\u76ee\u6807\uff0c\u80fd\u591f\u540c\u65f6\u4f30\u8ba1\u7126\u8ddd\u548c\u4e3b\u70b9\uff0c\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u8f66\u8f86\u7f16\u961f\u7b49\u5e94\u7528\u4e2d\uff0c\u9884\u5148\u6807\u5b9a\u7684\u8bbe\u7f6e\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u5b9e\u65f6\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u65e0\u9700\u5148\u9a8c\u573a\u666f\u77e5\u8bc6\u7684\u76f8\u673a\u5185\u53c2\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u6821\u51c6\u4e09\u7126\u5f20\u91cf\u5efa\u7acb\u65b9\u7a0b\u7ec4\uff0c\u4ece\u6700\u5c0f\u56fe\u50cf\u6570\u636e\u5b9e\u73b0\u6295\u5f71\u76f8\u673a\u81ea\u6807\u5b9a\uff0c\u65e0\u9700\u6807\u5b9a\u76ee\u6807\uff0c\u5bf9\u76f8\u673a\u8fd0\u52a8\u65e0\u7ea6\u675f\uff0c\u53ef\u540c\u65f6\u4f30\u8ba1\u7126\u8ddd\u548c\u4e3b\u70b9\u3002", "result": "\u5728\u7a0b\u5e8f\u751f\u6210\u7684\u5408\u6210\u73af\u5883\u548c\u7ed3\u6784\u5316\u6570\u636e\u96c6\u573a\u666f\u4e2d\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u548c\u7ecf\u5178\u65b9\u6cd5\u3002", "conclusion": "TrifocalCalib\u65b9\u6cd5\u4e3a\u65e0\u5148\u9a8c\u573a\u666f\u77e5\u8bc6\u7684\u76f8\u673a\u81ea\u6807\u5b9a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u4ee5\u652f\u6301\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2509.17622", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17622", "abs": "https://arxiv.org/abs/2509.17622", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of PlantCLEF 2023: Image-based Plant Identification at Global Scale", "comment": "10 pages, 1 figure, CLEF 2023 Conference and Labs of the Evaluation\n  Forum, September 18 to 21, 2023, Thessaloniki, Greece", "summary": "The world is estimated to be home to over 300,000 species of vascular plants.\nIn the face of the ongoing biodiversity crisis, expanding our understanding of\nthese species is crucial for the advancement of human civilization,\nencompassing areas such as agriculture, construction, and pharmacopoeia.\nHowever, the labor-intensive process of plant identification undertaken by\nhuman experts poses a significant obstacle to the accumulation of new data and\nknowledge. Fortunately, recent advancements in automatic identification,\nparticularly through the application of deep learning techniques, have shown\npromising progress. Despite challenges posed by data-related issues such as a\nvast number of classes, imbalanced class distribution, erroneous\nidentifications, duplications, variable visual quality, and diverse visual\ncontents (such as photos or herbarium sheets), deep learning approaches have\nreached a level of maturity which gives us hope that in the near future we will\nhave an identification system capable of accurately identifying all plant\nspecies worldwide. The PlantCLEF2023 challenge aims to contribute to this\npursuit by addressing a multi-image (and metadata) classification problem\ninvolving an extensive set of classes (80,000 plant species). This paper\nprovides an overview of the challenge's resources and evaluations, summarizes\nthe methods and systems employed by participating research groups, and presents\nan analysis of key findings.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PlantCLEF2023\u6311\u6218\u8d5b\uff0c\u8be5\u6311\u6218\u65e8\u5728\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u89e3\u51b3\u690d\u7269\u7269\u79cd\u81ea\u52a8\u8bc6\u522b\u95ee\u9898\uff0c\u6d89\u53ca80,000\u4e2a\u690d\u7269\u7269\u79cd\u7684\u591a\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u3002", "motivation": "\u9762\u5bf9\u751f\u7269\u591a\u6837\u6027\u5371\u673a\uff0c\u690d\u7269\u8bc6\u522b\u5bf9\u4eba\u7c7b\u6587\u660e\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4eba\u5de5\u8bc6\u522b\u8fc7\u7a0b\u8017\u65f6\u4e14\u963b\u788d\u65b0\u6570\u636e\u79ef\u7d2f\u3002\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u5e0c\u671b\u3002", "method": "PlantCLEF2023\u6311\u6218\u8d5b\u91c7\u7528\u591a\u56fe\u50cf\uff08\u548c\u5143\u6570\u636e\uff09\u5206\u7c7b\u65b9\u6cd5\uff0c\u5904\u7406\u5927\u89c4\u6a21\u7c7b\u522b\u7684\u690d\u7269\u7269\u79cd\u8bc6\u522b\uff0c\u5e94\u5bf9\u6570\u636e\u4e0d\u5e73\u8861\u3001\u8bc6\u522b\u9519\u8bef\u3001\u91cd\u590d\u548c\u8d28\u91cf\u4e0d\u4e00\u7b49\u6311\u6218\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5df2\u8d8b\u4e8e\u6210\u719f\uff0c\u6709\u671b\u5728\u4e0d\u4e45\u7684\u5c06\u6765\u5f00\u53d1\u51fa\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u5168\u7403\u6240\u6709\u690d\u7269\u7269\u79cd\u7684\u8bc6\u522b\u7cfb\u7edf\u3002", "conclusion": "PlantCLEF2023\u6311\u6218\u8d5b\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u63a8\u52a8\u4e86\u690d\u7269\u81ea\u52a8\u8bc6\u522b\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4e3a\u6784\u5efa\u5168\u7403\u690d\u7269\u8bc6\u522b\u7cfb\u7edf\u505a\u51fa\u4e86\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2509.17627", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17627", "abs": "https://arxiv.org/abs/2509.17627", "authors": ["Jinshu Chen", "Xinghui Li", "Xu Bai", "Tianxiang Ma", "Pengze Zhang", "Zhuowei Chen", "Gen Li", "Lijie Liu", "Songtao Zhao", "Bingchuan Li", "Qian He"], "title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models", "comment": "Github Page: https://phantom-video.github.io/OmniInsert/", "summary": "Recent advances in video insertion based on diffusion models are impressive.\nHowever, existing methods rely on complex control signals but struggle with\nsubject consistency, limiting their practical applicability. In this paper, we\nfocus on the task of Mask-free Video Insertion and aim to resolve three key\nchallenges: data scarcity, subject-scene equilibrium, and insertion\nharmonization. To address the data scarcity, we propose a new data pipeline\nInsertPipe, constructing diverse cross-pair data automatically. Building upon\nour data pipeline, we develop OmniInsert, a novel unified framework for\nmask-free video insertion from both single and multiple subject references.\nSpecifically, to maintain subject-scene equilibrium, we introduce a simple yet\neffective Condition-Specific Feature Injection mechanism to distinctly inject\nmulti-source conditions and propose a novel Progressive Training strategy that\nenables the model to balance feature injection from subjects and source video.\nMeanwhile, we design the Subject-Focused Loss to improve the detailed\nappearance of the subjects. To further enhance insertion harmonization, we\npropose an Insertive Preference Optimization methodology to optimize the model\nby simulating human preferences, and incorporate a Context-Aware Rephraser\nmodule during reference to seamlessly integrate the subject into the original\nscenes. To address the lack of a benchmark for the field, we introduce\nInsertBench, a comprehensive benchmark comprising diverse scenes with\nmeticulously selected subjects. Evaluation on InsertBench indicates OmniInsert\noutperforms state-of-the-art closed-source commercial solutions. The code will\nbe released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OmniInsert\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u63a9\u7801\u89c6\u9891\u63d2\u5165\u4efb\u52a1\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u6570\u636e\u7a00\u7f3a\u3001\u4e3b\u4f53-\u573a\u666f\u5e73\u8861\u548c\u63d2\u5165\u534f\u8c03\u3002\u901a\u8fc7InsertPipe\u6570\u636e\u7ba1\u9053\u3001\u6761\u4ef6\u7279\u5b9a\u7279\u5f81\u6ce8\u5165\u673a\u5236\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u63d2\u5165\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u63d2\u5165\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u63a7\u5236\u4fe1\u53f7\u4f46\u96be\u4ee5\u4fdd\u6301\u4e3b\u4f53\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u3001\u4e3b\u4f53-\u573a\u666f\u5e73\u8861\u548c\u63d2\u5165\u534f\u8c03\u4e09\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faInsertPipe\u6570\u636e\u7ba1\u9053\u81ea\u52a8\u6784\u5efa\u591a\u6837\u5316\u4ea4\u53c9\u5bf9\u6570\u636e\uff1b\u5f00\u53d1OmniInsert\u7edf\u4e00\u6846\u67b6\uff0c\u91c7\u7528\u6761\u4ef6\u7279\u5b9a\u7279\u5f81\u6ce8\u5165\u673a\u5236\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff1b\u8bbe\u8ba1\u4e3b\u4f53\u805a\u7126\u635f\u5931\u548c\u63d2\u5165\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff1b\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u91cd\u8868\u8ff0\u6a21\u5757\u3002", "result": "\u5728\u65b0\u5efa\u7684InsertBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOmniInsert\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u95ed\u6e90\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "OmniInsert\u4e3a\u65e0\u63a9\u7801\u89c6\u9891\u63d2\u5165\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u7ba1\u9053\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u63d2\u5165\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.17632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17632", "abs": "https://arxiv.org/abs/2509.17632", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of PlantCLEF 2022: Image-based plant identification at global scale", "comment": "13 pages, 2 figures, CLEF 2022 Conference and Labs of the Evaluation\n  Forum, September 05 to 08, 2022, Bologna, Italy", "summary": "It is estimated that there are more than 300,000 species of vascular plants\nin the world. Increasing our knowledge of these species is of paramount\nimportance for the development of human civilization (agriculture,\nconstruction, pharmacopoeia, etc.), especially in the context of the\nbiodiversity crisis. However, the burden of systematic plant identification by\nhuman experts strongly penalizes the aggregation of new data and knowledge.\nSince then, automatic identification has made considerable progress in recent\nyears as highlighted during all previous editions of PlantCLEF. Deep learning\ntechniques now seem mature enough to address the ultimate but realistic problem\nof global identification of plant biodiversity in spite of many problems that\nthe data may present (a huge number of classes, very strongly unbalanced\nclasses, partially erroneous identifications, duplications, variable visual\nquality, diversity of visual contents such as photos or herbarium sheets, etc).\nThe PlantCLEF2022 challenge edition proposes to take a step in this direction\nby tackling a multi-image (and metadata) classification problem with a very\nlarge number of classes (80k plant species). This paper presents the resources\nand evaluations of the challenge, summarizes the approaches and systems\nemployed by the participating research groups, and provides an analysis of key\nfindings.", "AI": {"tldr": "PlantCLEF2022\u6311\u6218\u8d5b\u65e8\u5728\u901a\u8fc7\u591a\u56fe\u50cf\u548c\u5143\u6570\u636e\u5206\u7c7b\u65b9\u6cd5\u89e3\u51b3\u5168\u7403\u690d\u7269\u7269\u79cd\u81ea\u52a8\u8bc6\u522b\u95ee\u9898\uff0c\u6d89\u53ca8\u4e07\u79cd\u690d\u7269\u7269\u79cd\u7684\u5927\u89c4\u6a21\u5206\u7c7b\u4efb\u52a1\u3002", "motivation": "\u5168\u7403\u6709\u8d85\u8fc730\u4e07\u79cd\u7ef4\u7ba1\u690d\u7269\uff0c\u4eba\u5de5\u4e13\u5bb6\u8bc6\u522b\u9650\u5236\u4e86\u65b0\u6570\u636e\u548c\u77e5\u8bc6\u7684\u79ef\u7d2f\u3002\u5728\u751f\u7269\u591a\u6837\u6027\u5371\u673a\u80cc\u666f\u4e0b\uff0c\u5f00\u53d1\u81ea\u52a8\u8bc6\u522b\u6280\u672f\u5bf9\u519c\u4e1a\u53d1\u5c55\u3001\u5efa\u7b51\u3001\u836f\u5178\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u8fdb\u884c\u591a\u56fe\u50cf\u548c\u5143\u6570\u636e\u5206\u7c7b\uff0c\u5904\u7406\u5927\u89c4\u6a21\u7c7b\u522b\uff088\u4e07\u79cd\u690d\u7269\u7269\u79cd\uff09\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u90e8\u5206\u9519\u8bef\u8bc6\u522b\u3001\u91cd\u590d\u6570\u636e\u3001\u89c6\u89c9\u8d28\u91cf\u4e0d\u4e00\u7b49\u591a\u79cd\u6570\u636e\u95ee\u9898\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5728\u5904\u7406\u5927\u89c4\u6a21\u690d\u7269\u8bc6\u522b\u95ee\u9898\u4e0a\u5df2\u76f8\u5bf9\u6210\u719f\uff0c\u80fd\u591f\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u7684\u690d\u7269\u8bc6\u522b\u6311\u6218\u3002", "conclusion": "PlantCLEF2022\u6311\u6218\u8d5b\u5728\u63a8\u52a8\u5168\u7403\u690d\u7269\u751f\u7269\u591a\u6837\u6027\u81ea\u52a8\u8bc6\u522b\u65b9\u9762\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u672a\u6765\u690d\u7269\u8bc6\u522b\u6280\u672f\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2509.17638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17638", "abs": "https://arxiv.org/abs/2509.17638", "authors": ["Zilin Gao", "Qilong Wang", "Bingbing Zhang", "Qinghua Hu", "Peihua Li"], "title": "A$^2$M$^2$-Net: Adaptively Aligned Multi-Scale Moment for Few-Shot Action Recognition", "comment": "27 pages, 13 figures, 7 tables", "summary": "Thanks to capability to alleviate the cost of large-scale annotation,\nfew-shot action recognition (FSAR) has attracted increased attention of\nresearchers in recent years. Existing FSAR approaches typically neglect the\nrole of individual motion pattern in comparison, and under-explore the feature\nstatistics for video dynamics. Thereby, they struggle to handle the challenging\ntemporal misalignment in video dynamics, particularly by using 2D backbones. To\novercome these limitations, this work proposes an adaptively aligned\nmulti-scale second-order moment network, namely A$^2$M$^2$-Net, to describe the\nlatent video dynamics with a collection of powerful representation candidates\nand adaptively align them in an instance-guided manner. To this end, our\nA$^2$M$^2$-Net involves two core components, namely, adaptive alignment (A$^2$\nmodule) for matching, and multi-scale second-order moment (M$^2$ block) for\nstrong representation. Specifically, M$^2$ block develops a collection of\nsemantic second-order descriptors at multiple spatio-temporal scales.\nFurthermore, A$^2$ module aims to adaptively select informative candidate\ndescriptors while considering the individual motion pattern. By such means, our\nA$^2$M$^2$-Net is able to handle the challenging temporal misalignment problem\nby establishing an adaptive alignment protocol for strong representation.\nNotably, our proposed method generalizes well to various few-shot settings and\ndiverse metrics. The experiments are conducted on five widely used FSAR\nbenchmarks, and the results show our A$^2$M$^2$-Net achieves very competitive\nperformance compared to state-of-the-arts, demonstrating its effectiveness and\ngeneralization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86A\u00b2M\u00b2-Net\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5bf9\u9f50\u7684\u591a\u5c3a\u5ea6\u4e8c\u9636\u77e9\u7f51\u7edc\u6765\u89e3\u51b3\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u65f6\u5e8f\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u5ffd\u89c6\u4e2a\u4f53\u8fd0\u52a8\u6a21\u5f0f\u5728\u6bd4\u8f83\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u4e14\u5bf9\u89c6\u9891\u52a8\u6001\u7279\u5f81\u7edf\u8ba1\u7684\u63a2\u7d22\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u4f7f\u75282D\u9aa8\u5e72\u7f51\u7edc\u65f6\u96be\u4ee5\u5904\u7406\u89c6\u9891\u52a8\u6001\u4e2d\u7684\u65f6\u5e8f\u4e0d\u5bf9\u9f50\u6311\u6218\u3002", "method": "\u63d0\u51faA\u00b2M\u00b2-Net\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u81ea\u9002\u5e94\u5bf9\u9f50\u6a21\u5757(A\u00b2)\u7528\u4e8e\u5339\u914d\uff0c\u591a\u5c3a\u5ea6\u4e8c\u9636\u77e9\u6a21\u5757(M\u00b2)\u7528\u4e8e\u751f\u6210\u5f3a\u8868\u5f81\u3002M\u00b2\u6a21\u5757\u5728\u591a\u4e2a\u65f6\u7a7a\u5c3a\u5ea6\u4e0a\u5f00\u53d1\u8bed\u4e49\u4e8c\u9636\u63cf\u8ff0\u7b26\uff0cA\u00b2\u6a21\u5757\u81ea\u9002\u5e94\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u5019\u9009\u63cf\u8ff0\u7b26\u5e76\u8003\u8651\u4e2a\u4f53\u8fd0\u52a8\u6a21\u5f0f\u3002", "result": "\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aA\u00b2M\u00b2-Net\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u53d6\u5f97\u4e86\u975e\u5e38\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7\u5efa\u7acb\u81ea\u9002\u5e94\u5bf9\u9f50\u534f\u8bae\u6765\u5904\u7406\u65f6\u5e8f\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5728\u591a\u79cd\u5c11\u6837\u672c\u8bbe\u7f6e\u548c\u4e0d\u540c\u5ea6\u91cf\u6807\u51c6\u4e0b\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2509.17647", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17647", "abs": "https://arxiv.org/abs/2509.17647", "authors": ["Yu Liu", "Baoxiong Jia", "Ruijie Lu", "Chuyue Gan", "Huayu Chen", "Junfeng Ni", "Song-Chun Zhu", "Siyuan Huang"], "title": "VideoArtGS: Building Digital Twins of Articulated Objects from Monocular Video", "comment": null, "summary": "Building digital twins of articulated objects from monocular video presents\nan essential challenge in computer vision, which requires simultaneous\nreconstruction of object geometry, part segmentation, and articulation\nparameters from limited viewpoint inputs. Monocular video offers an attractive\ninput format due to its simplicity and scalability; however, it's challenging\nto disentangle the object geometry and part dynamics with visual supervision\nalone, as the joint movement of the camera and parts leads to ill-posed\nestimation. While motion priors from pre-trained tracking models can alleviate\nthe issue, how to effectively integrate them for articulation learning remains\nlargely unexplored. To address this problem, we introduce VideoArtGS, a novel\napproach that reconstructs high-fidelity digital twins of articulated objects\nfrom monocular video. We propose a motion prior guidance pipeline that analyzes\n3D tracks, filters noise, and provides reliable initialization of articulation\nparameters. We also design a hybrid center-grid part assignment module for\narticulation-based deformation fields that captures accurate part motion.\nVideoArtGS demonstrates state-of-the-art performance in articulation and mesh\nreconstruction, reducing the reconstruction error by about two orders of\nmagnitude compared to existing methods. VideoArtGS enables practical digital\ntwin creation from monocular video, establishing a new benchmark for\nvideo-based articulated object reconstruction. Our work is made publicly\navailable at: https://videoartgs.github.io.", "AI": {"tldr": "VideoArtGS\u662f\u4e00\u79cd\u4ece\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u94f0\u63a5\u7269\u4f53\u6570\u5b57\u5b6a\u751f\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd0\u52a8\u5148\u9a8c\u5f15\u5bfc\u548c\u6df7\u5408\u4e2d\u5fc3\u7f51\u683c\u90e8\u4ef6\u5206\u914d\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u94f0\u63a5\u53c2\u6570\u548c\u7f51\u683c\u91cd\u5efa\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4ece\u5355\u76ee\u89c6\u9891\u6784\u5efa\u94f0\u63a5\u7269\u4f53\u7684\u6570\u5b57\u5b6a\u751f\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u9700\u8981\u540c\u65f6\u91cd\u5efa\u51e0\u4f55\u3001\u90e8\u4ef6\u5206\u5272\u548c\u94f0\u63a5\u53c2\u6570\u3002\u5355\u76ee\u89c6\u9891\u8f93\u5165\u7b80\u5355\u4e14\u53ef\u6269\u5c55\uff0c\u4f46\u4ec5\u51ed\u89c6\u89c9\u76d1\u7763\u96be\u4ee5\u89e3\u8026\u7269\u4f53\u51e0\u4f55\u548c\u90e8\u4ef6\u52a8\u529b\u5b66\uff0c\u56e0\u4e3a\u76f8\u673a\u548c\u90e8\u4ef6\u7684\u8054\u5408\u8fd0\u52a8\u4f1a\u5bfc\u81f4\u4f30\u8ba1\u95ee\u9898\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u8fd0\u52a8\u5148\u9a8c\u5f15\u5bfc\u6d41\u7a0b\uff0c\u5206\u67903D\u8f68\u8ff9\u3001\u8fc7\u6ee4\u566a\u58f0\uff0c\u5e76\u63d0\u4f9b\u53ef\u9760\u7684\u94f0\u63a5\u53c2\u6570\u521d\u59cb\u5316\u3002\u8bbe\u8ba1\u6df7\u5408\u4e2d\u5fc3\u7f51\u683c\u90e8\u4ef6\u5206\u914d\u6a21\u5757\uff0c\u7528\u4e8e\u57fa\u4e8e\u94f0\u63a5\u7684\u53d8\u5f62\u573a\uff0c\u6355\u6349\u51c6\u786e\u7684\u90e8\u4ef6\u8fd0\u52a8\u3002", "result": "VideoArtGS\u5728\u94f0\u63a5\u548c\u7f51\u683c\u91cd\u5efa\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5c06\u91cd\u5efa\u8bef\u5dee\u964d\u4f4e\u4e86\u7ea6\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "VideoArtGS\u5b9e\u73b0\u4e86\u4ece\u5355\u76ee\u89c6\u9891\u521b\u5efa\u5b9e\u7528\u6570\u5b57\u5b6a\u751f\u7684\u80fd\u529b\uff0c\u4e3a\u57fa\u4e8e\u89c6\u9891\u7684\u94f0\u63a5\u7269\u4f53\u91cd\u5efa\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2509.17650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17650", "abs": "https://arxiv.org/abs/2509.17650", "authors": ["Soroush Mahdi", "Fardin Ayar", "Ehsan Javanmardi", "Manabu Tsukada", "Mahdi Javanmardi"], "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming Visual Geometry Transformers", "comment": null, "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u3001\u63a8\u7406\u65f6\u7684\u4ee4\u724c\u9a71\u9010\u7b56\u7565\uff0c\u901a\u8fc7\u4e22\u5f03\u5197\u4f59\u4ee4\u724c\u6765\u9650\u5236\u5185\u5b58\u589e\u957f\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u6d41\u5f0f\u89c6\u89c9\u53d8\u6362\u5668\u5982StreamVGGT\u57283D\u611f\u77e5\u65b9\u9762\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5b58\u5728\u952e\u503c\u5185\u5b58\u65e0\u9650\u5236\u589e\u957f\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u4f7f\u7528\u63a8\u7406\u65f6\u4ee4\u724c\u9a71\u9010\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6700\u5177\u4fe1\u606f\u91cf\u7684\u4ee4\u724c\u7684\u540c\u65f6\u4e22\u5f03\u5197\u4f59\u4ee4\u724c\uff0c\u4ece\u800c\u9650\u5236\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u57287-Scenes\u957f\u5e8f\u5217\u4e0a\uff0c\u5cf0\u503c\u5185\u5b58\u4ece18.63GB\u964d\u81f39.39GB\uff0c\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u4ec5\u4e0b\u964d0.003\u3002\u5728\u4e25\u683c\u5185\u5b58\u9884\u7b97\u4e0b\uff0c\u9a71\u9010\u7b56\u7565\u5141\u8bb8\u66f4\u5bc6\u96c6\u7684\u5e27\u91c7\u6837\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u30013D\u91cd\u5efa\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7b49\u591a\u4e2a\u4efb\u52a1\u4e0a\u63a5\u8fd1StreamVGGT\u6027\u80fd\uff0c\u4f46\u5185\u5b58\u4f7f\u7528\u5927\u5e45\u51cf\u5c11\uff0c\u4f7f\u957f\u5e8f\u5217\u6d41\u5f0f\u63a8\u7406\u66f4\u52a0\u5b9e\u7528\u3002"}}
{"id": "2509.17651", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17651", "abs": "https://arxiv.org/abs/2509.17651", "authors": ["Filippo Botti", "Alex Ergasti", "Tomaso Fontanini", "Claudio Ferrari", "Massimo Bertozzi", "Andrea Prati"], "title": "SISMA: Semantic Face Image Synthesis with Mamba", "comment": null, "summary": "Diffusion Models have become very popular for Semantic Image Synthesis (SIS)\nof human faces. Nevertheless, their training and inference is computationally\nexpensive and their computational requirements are high due to the quadratic\ncomplexity of attention layers. In this paper, we propose a novel architecture\ncalled SISMA, based on the recently proposed Mamba. SISMA generates high\nquality samples by controlling their shape using a semantic mask at a reduced\ncomputational demand. We validated our approach through comprehensive\nexperiments with CelebAMask-HQ, revealing that our architecture not only\nachieves a better FID score yet also operates at three times the speed of\nstate-of-the-art architectures. This indicates that the proposed design is a\nviable, lightweight substitute to transformer-based models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u65b0\u578b\u67b6\u6784SISMA\uff0c\u7528\u4e8e\u8bed\u4e49\u56fe\u50cf\u5408\u6210\uff0c\u76f8\u6bd4\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u8bed\u4e49\u56fe\u50cf\u5408\u6210\u4e2d\u5f88\u53d7\u6b22\u8fce\uff0c\u4f46\u5176\u8bad\u7ec3\u548c\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e3b\u8981\u7531\u4e8e\u6ce8\u610f\u529b\u5c42\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u66f4\u8f7b\u91cf\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u6700\u8fd1\u63d0\u51fa\u7684Mamba\u67b6\u6784\u5f00\u53d1\u4e86SISMA\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u63a9\u7801\u63a7\u5236\u751f\u6210\u56fe\u50cf\u7684\u5f62\u72b6\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u3002", "result": "\u5728CelebAMask-HQ\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSISMA\u4e0d\u4ec5\u83b7\u5f97\u4e86\u66f4\u597d\u7684FID\u5206\u6570\uff0c\u800c\u4e14\u8fd0\u884c\u901f\u5ea6\u662f\u73b0\u6709\u6700\u5148\u8fdb\u67b6\u6784\u7684\u4e09\u500d\u3002", "conclusion": "SISMA\u662f\u57fa\u4e8eTransformer\u6a21\u578b\u7684\u4e00\u4e2a\u53ef\u884c\u4e14\u8f7b\u91cf\u7ea7\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u8bed\u4e49\u56fe\u50cf\u5408\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2509.17654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17654", "abs": "https://arxiv.org/abs/2509.17654", "authors": ["Sehyun Kim", "Hye Jun Lee", "Jiwoo Lee", "Taemin Lee"], "title": "Clothing agnostic Pre-inpainting Virtual Try-ON", "comment": null, "summary": "With the development of deep learning technology, virtual try-on technology\nhas become an important application value in the fields of e-commerce, fashion,\nand entertainment. The recently proposed Leffa has improved the texture\ndistortion problem of diffu-sion-based models, but there are limitations in\nthat the bottom detection inaccuracy and the existing clothing silhouette\nremain in the synthesis results. To solve this problem, this study proposes\nCaP-VTON (Clothing agnostic Pre-inpainting Virtual Try-ON). CaP-VTON has\nimproved the naturalness and consistency of whole-body clothing syn-thesis by\nintegrating multi-category masking based on Dress Code and skin inpainting\nbased on Stable Diffusion. In particular, a generate skin module was introduced\nto solve the skin restoration problem that occurs when long-sleeved images are\nconverted into short-sleeved or sleeveless ones, and high-quality restoration\nwas implemented consider-ing the human body posture and color. As a result,\nCaP-VTON recorded 92.5\\%, which is 15.4\\% better than Leffa in short-sleeved\nsynthesis accuracy, and showed the performance of consistently reproducing the\nstyle and shape of reference clothing in visual evaluation. These structures\nmaintain model-agnostic properties and are applicable to various\ndiffu-sion-based virtual inspection systems, and can contribute to applications\nthat require high-precision virtual wearing, such as e-commerce, custom\nstyling, and avatar creation.", "AI": {"tldr": "CaP-VTON\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u865a\u62df\u8bd5\u7a7f\u6280\u672f\uff0c\u901a\u8fc7\u591a\u7c7b\u522b\u63a9\u7801\u548c\u76ae\u80a4\u4fee\u590d\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u670d\u88c5\u8f6e\u5ed3\u4fdd\u6301\u548c\u76ae\u80a4\u4fee\u590d\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5728\u77ed\u8896\u5408\u6210\u51c6\u786e\u7387\u4e0a\u6bd4Leffa\u63d0\u5347\u4e8615.4%\u3002", "motivation": "\u73b0\u6709\u7684Leffa\u65b9\u6cd5\u867d\u7136\u6539\u8fdb\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7eb9\u7406\u5931\u771f\u95ee\u9898\uff0c\u4f46\u5728\u5e95\u90e8\u68c0\u6d4b\u4e0d\u51c6\u786e\u548c\u670d\u88c5\u8f6e\u5ed3\u4fdd\u7559\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u81ea\u7136\u548c\u4e00\u81f4\u7684\u5168\u8eab\u670d\u88c5\u5408\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCaP-VTON\u65b9\u6cd5\uff0c\u6574\u5408\u4e86\u57fa\u4e8eDress Code\u7684\u591a\u7c7b\u522b\u63a9\u7801\u548c\u57fa\u4e8eStable Diffusion\u7684\u76ae\u80a4\u4fee\u590d\uff0c\u7279\u522b\u5f15\u5165\u4e86\u751f\u6210\u76ae\u80a4\u6a21\u5757\u6765\u89e3\u51b3\u957f\u8896\u8f6c\u77ed\u8896/\u65e0\u8896\u65f6\u7684\u76ae\u80a4\u4fee\u590d\u95ee\u9898\u3002", "result": "CaP-VTON\u5728\u77ed\u8896\u5408\u6210\u51c6\u786e\u7387\u8fbe\u523092.5%\uff0c\u6bd4Leffa\u63d0\u534715.4%\uff0c\u5728\u89c6\u89c9\u8bc4\u4f30\u4e2d\u80fd\u4e00\u81f4\u5730\u590d\u73b0\u53c2\u8003\u670d\u88c5\u7684\u98ce\u683c\u548c\u5f62\u72b6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4fdd\u6301\u6a21\u578b\u65e0\u5173\u7279\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u5404\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u865a\u62df\u8bd5\u7a7f\u7cfb\u7edf\uff0c\u5bf9\u7535\u5546\u3001\u5b9a\u5236\u9020\u578b\u548c\u865a\u62df\u5f62\u8c61\u521b\u5efa\u7b49\u9ad8\u7cbe\u5ea6\u865a\u62df\u8bd5\u7a7f\u5e94\u7528\u6709\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2509.17660", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17660", "abs": "https://arxiv.org/abs/2509.17660", "authors": ["Yikun Ma", "Bo Li", "Ying Chen", "Zijie Yue", "Shuchang Xu", "Jingyao Li", "Lei Ma", "Liang Zhong", "Duowu Zou", "Leiming Xu", "Yunshi Zhong", "Xiaobo Li", "Weiqun Ding", "Minmin Zhang", "Dongli He", "Zhenghong Li", "Ye Chen", "Ye Zhao", "Jialong Zhuo", "Xiaofen Wu", "Lisha Yi", "Miaojing Shi", "Huihui Sun"], "title": "Development and validation of an AI foundation model for endoscopic diagnosis of esophagogastric junction adenocarcinoma: a cohort and deep learning study", "comment": null, "summary": "The early detection of esophagogastric junction adenocarcinoma (EGJA) is\ncrucial for improving patient prognosis, yet its current diagnosis is highly\noperator-dependent. This paper aims to make the first attempt to develop an\nartificial intelligence (AI) foundation model-based method for both screening\nand staging diagnosis of EGJA using endoscopic images. In this cohort and\nlearning study, we conducted a multicentre study across seven Chinese hospitals\nbetween December 28, 2016 and December 30, 2024. It comprises 12,302 images\nfrom 1,546 patients; 8,249 of them were employed for model training, while the\nremaining were divided into the held-out (112 patients, 914 images), external\n(230 patients, 1,539 images), and prospective (198 patients, 1,600 images) test\nsets for evaluation. The proposed model employs DINOv2 (a vision foundation\nmodel) and ResNet50 (a convolutional neural network) to extract features of\nglobal appearance and local details of endoscopic images for EGJA staging\ndiagnosis. Our model demonstrates satisfactory performance for EGJA staging\ndiagnosis across three test sets, achieving an accuracy of 0.9256, 0.8895, and\n0.8956, respectively. In contrast, among representative AI models, the best one\n(ResNet50) achieves an accuracy of 0.9125, 0.8382, and 0.8519 on the three test\nsets, respectively; the expert endoscopists achieve an accuracy of 0.8147 on\nthe held-out test set. Moreover, with the assistance of our model, the overall\naccuracy for the trainee, competent, and expert endoscopists improves from\n0.7035, 0.7350, and 0.8147 to 0.8497, 0.8521, and 0.8696, respectively. To our\nknowledge, our model is the first application of foundation models for EGJA\nstaging diagnosis and demonstrates great potential in both diagnostic accuracy\nand efficiency.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u98df\u7ba1\u80c3\u7ed3\u5408\u90e8\u817a\u764c\uff08EGJA\uff09\u7684\u7b5b\u67e5\u548c\u5206\u671f\u8bca\u65ad\uff0c\u4f7f\u7528DINOv2\u548cResNet50\u7ed3\u5408\u5168\u5c40\u5916\u89c2\u548c\u5c40\u90e8\u7ec6\u8282\u7279\u5f81\uff0c\u5728\u591a\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709AI\u6a21\u578b\u548c\u4e13\u5bb6\u7684\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "EGJA\u7684\u65e9\u671f\u68c0\u6d4b\u5bf9\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7684\u8bca\u65ad\u9ad8\u5ea6\u4f9d\u8d56\u64cd\u4f5c\u8005\u7ecf\u9a8c\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5ba2\u89c2\u3001\u51c6\u786e\u7684AI\u8f85\u52a9\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u4e2d\u5fc3\u7814\u7a76\u8bbe\u8ba1\uff0c\u6536\u96c612,302\u5f20\u5185\u955c\u56fe\u50cf\uff0c\u4f7f\u7528DINOv2\uff08\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff09\u548cResNet50\uff08\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u63d0\u53d6\u56fe\u50cf\u7279\u5f81\uff0c\u5206\u522b\u5173\u6ce8\u5168\u5c40\u5916\u89c2\u548c\u5c40\u90e8\u7ec6\u8282\uff0c\u7528\u4e8eEGJA\u5206\u671f\u8bca\u65ad\u3002", "result": "\u6a21\u578b\u5728\u4e09\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u5206\u522b\u8fbe\u52300.9256\u30010.8895\u548c0.8956\uff0c\u4f18\u4e8e\u6700\u4f73AI\u6a21\u578b\uff08ResNet50\uff09\u548c\u4e13\u5bb6\u5185\u955c\u533b\u5e08\u3002\u6a21\u578b\u8f85\u52a9\u4e0b\uff0c\u5404\u7ea7\u533b\u5e08\u7684\u8bca\u65ad\u51c6\u786e\u7387\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8fd9\u662f\u57fa\u7840\u6a21\u578b\u5728EGJA\u5206\u671f\u8bca\u65ad\u4e2d\u7684\u9996\u6b21\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5728\u8bca\u65ad\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u6709\u671b\u6210\u4e3a\u4e34\u5e8a\u8f85\u52a9\u8bca\u65ad\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2509.17664", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17664", "abs": "https://arxiv.org/abs/2509.17664", "authors": ["Pingyi Chen", "Yujing Lou", "Shen Cao", "Jinhui Guo", "Lubin Fan", "Yue Wu", "Lin Yang", "Lizhuang Ma", "Jieping Ye"], "title": "SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models", "comment": "Accepted by NeurIPS 2025", "summary": "While vision language models (VLMs) excel in 2D semantic visual\nunderstanding, their ability to quantitatively reason about 3D spatial\nrelationships remains under-explored, due to the deficiency of 2D images'\nspatial representation ability. In this paper, we analyze the problem hindering\nVLMs' spatial understanding abilities and propose SD-VLM, a novel framework\nthat significantly enhances fundamental spatial perception abilities of VLMs\nthrough two key contributions: (1) propose Massive Spatial Measuring and\nUnderstanding (MSMU) dataset with precise spatial annotations, and (2)\nintroduce a simple depth positional encoding method strengthening VLMs' spatial\nawareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA\npairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented\nsamples. We have trained SD-VLM, a strong generalist VLM which shows superior\nquantitative spatial measuring and understanding capability. SD-VLM not only\nachieves state-of-the-art performance on our proposed MSMU-Bench, but also\nshows spatial generalization abilities on other spatial understanding\nbenchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments\ndemonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and\n25.56% respectively on MSMU-Bench. Code and models are released at\nhttps://github.com/cpystan/SD-VLM.", "AI": {"tldr": "SD-VLM\u662f\u4e00\u4e2a\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b3D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u7a7a\u95f4\u6d4b\u91cf\u6570\u636e\u96c6\u548c\u6df1\u5ea6\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86VLMs\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57282D\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u57283D\u7a7a\u95f4\u5173\u7cfb\u7684\u5b9a\u91cf\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f2D\u56fe\u50cf\u7f3a\u4e4f\u8db3\u591f\u7684\u7a7a\u95f4\u8868\u793a\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u5173\u952e\u8d21\u732e\uff1a(1) MSMU\u6570\u636e\u96c6\uff0c\u5305\u542b\u7cbe\u786e\u7684\u7a7a\u95f4\u6807\u6ce8\uff0c\u6db5\u76d6700K\u95ee\u7b54\u5bf9\u3001250\u4e07\u7269\u7406\u6570\u503c\u6807\u6ce8\u548c10K\u601d\u7ef4\u94fe\u589e\u5f3a\u6837\u672c\uff1b(2) \u7b80\u5355\u7684\u6df1\u5ea6\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u589e\u5f3aVLMs\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u3002", "result": "SD-VLM\u5728MSMU-Bench\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4GPT-4o\u548cIntern-VL3-78B\u5206\u522b\u63d0\u534726.91%\u548c25.56%\uff0c\u5728\u5176\u4ed6\u7a7a\u95f4\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u663e\u793a\u51fa\u826f\u597d\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SD-VLM\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u5b9a\u91cf\u7a7a\u95f4\u6d4b\u91cf\u548c\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3VLMs\u57283D\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.17670", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17670", "abs": "https://arxiv.org/abs/2509.17670", "authors": ["Mariette Sch\u00f6nfeld", "Wannes Meert", "Hendrik Blockeel"], "title": "Tailored Transformation Invariance for Industrial Anomaly Detection", "comment": null, "summary": "Industrial Anomaly Detection (IAD) is a subproblem within Computer Vision\nAnomaly Detection that has been receiving increasing amounts of attention due\nto its applicability to real-life scenarios. Recent research has focused on how\nto extract the most informative features, contrasting older kNN-based methods\nthat use only pretrained features. These recent methods are much more expensive\nto train however and could complicate real-life application. Careful study of\nrelated work with regards to transformation invariance leads to the idea that\npopular benchmarks require robustness to only minor translations. With this\nidea we then formulate LWinNN, a local window based approach that creates a\nmiddle ground between kNN based methods that have either complete or no\ntranslation invariance. Our experiments demonstrate that this small change\nincreases accuracy considerably, while simultaneously decreasing both train and\ntest time. This teaches us two things: first, the gap between kNN-based\napproaches and more complex state-of-the-art methodology can still be narrowed\nby effective usage of the limited data available. Second, our assumption of\nrequiring only limited translation invariance highlights potential areas of\ninterest for future work and the need for more spatially diverse benchmarks,\nfor which our method can hopefully serve as a new baseline. Our code can be\nfound at https://github.com/marietteschonfeld/LWinNN .", "AI": {"tldr": "LWinNN\u662f\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u7a97\u53e3\u7684\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728kNN\u65b9\u6cd5\u548c\u590d\u6742SOTA\u65b9\u6cd5\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u70b9\uff0c\u901a\u8fc7\u6709\u9650\u5e73\u79fb\u4e0d\u53d8\u6027\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u5e76\u51cf\u5c11\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u8981\u4e48\u8bad\u7ec3\u6210\u672c\u9ad8\uff08\u590d\u6742SOTA\u65b9\u6cd5\uff09\uff0c\u8981\u4e48\u7279\u5f81\u5229\u7528\u4e0d\u8db3\uff08\u4f20\u7edfkNN\u65b9\u6cd5\uff09\uff0c\u4e14\u53d1\u73b0\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u9700\u5bf9\u5fae\u5c0f\u5e73\u79fb\u5177\u6709\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u5c40\u90e8\u7a97\u53e3\u65b9\u6cd5LWinNN\uff0c\u5728\u5b8c\u5168\u5e73\u79fb\u4e0d\u53d8\u6027\uff08kNN\uff09\u548c\u65e0\u5e73\u79fb\u4e0d\u53d8\u6027\uff08\u590d\u6742\u65b9\u6cd5\uff09\u4e4b\u95f4\u5efa\u7acb\u4e2d\u95f4\u65b9\u6848\uff0c\u901a\u8fc7\u6709\u9650\u5e73\u79fb\u4e0d\u53d8\u6027\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLWinNN\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u95f4\uff0c\u7f29\u5c0f\u4e86kNN\u65b9\u6cd5\u4e0eSOTA\u65b9\u6cd5\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "kNN\u65b9\u6cd5\u7684\u6f5c\u529b\u5c1a\u672a\u5b8c\u5168\u6316\u6398\uff0c\u6709\u9650\u5e73\u79fb\u4e0d\u53d8\u6027\u662f\u5173\u952e\u6539\u8fdb\u65b9\u5411\uff1b\u9700\u8981\u66f4\u5177\u7a7a\u95f4\u591a\u6837\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0cLWinNN\u53ef\u4f5c\u4e3a\u65b0\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2509.17684", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17684", "abs": "https://arxiv.org/abs/2509.17684", "authors": ["ThankGod Egbe", "Peng Wang", "Zhihao Guo", "Zidong Chen"], "title": "DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning", "comment": null, "summary": "This paper evaluates DINOv3, a recent large-scale self-supervised vision\nbackbone, for visuomotor diffusion policy learning in robotic manipulation. We\ninvestigate whether a purely self-supervised encoder can match or surpass\nconventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under\nthree regimes: training from scratch, frozen, and finetuned. Across four\nbenchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned\ndiffusion policy, we find that (i) finetuned DINOv3 matches or exceeds\nResNet-18 on several tasks, (ii) frozen DINOv3 remains competitive, indicating\nstrong transferable priors, and (iii) self-supervised features improve sample\nefficiency and robustness. These results support self-supervised large visual\nmodels as effective, generalizable perceptual front-ends for action diffusion\npolicies, motivating further exploration of scalable label-free pretraining in\nrobotic manipulation. Compared to using ResNet18 as a backbone, our approach\nwith DINOv3 achieves up to a 10% absolute increase in test-time success rates\non challenging tasks such as Can, and on-the-par performance in tasks like\nLift, PushT, and Square.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u81ea\u76d1\u7763\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edcDINOv3\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u5fae\u8c03\u540e\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8a\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u7684ResNet-18\uff0c\u7279\u522b\u662f\u5728\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u7814\u7a76\u81ea\u76d1\u7763\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\u662f\u5426\u80fd\u591f\u66ff\u4ee3\u4f20\u7edf\u7684\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u89c6\u89c9\u8fd0\u52a8\u6269\u6563\u7b56\u7565\u7684\u611f\u77e5\u524d\u7aef\uff0c\u63a2\u7d22\u65e0\u6807\u7b7e\u9884\u8bad\u7ec3\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u5728\u56db\u4e2a\u57fa\u51c6\u4efb\u52a1\uff08Push-T\u3001Lift\u3001Can\u3001Square\uff09\u4e0a\uff0c\u4f7f\u7528\u7edf\u4e00\u7684FiLM\u6761\u4ef6\u6269\u6563\u7b56\u7565\uff0c\u6bd4\u8f83DINOv3\u548cResNet-18\u5728\u4e09\u79cd\u8bad\u7ec3\u6a21\u5f0f\uff08\u4ece\u5934\u8bad\u7ec3\u3001\u51bb\u7ed3\u3001\u5fae\u8c03\uff09\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5fae\u8c03\u540e\u7684DINOv3\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5339\u914d\u6216\u8d85\u8d8aResNet-18\uff1b\u51bb\u7ed3\u7684DINOv3\u4ecd\u5177\u7ade\u4e89\u529b\uff1b\u81ea\u76d1\u7763\u7279\u5f81\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\uff1b\u5728Can\u4efb\u52a1\u4e0a\u6d4b\u8bd5\u6210\u529f\u7387\u63d0\u534710%\u3002", "conclusion": "\u81ea\u76d1\u7763\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\u53ef\u4f5c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u52a8\u4f5c\u6269\u6563\u7b56\u7565\u7684\u6709\u6548\u3001\u53ef\u6cdb\u5316\u611f\u77e5\u524d\u7aef\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u63a2\u7d22\u53ef\u6269\u5c55\u7684\u65e0\u6807\u7b7e\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2509.17686", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17686", "abs": "https://arxiv.org/abs/2509.17686", "authors": ["Mohamad Mofeed Chaar", "Jamal Raiyn", "Galia Weidl"], "title": "Predicting Depth Maps from Single RGB Images and Addressing Missing Information in Depth Estimation", "comment": "8 pages, 10 figures, VEHITS conference 2025", "summary": "Depth imaging is a crucial area in Autonomous Driving Systems (ADS), as it\nplays a key role in detecting and measuring objects in the vehicle's\nsurroundings. However, a significant challenge in this domain arises from\nmissing information in Depth images, where certain points are not measurable\ndue to gaps or inconsistencies in pixel data. Our research addresses two key\ntasks to overcome this challenge. First, we developed an algorithm using a\nmulti-layered training approach to generate Depth images from a single RGB\nimage. Second, we addressed the issue of missing information in Depth images by\napplying our algorithm to rectify these gaps, resulting in Depth images with\ncomplete and accurate data. We further tested our algorithm on the Cityscapes\ndataset and successfully resolved the missing information in its Depth images,\ndemonstrating the effectiveness of our approach in real-world urban\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u6210\u6df1\u5ea6\u56fe\u50cf\u5e76\u4fee\u590d\u6df1\u5ea6\u56fe\u50cf\u4e2d\u7f3a\u5931\u4fe1\u606f\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u5c42\u8bad\u7ec3\u65b9\u6cd5\u5728Cityscapes\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u6df1\u5ea6\u6210\u50cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6df1\u5ea6\u56fe\u50cf\u5e38\u5b58\u5728\u4fe1\u606f\u7f3a\u5931\u95ee\u9898\uff0c\u5f71\u54cd\u7269\u4f53\u68c0\u6d4b\u548c\u6d4b\u91cf\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u591a\u5c42\u8bad\u7ec3\u65b9\u6cd5\u5f00\u53d1\u7b97\u6cd5\uff0c\u80fd\u591f\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u6210\u6df1\u5ea6\u56fe\u50cf\uff0c\u5e76\u4fee\u590d\u6df1\u5ea6\u56fe\u50cf\u4e2d\u7684\u7f3a\u5931\u4fe1\u606f\u3002", "result": "\u5728Cityscapes\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u6210\u529f\u89e3\u51b3\u4e86\u6df1\u5ea6\u56fe\u50cf\u4e2d\u7684\u4fe1\u606f\u7f3a\u5931\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u7b97\u6cd5\u5728\u771f\u5b9e\u57ce\u5e02\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u5b8c\u6574\u7684\u6df1\u5ea6\u56fe\u50cf\u5e76\u4fee\u590d\u7f3a\u5931\u4fe1\u606f\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6df1\u5ea6\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17689", "abs": "https://arxiv.org/abs/2509.17689", "authors": ["\u017diga Babnik", "Deepak Kumar Jain", "Peter Peer", "Vitomir \u0160truc"], "title": "FROQ: Observing Face Recognition Models for Efficient Quality Assessment", "comment": "Presented at the International Joint Conference on Biometrics (IJCB\n  2025)", "summary": "Face Recognition (FR) plays a crucial role in many critical (high-stakes)\napplications, where errors in the recognition process can lead to serious\nconsequences. Face Image Quality Assessment (FIQA) techniques enhance FR\nsystems by providing quality estimates of face samples, enabling the systems to\ndiscard samples that are unsuitable for reliable recognition or lead to\nlow-confidence recognition decisions. Most state-of-the-art FIQA techniques\nrely on extensive supervised training to achieve accurate quality estimation.\nIn contrast, unsupervised techniques eliminate the need for additional training\nbut tend to be slower and typically exhibit lower performance. In this paper,\nwe introduce FROQ (Face Recognition Observer of Quality), a semi-supervised,\ntraining-free approach that leverages specific intermediate representations\nwithin a given FR model to estimate face-image quality, and combines the\nefficiency of supervised FIQA models with the training-free approach of\nunsupervised methods. A simple calibration step based on pseudo-quality labels\nallows FROQ to uncover specific representations, useful for quality assessment,\nin any modern FR model. To generate these pseudo-labels, we propose a novel\nunsupervised FIQA technique based on sample perturbations. Comprehensive\nexperiments with four state-of-the-art FR models and eight benchmark datasets\nshow that FROQ leads to highly competitive results compared to the\nstate-of-the-art, achieving both strong performance and efficient runtime,\nwithout requiring explicit training.", "AI": {"tldr": "FROQ\u662f\u4e00\u79cd\u534a\u76d1\u7763\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u9762\u90e8\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u76d1\u7763\u65b9\u6cd5\u7684\u9ad8\u6548\u6027\u548c\u65e0\u76d1\u7763\u65b9\u6cd5\u7684\u65e0\u9700\u8bad\u7ec3\u4f18\u52bf\uff0c\u901a\u8fc7\u5229\u7528FR\u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u4e2d\u95f4\u8868\u793a\u6765\u4f30\u8ba1\u9762\u90e8\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u5f53\u524dFIQA\u6280\u672f\u5927\u591a\u4f9d\u8d56\u5927\u91cf\u76d1\u7763\u8bad\u7ec3\uff0c\u800c\u65e0\u76d1\u7763\u65b9\u6cd5\u867d\u7136\u65e0\u9700\u8bad\u7ec3\u4f46\u6027\u80fd\u8f83\u4f4e\u4e14\u901f\u5ea6\u8f83\u6162\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u53c8\u65e0\u9700\u663e\u5f0f\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "method": "FROQ\u5229\u7528FR\u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u4e2d\u95f4\u8868\u793a\u8fdb\u884c\u8d28\u91cf\u4f30\u8ba1\uff0c\u901a\u8fc7\u57fa\u4e8e\u4f2a\u8d28\u91cf\u6807\u7b7e\u7684\u7b80\u5355\u6821\u51c6\u6b65\u9aa4\u6765\u53d1\u73b0\u9002\u7528\u4e8e\u8d28\u91cf\u8bc4\u4f30\u7684\u8868\u793a\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6837\u672c\u6270\u52a8\u7684\u65b0\u578b\u65e0\u76d1\u7763FIQA\u6280\u672f\u6765\u751f\u6210\u4f2a\u6807\u7b7e\u3002", "result": "\u57284\u4e2a\u6700\u5148\u8fdb\u7684FR\u6a21\u578b\u548c8\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cFROQ\u76f8\u6bd4\u6700\u5148\u8fdb\u6280\u672f\u53d6\u5f97\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5b9e\u73b0\u4e86\u5f3a\u6027\u80fd\u548c\u9ad8\u6548\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "FROQ\u6210\u529f\u5730\u5c06\u76d1\u7763FIQA\u6a21\u578b\u7684\u6548\u7387\u4e0e\u65e0\u76d1\u7763\u65b9\u6cd5\u7684\u65e0\u9700\u8bad\u7ec3\u4f18\u52bf\u76f8\u7ed3\u5408\uff0c\u4e3a\u9762\u90e8\u8bc6\u522b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u9760\u7684\u8d28\u91cf\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17702", "abs": "https://arxiv.org/abs/2509.17702", "authors": ["Patrick Schmidt", "Vasileios Belagiannis", "Lazaros Nalpantidis"], "title": "Depth Edge Alignment Loss: DEALing with Depth in Weakly Supervised Semantic Segmentation", "comment": "Submitted to IEEE", "summary": "Autonomous robotic systems applied to new domains require an abundance of\nexpensive, pixel-level dense labels to train robust semantic segmentation\nmodels under full supervision. This study proposes a model-agnostic Depth Edge\nAlignment Loss to improve Weakly Supervised Semantic Segmentation models across\ndifferent datasets. The methodology generates pixel-level semantic labels from\nimage-level supervision, avoiding expensive annotation processes. While weak\nsupervision is widely explored in traditional computer vision, our approach\nadds supervision with pixel-level depth information, a modality commonly\navailable in robotic systems. We demonstrate how our approach improves\nsegmentation performance across datasets and models, but can also be combined\nwith other losses for even better performance, with improvements up to +5.439,\n+1.274 and +16.416 points in mean Intersection over Union on the PASCAL VOC /\nMS COCO validation, and the HOPE static onboarding split, respectively. Our\ncode will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6df1\u5ea6\u8fb9\u7f18\u5bf9\u9f50\u635f\u5931\uff0c\u7528\u4e8e\u6539\u8fdb\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u5229\u7528\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5e38\u89c1\u7684\u6df1\u5ea6\u4fe1\u606f\u6765\u589e\u5f3a\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u65b0\u9886\u57df\u5e94\u7528\u65f6\u9700\u8981\u5927\u91cf\u6602\u8d35\u7684\u50cf\u7d20\u7ea7\u5bc6\u96c6\u6807\u7b7e\u6765\u8bad\u7ec3\u8bed\u4e49\u5206\u5272\u6a21\u578b\u3002\u5f31\u76d1\u7763\u65b9\u6cd5\u53ef\u4ee5\u907f\u514d\u6602\u8d35\u7684\u6807\u6ce8\u8fc7\u7a0b\uff0c\u4f46\u4f20\u7edf\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\u7f3a\u4e4f\u6df1\u5ea6\u4fe1\u606f\u7684\u5229\u7528\u3002", "method": "\u751f\u6210\u50cf\u7d20\u7ea7\u8bed\u4e49\u6807\u7b7e\u4ece\u56fe\u50cf\u7ea7\u76d1\u7763\u4e2d\uff0c\u5e76\u6dfb\u52a0\u50cf\u7d20\u7ea7\u6df1\u5ea6\u4fe1\u606f\u4f5c\u4e3a\u989d\u5916\u76d1\u7763\u6a21\u6001\uff0c\u63d0\u51fa\u6df1\u5ea6\u8fb9\u7f18\u5bf9\u9f50\u635f\u5931\u6765\u6539\u8fdb\u5206\u5272\u6a21\u578b\u3002", "result": "\u5728PASCAL VOC/MS COCO\u9a8c\u8bc1\u96c6\u548cHOPE\u9759\u6001\u767b\u673a\u5206\u5272\u4e0a\uff0c\u5e73\u5747\u4ea4\u5e76\u6bd4\u5206\u522b\u63d0\u9ad8\u4e86+5.439\u3001+1.274\u548c+16.416\u4e2a\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u5206\u5272\u6027\u80fd\uff0c\u5e76\u53ef\u4e0e\u5176\u4ed6\u635f\u5931\u51fd\u6570\u7ed3\u5408\u83b7\u5f97\u66f4\u597d\u6548\u679c\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2509.17704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17704", "abs": "https://arxiv.org/abs/2509.17704", "authors": ["Bo Li", "Yunkuo Lei", "Tingting Bao", "Yaxian Wang", "Lingling Zhang", "Jun Liu"], "title": "Neurodynamics-Driven Coupled Neural P Systems for Multi-Focus Image Fusion", "comment": "10 pages, 8 figures", "summary": "Multi-focus image fusion (MFIF) is a crucial technique in image processing,\nwith a key challenge being the generation of decision maps with precise\nboundaries. However, traditional methods based on heuristic rules and deep\nlearning methods with black-box mechanisms are difficult to generate\nhigh-quality decision maps. To overcome this challenge, we introduce\nneurodynamics-driven coupled neural P (CNP) systems, which are third-generation\nneural computation models inspired by spiking mechanisms, to enhance the\naccuracy of decision maps. Specifically, we first conduct an in-depth analysis\nof the model's neurodynamics to identify the constraints between the network\nparameters and the input signals. This solid analysis avoids abnormal\ncontinuous firing of neurons and ensures the model accurately distinguishes\nbetween focused and unfocused regions, generating high-quality decision maps\nfor MFIF. Based on this analysis, we propose a\n\\textbf{N}eurodynamics-\\textbf{D}riven \\textbf{CNP} \\textbf{F}usion model\n(\\textbf{ND-CNPFuse}) tailored for the challenging MFIF task. Unlike current\nideas of decision map generation, ND-CNPFuse distinguishes between focused and\nunfocused regions by mapping the source image into interpretable spike\nmatrices. By comparing the number of spikes, an accurate decision map can be\ngenerated directly without any post-processing. Extensive experimental results\nshow that ND-CNPFuse achieves new state-of-the-art performance on four\nclassical MFIF datasets, including Lytro, MFFW, MFI-WHU, and Real-MFF. The code\nis available at https://github.com/MorvanLi/ND-CNPFuse.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u52a8\u529b\u5b66\u9a71\u52a8\u7684\u8026\u5408\u795e\u7ecfP\u7cfb\u7edf\uff08ND-CNPFuse\uff09\u7684\u591a\u7126\u70b9\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6e90\u56fe\u50cf\u6620\u5c04\u4e3a\u53ef\u89e3\u91ca\u7684\u8109\u51b2\u77e9\u9635\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u51b3\u7b56\u56fe\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u542f\u53d1\u5f0f\u89c4\u5219\u7684\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u9ed1\u76d2\u673a\u5236\u96be\u4ee5\u751f\u6210\u5177\u6709\u7cbe\u786e\u8fb9\u754c\u7684\u9ad8\u8d28\u91cf\u51b3\u7b56\u56fe\uff0c\u591a\u7126\u70b9\u56fe\u50cf\u878d\u5408\u9762\u4e34\u51b3\u7b56\u56fe\u8fb9\u754c\u7cbe\u5ea6\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u7b2c\u4e09\u4ee3\u795e\u7ecf\u8ba1\u7b97\u6a21\u578b\u2014\u2014\u795e\u7ecf\u52a8\u529b\u5b66\u9a71\u52a8\u7684\u8026\u5408\u795e\u7ecfP\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u795e\u7ecf\u52a8\u529b\u5b66\u6765\u786e\u4fdd\u795e\u7ecf\u5143\u6b63\u5e38\u6fc0\u6d3b\uff0c\u5c06\u6e90\u56fe\u50cf\u6620\u5c04\u4e3a\u53ef\u89e3\u91ca\u7684\u8109\u51b2\u77e9\u9635\uff0c\u901a\u8fc7\u6bd4\u8f83\u8109\u51b2\u6570\u91cf\u76f4\u63a5\u751f\u6210\u51c6\u786e\u51b3\u7b56\u56fe\u800c\u65e0\u9700\u540e\u5904\u7406\u3002", "result": "\u5728Lytro\u3001MFFW\u3001MFI-WHU\u548cReal-MFF\u56db\u4e2a\u7ecf\u5178\u591a\u7126\u70b9\u56fe\u50cf\u878d\u5408\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cND-CNPFuse\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "ND-CNPFuse\u901a\u8fc7\u795e\u7ecf\u52a8\u529b\u5b66\u9a71\u52a8\u7684\u8026\u5408\u795e\u7ecfP\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u591a\u7126\u70b9\u56fe\u50cf\u878d\u5408\u4e2d\u51b3\u7b56\u56fe\u8fb9\u754c\u7cbe\u5ea6\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u65e0\u9700\u540e\u5904\u7406\u7684\u9ad8\u6548\u878d\u5408\u65b9\u6cd5\u3002"}}
{"id": "2509.17707", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17707", "abs": "https://arxiv.org/abs/2509.17707", "authors": ["Emre G\u00fclsoylu", "Alhassan Abdelhalim", "Derya Kara Boztas", "Ole Grasse", "Carlos Jahn", "Simone Frintrop", "Janick Edinger"], "title": "Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review", "comment": "Submission to Transport Reviews. 36 pages, 2 figures, 4 tables", "summary": "The standardisation of Intermodal Loading Units (ILUs), such as containers,\nsemi-trailers and swap bodies, has revolutionised global trade yet their\nefficient and robust identification remains a critical bottleneck in\nhigh-throughput ports and terminals. This paper reviews 63 empirical studies\nthat propose computer vision (CV) based solutions. It covers the last 35 years\n(1990-2025), tracing the field's evolution from early digital image processing\n(DIP) and traditional machine learning (ML) to the current dominance of deep\nlearning (DL) techniques. While CV offers cost-effective alternatives for other\ntypes of identification techniques, its development is hindered by the lack of\npublicly available benchmarking datasets. This results in high variance for the\nreported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond\ndataset limitations, this review highlights the emerging challenges especially\nintroduced by the shift from character-based text recognition to scene-text\nspotting and the integration of mobile cameras (e.g. drones, sensor equipped\nground vehicles) for dynamic terminal monitoring. To advance the field, the\npaper calls for standardised terminology, open-access datasets, shared source\ncode, while outlining future research directions such as contextless text\nrecognition optimised for ISO6346 codes.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e8663\u9879\u5173\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u5728\u96c6\u88c5\u7bb1\u7b49\u8054\u8fd0\u88c5\u8f7d\u5355\u5143\u8bc6\u522b\u4e2d\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86\u8be5\u9886\u57df35\u5e74\u6765\u7684\u6280\u672f\u6f14\u8fdb\uff0c\u6307\u51fa\u4e86\u6570\u636e\u96c6\u7f3a\u4e4f\u548c\u7ed3\u679c\u5dee\u5f02\u5927\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u8054\u8fd0\u88c5\u8f7d\u5355\u5143\u7684\u9ad8\u6548\u8bc6\u522b\u662f\u6e2f\u53e3\u8fd0\u8425\u7684\u5173\u952e\u74f6\u9888\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\u63d0\u4f9b\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6\u548c\u6807\u51c6\u5316\u963b\u788d\u4e86\u8be5\u9886\u57df\u53d1\u5c55\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e861990-2025\u5e74\u95f463\u9879\u5b9e\u8bc1\u7814\u7a76\uff0c\u8ffd\u8e2a\u4e86\u4ece\u65e9\u671f\u6570\u5b57\u56fe\u50cf\u5904\u7406\u548c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5230\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u4e3b\u5bfc\u7684\u6280\u672f\u6f14\u8fdb\u8def\u5f84\u3002", "result": "\u53d1\u73b0\u8ba1\u7b97\u673a\u89c6\u89c9\u8bc6\u522b\u7ed3\u679c\u7684\u7aef\u5230\u7aef\u51c6\u786e\u7387\u5dee\u5f02\u5de8\u5927\uff085%-96%\uff09\uff0c\u4e3b\u8981\u53d7\u6570\u636e\u96c6\u9650\u5236\u5f71\u54cd\uff1b\u540c\u65f6\u9762\u4e34\u4ece\u5b57\u7b26\u8bc6\u522b\u5411\u573a\u666f\u6587\u672c\u8bc6\u522b\u8f6c\u53d8\u7684\u65b0\u6311\u6218\u3002", "conclusion": "\u547c\u5401\u5efa\u7acb\u6807\u51c6\u5316\u672f\u8bed\u3001\u5f00\u653e\u6570\u636e\u96c6\u548c\u5171\u4eab\u6e90\u4ee3\u7801\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u9488\u5bf9ISO6346\u4ee3\u7801\u7684\u65e0\u4e0a\u4e0b\u6587\u6587\u672c\u8bc6\u522b\u4f18\u5316\u3002"}}
{"id": "2509.17712", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17712", "abs": "https://arxiv.org/abs/2509.17712", "authors": ["Geonho Bang", "Minjae Seong", "Jisong Kim", "Geunju Baek", "Daye Oh", "Junhyung Kim", "Junho Koh", "Jun Won Choi"], "title": "RCTDistill: Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion", "comment": "Accepted at ICCV 2025", "summary": "Radar-camera fusion methods have emerged as a cost-effective approach for 3D\nobject detection but still lag behind LiDAR-based methods in performance.\nRecent works have focused on employing temporal fusion and Knowledge\nDistillation (KD) strategies to overcome these limitations. However, existing\napproaches have not sufficiently accounted for uncertainties arising from\nobject motion or sensor-specific errors inherent in radar and camera\nmodalities. In this work, we propose RCTDistill, a novel cross-modal KD method\nbased on temporal fusion, comprising three key modules: Range-Azimuth Knowledge\nDistillation (RAKD), Temporal Knowledge Distillation (TKD), and\nRegion-Decoupled Knowledge Distillation (RDKD). RAKD is designed to consider\nthe inherent errors in the range and azimuth directions, enabling effective\nknowledge transfer from LiDAR features to refine inaccurate BEV\nrepresentations. TKD mitigates temporal misalignment caused by dynamic objects\nby aligning historical radar-camera BEV features with current LiDAR\nrepresentations. RDKD enhances feature discrimination by distilling relational\nknowledge from the teacher model, allowing the student to differentiate\nforeground and background features. RCTDistill achieves state-of-the-art\nradar-camera fusion performance on both the nuScenes and View-of-Delft (VoD)\ndatasets, with the fastest inference speed of 26.2 FPS.", "AI": {"tldr": "RCTDistill\u662f\u4e00\u79cd\u57fa\u4e8e\u65f6\u5e8f\u878d\u5408\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6a21\u5757\u89e3\u51b3\u96f7\u8fbe-\u76f8\u673a\u878d\u5408\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5728nuScenes\u548cVoD\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u96f7\u8fbe-\u76f8\u673a\u878d\u5408\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4ecd\u843d\u540e\u4e8eLiDAR\u65b9\u6cd5\uff0c\u4e14\u672a\u5145\u5206\u8003\u8651\u7269\u4f53\u8fd0\u52a8\u548c\u4f20\u611f\u5668\u7279\u5b9a\u8bef\u5dee\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51faRCTDistill\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aRAKD\uff08\u8003\u8651\u8ddd\u79bb\u548c\u65b9\u4f4d\u89d2\u8bef\u5dee\uff09\u3001TKD\uff08\u7f13\u89e3\u52a8\u6001\u7269\u4f53\u7684\u65f6\u5e8f\u9519\u4f4d\uff09\u3001RDKD\uff08\u589e\u5f3a\u7279\u5f81\u533a\u5206\u80fd\u529b\uff09\u3002", "result": "\u5728nuScenes\u548cView-of-Delft\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u96f7\u8fbe-\u76f8\u673a\u878d\u5408\u6027\u80fd\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe26.2 FPS\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u548c\u65f6\u5e8f\u878d\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f7\u8fbe-\u76f8\u673a\u878d\u5408\u76843D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.17726", "categories": ["cs.CV", "cs.LG", "I.4.0"], "pdf": "https://arxiv.org/pdf/2509.17726", "abs": "https://arxiv.org/abs/2509.17726", "authors": ["Javier Bisbal", "Patrick Winter", "Sebastian Jofre", "Aaron Ponce", "Sameer A. Ansari", "Ramez Abdalla", "Michael Markl", "Oliver Welin Odeback", "Sergio Uribe", "Cristian Tejos", "Julio Sotelo", "Susanne Schnell", "David Marlevi"], "title": "Automated Labeling of Intracranial Arteries with Uncertainty Quantification Using Deep Learning", "comment": "16 pages, 6 figures", "summary": "Accurate anatomical labeling of intracranial arteries is essential for\ncerebrovascular diagnosis and hemodynamic analysis but remains time-consuming\nand subject to interoperator variability. We present a deep learning-based\nframework for automated artery labeling from 3D Time-of-Flight Magnetic\nResonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating\nuncertainty quantification to enhance interpretability and reliability. We\nevaluated three convolutional neural network architectures: (1) a UNet with\nresidual encoder blocks, reflecting commonly used baselines in vascular\nlabeling; (2) CS-Net, an attention-augmented UNet incorporating channel and\nspatial attention mechanisms for enhanced curvilinear structure recognition;\nand (3) nnUNet, a self-configuring framework that automates preprocessing,\ntraining, and architectural adaptation based on dataset characteristics. Among\nthese, nnUNet achieved the highest labeling performance (average Dice score:\n0.922; average surface distance: 0.387 mm), with improved robustness in\nanatomically complex vessels. To assess predictive confidence, we implemented\ntest-time augmentation (TTA) and introduced a novel coordinate-guided strategy\nto reduce interpolation errors during augmented inference. The resulting\nuncertainty maps reliably indicated regions of anatomical ambiguity,\npathological variation, or manual labeling inconsistency. We further validated\nclinical utility by comparing flow velocities derived from automated and manual\nlabels in co-registered 4D Flow MRI datasets, observing close agreement with no\nstatistically significant differences. Our framework offers a scalable,\naccurate, and uncertainty-aware solution for automated cerebrovascular\nlabeling, supporting downstream hemodynamic analysis and facilitating clinical\nintegration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece3D TOF-MRA\u5206\u5272\u4e2d\u81ea\u52a8\u6807\u8bb0\u9885\u5185\u52a8\u8109\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002\u8bc4\u4f30\u4e86\u4e09\u79cdCNN\u67b6\u6784\uff0c\u5176\u4e2dnnUNet\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u901a\u8fc7\u8840\u6d41\u901f\u5ea6\u9a8c\u8bc1\u4e86\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "motivation": "\u9885\u5185\u52a8\u8109\u7684\u7cbe\u786e\u89e3\u5256\u6807\u8bb0\u5bf9\u8111\u8840\u7ba1\u8bca\u65ad\u548c\u8840\u6d41\u52a8\u529b\u5b66\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4ecd\u8017\u65f6\u4e14\u5b58\u5728\u64cd\u4f5c\u8005\u95f4\u53d8\u5f02\u6027\u3002", "method": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff1a\u5e26\u6b8b\u5dee\u7f16\u7801\u5757\u7684UNet\u3001\u5177\u6709\u901a\u9053\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u7684CS-Net\u3001\u4ee5\u53ca\u81ea\u914d\u7f6e\u7684nnUNet\u6846\u67b6\u3002\u5b9e\u65bd\u4e86\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a\u548c\u65b0\u7684\u5750\u6807\u5f15\u5bfc\u7b56\u7565\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "nnUNet\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u6807\u8bb0\u6027\u80fd\uff08\u5e73\u5747Dice\u5f97\u5206\uff1a0.922\uff1b\u5e73\u5747\u8868\u9762\u8ddd\u79bb\uff1a0.387 mm\uff09\uff0c\u5728\u89e3\u5256\u590d\u6742\u8840\u7ba1\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002\u4e0d\u786e\u5b9a\u6027\u56fe\u53ef\u9760\u5730\u6307\u793a\u4e86\u89e3\u5256\u6a21\u7cca\u3001\u75c5\u7406\u53d8\u5f02\u6216\u624b\u52a8\u6807\u8bb0\u4e0d\u4e00\u81f4\u7684\u533a\u57df\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u8111\u8840\u7ba1\u6807\u8bb0\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u51c6\u786e\u4e14\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u4e0b\u6e38\u8840\u6d41\u52a8\u529b\u5b66\u5206\u6790\u5e76\u4fc3\u8fdb\u4e34\u5e8a\u6574\u5408\u3002"}}
{"id": "2509.17740", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17740", "abs": "https://arxiv.org/abs/2509.17740", "authors": ["Yiwen Jiang", "Deval Mehta", "Siyuan Yan", "Yaling Shen", "Zimu Wang", "Zongyuan Ge"], "title": "WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification", "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Multimodal Large Language Models (MLLMs) have shown promise in visual-textual\nreasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly\nenhancing interpretability. However, existing MCoT methods rely on\nrationale-rich datasets and largely focus on inter-object reasoning,\noverlooking the intra-object understanding crucial for image classification. To\naddress this gap, we propose WISE, a Weak-supervision-guided Step-by-step\nExplanation method that augments any image classification dataset with MCoTs by\nreformulating the concept-based representations from Concept Bottleneck Models\n(CBMs) into concise, interpretable reasoning chains under weak supervision.\nExperiments across ten datasets show that our generated MCoTs not only improve\ninterpretability by 37% but also lead to gains in classification accuracy when\nused to fine-tune MLLMs. Our work bridges concept-based interpretability and\ngenerative MCoT reasoning, providing a generalizable framework for enhancing\nMLLMs in fine-grained visual understanding.", "AI": {"tldr": "WISE\u65b9\u6cd5\u901a\u8fc7\u5f31\u76d1\u7763\u5f15\u5bfc\u7684\u9010\u6b65\u89e3\u91ca\uff0c\u5c06\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u7684\u6982\u5ff5\u8868\u793a\u8f6c\u5316\u4e3a\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u94fe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u65b9\u6cd5\u5ffd\u89c6\u7ec6\u7c92\u5ea6\u7269\u4f53\u5185\u90e8\u7406\u89e3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709MCoT\u65b9\u6cd5\u4f9d\u8d56\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u4e14\u4e3b\u8981\u5173\u6ce8\u7269\u4f53\u95f4\u63a8\u7406\uff0c\u5ffd\u89c6\u4e86\u56fe\u50cf\u5206\u7c7b\u4e2d\u5173\u952e\u7684\u7269\u4f53\u5185\u90e8\u7406\u89e3\u9700\u6c42\u3002", "method": "\u63d0\u51faWISE\u65b9\u6cd5\uff0c\u5728\u5f31\u76d1\u7763\u4e0b\u5c06\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u7684\u6982\u5ff5\u8868\u793a\u91cd\u65b0\u8868\u8ff0\u4e3a\u7b80\u6d01\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u94fe\uff0c\u4e3a\u4efb\u4f55\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u751f\u6210MCoT\u3002", "result": "\u572810\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u751f\u6210\u7684MCoT\u4e0d\u4ec5\u5c06\u53ef\u89e3\u91ca\u6027\u63d0\u9ad8\u4e8637%\uff0c\u800c\u4e14\u5728\u5fae\u8c03MLLM\u65f6\u8fd8\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8fde\u63a5\u4e86\u57fa\u4e8e\u6982\u5ff5\u7684\u53ef\u89e3\u91ca\u6027\u548c\u751f\u6210\u5f0fMCoT\u63a8\u7406\uff0c\u4e3a\u589e\u5f3aMLLM\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2509.17743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17743", "abs": "https://arxiv.org/abs/2509.17743", "authors": ["Chenglin Li", "Feng Han", "FengTao", "Ruilin Li", "Qianglong Chen", "Jingqi Tong", "Yin Zhang", "Jiaqi Wang"], "title": "Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA", "comment": null, "summary": "Large language models (LLMs) have shown promise in generating program\nworkflows for visual tasks. However, previous approaches often rely on\nclosed-source models, lack systematic reasoning, and struggle with long-form\nvideo question answering (videoQA). To address these challenges, we introduce\nthe FS-VisPR framework, an adaptive visual program reasoning approach that\nbalances fast reasoning for simple queries with slow reasoning for difficult\nones. First, we design efficient visual modules (e.g., key clip retrieval and\nsubtitle retrieval) to support long-form video tasks. Then, we construct a\ndiverse and high-quality fast-slow reasoning dataset with a strong LLM to align\nopen-source language models' ability to generate visual program workflows as\nFS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple\nqueries are directly solved by VideoLLMs, while difficult ones invoke visual\nprogram reasoning, motivated by human-like reasoning processes. During this\nprocess, low-confidence fast-thinking answers will trigger a second-stage\nslow-reasoning process, and a fallback mechanism to fast reasoning is activated\nif the program execution fails. Moreover, we improve visual programs through\nparameter search during both training and inference. By adjusting the\nparameters of the visual modules within the program, multiple variants are\ngenerated: during training, programs that yield correct answers are selected,\nwhile during inference, the program with the highest confidence result is\napplied. Experiments show that FS-VisPR improves both efficiency and\nreliability in visual program workflows. It achieves 50.4% accuracy on LVBench,\nsurpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.", "AI": {"tldr": "FS-VisPR\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u89c6\u89c9\u7a0b\u5e8f\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5feb\u901f-\u6162\u901f\u63a8\u7406\u673a\u5236\u5e73\u8861\u7b80\u5355\u548c\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\uff0c\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u4f9d\u8d56\u95ed\u6e90\u6a21\u578b\u3001\u7f3a\u4e4f\u7cfb\u7edf\u63a8\u7406\u80fd\u529b\u3001\u96be\u4ee5\u5904\u7406\u957f\u89c6\u9891\u95ee\u7b54\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u9ad8\u6548\u89c6\u89c9\u6a21\u5757\uff0c\u6784\u5efa\u5feb\u901f-\u6162\u901f\u63a8\u7406\u6570\u636e\u96c6\uff0c\u5f00\u53d1FS-LLM\u751f\u6210\u89c6\u89c9\u7a0b\u5e8f\u5de5\u4f5c\u6d41\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u63a8\u7406\u673a\u5236\u548c\u53c2\u6570\u641c\u7d22\u4f18\u5316\u3002", "result": "\u5728LVBench\u4e0a\u8fbe\u523050.4%\u51c6\u786e\u7387\uff0c\u8d85\u8d8aGPT-4o\uff0c\u5728VideoMME\u4e0a\u4e0eQwen2.5VL-72B\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "FS-VisPR\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u7a0b\u5e8f\u5de5\u4f5c\u6d41\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17747", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17747", "abs": "https://arxiv.org/abs/2509.17747", "authors": ["Sheng Huang", "Jiexuan Yan", "Beiyan Liu", "Bo Liu", "Richang Hong"], "title": "Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification", "comment": "accepted by IEEE Transactions on Image Processing", "summary": "Real-world datasets often exhibit class imbalance across multiple categories,\nmanifesting as long-tailed distributions and few-shot scenarios. This is\nespecially challenging in Class-Imbalanced Multi-Label Image Classification\n(CI-MLIC) tasks, where data imbalance and multi-object recognition present\nsignificant obstacles. To address these challenges, we propose a novel method\ntermed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which\nleverages multi-modal knowledge from vision-language pretrained (VLP) models to\nmitigate the class-imbalance problem in multi-label settings. Specifically,\nHP-DVAL employs dual-view alignment learning to transfer the powerful feature\nrepresentation capabilities from VLP models by extracting complementary\nfeatures for accurate image-text alignment. To better adapt VLP models for\nCI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes\nglobal and local prompts to learn task-specific and context-related prior\nknowledge. Additionally, we design a semantic consistency loss during prompt\ntuning to prevent learned prompts from deviating from general knowledge\nembedded in VLP models. The effectiveness of our approach is validated on two\nCI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results\ndemonstrate the superiority of our method over SOTA approaches, achieving mAP\nimprovements of 10.0\\% and 5.2\\% on the long-tailed multi-label image\nclassification task, and 6.8\\% and 2.9\\% on the multi-label few-shot image\nclassification task.", "AI": {"tldr": "\u63d0\u51faHP-DVAL\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u89e3\u51b3\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u89c6\u89d2\u5bf9\u9f50\u5b66\u4e60\u548c\u5206\u5c42\u63d0\u793a\u8c03\u4f18\u7b56\u7565\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u5e38\u5448\u73b0\u957f\u5c3e\u5206\u5e03\u548c\u5c11\u6837\u672c\u573a\u666f\uff0c\u7279\u522b\u662f\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u6570\u636e\u4e0d\u5e73\u8861\u548c\u591a\u76ee\u6807\u8bc6\u522b\u5e26\u6765\u91cd\u5927\u6311\u6218", "method": "\u91c7\u7528\u53cc\u89c6\u89d2\u5bf9\u9f50\u5b66\u4e60\u4eceVLP\u6a21\u578b\u8fc1\u79fb\u5f3a\u5927\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u5f15\u5165\u5206\u5c42\u63d0\u793a\u8c03\u4f18\u7b56\u7565\u4f7f\u7528\u5168\u5c40\u548c\u5c40\u90e8\u63d0\u793a\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\uff0c\u5e76\u8bbe\u8ba1\u8bed\u4e49\u4e00\u81f4\u6027\u635f\u5931\u9632\u6b62\u63d0\u793a\u504f\u79bbVLP\u6a21\u578b\u7684\u901a\u7528\u77e5\u8bc6", "result": "\u5728MS-COCO\u548cVOC2007\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u957f\u5c3e\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1mAP\u5206\u522b\u63d0\u534710.0%\u548c5.2%\uff0c\u591a\u6807\u7b7e\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1mAP\u5206\u522b\u63d0\u53476.8%\u548c2.9%", "conclusion": "HP-DVAL\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5"}}
{"id": "2509.17757", "categories": ["cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.17757", "abs": "https://arxiv.org/abs/2509.17757", "authors": ["Hongxing Fan", "Lipeng Wang", "Haohua Chen", "Zehuan Huang", "Jiangtao Wu", "Lu Sheng"], "title": "Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained Semantic Guidance", "comment": null, "summary": "Amodal completion, generating invisible parts of occluded objects, is vital\nfor applications like image editing and AR. Prior methods face challenges with\ndata needs, generalization, or error accumulation in progressive pipelines. We\npropose a Collaborative Multi-Agent Reasoning Framework based on upfront\ncollaborative reasoning to overcome these issues. Our framework uses multiple\nagents to collaboratively analyze occlusion relationships and determine\nnecessary boundary expansion, yielding a precise mask for inpainting.\nConcurrently, an agent generates fine-grained textual descriptions, enabling\nFine-Grained Semantic Guidance. This ensures accurate object synthesis and\nprevents the regeneration of occluders or other unwanted elements, especially\nwithin large inpainting areas. Furthermore, our method directly produces\nlayered RGBA outputs guided by visible masks and attention maps from a\nDiffusion Transformer, eliminating extra segmentation. Extensive evaluations\ndemonstrate our framework achieves state-of-the-art visual quality.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5206\u6790\u906e\u6321\u5173\u7cfb\u548c\u8fb9\u754c\u6269\u5c55\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u8bed\u4e49\u6307\u5bfc\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u65e0\u6a21\u6001\u56fe\u50cf\u8865\u5168\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u9700\u6c42\u3001\u6cdb\u5316\u80fd\u529b\u548c\u6e10\u8fdb\u5f0f\u7ba1\u9053\u4e2d\u9519\u8bef\u7d2f\u79ef\u65b9\u9762\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u906e\u6321\u5bf9\u8c61\u8865\u5168\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63a8\u7406\u6846\u67b6\u5206\u6790\u906e\u6321\u5173\u7cfb\uff0c\u786e\u5b9a\u8fb9\u754c\u6269\u5c55\u9700\u6c42\uff1b\u540c\u65f6\u751f\u6210\u7ec6\u7c92\u5ea6\u6587\u672c\u63cf\u8ff0\u63d0\u4f9b\u8bed\u4e49\u6307\u5bfc\uff1b\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u6ce8\u610f\u529b\u56fe\u76f4\u63a5\u751f\u6210\u5206\u5c42RGBA\u8f93\u51fa\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\u8bc1\u660e\u8be5\u6846\u67b6\u5728\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u80fd\u591f\u51c6\u786e\u5408\u6210\u5bf9\u8c61\u5e76\u9632\u6b62\u906e\u6321\u7269\u6216\u5176\u4ed6\u4e0d\u9700\u8981\u5143\u7d20\u7684\u518d\u751f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u6a21\u6001\u8865\u5168\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u56fe\u50cf\u7f16\u8f91\u548c\u589e\u5f3a\u73b0\u5b9e\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.17762", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17762", "abs": "https://arxiv.org/abs/2509.17762", "authors": ["Sitian Shen", "Georgi Pramatarov", "Yifu Tao", "Daniele De Martini"], "title": "Neural-MMGS: Multi-modal Neural Gaussian Splats for Large-Scale Scene Reconstruction", "comment": null, "summary": "This paper proposes Neural-MMGS, a novel neural 3DGS framework for multimodal\nlarge-scale scene reconstruction that fuses multiple sensing modalities in a\nper-gaussian compact, learnable embedding. While recent works focusing on\nlarge-scale scene reconstruction have incorporated LiDAR data to provide more\naccurate geometric constraints, we argue that LiDAR's rich physical properties\nremain underexplored. Similarly, semantic information has been used for object\nretrieval, but could provide valuable high-level context for scene\nreconstruction. Traditional approaches append these properties to Gaussians as\nseparate parameters, increasing memory usage and limiting information exchange\nacross modalities. Instead, our approach fuses all modalities -- image, LiDAR,\nand semantics -- into a compact, learnable embedding that implicitly encodes\noptical, physical, and semantic features in each Gaussian. We then train\nlightweight neural decoders to map these embeddings to Gaussian parameters,\nenabling the reconstruction of each sensing modality with lower memory overhead\nand improved scalability. We evaluate Neural-MMGS on the Oxford Spires and\nKITTI-360 datasets. On Oxford Spires, we achieve higher-quality\nreconstructions, while on KITTI-360, our method reaches competitive results\nwith less storage consumption compared with current approaches in LiDAR-based\nnovel-view synthesis.", "AI": {"tldr": "Neural-MMGS\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u795e\u7ecf3D\u9ad8\u65af\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\uff0c\u901a\u8fc7\u5c06\u591a\u79cd\u611f\u77e5\u6a21\u6001\u878d\u5408\u5230\u6bcf\u4e2a\u9ad8\u65af\u7684\u7d27\u51d1\u53ef\u5b66\u4e60\u5d4c\u5165\u4e2d\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u5de5\u4f5c\u867d\u7136\u6574\u5408\u4e86LiDAR\u6570\u636e\u63d0\u4f9b\u51e0\u4f55\u7ea6\u675f\uff0c\u4f46LiDAR\u7684\u4e30\u5bcc\u7269\u7406\u5c5e\u6027\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u6f5c\u529b\u672a\u88ab\u5145\u5206\u6316\u6398\u3002\u4f20\u7edf\u65b9\u6cd5\u5c06\u4e0d\u540c\u6a21\u6001\u5c5e\u6027\u4f5c\u4e3a\u72ec\u7acb\u53c2\u6570\u9644\u52a0\u5230\u9ad8\u65af\u4e0a\uff0c\u5bfc\u81f4\u5185\u5b58\u4f7f\u7528\u589e\u52a0\u4e14\u6a21\u6001\u95f4\u4fe1\u606f\u4ea4\u6362\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u5c06\u56fe\u50cf\u3001LiDAR\u548c\u8bed\u4e49\u4fe1\u606f\u878d\u5408\u5230\u7d27\u51d1\u7684\u53ef\u5b66\u4e60\u5d4c\u5165\u4e2d\uff0c\u9690\u5f0f\u7f16\u7801\u5149\u5b66\u3001\u7269\u7406\u548c\u8bed\u4e49\u7279\u5f81\u3002\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u795e\u7ecf\u89e3\u7801\u5668\u5c06\u8fd9\u4e9b\u5d4c\u5165\u6620\u5c04\u5230\u9ad8\u65af\u53c2\u6570\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u91cd\u5efa\u3002", "result": "\u5728Oxford Spires\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u66f4\u9ad8\u8d28\u91cf\u7684\u91cd\u5efa\u7ed3\u679c\uff0c\u5728KITTI-360\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e0e\u5f53\u524dLiDAR\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u7ade\u4e89\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u5b58\u50a8\u6d88\u8017\u66f4\u4f4e\u3002", "conclusion": "Neural-MMGS\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u7d27\u51d1\u5d4c\u5165\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u57283D\u9ad8\u65af\u91cd\u5efa\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2509.17769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17769", "abs": "https://arxiv.org/abs/2509.17769", "authors": ["Yang Li", "Xinyi Zeng", "Zhe Xue", "Pinxian Zeng", "Zikai Zhang", "Yan Wang"], "title": "Incorporating the Refractory Period into Spiking Neural Networks through Spike-Triggered Threshold Dynamics", "comment": null, "summary": "As the third generation of neural networks, spiking neural networks (SNNs)\nhave recently gained widespread attention for their biological plausibility,\nenergy efficiency, and effectiveness in processing neuromorphic datasets. To\nbetter emulate biological neurons, various models such as Integrate-and-Fire\n(IF) and Leaky Integrate-and-Fire (LIF) have been widely adopted in SNNs.\nHowever, these neuron models overlook the refractory period, a fundamental\ncharacteristic of biological neurons. Research on excitable neurons reveal that\nafter firing, neurons enter a refractory period during which they are\ntemporarily unresponsive to subsequent stimuli. This mechanism is critical for\npreventing over-excitation and mitigating interference from aberrant signals.\nTherefore, we propose a simple yet effective method to incorporate the\nrefractory period into spiking LIF neurons through spike-triggered threshold\ndynamics, termed RPLIF. Our method ensures that each spike accurately encodes\nneural information, effectively preventing neuron over-excitation under\ncontinuous inputs and interference from anomalous inputs. Incorporating the\nrefractory period into LIF neurons is seamless and computationally efficient,\nenhancing robustness and efficiency while yielding better performance with\nnegligible overhead. To the best of our knowledge, RPLIF achieves\nstate-of-the-art performance on Cifar10-DVS(82.40%) and N-Caltech101(83.35%)\nwith fewer timesteps and demonstrates superior performance on DVS128\nGesture(97.22%) at low latency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRPLIF\u7684\u65b0\u578b\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u901a\u8fc7\u5728LIF\u795e\u7ecf\u5143\u4e2d\u5f15\u5165\u4e0d\u5e94\u671f\u673a\u5236\u6765\u66f4\u597d\u5730\u6a21\u62df\u751f\u7269\u795e\u7ecf\u5143\u7279\u6027\uff0c\u4ece\u800c\u63d0\u5347SNN\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709SNN\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684IF\u548cLIF\u795e\u7ecf\u5143\u6a21\u578b\u5ffd\u7565\u4e86\u751f\u7269\u795e\u7ecf\u5143\u7684\u4e0d\u5e94\u671f\u7279\u6027\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u795e\u7ecf\u5143\u8fc7\u5ea6\u5174\u594b\u548c\u5bf9\u5f02\u5e38\u4fe1\u53f7\u7684\u5e72\u6270\u3002\u751f\u7269\u795e\u7ecf\u5143\u5728\u53d1\u653e\u8109\u51b2\u540e\u4f1a\u8fdb\u5165\u4e0d\u5e94\u671f\uff0c\u6682\u65f6\u65e0\u6cd5\u54cd\u5e94\u540e\u7eed\u523a\u6fc0\uff0c\u8fd9\u4e00\u673a\u5236\u5bf9\u4e8e\u9632\u6b62\u8fc7\u5ea6\u5174\u594b\u548c\u51cf\u5c11\u5e72\u6270\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faRPLIF\u65b9\u6cd5\uff0c\u901a\u8fc7\u8109\u51b2\u89e6\u53d1\u7684\u9608\u503c\u52a8\u6001\u673a\u5236\u5c06\u4e0d\u5e94\u671f\u6574\u5408\u5230LIF\u795e\u7ecf\u5143\u4e2d\u3002\u8be5\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u80fd\u591f\u786e\u4fdd\u6bcf\u4e2a\u8109\u51b2\u51c6\u786e\u7f16\u7801\u795e\u7ecf\u4fe1\u606f\uff0c\u6709\u6548\u9632\u6b62\u795e\u7ecf\u5143\u5728\u8fde\u7eed\u8f93\u5165\u4e0b\u7684\u8fc7\u5ea6\u5174\u594b\u548c\u5f02\u5e38\u8f93\u5165\u7684\u5e72\u6270\u3002", "result": "RPLIF\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff1aCifar10-DVS\u8fbe\u523082.40%\uff0cN-Caltech101\u8fbe\u523083.35%\uff0c\u4e14\u4f7f\u7528\u66f4\u5c11\u7684\u65f6\u95f4\u6b65\u957f\u3002\u5728DVS128 Gesture\u6570\u636e\u96c6\u4e0a\u4f4e\u5ef6\u8fdf\u6761\u4ef6\u4e0b\u8fbe\u523097.22%\u7684\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "\u5c06\u4e0d\u5e94\u671f\u673a\u5236\u6574\u5408\u5230LIF\u795e\u7ecf\u5143\u4e2d\u662f\u7b80\u5355\u6709\u6548\u7684\uff0c\u80fd\u591f\u4ee5\u53ef\u5ffd\u7565\u7684\u5f00\u9500\u63d0\u5347SNN\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u540c\u65f6\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2509.17773", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17773", "abs": "https://arxiv.org/abs/2509.17773", "authors": ["Guanjie Wang", "Zehua Ma", "Han Fang", "Weiming Zhang"], "title": "I2VWM: Robust Watermarking for Image to Video Generation", "comment": "10 pages", "summary": "The rapid progress of image-guided video generation (I2V) has raised concerns\nabout its potential misuse in misinformation and fraud, underscoring the urgent\nneed for effective digital watermarking. While existing watermarking methods\ndemonstrate robustness within a single modality, they fail to trace source\nimages in I2V settings. To address this gap, we introduce the concept of Robust\nDiffusion Distance, which measures the temporal persistence of watermark\nsignals in generated videos. Building on this, we propose I2VWM, a cross-modal\nwatermarking framework designed to enhance watermark robustness across time.\nI2VWM leverages a video-simulation noise layer during training and employs an\noptical-flow-based alignment module during inference. Experiments on both\nopen-source and commercial I2V models demonstrate that I2VWM significantly\nimproves robustness while maintaining imperceptibility, establishing a new\nparadigm for cross-modal watermarking in the era of generative video.\n\\href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code\nReleased.}", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86I2VWM\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8de8\u6a21\u6001\u6c34\u5370\u95ee\u9898\uff0c\u901a\u8fc7\u9c81\u68d2\u6269\u6563\u8ddd\u79bb\u548c\u5149\u5b66\u6d41\u5bf9\u9f50\u6a21\u5757\u63d0\u9ad8\u6c34\u5370\u5728\u89c6\u9891\u4e2d\u7684\u65f6\u95f4\u6301\u4e45\u6027\u3002", "motivation": "\u968f\u7740\u56fe\u50cf\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u5728\u9519\u8bef\u4fe1\u606f\u548c\u6b3a\u8bc8\u4e2d\u7684\u6f5c\u5728\u6ee5\u7528\u5f15\u53d1\u4e86\u62c5\u5fe7\uff0c\u8feb\u5207\u9700\u8981\u6709\u6548\u7684\u6570\u5b57\u6c34\u5370\u6280\u672f\u3002\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5728\u5355\u4e00\u6a21\u6001\u5185\u8868\u73b0\u826f\u597d\uff0c\u4f46\u65e0\u6cd5\u5728I2V\u8bbe\u7f6e\u4e2d\u8ffd\u8e2a\u6e90\u56fe\u50cf\u3002", "method": "\u63d0\u51fa\u9c81\u68d2\u6269\u6563\u8ddd\u79bb\u6982\u5ff5\u6765\u8861\u91cf\u751f\u6210\u89c6\u9891\u4e2d\u6c34\u5370\u4fe1\u53f7\u7684\u65f6\u95f4\u6301\u4e45\u6027\u3002I2VWM\u6846\u67b6\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u89c6\u9891\u6a21\u62df\u566a\u58f0\u5c42\uff0c\u5728\u63a8\u7406\u65f6\u91c7\u7528\u57fa\u4e8e\u5149\u5b66\u6d41\u7684\u5bf9\u9f50\u6a21\u5757\u3002", "result": "\u5728\u5f00\u6e90\u548c\u5546\u4e1aI2V\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cI2VWM\u663e\u8457\u63d0\u9ad8\u4e86\u6c34\u5370\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "conclusion": "I2VWM\u4e3a\u751f\u6210\u89c6\u9891\u65f6\u4ee3\u7684\u8de8\u6a21\u6001\u6c34\u5370\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.17786", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17786", "abs": "https://arxiv.org/abs/2509.17786", "authors": ["Aniello Panariello", "Daniel Marczak", "Simone Magistri", "Angelo Porrello", "Bart\u0142omiej Twardowski", "Andrew D. Bagdanov", "Simone Calderara", "Joost van de Weijer"], "title": "Accurate and Efficient Low-Rank Model Merging in Core Space", "comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025), San Diego, USA", "summary": "In this paper, we address the challenges associated with merging low-rank\nadaptations of large neural networks. With the rise of parameter-efficient\nadaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning\nhas become more accessible. While fine-tuning models with LoRA is highly\nefficient, existing merging methods often sacrifice this efficiency by merging\nfully-sized weight matrices. We propose the Core Space merging framework, which\nenables the merging of LoRA-adapted models within a common alignment basis,\nthereby preserving the efficiency of low-rank adaptation while substantially\nimproving accuracy across tasks. We further provide a formal proof that\nprojection into Core Space ensures no loss of information and provide a\ncomplexity analysis showing the efficiency gains. Extensive empirical results\ndemonstrate that Core Space significantly improves existing merging techniques\nand achieves state-of-the-art results on both vision and language tasks while\nutilizing a fraction of the computational resources. Codebase is available at\nhttps://github.com/apanariello4/core-space-merging.", "AI": {"tldr": "\u63d0\u51fa\u4e86Core Space\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5171\u4eab\u5bf9\u9f50\u57fa\u4e0a\u5408\u5e76LoRA\u9002\u914d\u6a21\u578b\uff0c\u4fdd\u6301\u4f4e\u79e9\u9002\u914d\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8de8\u4efb\u52a1\u51c6\u786e\u6027", "motivation": "\u73b0\u6709LoRA\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5f80\u5f80\u727a\u7272\u6548\u7387\uff0c\u901a\u8fc7\u5408\u5e76\u5168\u5c3a\u5bf8\u6743\u91cd\u77e9\u9635\uff0c\u65e0\u6cd5\u4fdd\u6301\u4f4e\u79e9\u9002\u914d\u7684\u9ad8\u6548\u6027", "method": "Core Space\u5408\u5e76\u6846\u67b6\uff0c\u5c06\u6a21\u578b\u6295\u5f71\u5230\u5171\u540c\u5bf9\u9f50\u57fa\u7684\u6838\u5fc3\u7a7a\u95f4\uff0c\u786e\u4fdd\u4fe1\u606f\u65e0\u635f\uff0c\u63d0\u4f9b\u5f62\u5f0f\u5316\u8bc1\u660e\u548c\u590d\u6742\u5ea6\u5206\u6790", "result": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\u663e\u8457\u6539\u8fdb\u73b0\u6709\u5408\u5e76\u6280\u672f\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u540c\u65f6\u4ec5\u4f7f\u7528\u5c11\u91cf\u8ba1\u7b97\u8d44\u6e90", "conclusion": "Core Space\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86LoRA\u6a21\u578b\u5408\u5e76\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u53c2\u6570\u9ad8\u6548\u9002\u914d\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5408\u5e76\u65b9\u6848"}}
{"id": "2509.17789", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17789", "abs": "https://arxiv.org/abs/2509.17789", "authors": ["Guoxi Huang", "Haoran Wang", "Zipeng Qi", "Wenjun Lu", "David Bull", "Nantheera Anantrasirichai"], "title": "From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes", "comment": null, "summary": "Underwater image degradation poses significant challenges for 3D\nreconstruction, where simplified physical models often fail in complex scenes.\nWe propose \\textbf{R-Splatting}, a unified framework that bridges underwater\nimage restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both\nrendering quality and geometric fidelity. Our method integrates multiple\nenhanced views produced by diverse UIR models into a single reconstruction\npipeline. During inference, a lightweight illumination generator samples latent\ncodes to support diverse yet coherent renderings, while a contrastive loss\nensures disentangled and stable illumination representations. Furthermore, we\npropose \\textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models\nopacity as a stochastic function to regularize training. This suppresses abrupt\ngradient responses triggered by illumination variation and mitigates\noverfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF\nand our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong\nbaselines in both rendering quality and geometric accuracy.", "AI": {"tldr": "R-Splatting\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u591a\u589e\u5f3a\u89c6\u56fe\u96c6\u6210\u3001\u8f7b\u91cf\u7ea7\u5149\u7167\u751f\u6210\u5668\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4e0d\u900f\u660e\u5ea6\u4f18\u5316\uff0c\u63d0\u5347\u6c34\u4e0b3D\u91cd\u5efa\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u51e0\u4f55\u7cbe\u5ea6", "motivation": "\u6c34\u4e0b\u56fe\u50cf\u9000\u5316\u5bf93D\u91cd\u5efa\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u7b80\u5316\u7684\u7269\u7406\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u5f80\u5f80\u5931\u6548\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u6c34\u4e0b\u73af\u5883\u7684\u590d\u6742\u6027", "method": "1) \u5c06\u591a\u79cdUIR\u6a21\u578b\u4ea7\u751f\u7684\u589e\u5f3a\u89c6\u56fe\u96c6\u6210\u5230\u5355\u4e00\u91cd\u5efa\u6d41\u7a0b\uff1b2) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u5149\u7167\u751f\u6210\u5668\u91c7\u6837\u6f5c\u7801\u652f\u6301\u591a\u6837\u5316\u4f46\u4e00\u81f4\u7684\u6e32\u67d3\uff1b3) \u5bf9\u6bd4\u635f\u5931\u786e\u4fdd\u89e3\u8026\u7a33\u5b9a\u7684\u5149\u7167\u8868\u793a\uff1b4) \u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4e0d\u900f\u660e\u5ea6\u4f18\u5316\uff0c\u5c06\u4e0d\u900f\u660e\u5ea6\u5efa\u6a21\u4e3a\u968f\u673a\u51fd\u6570\u6765\u6b63\u5219\u5316\u8bad\u7ec3", "result": "\u5728Seathru-NeRF\u548c\u65b0BlueCoral3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cR-Splatting\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u51e0\u4f55\u7cbe\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u6c34\u4e0b3D\u91cd\u5efa\u4e2d\u7684\u56fe\u50cf\u9000\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u6846\u67b6\u5b9e\u73b0\u4e86\u6e32\u67d3\u8d28\u91cf\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6\u7684\u663e\u8457\u63d0\u5347"}}
{"id": "2509.17792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17792", "abs": "https://arxiv.org/abs/2509.17792", "authors": ["S M A Sharif", "Abdur Rehman", "Fayaz Ali Dharejo", "Radu Timofte", "Rizwan Ali Naqvi"], "title": "Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding", "comment": null, "summary": "Real-world images often suffer from spatially diverse degradations such as\nhaze, rain, snow, and low-light, significantly impacting visual quality and\ndownstream vision tasks. Existing all-in-one restoration (AIR) approaches\neither depend on external text prompts or embed hand-crafted architectural\npriors (e.g., frequency heuristics); both impose discrete, brittle assumptions\nthat weaken generalization to unseen or mixed degradations. To address this\nlimitation, we propose to reframe AIR as learned latent prior inference, where\ndegradation-aware representations are automatically inferred from the input\nwithout explicit task cues. Based on latent priors, we formulate AIR as a\nstructured reasoning paradigm: (1) which features to route (adaptive feature\nselection), (2) where to restore (spatial localization), and (3) what to\nrestore (degradation semantics). We design a lightweight decoding module that\nefficiently leverages these latent encoded cues for spatially-adaptive\nrestoration. Extensive experiments across six common degradation tasks, five\ncompound settings, and previously unseen degradations demonstrate that our\nmethod outperforms state-of-the-art (SOTA) approaches, achieving an average\nPSNR improvement of 1.68 dB while being three times more efficient.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u6f5c\u5728\u5148\u9a8c\u63a8\u7406\u7684\u5168\u80fd\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7279\u5f81\u9009\u62e9\u3001\u7a7a\u95f4\u5b9a\u4f4d\u548c\u9000\u5316\u8bed\u4e49\u4e09\u4e2a\u7ed3\u6784\u5316\u63a8\u7406\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u79cd\u9000\u5316\u7c7b\u578b\u7684\u6709\u6548\u6062\u590d\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf\u5e38\u53d7\u5230\u96fe\u973e\u3001\u96e8\u96ea\u3001\u4f4e\u5149\u7167\u7b49\u591a\u79cd\u7a7a\u95f4\u591a\u6837\u5316\u9000\u5316\u7684\u5f71\u54cd\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u6587\u672c\u63d0\u793a\u6216\u624b\u5de5\u8bbe\u8ba1\u7684\u67b6\u6784\u5148\u9a8c\uff0c\u8fd9\u4e9b\u79bb\u6563\u5047\u8bbe\u9650\u5236\u4e86\u65b9\u6cd5\u5bf9\u672a\u89c1\u6216\u6df7\u5408\u9000\u5316\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5c06\u5168\u80fd\u56fe\u50cf\u6062\u590d\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5b66\u4e60\u6f5c\u5728\u5148\u9a8c\u63a8\u7406\uff0c\u4ece\u8f93\u5165\u4e2d\u81ea\u52a8\u63a8\u65ad\u9000\u5316\u611f\u77e5\u8868\u793a\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u89e3\u7801\u6a21\u5757\u5229\u7528\u8fd9\u4e9b\u6f5c\u5728\u7f16\u7801\u7ebf\u7d22\u8fdb\u884c\u7a7a\u95f4\u81ea\u9002\u5e94\u6062\u590d\u3002", "result": "\u5728\u516d\u79cd\u5e38\u89c1\u9000\u5316\u4efb\u52a1\u3001\u4e94\u79cd\u590d\u5408\u8bbe\u7f6e\u548c\u672a\u89c1\u9000\u5316\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5e73\u5747PSNR\u63d0\u53471.68 dB\uff0c\u540c\u65f6\u6548\u7387\u63d0\u9ad8\u4e09\u500d\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5168\u80fd\u56fe\u50cf\u6062\u590d\u6784\u5efa\u4e3a\u7ed3\u6784\u5316\u63a8\u7406\u8303\u5f0f\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6062\u590d\u6027\u80fd\uff0c\u5bf9\u591a\u79cd\u9000\u5316\u7c7b\u578b\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.17802", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17802", "abs": "https://arxiv.org/abs/2509.17802", "authors": ["Qi'ao Xu", "Pengfei Wang", "Bo Zhong", "Tianwen Qian", "Xiaoling Wang", "Ye Wang", "Hong Yu"], "title": "TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided Medical Time Series Classification", "comment": "12 pages, 4 figures", "summary": "Medical time series (MedTS) classification is pivotal for intelligent\nhealthcare, yet its efficacy is severely limited by poor cross-subject\ngeneration due to the profound cross-individual heterogeneity. Despite advances\nin architectural innovations and transfer learning techniques, current methods\nremain constrained by modality-specific inductive biases that limit their\nability to learn universally invariant representations. To overcome this, we\npropose TS-P$^2$CL, a novel plug-and-play framework that leverages the\nuniversal pattern recognition capabilities of pre-trained vision models. We\nintroduce a vision-guided paradigm that transforms 1D physiological signals\ninto 2D pseudo-images, establishing a bridge to the visual domain. This\ntransformation enables implicit access to rich semantic priors learned from\nnatural images. Within this unified space, we employ a dual-contrastive\nlearning strategy: intra-modal consistency enforces temporal coherence, while\ncross-modal alignment aligns time-series dynamics with visual semantics,\nthereby mitigating individual-specific biases and learning robust,\ndomain-invariant features. Extensive experiments on six MedTS datasets\ndemonstrate that TS-P$^2$CL consistently outperforms fourteen methods in both\nsubject-dependent and subject-independent settings.", "AI": {"tldr": "TS-P^2CL\u662f\u4e00\u4e2a\u7528\u4e8e\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5c061D\u751f\u7406\u4fe1\u53f7\u8f6c\u6362\u4e3a2D\u4f2a\u56fe\u50cf\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u6a21\u5f0f\u8bc6\u522b\u80fd\u529b\uff0c\u91c7\u7528\u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u6765\u7f13\u89e3\u8de8\u4e2a\u4f53\u5f02\u8d28\u6027\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u5728\u667a\u80fd\u533b\u7597\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8de8\u4e2a\u4f53\u5f02\u8d28\u6027\u4e25\u91cd\uff0c\u73b0\u6709\u65b9\u6cd5\u53d7\u5230\u6a21\u6001\u7279\u5b9a\u5f52\u7eb3\u504f\u7f6e\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u5b66\u4e60\u666e\u904d\u4e0d\u53d8\u7684\u8868\u5f81\u3002", "method": "\u63d0\u51faTS-P^2CL\u6846\u67b6\uff1a1\uff09\u5c061D\u751f\u7406\u4fe1\u53f7\u8f6c\u6362\u4e3a2D\u4f2a\u56fe\u50cf\uff0c\u5efa\u7acb\u4e0e\u89c6\u89c9\u9886\u57df\u7684\u6865\u6881\uff1b2\uff09\u91c7\u7528\u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff1a\u6a21\u6001\u5185\u4e00\u81f4\u6027\u5f3a\u5236\u65f6\u95f4\u8fde\u8d2f\u6027\uff0c\u8de8\u6a21\u6001\u5bf9\u9f50\u5c06\u65f6\u95f4\u5e8f\u5217\u52a8\u6001\u4e0e\u89c6\u89c9\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728\u516d\u4e2a\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTS-P^2CL\u5728\u53d7\u8bd5\u8005\u4f9d\u8d56\u548c\u53d7\u8bd5\u8005\u72ec\u7acb\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e14\u79cd\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u5f15\u5bfc\u8303\u5f0f\u6210\u529f\u7f13\u89e3\u4e86\u4e2a\u4f53\u7279\u5f02\u6027\u504f\u5dee\uff0c\u5b66\u4e60\u4e86\u9c81\u68d2\u7684\u9886\u57df\u4e0d\u53d8\u7279\u5f81\uff0c\u4e3a\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17805", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17805", "abs": "https://arxiv.org/abs/2509.17805", "authors": ["Dong Chen", "Huili Peng", "Yong Hu", "Kenneth MC. Cheung"], "title": "Selecting Optimal Camera Views for Gait Analysis: A Multi-Metric Assessment of 2D Projections", "comment": null, "summary": "Objective: To systematically quantify the effect of the camera view (frontal\nvs. lateral) on the accuracy of 2D markerless gait analysis relative to 3D\nmotion capture ground truth. Methods: Gait data from 18 subjects were recorded\nsimultaneously using frontal, lateral and 3D motion capture systems. Pose\nestimation used YOLOv8. Four metrics were assessed to evaluate agreement:\nDynamic Time Warping (DTW) for temporal alignment, Maximum Cross-Correlation\n(MCC) for signal similarity, Kullback-Leibler Divergence (KLD) for distribution\ndifferences, and Information Entropy (IE) for complexity. Wilcoxon signed-rank\ntests (significance: $p < 0.05$) and Cliff's delta ($\\delta$) were used to\nmeasure statistical differences and effect sizes. Results: Lateral views\nsignificantly outperformed frontal views for sagittal plane kinematics: step\nlength (DTW: $53.08 \\pm 24.50$ vs. $69.87 \\pm 25.36$, $p = 0.005$) and knee\nrotation (DTW: $106.46 \\pm 38.57$ vs. $155.41 \\pm 41.77$, $p = 0.004$). Frontal\nviews were superior for symmetry parameters: trunk rotation (KLD: $0.09 \\pm\n0.06$ vs. $0.30 \\pm 0.19$, $p < 0.001$) and wrist-to-hipmid distance (MCC:\n$105.77 \\pm 29.72$ vs. $75.20 \\pm 20.38$, $p = 0.003$). Effect sizes were\nmedium-to-large ($\\delta: 0.34$--$0.76$). Conclusion: Camera view critically\nimpacts gait parameter accuracy. Lateral views are optimal for sagittal\nkinematics; frontal views excel for trunk symmetry. Significance: This first\nsystematic evidence enables data-driven camera deployment in 2D gait analysis,\nenhancing clinical utility. Future implementations should leverage both views\nvia disease-oriented setups.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u91cf\u5316\u4e86\u6444\u50cf\u673a\u89c6\u89d2\uff08\u6b63\u9762vs\u4fa7\u9762\uff09\u5bf92D\u65e0\u6807\u8bb0\u6b65\u6001\u5206\u6790\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4fa7\u9762\u89c6\u89d2\u5728\u77e2\u72b6\u9762\u8fd0\u52a8\u5b66\u53c2\u6570\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u800c\u6b63\u9762\u89c6\u89d2\u5728\u8eaf\u5e72\u5bf9\u79f0\u6027\u53c2\u6570\u4e0a\u66f4\u51c6\u786e\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5173\u4e8e\u6444\u50cf\u673a\u89c6\u89d2\u5982\u4f55\u5f71\u54cd2D\u65e0\u6807\u8bb0\u6b65\u6001\u5206\u6790\u51c6\u786e\u6027\u7684\u7cfb\u7edf\u6027\u8bc1\u636e\uff0c\u9700\u8981\u4e3a\u4e34\u5e8a\u5e94\u7528\u4e2d\u6444\u50cf\u673a\u90e8\u7f72\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u6307\u5bfc\u3002", "method": "\u4f7f\u752818\u540d\u53d7\u8bd5\u8005\u7684\u6b65\u6001\u6570\u636e\uff0c\u540c\u65f6\u8bb0\u5f55\u6b63\u9762\u3001\u4fa7\u9762\u548c3D\u8fd0\u52a8\u6355\u6349\u6570\u636e\uff0c\u91c7\u7528YOLOv8\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\uff0c\u4f7f\u7528DTW\u3001MCC\u3001KLD\u548cIE\u56db\u79cd\u6307\u6807\u8bc4\u4f30\u4e00\u81f4\u6027\uff0c\u5e76\u8fdb\u884c\u7edf\u8ba1\u663e\u8457\u6027\u68c0\u9a8c\u3002", "result": "\u4fa7\u9762\u89c6\u89d2\u5728\u6b65\u957f\u548c\u819d\u5173\u8282\u65cb\u8f6c\u7b49\u77e2\u72b6\u9762\u8fd0\u52a8\u5b66\u53c2\u6570\u4e0a\u663e\u8457\u4f18\u4e8e\u6b63\u9762\u89c6\u89d2\uff0c\u800c\u6b63\u9762\u89c6\u89d2\u5728\u8eaf\u5e72\u65cb\u8f6c\u548c\u8155-\u9acb\u4e2d\u8ddd\u79bb\u7b49\u5bf9\u79f0\u6027\u53c2\u6570\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u6548\u5e94\u91cf\u4e3a\u4e2d\u7b49\u81f3\u5927\u3002", "conclusion": "\u6444\u50cf\u673a\u89c6\u89d2\u5bf9\u6b65\u6001\u53c2\u6570\u51c6\u786e\u6027\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4fa7\u9762\u89c6\u89d2\u9002\u5408\u77e2\u72b6\u9762\u8fd0\u52a8\u5b66\u5206\u6790\uff0c\u6b63\u9762\u89c6\u89d2\u9002\u5408\u8eaf\u5e72\u5bf9\u79f0\u6027\u5206\u6790\uff0c\u672a\u6765\u5e94\u57fa\u4e8e\u75be\u75c5\u7279\u70b9\u91c7\u7528\u53cc\u89c6\u89d2\u8bbe\u7f6e\u3002"}}
{"id": "2509.17816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17816", "abs": "https://arxiv.org/abs/2509.17816", "authors": ["Brown Ebouky", "Ajad Chhatkuli", "Cristiano Malossi", "Christoph Studer", "Roy Assaf", "Andrea Bartezzaghi"], "title": "Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training", "comment": "24 pages, 5 figures", "summary": "Self-supervised learning (SSL) has emerged as a central paradigm for training\nfoundation models by leveraging large-scale unlabeled datasets, often producing\nrepresentations with strong generalization capabilities. These models are\ntypically pre-trained on general-purpose datasets such as ImageNet and\nsubsequently adapted to various downstream tasks through finetuning. While\nrecent advances have explored parameter-efficient strategies for adapting\npre-trained models, extending SSL pre-training itself to new domains -\nparticularly under limited data regimes and for dense prediction tasks -\nremains underexplored. In this work, we address the problem of adapting vision\nfoundation models to new domains in an unsupervised and data-efficient manner,\nspecifically targeting downstream semantic segmentation. We propose GLARE\n(Global Local and Regional Enforcement), a novel continual self-supervised\npre-training task designed to enhance downstream segmentation performance.\nGLARE introduces patch-level augmentations to encourage local consistency and\nincorporates a regional consistency constraint that leverages spatial semantics\nin the data. For efficient continual pre-training, we initialize Vision\nTransformers (ViTs) with weights from existing SSL models and update only\nlightweight adapter modules - specifically UniAdapter - while keeping the rest\nof the backbone frozen. Experiments across multiple semantic segmentation\nbenchmarks on different domains demonstrate that GLARE consistently improves\ndownstream performance with minimal computational and parameter overhead.", "AI": {"tldr": "GLARE\u662f\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u65e0\u76d1\u7763\u3001\u6570\u636e\u9ad8\u6548\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40-\u5c40\u90e8-\u533a\u57df\u4e00\u81f4\u6027\u7ea6\u675f\u6765\u63d0\u5347\u4e0b\u6e38\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u901a\u7528\u6570\u636e\u96c6\u9884\u8bad\u7ec3\uff0c\u4f46\u5728\u65b0\u9886\u57df\u7279\u522b\u662f\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u9488\u5bf9\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51faGLARE\u6301\u7eed\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u7ed3\u5408\u5c40\u90e8\u4e00\u81f4\u6027\u3001\u533a\u57df\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u6a21\u5757\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u66f4\u65b0\u3002", "result": "\u5728\u591a\u4e2a\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGLARE\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u8ba1\u7b97\u548c\u53c2\u6570\u5f00\u9500\u6301\u7eed\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "GLARE\u4e3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u65b0\u9886\u57df\u7684\u65e0\u76d1\u7763\u81ea\u9002\u5e94\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6570\u636e\u53d7\u9650\u7684\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2509.17818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17818", "abs": "https://arxiv.org/abs/2509.17818", "authors": ["Yiyang Chen", "Xuanhua He", "Xiujun Ma", "Yue Ma"], "title": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment", "comment": "The project page is at https://yychen233.github.io/ContextFlow-page", "summary": "Training-free video object editing aims to achieve precise object-level\nmanipulation, including object insertion, swapping, and deletion. However, it\nfaces significant challenges in maintaining fidelity and temporal consistency.\nExisting methods, often designed for U-Net architectures, suffer from two\nprimary limitations: inaccurate inversion due to first-order solvers, and\ncontextual conflicts caused by crude \"hard\" feature replacement. These issues\nare more challenging in Diffusion Transformers (DiTs), where the unsuitability\nof prior layer-selection heuristics makes effective guidance challenging. To\naddress these limitations, we introduce ContextFlow, a novel training-free\nframework for DiT-based video object editing. In detail, we first employ a\nhigh-order Rectified Flow solver to establish a robust editing foundation. The\ncore of our framework is Adaptive Context Enrichment (for specifying what to\nedit), a mechanism that addresses contextual conflicts. Instead of replacing\nfeatures, it enriches the self-attention context by concatenating Key-Value\npairs from parallel reconstruction and editing paths, empowering the model to\ndynamically fuse information. Additionally, to determine where to apply this\nenrichment (for specifying where to edit), we propose a systematic, data-driven\nanalysis to identify task-specific vital layers. Based on a novel Guidance\nResponsiveness Metric, our method pinpoints the most influential DiT blocks for\ndifferent tasks (e.g., insertion, swapping), enabling targeted and highly\neffective guidance. Extensive experiments show that ContextFlow significantly\noutperforms existing training-free methods and even surpasses several\nstate-of-the-art training-based approaches, delivering temporally coherent,\nhigh-fidelity results.", "AI": {"tldr": "ContextFlow\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u4e8eDiT\u7684\u89c6\u9891\u5bf9\u8c61\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u9636Rectified Flow\u6c42\u89e3\u5668\u548c\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u589e\u5f3a\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u9700\u8bad\u7ec3\u89c6\u9891\u5bf9\u8c61\u7f16\u8f91\u65b9\u6cd5\u5728DiT\u67b6\u6784\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u4e00\u9636\u6c42\u89e3\u5668\u5bfc\u81f4\u7684\u53cd\u8f6c\u4e0d\u51c6\u786e\uff0c\u4ee5\u53ca\u7c97\u7cd9\u7684\"\u786c\"\u7279\u5f81\u66ff\u6362\u5f15\u8d77\u7684\u4e0a\u4e0b\u6587\u51b2\u7a81\u3002DiT\u4e2d\u5148\u524d\u7684\u5c42\u9009\u62e9\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e0d\u9002\u7528\uff0c\u4f7f\u5f97\u6709\u6548\u6307\u5bfc\u53d8\u5f97\u56f0\u96be\u3002", "method": "1. \u4f7f\u7528\u9ad8\u9636Rectified Flow\u6c42\u89e3\u5668\u5efa\u7acb\u7a33\u5065\u7684\u7f16\u8f91\u57fa\u7840\uff1b2. \u63d0\u51fa\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u589e\u5f3a\u673a\u5236\uff0c\u901a\u8fc7\u5e76\u884c\u91cd\u5efa\u548c\u7f16\u8f91\u8def\u5f84\u7684Key-Value\u5bf9\u8fde\u63a5\u6765\u4e30\u5bcc\u81ea\u6ce8\u610f\u529b\u4e0a\u4e0b\u6587\uff1b3. \u57fa\u4e8e\u65b0\u7684\u6307\u5bfc\u54cd\u5e94\u5ea6\u6307\u6807\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u5206\u6790\uff0c\u8bc6\u522b\u4efb\u52a1\u7279\u5b9a\u7684\u5173\u952e\u5c42\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cContextFlow\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u9700\u8bad\u7ec3\u65b9\u6cd5\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u51e0\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u4f9b\u65f6\u95f4\u4e00\u81f4\u3001\u9ad8\u4fdd\u771f\u7684\u7ed3\u679c\u3002", "conclusion": "ContextFlow\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u5c42\u9009\u62e9\u548c\u52a8\u6001\u4fe1\u606f\u878d\u5408\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86DiT\u67b6\u6784\u4e2d\u89c6\u9891\u5bf9\u8c61\u7f16\u8f91\u7684\u6311\u6218\uff0c\u4e3a\u8bad\u7ec3\u81ea\u7531\u7684\u89c6\u9891\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17847", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17847", "abs": "https://arxiv.org/abs/2509.17847", "authors": ["Saghir Alfasly", "Wataru Uegami", "MD Enamul Hoq", "Ghazal Alabtah", "H. R. Tizhoosh"], "title": "Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous Tissue Synthesis in Histopathology", "comment": "NeurIPS 2025", "summary": "Synthetic data generation in histopathology faces unique challenges:\npreserving tissue heterogeneity, capturing subtle morphological features, and\nscaling to unannotated datasets. We present a latent diffusion model that\ngenerates realistic heterogeneous histopathology images through a novel\ndual-conditioning approach combining semantic segmentation maps with\ntissue-specific visual crops. Unlike existing methods that rely on text prompts\nor abstract visual embeddings, our approach preserves critical morphological\ndetails by directly incorporating raw tissue crops from corresponding semantic\nregions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches\nensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we\nintroduce a self-supervised extension that clusters whole-slide images into 100\ntissue types using foundation model embeddings, automatically generating\npseudo-semantic maps for training. Our method synthesizes high-fidelity images\nwith precise region-wise annotations, achieving superior performance on\ndownstream segmentation tasks. When evaluated on annotated datasets, models\ntrained on our synthetic data show competitive performance to those trained on\nreal data, demonstrating the utility of controlled heterogeneous tissue\ngeneration. In quantitative evaluation, prompt-guided synthesis reduces Frechet\nDistance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower\nFD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on\nsynthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within\n1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA\nwhole-slide images without manual annotations, our framework offers a practical\nsolution for an urgent need for generating diverse, annotated histopathology\ndata, addressing a critical bottleneck in computational pathology.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u751f\u6210\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u6761\u4ef6\u65b9\u6cd5\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u56fe\u548c\u7279\u5b9a\u7ec4\u7ec7\u89c6\u89c9\u88c1\u526a\uff0c\u751f\u6210\u5177\u6709\u7cbe\u786e\u533a\u57df\u6ce8\u91ca\u7684\u9ad8\u4fdd\u771f\u5f02\u8d28\u6027\u7ec4\u7ec7\u56fe\u50cf\u3002", "motivation": "\u89e3\u51b3\u7ec4\u7ec7\u75c5\u7406\u5b66\u5408\u6210\u6570\u636e\u751f\u6210\u9762\u4e34\u7684\u6311\u6218\uff1a\u4fdd\u6301\u7ec4\u7ec7\u5f02\u8d28\u6027\u3001\u6355\u6349\u7ec6\u5fae\u5f62\u6001\u7279\u5f81\uff0c\u4ee5\u53ca\u5728\u65e0\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u6761\u4ef6\u65b9\u6cd5\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u56fe\u548c\u539f\u59cb\u7ec4\u7ec7\u88c1\u526a\u3002\u5bf9\u4e8e\u65e0\u6807\u6ce8\u6570\u636e\uff0c\u5f15\u5165\u81ea\u76d1\u7763\u6269\u5c55\uff0c\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u5d4c\u5165\u5c06\u5168\u5207\u7247\u56fe\u50cf\u805a\u7c7b\u4e3a100\u79cd\u7ec4\u7ec7\u7c7b\u578b\uff0c\u81ea\u52a8\u751f\u6210\u4f2a\u8bed\u4e49\u56fe\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728Camelyon16\u6570\u636e\u96c6\u4e0aFr\u00e9chet\u8ddd\u79bb\u964d\u4f4e6\u500d\uff08\u4ece430.1\u964d\u81f372.0\uff09\uff0c\u5728Panda\u548cTCGA\u6570\u636e\u96c6\u4e0a\u964d\u4f4e2-3\u500d\u3002\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684DeepLabv3+\u6a21\u578b\u5728Camelyon16\u548cPanda\u4e0a\u5206\u522b\u8fbe\u52300.71\u548c0.95\u7684IoU\uff0c\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u57fa\u7ebf\uff080.72\u548c0.96\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u751f\u6210\u591a\u6837\u5316\u3001\u6807\u6ce8\u7684\u7ec4\u7ec7\u75c5\u7406\u5b66\u6570\u636e\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u5173\u952e\u74f6\u9888\uff0c\u80fd\u591f\u6269\u5c55\u523011,765\u4e2aTCGA\u5168\u5207\u7247\u56fe\u50cf\u800c\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u3002"}}
{"id": "2509.17864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17864", "abs": "https://arxiv.org/abs/2509.17864", "authors": ["Shi Chen", "Erik Sandstr\u00f6m", "Sandro Lombardi", "Siyuan Li", "Martin R. Oswald"], "title": "ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos", "comment": null, "summary": "Achieving truly practical dynamic 3D reconstruction requires online\noperation, global pose and map consistency, detailed appearance modeling, and\nthe flexibility to handle both RGB and RGB-D inputs. However, existing SLAM\nmethods typically merely remove the dynamic parts or require RGB-D input, while\noffline methods are not scalable to long video sequences, and current\ntransformer-based feedforward methods lack global consistency and appearance\ndetails. To this end, we achieve online dynamic scene reconstruction by\ndisentangling the static and dynamic parts within a SLAM system. The poses are\ntracked robustly with a novel motion masking strategy, and dynamic parts are\nreconstructed leveraging a progressive adaptation of a Motion Scaffolds graph.\nOur method yields novel view renderings competitive to offline methods and\nachieves on-par tracking with state-of-the-art dynamic SLAM methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u52a8\u6001\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7SLAM\u7cfb\u7edf\u5206\u79bb\u9759\u6001\u548c\u52a8\u6001\u90e8\u5206\uff0c\u4f7f\u7528\u8fd0\u52a8\u63a9\u7801\u7b56\u7565\u8fdb\u884c\u9c81\u68d2\u59ff\u6001\u8ddf\u8e2a\uff0c\u5e76\u5229\u7528\u8fd0\u52a8\u652f\u67b6\u56fe\u8fdb\u884c\u52a8\u6001\u90e8\u5206\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709SLAM\u65b9\u6cd5\u901a\u5e38\u4ec5\u79fb\u9664\u52a8\u6001\u90e8\u5206\u6216\u9700\u8981RGB-D\u8f93\u5165\uff0c\u79bb\u7ebf\u65b9\u6cd5\u65e0\u6cd5\u6269\u5c55\u5230\u957f\u89c6\u9891\u5e8f\u5217\uff0c\u800c\u57fa\u4e8etransformer\u7684\u524d\u9988\u65b9\u6cd5\u7f3a\u4e4f\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5916\u89c2\u7ec6\u8282\u3002", "method": "\u5728SLAM\u7cfb\u7edf\u4e2d\u5206\u79bb\u9759\u6001\u548c\u52a8\u6001\u90e8\u5206\uff0c\u91c7\u7528\u65b0\u9896\u7684\u8fd0\u52a8\u63a9\u7801\u7b56\u7565\u8fdb\u884c\u9c81\u68d2\u59ff\u6001\u8ddf\u8e2a\uff0c\u52a8\u6001\u90e8\u5206\u901a\u8fc7\u6e10\u8fdb\u9002\u5e94\u7684\u8fd0\u52a8\u652f\u67b6\u56fe\u8fdb\u884c\u91cd\u5efa\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u4e0e\u79bb\u7ebf\u65b9\u6cd5\u76f8\u7ade\u4e89\u7684\u65b0\u89c6\u89d2\u6e32\u67d3\u7ed3\u679c\uff0c\u5e76\u5728\u8ddf\u8e2a\u6027\u80fd\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u52a8\u6001SLAM\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u771f\u6b63\u5b9e\u7528\u7684\u52a8\u60013D\u91cd\u5efa\uff0c\u5177\u5907\u5728\u7ebf\u64cd\u4f5c\u3001\u5168\u5c40\u59ff\u6001\u548c\u5730\u56fe\u4e00\u81f4\u6027\u3001\u8be6\u7ec6\u5916\u89c2\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u80fd\u7075\u6d3b\u5904\u7406RGB\u548cRGB-D\u8f93\u5165\u3002"}}
{"id": "2509.17888", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17888", "abs": "https://arxiv.org/abs/2509.17888", "authors": ["Divya Mereddy", "Marcos Quinones-Grueiro", "Ashwin T S", "Eduardo Davalos", "Gautam Biswas", "Kent Etherton", "Tyler Davis", "Katelyn Kay", "Jill Lear", "Benjamin Goldberg"], "title": "Trainee Action Recognition through Interaction Analysis in CCATT Mixed-Reality Training", "comment": null, "summary": "This study examines how Critical Care Air Transport Team (CCATT) members are\ntrained using mixed-reality simulations that replicate the high-pressure\nconditions of aeromedical evacuation. Each team - a physician, nurse, and\nrespiratory therapist - must stabilize severely injured soldiers by managing\nventilators, IV pumps, and suction devices during flight. Proficient\nperformance requires clinical expertise and cognitive skills, such as\nsituational awareness, rapid decision-making, effective communication, and\ncoordinated task management, all of which must be maintained under stress.\nRecent advances in simulation and multimodal data analytics enable more\nobjective and comprehensive performance evaluation. In contrast, traditional\ninstructor-led assessments are subjective and may overlook critical events,\nthereby limiting generalizability and consistency. However, AI-based automated\nand more objective evaluation metrics still demand human input to train the AI\nalgorithms to assess complex team dynamics in the presence of environmental\nnoise and the need for accurate re-identification in multi-person tracking. To\naddress these challenges, we introduce a systematic, data-driven assessment\nframework that combines Cognitive Task Analysis (CTA) with Multimodal Learning\nAnalytics (MMLA). We have developed a domain-specific CTA model for CCATT\ntraining and a vision-based action recognition pipeline using a fine-tuned\nHuman-Object Interaction model, the Cascade Disentangling Network (CDN), to\ndetect and track trainee-equipment interactions over time. These interactions\nautomatically yield performance indicators (e.g., reaction time, task\nduration), which are mapped onto a hierarchical CTA model tailored to CCATT\noperations, enabling interpretable, domain-relevant performance evaluations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u8ba4\u77e5\u4efb\u52a1\u5206\u6790\u548c\u591a\u6a21\u6001\u5b66\u4e60\u5206\u6790\u7684\u7cfb\u7edf\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5ba2\u89c2\u8bc4\u4f30\u91cd\u75c7\u76d1\u62a4\u822a\u7a7a\u8fd0\u8f93\u56e2\u961f\u5728\u6df7\u5408\u73b0\u5b9e\u6a21\u62df\u8bad\u7ec3\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u5de5\u8bc4\u4f30\u4e3b\u89c2\u6027\u5f3a\u4e14\u53ef\u80fd\u9057\u6f0f\u5173\u952e\u4e8b\u4ef6\uff0c\u800c\u73b0\u6709\u7684AI\u8bc4\u4f30\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6765\u8bad\u7ec3\u7b97\u6cd5\u8bc4\u4f30\u590d\u6742\u56e2\u961f\u52a8\u6001\u3002\u9700\u8981\u66f4\u5ba2\u89c2\u3001\u53ef\u89e3\u91ca\u7684\u6027\u80fd\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u9886\u57df\u7279\u5b9a\u7684\u8ba4\u77e5\u4efb\u52a1\u5206\u6790\u6a21\u578b\u548c\u57fa\u4e8e\u89c6\u89c9\u7684\u52a8\u4f5c\u8bc6\u522b\u7ba1\u9053\uff0c\u4f7f\u7528\u5fae\u8c03\u7684\u4eba-\u7269\u4ea4\u4e92\u6a21\u578b\uff08Cascade Disentangling Network\uff09\u6765\u68c0\u6d4b\u548c\u8ddf\u8e2a\u5b66\u5458\u4e0e\u8bbe\u5907\u7684\u4ea4\u4e92\uff0c\u81ea\u52a8\u751f\u6210\u6027\u80fd\u6307\u6807\u5e76\u6620\u5c04\u5230\u5206\u5c42\u8ba4\u77e5\u4efb\u52a1\u5206\u6790\u6a21\u578b\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u53cd\u5e94\u65f6\u95f4\u3001\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u7b49\u6027\u80fd\u6307\u6807\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u3001\u9886\u57df\u76f8\u5173\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u91cd\u75c7\u76d1\u62a4\u822a\u7a7a\u8fd0\u8f93\u56e2\u961f\u57f9\u8bad\u63d0\u4f9b\u4e86\u66f4\u5ba2\u89c2\u3001\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e3b\u89c2\u6027\u548c\u4e00\u81f4\u6027\u9650\u5236\u95ee\u9898\u3002"}}
{"id": "2509.17901", "categories": ["cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17901", "abs": "https://arxiv.org/abs/2509.17901", "authors": ["Geewook Kim", "Minjoon Seo"], "title": "Does Audio Matter for Modern Video-LLMs and Their Benchmarks?", "comment": "5 pages, 2 figures, under review. Project page:\n  https://github.com/naver-ai/LLaVA-AV-SSM", "summary": "Modern multimodal large language models often claim \"video understanding,\"\nyet most evaluations use muted videos or simply discard audio. We ask a direct\nquestion: how much does audio actually matter for contemporary Video-LLMs and\nthe benchmarks that certify them? We audit widely used suites and observe that\nmany items are even solvable from a single frame, rendering audio largely\nredundant. Building on LLaVA-OneVision architecture, we attach a speech/audio\nencoder (e.g., Whisper) and analyze when audio helps, while addressing audio\ntoken explosion with a lightweight Mamba-based state-space token compressor. We\nfind that audio yields minimal gains on recent video benchmarks but is decisive\non curated, audio-sensitive subsets. To enable faithful evaluation, we release\nAVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a\ngrowing gap between current academic practice and real-world expectations, and\nprovide practical tools for scalable audio-visual Video-LLMs. We will fully\nopen-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u89c6\u9891\u7406\u89e3\u6a21\u578b\u548c\u8bc4\u6d4b\u57fa\u51c6\u8fc7\u5ea6\u4f9d\u8d56\u89c6\u89c9\u4fe1\u606f\uff0c\u97f3\u9891\u8d21\u732e\u6709\u9650\u3002\u4f5c\u8005\u5f00\u53d1\u4e86\u5305\u542b\u97f3\u9891\u7f16\u7801\u5668\u7684\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u521b\u5efa\u4e86\u66f4\u4e25\u683c\u7684\u97f3\u9891\u654f\u611f\u8bc4\u6d4b\u96c6\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u6a21\u578b\u548c\u8bc4\u6d4b\u57fa\u51c6\u5927\u591a\u5ffd\u7565\u97f3\u9891\u4fe1\u606f\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u4e2d\u97f3\u9891\u5bf9\u89c6\u9891\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002\u4f5c\u8005\u5e0c\u671b\u8bc4\u4f30\u97f3\u9891\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5b9e\u9645\u8d21\u732e\uff0c\u5e76\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5904\u7406\u97f3\u9891\u7684\u6a21\u578b\u3002", "method": "\u57fa\u4e8eLLaVA-OneVision\u67b6\u6784\uff0c\u96c6\u6210\u8bed\u97f3/\u97f3\u9891\u7f16\u7801\u5668\uff08\u5982Whisper\uff09\uff0c\u4f7f\u7528\u57fa\u4e8eMamba\u7684\u72b6\u6001\u7a7a\u95f4\u6807\u8bb0\u538b\u7f29\u5668\u89e3\u51b3\u97f3\u9891\u6807\u8bb0\u7206\u70b8\u95ee\u9898\uff0c\u5e76\u521b\u5efa\u4e86AVQA-Hard\u548cMusic-AVQA-Hard\u8bc4\u6d4b\u96c6\u3002", "result": "\u97f3\u9891\u5728\u5f53\u524d\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d21\u732e\u6709\u9650\uff0c\u4f46\u5728\u97f3\u9891\u654f\u611f\u4efb\u52a1\u4e2d\u8d77\u51b3\u5b9a\u6027\u4f5c\u7528\u3002\u65b0\u8bc4\u6d4b\u96c6\u80fd\u66f4\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u7684\u97f3\u9891\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u5f53\u524d\u5b66\u672f\u5b9e\u8df5\u4e0e\u771f\u5b9e\u4e16\u754c\u671f\u671b\u5b58\u5728\u5dee\u8ddd\uff0c\u9700\u8981\u66f4\u91cd\u89c6\u97f3\u9891\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u4f5c\u7528\u3002\u4f5c\u8005\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u97f3\u9891-\u89c6\u89c9\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u5de5\u5177\u3002"}}
{"id": "2509.17925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17925", "abs": "https://arxiv.org/abs/2509.17925", "authors": ["Yuanhan Wang", "Yifei Chen", "Shuo Jiang", "Wenjing Yu", "Mingxuan Liu", "Beining Wu", "Jinying Zong", "Feiwei Qin", "Changmiao Wang", "Qiyuan Tian"], "title": "SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain Brain Tumor Segmentation in MRI", "comment": "11 pages, 6 figures", "summary": "Reliable brain tumor segmentation in MRI is indispensable for treatment\nplanning and outcome monitoring, yet models trained on curated benchmarks often\nfail under domain shifts arising from scanner and protocol variability as well\nas population heterogeneity. Such gaps are especially severe in low-resource\nand pediatric cohorts, where conventional test-time or source-free adaptation\nstrategies often suffer from instability and structural inconsistency. We\npropose SmaRT, a style-modulated robust test-time adaptation framework that\nenables source-free cross-domain generalization. SmaRT integrates style-aware\naugmentation to mitigate appearance discrepancies, a dual-branch momentum\nstrategy for stable pseudo-label refinement, and structural priors enforcing\nconsistency, integrity, and connectivity. This synergy ensures both adaptation\nstability and anatomical fidelity under extreme domain shifts. Extensive\nevaluations on sub-Saharan Africa and pediatric glioma datasets show that SmaRT\nconsistently outperforms state-of-the-art methods, with notable gains in Dice\naccuracy and boundary precision. Overall, SmaRT bridges the gap between\nalgorithmic advances and equitable clinical applicability, supporting robust\ndeployment of MRI-based neuro-oncology tools in diverse clinical environments.\nOur source code is available at https://github.com/baiyou1234/SmaRT.", "AI": {"tldr": "SmaRT\u662f\u4e00\u4e2a\u7528\u4e8e\u8111\u80bf\u7624MRI\u5206\u5272\u7684\u6e90\u81ea\u7531\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u98ce\u683c\u8c03\u5236\u589e\u5f3a\u3001\u53cc\u5206\u652f\u52a8\u91cf\u7b56\u7565\u548c\u7ed3\u6784\u5148\u9a8c\u6765\u5e94\u5bf9\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5728\u4f4e\u8d44\u6e90\u548c\u513f\u79d1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02", "motivation": "\u89e3\u51b3\u8111\u80bf\u7624MRI\u5206\u5272\u6a21\u578b\u5728\u57df\u504f\u79fb\uff08\u626b\u63cf\u4eea\u3001\u534f\u8bae\u3001\u4eba\u7fa4\u5dee\u5f02\uff09\u4e0b\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u548c\u513f\u79d1\u961f\u5217\u4e2d\u4f20\u7edf\u81ea\u9002\u5e94\u65b9\u6cd5\u5b58\u5728\u4e0d\u7a33\u5b9a\u548c\u7ed3\u6784\u4e0d\u4e00\u81f4\u7684\u95ee\u9898", "method": "\u96c6\u6210\u98ce\u683c\u611f\u77e5\u589e\u5f3a\u6765\u51cf\u8f7b\u5916\u89c2\u5dee\u5f02\uff0c\u53cc\u5206\u652f\u52a8\u91cf\u7b56\u7565\u7528\u4e8e\u7a33\u5b9a\u4f2a\u6807\u7b7e\u7ec6\u5316\uff0c\u7ed3\u6784\u5148\u9a8c\u786e\u4fdd\u4e00\u81f4\u6027\u3001\u5b8c\u6574\u6027\u548c\u8fde\u901a\u6027", "result": "\u5728\u6492\u54c8\u62c9\u4ee5\u5357\u975e\u6d32\u548c\u513f\u79d1\u80f6\u8d28\u7624\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0cSmaRT\u5728Dice\u51c6\u786e\u7387\u548c\u8fb9\u754c\u7cbe\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "SmaRT\u5f25\u5408\u4e86\u7b97\u6cd5\u8fdb\u6b65\u4e0e\u516c\u5e73\u4e34\u5e8a\u9002\u7528\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u652f\u6301MRI\u795e\u7ecf\u80bf\u7624\u5de5\u5177\u5728\u4e0d\u540c\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u7a33\u5065\u90e8\u7f72"}}
{"id": "2509.17931", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2509.17931", "abs": "https://arxiv.org/abs/2509.17931", "authors": ["Zhuo Xiao", "Fugen Zhou", "Jingjing Wang", "Chongyu He", "Bo Liu", "Haitao Sun", "Zhe Ji", "Yuliang Jiang", "Junjie Wang", "Qiuwen Wu"], "title": "Multi-needle Localization for Pelvic Seed Implant Brachytherapy based on Tip-handle Detection and Matching", "comment": null, "summary": "Accurate multi-needle localization in intraoperative CT images is crucial for\noptimizing seed placement in pelvic seed implant brachytherapy. However, this\ntask is challenging due to poor image contrast and needle adhesion. This paper\npresents a novel approach that reframes needle localization as a tip-handle\ndetection and matching problem to overcome these difficulties. An anchor-free\nnetwork, based on HRNet, is proposed to extract multi-scale features and\naccurately detect needle tips and handles by predicting their centers and\norientations using decoupled branches for heatmap regression and polar angle\nprediction. To associate detected tips and handles into individual needles, a\ngreedy matching and merging (GMM) method designed to solve the unbalanced\nassignment problem with constraints (UAP-C) is presented. The GMM method\niteratively selects the most probable tip-handle pairs and merges them based on\na distance metric to reconstruct 3D needle paths. Evaluated on a dataset of 100\npatients, the proposed method demonstrates superior performance, achieving\nhigher precision and F1 score compared to a segmentation-based method utilizing\nthe nnUNet model,thereby offering a more robust and accurate solution for\nneedle localization in complex clinical scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHRNet\u7684\u65e0\u951a\u7f51\u7edc\u65b9\u6cd5\uff0c\u5c06\u9488\u5934\u5b9a\u4f4d\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5c16\u7aef-\u624b\u67c4\u68c0\u6d4b\u548c\u5339\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u70ed\u56fe\u56de\u5f52\u548c\u6781\u89d2\u9884\u6d4b\u6765\u51c6\u786e\u68c0\u6d4b\u9488\u5c16\u548c\u624b\u67c4\uff0c\u5e76\u4f7f\u7528\u8d2a\u5a6a\u5339\u914d\u5408\u5e76\u65b9\u6cd5\u5173\u8054\u68c0\u6d4b\u7ed3\u679c\u4ee5\u91cd\u5efa3D\u9488\u8def\u5f84\u3002", "motivation": "\u5728\u76c6\u8154\u79cd\u5b50\u690d\u5165\u8fd1\u8ddd\u79bb\u653e\u5c04\u6cbb\u7597\u4e2d\uff0c\u672f\u4e2dCT\u56fe\u50cf\u7684\u591a\u9488\u51c6\u786e\u5b9a\u4f4d\u5bf9\u4f18\u5316\u79cd\u5b50\u653e\u7f6e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u5dee\u548c\u9488\u5934\u7c98\u8fde\uff0c\u8fd9\u4e00\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eHRNet\u7684\u65e0\u951a\u7f51\u7edc\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u901a\u8fc7\u5206\u79bb\u7684\u70ed\u56fe\u56de\u5f52\u548c\u6781\u89d2\u9884\u6d4b\u5206\u652f\u68c0\u6d4b\u9488\u5c16\u548c\u624b\u67c4\u4e2d\u5fc3\u53ca\u65b9\u5411\uff1b\u8bbe\u8ba1\u8d2a\u5a6a\u5339\u914d\u5408\u5e76\u65b9\u6cd5\u89e3\u51b3\u4e0d\u5e73\u8861\u5206\u914d\u95ee\u9898\uff0c\u8fed\u4ee3\u9009\u62e9\u6700\u53ef\u80fd\u7684\u5c16\u7aef-\u624b\u67c4\u5bf9\u5e76\u57fa\u4e8e\u8ddd\u79bb\u5ea6\u91cf\u5408\u5e76\u4ee5\u91cd\u5efa3D\u9488\u8def\u5f84\u3002", "result": "\u5728100\u540d\u60a3\u8005\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u4e8ennUNet\u6a21\u578b\u7684\u5206\u5272\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u786e\u5ea6\u548cF1\u5206\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u9488\u5934\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u548c\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17943", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17943", "abs": "https://arxiv.org/abs/2509.17943", "authors": ["Romain Thoreau", "Jessie Levillain", "Dawa Derksen"], "title": "Can multimodal representation learning by alignment preserve modality-specific information?", "comment": "Accepted as a workshop paper at MACLEAN - ECML/PKDD 2025", "summary": "Combining multimodal data is a key issue in a wide range of machine learning\ntasks, including many remote sensing problems. In Earth observation, early\nmultimodal data fusion methods were based on specific neural network\narchitectures and supervised learning. Ever since, the scarcity of labeled data\nhas motivated self-supervised learning techniques. State-of-the-art multimodal\nrepresentation learning techniques leverage the spatial alignment between\nsatellite data from different modalities acquired over the same geographic area\nin order to foster a semantic alignment in the latent space. In this paper, we\ninvestigate how this methods can preserve task-relevant information that is not\nshared across modalities. First, we show, under simplifying assumptions, when\nalignment strategies fundamentally lead to an information loss. Then, we\nsupport our theoretical insight through numerical experiments in more realistic\nsettings. With those theoretical and empirical evidences, we hope to support\nnew developments in contrastive learning for the combination of multimodal\nsatellite data. Our code and data is publicly available at\nhttps://github.com/Romain3Ch216/alg_maclean_25.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u536b\u661f\u6570\u636e\u878d\u5408\u4e2d\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u53d1\u73b0\u5728\u8ffd\u6c42\u6a21\u6001\u95f4\u8bed\u4e49\u5bf9\u9f50\u7684\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u5bfc\u81f4\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u7684\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u73b0\u8c61\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u878d\u5408\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6a21\u6001\u95f4\u7684\u7a7a\u95f4\u5bf9\u9f50\u6765\u4fc3\u8fdb\u6f5c\u5728\u7a7a\u95f4\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u4fdd\u7559\u6a21\u6001\u7279\u6709\u7684\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u3002\u4f5c\u8005\u65e8\u5728\u63a2\u7d22\u8fd9\u79cd\u5bf9\u9f50\u7b56\u7565\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\u4f1a\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\u3002", "method": "\u9996\u5148\u5728\u7b80\u5316\u5047\u8bbe\u4e0b\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u5bf9\u9f50\u7b56\u7565\u4f55\u65f6\u4f1a\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\uff1b\u7136\u540e\u5728\u66f4\u73b0\u5b9e\u7684\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\u6765\u652f\u6301\u7406\u8bba\u53d1\u73b0\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u636e\u8868\u660e\uff0c\u73b0\u6709\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5728\u8ffd\u6c42\u6a21\u6001\u95f4\u5bf9\u9f50\u65f6\u786e\u5b9e\u4f1a\u635f\u5931\u4e00\u4e9b\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u7279\u522b\u662f\u5728\u8fd9\u4e9b\u4fe1\u606f\u4e0d\u8de8\u6a21\u6001\u5171\u4eab\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u591a\u6a21\u6001\u536b\u661f\u6570\u636e\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b0\u53d1\u5c55\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u5728\u6a21\u6001\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u9700\u8981\u8003\u8651\u4fdd\u7559\u6a21\u6001\u7279\u6709\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.17951", "categories": ["cs.CV", "I.5.4"], "pdf": "https://arxiv.org/pdf/2509.17951", "abs": "https://arxiv.org/abs/2509.17951", "authors": ["Kai Li", "Xingxing Weng", "Yupeng Deng", "Yu Meng", "Chao Pang", "Gui-Song Xia", "Xiangyu Zhao"], "title": "DragOSM: Extract Building Roofs and Footprints from Aerial Images by Aligning Historical Labels", "comment": "17 Pages", "summary": "Extracting polygonal roofs and footprints from remote sensing images is\ncritical for large-scale urban analysis. Most existing methods rely on\nsegmentation-based models that assume clear semantic boundaries of roofs, but\nthese approaches struggle in off- nadir images, where the roof and footprint\nare significantly displaced, and facade pixels are fused with the roof\nboundary. With the increasing availability of open vector map annotations,\ne.g., OpenStreetMap, utilizing historical labels for off-nadir image annotation\nhas become viable because remote sensing images are georeferenced once\ncaptured. However, these historical labels commonly suffer from significant\npositional discrepancies with new images and only have one annotation (roof or\nfootprint), which fails to describe the correct structures of a building. To\naddress these discrepancies, we first introduce a concept of an alignment\ntoken, which encodes the correction vector to guide the label correction. Based\non this concept, we then propose Drag OpenStreetMap Labels (DragOSM), a novel\nmodel designed to align dislocated historical labels with roofs and footprints.\nSpecifically, DragOSM formulates the label alignment as an interactive\ndenoising process, modeling the positional discrepancy as a Gaussian\ndistribution. During training, it learns to correct these errors by simulating\nmisalignment with random Gaussian perturbations; during inference, it\niteratively refines the positions of input labels. To validate our method, we\nfurther present a new dataset, Repairing Buildings in OSM (ReBO), comprising\n179,265 buildings with both OpenStreetMap and manually corrected annotations\nacross 5,473 images from 41 cities. Experimental results on ReBO demonstrate\nthe effectiveness of DragOSM. Code, dataset, and trained models are publicly\navailable at https://github.com/likaiucas/DragOSM.git.", "AI": {"tldr": "\u63d0\u51faDragOSM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5bf9\u9f50\u4ee4\u724c\u6982\u5ff5\uff0c\u5c06\u5386\u53f2OpenStreetMap\u6807\u7b7e\u4e0e\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u5efa\u7b51\u5c4b\u9876\u548c\u8db3\u8ff9\u8fdb\u884c\u5bf9\u9f50\u6821\u6b63\uff0c\u89e3\u51b3\u503e\u659c\u56fe\u50cf\u4e2d\u5efa\u7b51\u6807\u6ce8\u7684\u4f4d\u7f6e\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5206\u5272\u7684\u65b9\u6cd5\u5728\u503e\u659c\u9065\u611f\u56fe\u50cf\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5c4b\u9876\u548c\u8db3\u8ff9\u5b58\u5728\u663e\u8457\u4f4d\u79fb\uff0c\u4e14\u7acb\u9762\u50cf\u7d20\u4e0e\u5c4b\u9876\u8fb9\u754c\u878d\u5408\u3002\u867d\u7136OpenStreetMap\u7b49\u5f00\u653e\u77e2\u91cf\u5730\u56fe\u6807\u6ce8\u53ef\u7528\uff0c\u4f46\u8fd9\u4e9b\u5386\u53f2\u6807\u7b7e\u4e0e\u65b0\u56fe\u50cf\u5b58\u5728\u4f4d\u7f6e\u504f\u5dee\u4e14\u53ea\u6709\u5355\u4e00\u6807\u6ce8\uff08\u5c4b\u9876\u6216\u8db3\u8ff9\uff09\uff0c\u65e0\u6cd5\u51c6\u786e\u63cf\u8ff0\u5efa\u7b51\u7ed3\u6784\u3002", "method": "\u63d0\u51faDragOSM\u6a21\u578b\uff0c\u5f15\u5165\u5bf9\u9f50\u4ee4\u724c\u7f16\u7801\u6821\u6b63\u5411\u91cf\u6765\u6307\u5bfc\u6807\u7b7e\u6821\u6b63\u3002\u5c06\u6807\u7b7e\u5bf9\u9f50\u5efa\u6a21\u4e3a\u4ea4\u4e92\u5f0f\u53bb\u566a\u8fc7\u7a0b\uff0c\u5c06\u4f4d\u7f6e\u504f\u5dee\u5efa\u6a21\u4e3a\u9ad8\u65af\u5206\u5e03\u3002\u8bad\u7ec3\u65f6\u901a\u8fc7\u968f\u673a\u9ad8\u65af\u6270\u52a8\u6a21\u62df\u9519\u4f4d\u6765\u5b66\u4e60\u6821\u6b63\u8bef\u5dee\uff0c\u63a8\u7406\u65f6\u8fed\u4ee3\u4f18\u5316\u8f93\u5165\u6807\u7b7e\u4f4d\u7f6e\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b179,265\u4e2a\u5efa\u7b51\u7684\u65b0\u6570\u636e\u96c6ReBO\uff0c\u8986\u76d641\u4e2a\u57ce\u5e02\u76845,473\u5f20\u56fe\u50cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eDragOSM\u65b9\u6cd5\u6709\u6548\u3002", "conclusion": "DragOSM\u80fd\u591f\u6709\u6548\u6821\u6b63\u5386\u53f2OpenStreetMap\u6807\u7b7e\u4e0e\u9065\u611f\u56fe\u50cf\u4e2d\u5efa\u7b51\u7ed3\u6784\u7684\u4f4d\u7f6e\u504f\u5dee\uff0c\u4e3a\u5927\u89c4\u6a21\u57ce\u5e02\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5efa\u7b51\u8f6e\u5ed3\u63d0\u53d6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17955", "abs": "https://arxiv.org/abs/2509.17955", "authors": ["Fan Xu", "Hao Wu", "Nan Wang", "Lilan Peng", "Kun Wang", "Wei Gong", "Xibin Zhao"], "title": "Breaking the Discretization Barrier of Continuous Physics Simulation Learning", "comment": null, "summary": "The modeling of complicated time-evolving physical dynamics from partial\nobservations is a long-standing challenge. Particularly, observations can be\nsparsely distributed in a seemingly random or unstructured manner, making it\ndifficult to capture highly nonlinear features in a variety of scientific and\nengineering problems. However, existing data-driven approaches are often\nconstrained by fixed spatial and temporal discretization. While some\nresearchers attempt to achieve spatio-temporal continuity by designing novel\nstrategies, they either overly rely on traditional numerical methods or fail to\ntruly overcome the limitations imposed by discretization. To address these, we\npropose CoPS, a purely data-driven methods, to effectively model continuous\nphysics simulation from partial observations. Specifically, we employ\nmultiplicative filter network to fuse and encode spatial information with the\ncorresponding observations. Then we customize geometric grids and use\nmessage-passing mechanism to map features from original spatial domain to the\ncustomized grids. Subsequently, CoPS models continuous-time dynamics by\ndesigning multi-scale graph ODEs, while introducing a Markov-based neural\nauto-correction module to assist and constrain the continuous extrapolations.\nComprehensive experiments demonstrate that CoPS advances the state-of-the-art\nmethods in space-time continuous modeling across various scenarios.", "AI": {"tldr": "CoPS\u662f\u4e00\u79cd\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u90e8\u5206\u89c2\u6d4b\u4e2d\u6709\u6548\u5efa\u6a21\u8fde\u7eed\u7269\u7406\u6a21\u62df\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u56feODE\u548c\u795e\u7ecf\u81ea\u6821\u6b63\u6a21\u5757\u5b9e\u73b0\u65f6\u7a7a\u8fde\u7eed\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u53d7\u9650\u4e8e\u56fa\u5b9a\u7684\u65f6\u7a7a\u79bb\u6563\u5316\uff0c\u96be\u4ee5\u5904\u7406\u7a00\u758f\u5206\u5e03\u7684\u975e\u7ebf\u6027\u7269\u7406\u52a8\u529b\u5b66\u89c2\u6d4b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e58\u6cd5\u6ee4\u6ce2\u5668\u7f51\u7edc\u7f16\u7801\u7a7a\u95f4\u4fe1\u606f\uff0c\u5b9a\u5236\u51e0\u4f55\u7f51\u683c\u5e76\u901a\u8fc7\u6d88\u606f\u4f20\u9012\u673a\u5236\u6620\u5c04\u7279\u5f81\uff0c\u8bbe\u8ba1\u591a\u5c3a\u5ea6\u56feODE\u5efa\u6a21\u8fde\u7eed\u65f6\u95f4\u52a8\u529b\u5b66\uff0c\u5f15\u5165\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u7684\u795e\u7ecf\u81ea\u6821\u6b63\u6a21\u5757\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660eCoPS\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u65f6\u7a7a\u8fde\u7eed\u5efa\u6a21\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "CoPS\u6210\u529f\u514b\u670d\u4e86\u79bb\u6563\u5316\u9650\u5236\uff0c\u4e3a\u590d\u6742\u7269\u7406\u52a8\u529b\u5b66\u7684\u8fde\u7eed\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17968", "abs": "https://arxiv.org/abs/2509.17968", "authors": ["Qizhen Lan", "Jung Im Choi", "Qing Tian"], "title": "Visual Detector Compression via Location-Aware Discriminant Analysis", "comment": null, "summary": "Deep neural networks are powerful, yet their high complexity greatly limits\ntheir potential to be deployed on billions of resource-constrained edge\ndevices. Pruning is a crucial network compression technique, yet most existing\nmethods focus on classification models, with limited attention to detection.\nEven among those addressing detection, there is a lack of utilization of\nessential localization information. Also, many pruning methods passively rely\non pre-trained models, in which useful and useless components are intertwined,\nmaking it difficult to remove the latter without harming the former at the\nneuron/filter level. To address the above issues, in this paper, we propose a\nproactive detection-discriminants-based network compression approach for deep\nvisual detectors, which alternates between two steps: (1) maximizing and\ncompressing detection-related discriminants and aligning them with a subset of\nneurons/filters immediately before the detection head, and (2) tracing the\ndetection-related discriminating power across the layers and discarding\nfeatures of lower importance. Object location information is exploited in both\nsteps. Extensive experiments, employing four advanced detection models and four\nstate-of-the-art competing methods on the KITTI and COCO datasets, highlight\nthe superiority of our approach. Remarkably, our compressed models can even\nbeat the original base models with a substantial reduction in complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u6d4b\u5224\u522b\u5668\u7684\u4e3b\u52a8\u7f51\u7edc\u538b\u7f29\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u89c6\u89c9\u68c0\u6d4b\u5668\u8fdb\u884c\u526a\u679d\uff0c\u5229\u7528\u76ee\u6807\u5b9a\u4f4d\u4fe1\u606f\uff0c\u5728\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6", "motivation": "\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5206\u7c7b\u6a21\u578b\uff0c\u5bf9\u68c0\u6d4b\u6a21\u578b\u7684\u5173\u6ce8\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u5173\u952e\u5b9a\u4f4d\u4fe1\u606f\u7684\u5229\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u88ab\u52a8\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u96be\u4ee5\u5728\u4e0d\u635f\u5bb3\u6709\u7528\u7ec4\u4ef6\u7684\u60c5\u51b5\u4e0b\u79fb\u9664\u65e0\u7528\u7ec4\u4ef6", "method": "\u4ea4\u66ff\u6267\u884c\u4e24\u4e2a\u6b65\u9aa4\uff1a(1)\u6700\u5927\u5316\u5e76\u538b\u7f29\u68c0\u6d4b\u76f8\u5173\u5224\u522b\u5668\uff0c\u5c06\u5176\u4e0e\u68c0\u6d4b\u5934\u524d\u7684\u795e\u7ecf\u5143/\u6ee4\u6ce2\u5668\u5b50\u96c6\u5bf9\u9f50\uff1b(2)\u8de8\u5c42\u8ffd\u8e2a\u68c0\u6d4b\u76f8\u5173\u5224\u522b\u80fd\u529b\uff0c\u4e22\u5f03\u91cd\u8981\u6027\u8f83\u4f4e\u7684\u7279\u5f81\u3002\u4e24\u4e2a\u6b65\u9aa4\u90fd\u5229\u7528\u4e86\u76ee\u6807\u5b9a\u4f4d\u4fe1\u606f", "result": "\u5728KITTI\u548cCOCO\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u56db\u79cd\u5148\u8fdb\u68c0\u6d4b\u6a21\u578b\u548c\u56db\u79cd\u7ade\u4e89\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\uff0c\u538b\u7f29\u540e\u7684\u6a21\u578b\u751a\u81f3\u80fd\u591f\u8d85\u8d8a\u539f\u59cb\u57fa\u7840\u6a21\u578b\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u590d\u6742\u5ea6", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u68c0\u6d4b\u5668\u538b\u7f29\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u68c0\u6d4b\u4efb\u52a1\u7279\u6709\u7684\u5b9a\u4f4d\u4fe1\u606f\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u6a21\u578b\u538b\u7f29"}}
{"id": "2509.17993", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17993", "abs": "https://arxiv.org/abs/2509.17993", "authors": ["Haoxin Yang", "Bangzhen Liu", "Xuemiao Xu", "Cheng Xu", "Yuyang Yu", "Zikai Huang", "Yi Wang", "Shengfeng He"], "title": "StableGuard: Towards Unified Copyright Protection and Tamper Localization in Latent Diffusion Models", "comment": "Accepted by NeurIPS 2025", "summary": "The advancement of diffusion models has enhanced the realism of AI-generated\ncontent but also raised concerns about misuse, necessitating robust copyright\nprotection and tampering localization. Although recent methods have made\nprogress toward unified solutions, their reliance on post hoc processing\nintroduces considerable application inconvenience and compromises forensic\nreliability. We propose StableGuard, a novel framework that seamlessly\nintegrates a binary watermark into the diffusion generation process, ensuring\ncopyright protection and tampering localization in Latent Diffusion Models\nthrough an end-to-end design. We develop a Multiplexing Watermark VAE (MPW-VAE)\nby equipping a pretrained Variational Autoencoder (VAE) with a lightweight\nlatent residual-based adapter, enabling the generation of paired watermarked\nand watermark-free images. These pairs, fused via random masks, create a\ndiverse dataset for training a tampering-agnostic forensic network. To further\nenhance forensic synergy, we introduce a Mixture-of-Experts Guided Forensic\nNetwork (MoE-GFN) that dynamically integrates holistic watermark patterns,\nlocal tampering traces, and frequency-domain cues for precise watermark\nverification and tampered region detection. The MPW-VAE and MoE-GFN are jointly\noptimized in a self-supervised, end-to-end manner, fostering a reciprocal\ntraining between watermark embedding and forensic accuracy. Extensive\nexperiments demonstrate that StableGuard consistently outperforms\nstate-of-the-art methods in image fidelity, watermark verification, and\ntampering localization.", "AI": {"tldr": "StableGuard\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u65e0\u7f1d\u96c6\u6210\u4e8c\u8fdb\u5236\u6c34\u5370\uff0c\u5b9e\u73b0\u7248\u6743\u4fdd\u62a4\u548c\u7be1\u6539\u5b9a\u4f4d\uff0c\u907f\u514d\u4e86\u540e\u5904\u7406\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u8fdb\u6b65\u589e\u5f3a\u4e86AI\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u6ee5\u7528\u62c5\u5fe7\uff0c\u9700\u8981\u5f3a\u5927\u7684\u7248\u6743\u4fdd\u62a4\u548c\u7be1\u6539\u5b9a\u4f4d\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u540e\u5904\u7406\uff0c\u5bfc\u81f4\u5e94\u7528\u4e0d\u4fbf\u548c\u53d6\u8bc1\u53ef\u9760\u6027\u964d\u4f4e\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u8def\u590d\u7528\u6c34\u5370VAE\uff08MPW-VAE\uff09\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6f5c\u5728\u6b8b\u5dee\u9002\u914d\u5668\u751f\u6210\u914d\u5bf9\u7684\u6c34\u5370\u548c\u65e0\u6c34\u5370\u56fe\u50cf\uff1b\u5f15\u5165\u4e13\u5bb6\u6df7\u5408\u5f15\u5bfc\u53d6\u8bc1\u7f51\u7edc\uff08MoE-GFN\uff09\uff0c\u52a8\u6001\u6574\u5408\u5168\u5c40\u6c34\u5370\u6a21\u5f0f\u3001\u5c40\u90e8\u7be1\u6539\u75d5\u8ff9\u548c\u9891\u57df\u7ebf\u7d22\uff1b\u4e24\u8005\u4ee5\u81ea\u76d1\u7763\u7aef\u5230\u7aef\u65b9\u5f0f\u8054\u5408\u4f18\u5316\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cStableGuard\u5728\u56fe\u50cf\u4fdd\u771f\u5ea6\u3001\u6c34\u5370\u9a8c\u8bc1\u548c\u7be1\u6539\u5b9a\u4f4d\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "StableGuard\u901a\u8fc7\u7aef\u5230\u7aef\u8bbe\u8ba1\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u7684\u7248\u6743\u4fdd\u62a4\u548c\u7be1\u6539\u5b9a\u4f4d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6c34\u5370\u5d4c\u5165\u548c\u53d6\u8bc1\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u4e92\u60e0\u8bad\u7ec3\u3002"}}
{"id": "2509.18015", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18015", "abs": "https://arxiv.org/abs/2509.18015", "authors": ["Advait Gosai", "Arun Kavishwar", "Stephanie L. McNamara", "Soujanya Samineni", "Renato Umeton", "Alexander Chowdhury", "William Lotter"], "title": "Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization in Chest Radiographs", "comment": null, "summary": "Recent work has shown promising performance of frontier large language models\n(LLMs) and their multimodal counterparts in medical quizzes and diagnostic\ntasks, highlighting their potential for broad clinical utility given their\naccessible, general-purpose nature. However, beyond diagnosis, a fundamental\naspect of medical image interpretation is the ability to localize pathological\nfindings. Evaluating localization not only has clinical and educational\nrelevance but also provides insight into a model's spatial understanding of\nanatomy and disease. Here, we systematically assess two general-purpose MLLMs\n(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to\nlocalize pathologies on chest radiographs, using a prompting pipeline that\noverlays a spatial grid and elicits coordinate-based predictions. Averaged\nacross nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a\nlocalization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),\nall lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark\n(80.1%). Despite modest performance, error analysis revealed that GPT-5's\npredictions were largely in anatomically plausible regions, just not always\nprecisely localized. GPT-4 performed well on pathologies with fixed anatomical\nlocations, but struggled with spatially variable findings and exhibited\nanatomically implausible predictions more frequently. MedGemma demonstrated the\nlowest performance on all pathologies, showing limited capacity to generalize\nto this novel task. Our findings highlight both the promise and limitations of\ncurrent MLLMs in medical imaging and underscore the importance of integrating\nthem with task-specific tools for reliable use.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08GPT-4\u3001GPT-5\uff09\u548c\u9886\u57df\u4e13\u7528\u6a21\u578b\uff08MedGemma\uff09\u5728\u80f8\u90e8X\u5149\u7247\u75c5\u7406\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u867d\u7136\u6709\u4e00\u5b9a\u6f5c\u529b\uff0c\u4f46\u51c6\u786e\u7387\u4ecd\u4f4e\u4e8e\u4e13\u7528CNN\u6a21\u578b\u548c\u653e\u5c04\u79d1\u533b\u751f\u57fa\u51c6\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u89e3\u8bfb\u4e0d\u4ec5\u9700\u8981\u8bca\u65ad\u80fd\u529b\uff0c\u8fd8\u9700\u8981\u5b9a\u4f4d\u75c5\u7406\u53d1\u73b0\u7684\u80fd\u529b\u3002\u8bc4\u4f30\u5b9a\u4f4d\u80fd\u529b\u6709\u52a9\u4e8e\u4e86\u89e3\u6a21\u578b\u5bf9\u89e3\u5256\u7ed3\u6784\u548c\u75be\u75c5\u7a7a\u95f4\u5173\u7cfb\u7684\u7406\u89e3\uff0c\u5177\u6709\u4e34\u5e8a\u548c\u6559\u80b2\u610f\u4e49\u3002", "method": "\u4f7f\u7528CheXlocalize\u6570\u636e\u96c6\u4e2d\u76849\u79cd\u75c5\u7406\uff0c\u901a\u8fc7\u7a7a\u95f4\u7f51\u683c\u63d0\u793a\u7ba1\u9053\u83b7\u53d6\u57fa\u4e8e\u5750\u6807\u7684\u9884\u6d4b\uff0c\u7cfb\u7edf\u8bc4\u4f30GPT-4\u3001GPT-5\u548cMedGemma\u7684\u5b9a\u4f4d\u51c6\u786e\u6027\u3002", "result": "GPT-5\u5b9a\u4f4d\u51c6\u786e\u7387\u4e3a49.7%\uff0cGPT-4\u4e3a39.1%\uff0cMedGemma\u4e3a17.7%\uff0c\u5747\u4f4e\u4e8e\u4e13\u7528CNN\u6a21\u578b\uff0859.9%\uff09\u548c\u653e\u5c04\u79d1\u533b\u751f\u57fa\u51c6\uff0880.1%\uff09\u3002GPT-5\u7684\u9884\u6d4b\u5728\u89e3\u5256\u5b66\u4e0a\u5408\u7406\u4f46\u4e0d\u7cbe\u786e\uff0cGPT-4\u5bf9\u56fa\u5b9a\u4f4d\u7f6e\u75c5\u7406\u8868\u73b0\u8f83\u597d\u4f46\u5b58\u5728\u4e0d\u5408\u7406\u9884\u6d4b\uff0cMedGemma\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "conclusion": "\u5f53\u524dMLLMs\u5728\u533b\u5b66\u5f71\u50cf\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\u4f46\u4ecd\u6709\u5c40\u9650\uff0c\u9700\u8981\u4e0e\u4efb\u52a1\u4e13\u7528\u5de5\u5177\u7ed3\u5408\u624d\u80fd\u5b9e\u73b0\u53ef\u9760\u5e94\u7528\u3002"}}
{"id": "2509.18041", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18041", "abs": "https://arxiv.org/abs/2509.18041", "authors": ["Sahil Shah", "S P Sharan", "Harsh Goel", "Minkyu Choi", "Mustafa Munir", "Manvik Pasula", "Radu Marculescu", "Sandeep Chinchali"], "title": "NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning", "comment": null, "summary": "Long-Form Video Question Answering (LVQA) poses challenges beyond traditional\nvisual question answering (VQA), which is often limited to static images or\nshort video clips. While current vision-language models (VLMs) perform well in\nthose settings, they struggle with complex queries in LVQA over long videos\ninvolving multi-step temporal reasoning and causality. Vanilla approaches,\nwhich sample frames uniformly and feed them to a VLM with the question, incur\nsignificant token overhead, forcing severe downsampling. As a result, the model\noften misses fine-grained visual structure, subtle event transitions, or key\ntemporal cues, ultimately leading to incorrect answers. To address these\nlimitations, recent works have explored query-adaptive frame sampling,\nhierarchical keyframe selection, and agent-based iterative querying. However,\nthese methods remain fundamentally heuristic: they lack explicit temporal\nrepresentations and cannot enforce or verify logical event relationships. As a\nresult, there are no formal guarantees that the sampled context actually\nencodes the compositional or causal logic demanded by the question. To address\nthese foundational gaps, we introduce NeuS-QA, a training-free, plug-and-play\nneuro-symbolic pipeline for LVQA. NeuS-QA translates a natural language\nquestion into a formal temporal logic expression, constructs a video automaton\nfrom frame-level semantic propositions, and applies model checking to\nrigorously identify video segments satisfying the question's logical\nrequirements. Only these logic-verified segments are submitted to the VLM, thus\nimproving interpretability, reducing hallucinations, and enabling compositional\nreasoning without modifying or fine-tuning the model. Experiments on\nLongVideoBench and CinePile show NeuS-QA improves performance by over 10%,\nespecially on questions involving event ordering, causality, and multi-step\ncompositional reasoning.", "AI": {"tldr": "NeuS-QA\u662f\u4e00\u4e2a\u514d\u8bad\u7ec3\u7684\u795e\u7ecf\u7b26\u53f7\u5316\u7ba1\u9053\uff0c\u7528\u4e8e\u957f\u89c6\u9891\u95ee\u7b54\uff0c\u901a\u8fc7\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u65f6\u5e8f\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u6784\u5efa\u89c6\u9891\u81ea\u52a8\u673a\uff0c\u5e76\u5e94\u7528\u6a21\u578b\u68c0\u67e5\u6765\u8bc6\u522b\u6ee1\u8db3\u903b\u8f91\u8981\u6c42\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u4ece\u800c\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u7684\u65f6\u95f4\u63a8\u7406\u548c\u56e0\u679c\u5173\u7cfb\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u660e\u786e\u7684\u65f6\u5e8f\u8868\u793a\u548c\u903b\u8f91\u9a8c\u8bc1\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u91c7\u6837\u7684\u4e0a\u4e0b\u6587\u6ee1\u8db3\u95ee\u9898\u7684\u7ec4\u5408\u6216\u56e0\u679c\u903b\u8f91\u8981\u6c42\u3002", "method": "NeuS-QA\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u5316\u4e3a\u5f62\u5f0f\u5316\u7684\u65f6\u5e8f\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u4ece\u5e27\u7ea7\u8bed\u4e49\u547d\u9898\u6784\u5efa\u89c6\u9891\u81ea\u52a8\u673a\uff0c\u5e94\u7528\u6a21\u578b\u68c0\u67e5\u6765\u4e25\u683c\u8bc6\u522b\u6ee1\u8db3\u95ee\u9898\u903b\u8f91\u8981\u6c42\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u4ec5\u5c06\u8fd9\u4e9b\u7ecf\u8fc7\u903b\u8f91\u9a8c\u8bc1\u7684\u7247\u6bb5\u63d0\u4ea4\u7ed9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728LongVideoBench\u548cCinePile\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNeuS-QA\u5c06\u6027\u80fd\u63d0\u9ad8\u4e86\u8d85\u8fc710%\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u4e8b\u4ef6\u6392\u5e8f\u3001\u56e0\u679c\u5173\u7cfb\u548c\u591a\u6b65\u7ec4\u5408\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "NeuS-QA\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u5316\u65b9\u6cd5\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3001\u51cf\u5c11\u5e7b\u89c9\uff0c\u5e76\u5b9e\u73b0\u4e86\u65e0\u9700\u4fee\u6539\u6216\u5fae\u8c03\u6a21\u578b\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.18056", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18056", "abs": "https://arxiv.org/abs/2509.18056", "authors": ["Yunheng Li", "Jing Cheng", "Shaoyong Jia", "Hangyi Kuang", "Shaohui Jiao", "Qibin Hou", "Ming-Ming Cheng"], "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs", "comment": "Accepted at NeurIPS 2025", "summary": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1", "AI": {"tldr": "TempSamp-R1\u662f\u4e00\u4e2a\u65b0\u7684\u5f3a\u5316\u5fae\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002\u5b83\u901a\u8fc7\u4f7f\u7528\u771f\u5b9e\u6807\u6ce8\u4f5c\u4e3a\u79bb\u7b56\u7565\u76d1\u7763\u6765\u63d0\u4f9b\u65f6\u5e8f\u7cbe\u786e\u6307\u5bfc\uff0c\u5e76\u91c7\u7528\u975e\u7ebf\u6027\u8f6f\u4f18\u52bf\u8ba1\u7b97\u6765\u7a33\u5b9a\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5728\u5927\u578b\u65f6\u5e8f\u641c\u7d22\u7a7a\u95f4\u4efb\u52a1\u4e2d\u6548\u7387\u4f4e\u4e0b\u4e14\u6027\u80fd\u53d7\u9650\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u7b56\u7565\u5185\u91c7\u6837\uff0c\u5f80\u5f80\u65e0\u6cd5\u627e\u5230\u65f6\u5e8f\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "TempSamp-R1\u5229\u7528\u771f\u5b9e\u6807\u6ce8\u4f5c\u4e3a\u79bb\u7b56\u7565\u76d1\u7763\u6765\u63d0\u4f9b\u65f6\u5e8f\u7cbe\u786e\u6307\u5bfc\uff0c\u91c7\u7528\u975e\u7ebf\u6027\u8f6f\u4f18\u52bf\u8ba1\u7b97\u65b9\u6cd5\u52a8\u6001\u91cd\u5851\u5956\u52b1\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u601d\u7ef4\u94fe\u8bad\u7ec3\u8303\u5f0f\u4f18\u5316\u5355\u4e00\u7edf\u4e00\u6a21\u578b\u4ee5\u652f\u6301\u601d\u7ef4\u94fe\u548c\u975e\u601d\u7ef4\u94fe\u63a8\u7406\u6a21\u5f0f\u3002", "result": "TempSamp-R1\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u57fa\u4e8eGRPO\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728Charades-STA\uff08R1@0.7: 52.9%, +2.7%\uff09\u3001ActivityNet Captions\uff08R1@0.5: 56.0%, +5.3%\uff09\u548cQVHighlights\uff08mAP: 30.0%, +3.0%\uff09\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "TempSamp-R1\u901a\u8fc7\u79bb\u7b56\u7565\u76d1\u7763\u548c\u975e\u7ebf\u6027\u8f6f\u4f18\u52bf\u8ba1\u7b97\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u5e8f\u5b9a\u4f4d\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f3a\u5316\u5fae\u8c03\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18081", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18081", "abs": "https://arxiv.org/abs/2509.18081", "authors": ["Md. Mahmudul Hasan", "Ahmed Nesar Tahsin Choudhury", "Mahmudul Hasan", "Md. Mosaddek Khan"], "title": "GraDeT-HTR: A Resource-Efficient Bengali Handwritten Text Recognition System utilizing Grapheme-based Tokenizer and Decoder-only Transformer", "comment": "7 pages. Accepted at the 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP) System Demonstrations. Equal\n  Contribution: Md. Mahmudul Hasan and Ahmed Nesar Tahsin Choudhury", "summary": "Despite Bengali being the sixth most spoken language in the world,\nhandwritten text recognition (HTR) systems for Bengali remain severely\nunderdeveloped. The complexity of Bengali script--featuring conjuncts,\ndiacritics, and highly variable handwriting styles--combined with a scarcity of\nannotated datasets makes this task particularly challenging. We present\nGraDeT-HTR, a resource-efficient Bengali handwritten text recognition system\nbased on a Grapheme-aware Decoder-only Transformer architecture. To address the\nunique challenges of Bengali script, we augment the performance of a\ndecoder-only transformer by integrating a grapheme-based tokenizer and\ndemonstrate that it significantly improves recognition accuracy compared to\nconventional subword tokenizers. Our model is pretrained on large-scale\nsynthetic data and fine-tuned on real human-annotated samples, achieving\nstate-of-the-art performance on multiple benchmark datasets.", "AI": {"tldr": "GraDeT-HTR\u662f\u4e00\u4e2a\u57fa\u4e8eGrapheme-aware Decoder-only Transformer\u67b6\u6784\u7684\u8d44\u6e90\u9ad8\u6548\u5b5f\u52a0\u62c9\u8bed\u624b\u5199\u6587\u672c\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210\u57fa\u4e8e\u5b57\u7d20\u7684\u6807\u8bb0\u5668\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u4f5c\u4e3a\u4e16\u754c\u7b2c\u516d\u5927\u8bed\u8a00\uff0c\u5176\u624b\u5199\u6587\u672c\u8bc6\u522b\u7cfb\u7edf\u4e25\u91cd\u4e0d\u8db3\u3002\u5b5f\u52a0\u62c9\u8bed\u6587\u5b57\u7684\u590d\u6742\u6027\uff08\u5305\u542b\u8fde\u5b57\u3001\u53d8\u97f3\u7b26\u53f7\u548c\u9ad8\u5ea6\u53ef\u53d8\u7684\u4e66\u5199\u98ce\u683c\uff09\u4ee5\u53ca\u6807\u6ce8\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\u4f7f\u5f97\u8fd9\u9879\u4efb\u52a1\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5b57\u7d20\u7684\u6807\u8bb0\u5668\u589e\u5f3a\u4ec5\u89e3\u7801\u5668Transformer\u7684\u6027\u80fd\uff0c\u5728\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u771f\u5b9e\u4eba\u5de5\u6807\u6ce8\u6837\u672c\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "GraDeT-HTR\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u5b5f\u52a0\u62c9\u8bed\u624b\u5199\u6587\u672c\u8bc6\u522b\u7684\u72ec\u7279\u6311\u6218\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u5b57\u7d20\u7684\u6807\u8bb0\u5668\u76f8\u6bd4\u4f20\u7edf\u5b50\u8bcd\u6807\u8bb0\u5668\u80fd\u663e\u8457\u63d0\u9ad8\u8bc6\u522b\u51c6\u786e\u7387\u3002"}}
{"id": "2509.18090", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18090", "abs": "https://arxiv.org/abs/2509.18090", "authors": ["Jiahe Li", "Jiawei Zhang", "Youmin Zhang", "Xiao Bai", "Jin Zheng", "Xiaohan Yu", "Lin Gu"], "title": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction", "comment": "Accepted at NeurIPS 2025 (Spotlight). Project page:\n  https://fictionarry.github.io/GeoSVR-project/", "summary": "Reconstructing accurate surfaces with radiance fields has achieved remarkable\nprogress in recent years. However, prevailing approaches, primarily based on\nGaussian Splatting, are increasingly constrained by representational\nbottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based\nframework that explores and extends the under-investigated potential of sparse\nvoxels for achieving accurate, detailed, and complete surface reconstruction.\nAs strengths, sparse voxels support preserving the coverage completeness and\ngeometric clarity, while corresponding challenges also arise from absent scene\nconstraints and locality in surface refinement. To ensure correct scene\nconvergence, we first propose a Voxel-Uncertainty Depth Constraint that\nmaximizes the effect of monocular depth cues while presenting a voxel-oriented\nuncertainty to avoid quality degradation, enabling effective and robust scene\nconstraints yet preserving highly accurate geometries. Subsequently, Sparse\nVoxel Surface Regularization is designed to enhance geometric consistency for\ntiny voxels and facilitate the voxel-based formation of sharp and accurate\nsurfaces. Extensive experiments demonstrate our superior performance compared\nto existing methods across diverse challenging scenarios, excelling in\ngeometric accuracy, detail preservation, and reconstruction completeness while\nmaintaining high efficiency. Code is available at\nhttps://github.com/Fictionarry/GeoSVR.", "AI": {"tldr": "GeoSVR\u662f\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u4f53\u7d20\u7684\u663e\u5f0f\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u7cbe\u786e\u3001\u8be6\u7ec6\u548c\u5b8c\u6574\u7684\u8868\u9762\u91cd\u5efa\uff0c\u901a\u8fc7\u4f53\u7d20\u4e0d\u786e\u5b9a\u6027\u6df1\u5ea6\u7ea6\u675f\u548c\u7a00\u758f\u4f53\u7d20\u8868\u9762\u6b63\u5219\u5316\u6765\u89e3\u51b3\u73b0\u6709\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\u7684\u8868\u793a\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u7684\u4e3b\u6d41\u65b9\u6cd5\u53d7\u5230\u8868\u793a\u74f6\u9888\u7684\u9650\u5236\uff0c\u7a00\u758f\u4f53\u7d20\u867d\u7136\u80fd\u591f\u4fdd\u6301\u8986\u76d6\u5b8c\u6574\u6027\u548c\u51e0\u4f55\u6e05\u6670\u5ea6\uff0c\u4f46\u5728\u573a\u666f\u7ea6\u675f\u548c\u5c40\u90e8\u8868\u9762\u7ec6\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4f53\u7d20\u4e0d\u786e\u5b9a\u6027\u6df1\u5ea6\u7ea6\u675f\u6765\u6700\u5927\u5316\u5355\u76ee\u6df1\u5ea6\u7ebf\u7d22\u7684\u6548\u679c\uff0c\u540c\u65f6\u5f15\u5165\u4f53\u7d20\u5bfc\u5411\u7684\u4e0d\u786e\u5b9a\u6027\u4ee5\u907f\u514d\u8d28\u91cf\u4e0b\u964d\uff1b\u8bbe\u8ba1\u7a00\u758f\u4f53\u7d20\u8868\u9762\u6b63\u5219\u5316\u6765\u589e\u5f3a\u5fae\u5c0f\u4f53\u7d20\u7684\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u4fc3\u8fdb\u57fa\u4e8e\u4f53\u7d20\u7684\u9510\u5229\u51c6\u786e\u8868\u9762\u5f62\u6210\u3002", "result": "\u5728\u591a\u79cd\u6311\u6218\u6027\u573a\u666f\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u7cbe\u5ea6\u3001\u7ec6\u8282\u4fdd\u6301\u548c\u91cd\u5efa\u5b8c\u6574\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u7387\u3002", "conclusion": "GeoSVR\u901a\u8fc7\u521b\u65b0\u7684\u4f53\u7d20\u4e0d\u786e\u5b9a\u6027\u6df1\u5ea6\u7ea6\u675f\u548c\u7a00\u758f\u4f53\u7d20\u8868\u9762\u6b63\u5219\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758f\u4f53\u7d20\u5728\u8868\u9762\u91cd\u5efa\u4e2d\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u91cd\u5efa\u6027\u80fd\u3002"}}
{"id": "2509.18092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18092", "abs": "https://arxiv.org/abs/2509.18092", "authors": ["Guocheng Gordon Qian", "Daniil Ostashev", "Egor Nemchinov", "Avihay Assouline", "Sergey Tulyakov", "Kuan-Chieh Jackson Wang", "Kfir Aberman"], "title": "ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation", "comment": "Accepted to SIGGRAPH Asia 2025, webpage:\n  https://snap-research.github.io/composeme/", "summary": "Generating high-fidelity images of humans with fine-grained control over\nattributes such as hairstyle and clothing remains a core challenge in\npersonalized text-to-image synthesis. While prior methods emphasize identity\npreservation from a reference image, they lack modularity and fail to provide\ndisentangled control over specific visual attributes. We introduce a new\nparadigm for attribute-specific image prompting, in which distinct sets of\nreference images are used to guide the generation of individual aspects of\nhuman appearance, such as hair, clothing, and identity. Our method encodes\nthese inputs into attribute-specific tokens, which are injected into a\npre-trained text-to-image diffusion model. This enables compositional and\ndisentangled control over multiple visual factors, even across multiple people\nwithin a single image. To promote natural composition and robust\ndisentanglement, we curate a cross-reference training dataset featuring\nsubjects in diverse poses and expressions, and propose a multi-attribute\ncross-reference training strategy that encourages the model to generate\nfaithful outputs from misaligned attribute inputs while adhering to both\nidentity and textual conditioning. Extensive experiments show that our method\nachieves state-of-the-art performance in accurately following both visual and\ntextual prompts. Our framework paves the way for more configurable human image\nsynthesis by combining visual prompting with text-driven generation. Webpage is\navailable at: https://snap-research.github.io/composeme/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c5e\u6027\u7279\u5b9a\u56fe\u50cf\u63d0\u793a\u8303\u5f0f\uff0c\u901a\u8fc7\u591a\u7ec4\u53c2\u8003\u56fe\u50cf\u5206\u522b\u63a7\u5236\u4eba\u7c7b\u5916\u89c2\u7684\u5404\u4e2a\u5c5e\u6027\uff08\u5982\u53d1\u578b\u3001\u670d\u88c5\u3001\u8eab\u4efd\uff09\uff0c\u5b9e\u73b0\u53ef\u7ec4\u5408\u548c\u5206\u79bb\u7684\u591a\u5c5e\u6027\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u4e2d\u4e3b\u8981\u5173\u6ce8\u8eab\u4efd\u4fdd\u6301\uff0c\u4f46\u7f3a\u4e4f\u6a21\u5757\u5316\u80fd\u529b\uff0c\u65e0\u6cd5\u5bf9\u7279\u5b9a\u89c6\u89c9\u5c5e\u6027\u8fdb\u884c\u5206\u79bb\u63a7\u5236\u3002", "method": "\u5c06\u4e0d\u540c\u53c2\u8003\u56fe\u50cf\u7f16\u7801\u4e3a\u5c5e\u6027\u7279\u5b9atoken\uff0c\u6ce8\u5165\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff1b\u91c7\u7528\u8de8\u53c2\u8003\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u591a\u5c5e\u6027\u4ea4\u53c9\u53c2\u8003\u8bad\u7ec3\u7b56\u7565\uff0c\u786e\u4fdd\u81ea\u7136\u7ec4\u5408\u548c\u9c81\u68d2\u5206\u79bb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u9075\u5faa\u89c6\u89c9\u548c\u6587\u672c\u63d0\u793a\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u652f\u6301\u5355\u5f20\u56fe\u50cf\u4e2d\u591a\u4e2a\u4eba\u7684\u591a\u5c5e\u6027\u63a7\u5236\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u4e0e\u6587\u672c\u9a71\u52a8\u751f\u6210\u7684\u7ed3\u5408\uff0c\u4e3a\u66f4\u53ef\u914d\u7f6e\u7684\u4eba\u7c7b\u56fe\u50cf\u5408\u6210\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.18094", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18094", "abs": "https://arxiv.org/abs/2509.18094", "authors": ["Ye Liu", "Zongyang Ma", "Junfu Pu", "Zhongang Qi", "Yang Wu", "Ying Shan", "Chang Wen Chen"], "title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning", "comment": "NeurIPS 2025 Camera Ready. Project Page:\n  https://polyu-chenlab.github.io/unipixel/", "summary": "Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method.", "AI": {"tldr": "UniPixel\u662f\u4e00\u4e2a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u80fd\u591f\u7075\u6d3b\u7406\u89e3\u89c6\u89c9\u63d0\u793a\u8f93\u5165\u5e76\u751f\u6210\u57fa\u4e8e\u63a9\u7801\u7684\u54cd\u5e94\uff0c\u5c06\u50cf\u7d20\u7ea7\u611f\u77e5\u4e0e\u901a\u7528\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u65e0\u7f1d\u96c6\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u6574\u4f53\u56fe\u50cf\u548c\u89c6\u9891\u8bed\u8a00\u7406\u89e3\uff0c\u800c\u50cf\u7d20\u7ea7\u7ec6\u7c92\u5ea6\u7406\u89e3\u80fd\u529b\u7684\u7814\u7a76\u76f8\u5bf9\u8f83\u5c11\u3002\u4e4b\u524d\u7684\u6a21\u578b\u53ea\u80fd\u72ec\u7acb\u6267\u884c\u53c2\u8003\u6216\u5206\u5272\u4efb\u52a1\uff0c\u672a\u80fd\u5c06\u8fd9\u4e9b\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\u6574\u5408\u5230\u89c6\u89c9\u63a8\u7406\u4e2d\u3002", "method": "UniPixel\u5904\u7406\u89c6\u89c9\u63d0\u793a\u5e76\u6839\u636e\u9700\u6c42\u751f\u6210\u76f8\u5173\u63a9\u7801\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u57fa\u4e8e\u8fd9\u4e9b\u4e2d\u95f4\u6307\u9488\u8fdb\u884c\u540e\u7eed\u63a8\u7406\uff0c\u4ece\u800c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u50cf\u7d20\u7ea7\u63a8\u7406\u3002", "result": "\u8be5\u65b9\u6cd5\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u6db5\u76d6\u50cf\u7d20\u7ea7\u53c2\u8003/\u5206\u5272\u548c\u56fe\u50cf/\u89c6\u9891\u4e2d\u7684\u5bf9\u8c61\u4e2d\u5fc3\u7406\u89e3\u7b49\u591a\u79cd\u4efb\u52a1\u3002\u8fd8\u8bbe\u8ba1\u4e86\u65b0\u7684PixelQA\u4efb\u52a1\u6765\u9a8c\u8bc1\u65b9\u6cd5\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "UniPixel\u6210\u529f\u5730\u5c06\u50cf\u7d20\u7ea7\u611f\u77e5\u4e0e\u901a\u7528\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u50cf\u7d20\u7ea7\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18096", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18096", "abs": "https://arxiv.org/abs/2509.18096", "authors": ["Chaehyun Kim", "Heeseong Shin", "Eunbeen Hong", "Heeji Yoon", "Anurag Arnab", "Paul Hongsuck Seo", "Sunghwan Hong", "Seungryong Kim"], "title": "Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers", "comment": "NeurIPS 2025. Project page: https://cvlab-kaist.github.io/Seg4Diff/", "summary": "Text-to-image diffusion models excel at translating language prompts into\nphotorealistic images by implicitly grounding textual concepts through their\ncross-modal attention mechanisms. Recent multi-modal diffusion transformers\nextend this by introducing joint self-attention over concatenated image and\ntext tokens, enabling richer and more scalable cross-modal alignment. However,\na detailed understanding of how and where these attention maps contribute to\nimage generation remains limited. In this paper, we introduce Seg4Diff\n(Segmentation for Diffusion), a systematic framework for analyzing the\nattention structures of MM-DiT, with a focus on how specific layers propagate\nsemantic information from text to image. Through comprehensive analysis, we\nidentify a semantic grounding expert layer, a specific MM-DiT block that\nconsistently aligns text tokens with spatially coherent image regions,\nnaturally producing high-quality semantic segmentation masks. We further\ndemonstrate that applying a lightweight fine-tuning scheme with mask-annotated\nimage data enhances the semantic grouping capabilities of these layers and\nthereby improves both segmentation performance and generated image fidelity.\nOur findings demonstrate that semantic grouping is an emergent property of\ndiffusion transformers and can be selectively amplified to advance both\nsegmentation and generation performance, paving the way for unified models that\nbridge visual perception and generation.", "AI": {"tldr": "Seg4Diff\u662f\u4e00\u4e2a\u5206\u6790\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff08MM-DiT\uff09\u6ce8\u610f\u529b\u7ed3\u6784\u7684\u6846\u67b6\uff0c\u53d1\u73b0\u7279\u5b9a\u5c42\u80fd\u81ea\u7136\u4ea7\u751f\u9ad8\u8d28\u91cf\u8bed\u4e49\u5206\u5272\u63a9\u7801\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u5fae\u8c03\u63d0\u5347\u5206\u5272\u548c\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\u901a\u8fc7\u8054\u5408\u81ea\u6ce8\u610f\u529b\u5b9e\u73b0\u4e86\u66f4\u4e30\u5bcc\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u4f46\u5bf9\u6ce8\u610f\u529b\u56fe\u5982\u4f55\u5177\u4f53\u8d21\u732e\u4e8e\u56fe\u50cf\u751f\u6210\u7684\u7406\u89e3\u4ecd\u7136\u6709\u9650\u3002", "method": "\u5f15\u5165Seg4Diff\u6846\u67b6\u7cfb\u7edf\u5206\u6790MM-DiT\u7684\u6ce8\u610f\u529b\u7ed3\u6784\uff0c\u8bc6\u522b\u8bed\u4e49\u63a5\u5730\u4e13\u5bb6\u5c42\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u5fae\u8c03\u65b9\u6848\u589e\u5f3a\u8bed\u4e49\u5206\u7ec4\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u8bed\u4e49\u5206\u7ec4\u662f\u6269\u6563\u53d8\u6362\u5668\u7684\u6d8c\u73b0\u7279\u6027\uff0c\u53ef\u4ee5\u901a\u8fc7\u9009\u62e9\u6027\u653e\u5927\u6765\u63d0\u5347\u5206\u5272\u6027\u80fd\u548c\u751f\u6210\u56fe\u50cf\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7edf\u4e00\u89c6\u89c9\u611f\u77e5\u548c\u751f\u6210\u7684\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5c55\u793a\u4e86\u8bed\u4e49\u5206\u7ec4\u80fd\u529b\u53ef\u4ee5\u540c\u65f6\u63d0\u5347\u5206\u5272\u548c\u751f\u6210\u4efb\u52a1\u3002"}}
{"id": "2509.18097", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2509.18097", "abs": "https://arxiv.org/abs/2509.18097", "authors": ["Julian Kaltheuner", "Alexander Oebel", "Hannah Droege", "Patrick Stotko", "Reinhard Klein"], "title": "Preconditioned Deformation Grids", "comment": "GitHub: https://github.com/vc-bonn/preconditioned-deformation-grids", "summary": "Dynamic surface reconstruction of objects from point cloud sequences is a\nchallenging field in computer graphics. Existing approaches either require\nmultiple regularization terms or extensive training data which, however, lead\nto compromises in reconstruction accuracy as well as over-smoothing or poor\ngeneralization to unseen objects and motions. To address these lim- itations,\nwe introduce Preconditioned Deformation Grids, a novel technique for estimating\ncoherent deformation fields directly from unstructured point cloud sequences\nwithout requiring or forming explicit correspondences. Key to our approach is\nthe use of multi-resolution voxel grids that capture the overall motion at\nvarying spatial scales, enabling a more flexible deformation representation. In\nconjunction with incorporating grid-based Sobolev preconditioning into\ngradient-based optimization, we show that applying a Chamfer loss between the\ninput point clouds as well as to an evolving template mesh is sufficient to\nobtain accurate deformations. To ensure temporal consistency along the object\nsurface, we include a weak isometry loss on mesh edges which complements the\nmain objective without constraining deformation fidelity. Extensive evaluations\ndemonstrate that our method achieves superior results, particularly for long\nsequences, compared to state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u6761\u4ef6\u53d8\u5f62\u7f51\u683c\u7684\u65b0\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u70b9\u4e91\u5e8f\u5217\u4f30\u8ba1\u8fde\u8d2f\u7684\u53d8\u5f62\u573a\uff0c\u65e0\u9700\u663e\u5f0f\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728\u957f\u5e8f\u5217\u91cd\u5efa\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f", "motivation": "\u89e3\u51b3\u73b0\u6709\u52a8\u6001\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u9700\u8981\u591a\u4e2a\u6b63\u5219\u5316\u9879\u6216\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u5bfc\u81f4\u7684\u7cbe\u5ea6\u59a5\u534f\u3001\u8fc7\u5ea6\u5e73\u6ed1\u4ee5\u53ca\u5bf9\u672a\u89c1\u7269\u4f53\u548c\u8fd0\u52a8\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u591a\u5206\u8fa8\u7387\u4f53\u7d20\u7f51\u683c\u6355\u6349\u4e0d\u540c\u7a7a\u95f4\u5c3a\u5ea6\u7684\u6574\u4f53\u8fd0\u52a8\uff0c\u7ed3\u5408\u57fa\u4e8e\u7f51\u683c\u7684Sobolev\u9884\u6761\u4ef6\u68af\u5ea6\u4f18\u5316\uff0c\u5e94\u7528Chamfer\u635f\u5931\u548c\u5f31\u7b49\u8ddd\u635f\u5931\u786e\u4fdd\u53d8\u5f62\u7cbe\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027", "result": "\u5728\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u957f\u5e8f\u5217\u91cd\u5efa\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u4e8e\u6700\u5148\u8fdb\u6280\u672f\u7684\u7ed3\u679c", "conclusion": "Preconditioned Deformation Grids\u65b9\u6cd5\u80fd\u591f\u76f4\u63a5\u4ece\u975e\u7ed3\u6784\u5316\u70b9\u4e91\u5e8f\u5217\u83b7\u5f97\u51c6\u786e\u7684\u53d8\u5f62\uff0c\u65e0\u9700\u663e\u5f0f\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728\u52a8\u6001\u8868\u9762\u91cd\u5efa\u9886\u57df\u5177\u6709\u663e\u8457\u4f18\u52bf"}}
