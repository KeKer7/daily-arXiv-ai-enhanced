<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 43]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding](https://arxiv.org/abs/2602.17768)
*Boda Lin,Yongjie Zhu,Xiaocheng Gong,Wenyu Qin,Meng Wang*

Main category: cs.CV

TL;DR: 该论文提出了KPM-Bench数据集和MoPE算法，用于解决视频描述中细粒度运动细节描述不准确和幻觉问题，通过运动解析和提取技术提升运动中心视频描述的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前视频描述模型在描述细粒度运动细节方面存在局限，且存在严重的幻觉问题，特别是在运动中心视频中，对复杂动作和肢体动态的精确描述至关重要但常被忽视。

Method: 1) 提出自动化标注流程，整合基于运动学的运动计算和语言解析；2) 构建KPM-Bench数据集，包含细粒度视频-描述对、运动理解问答对和幻觉评估集；3) 提出基于语言的运动解析与提取(MoPE)算法；4) 将MoPE集成到GRPO后训练框架中。

Result: 创建了KPM-Bench开源数据集，提出了MoPE算法和独立的幻觉评估指标，通过GRPO框架有效缓解了幻觉问题，显著提升了运动中心视频描述模型的可靠性。

Conclusion: 该研究通过创新的数据集构建和算法设计，系统解决了视频描述中的细粒度运动理解和幻觉问题，为运动中心视频理解提供了重要工具和评估框架。

Abstract: Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.

</details>


### [2] [CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild](https://arxiv.org/abs/2602.17770)
*Balamurugan Thambiraja,Omid Taheri,Radek Danecek,Giorgio Becherini,Gerard Pons-Moll,Justus Thies*

Main category: cs.CV

TL;DR: 提出了3D-HIW数据集和CLUTCH系统，用于野外环境下的手部动作建模，在文本到手部动作生成和动作描述任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有手部动作建模方法依赖工作室采集的数据集，动作和场景有限，难以扩展到真实野外环境，且现有模型在动画保真度和文本-动作对齐方面存在不足。

Method: 1) 构建3D-HIW数据集：结合视觉语言模型和3D手部追踪器，从大量第一人称动作视频中提取32K个3D手部动作序列和对应文本；2) 提出CLUTCH系统：包含SHIFT（基于VQ-VAE的手部动作token化架构）和几何精炼阶段（通过重建损失微调LLM）。

Result: 在文本到手部动作生成和动作到文本描述任务上实现了最先进的性能，建立了可扩展的野外手部动作建模的首个基准。

Conclusion: 通过3D-HIW数据集和CLUTCH系统，成功解决了野外环境下手部动作建模的挑战，为可扩展的手部动画建模提供了新基准。

Abstract: Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to "in-the-wild" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.

</details>


### [3] [Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision](https://arxiv.org/abs/2602.17785)
*Xinwei Ju,Rema Daher,Danail Stoyanov,Sophia Bano,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: PRISM是一种用于结肠镜单目深度和姿态估计的自监督学习框架，利用边缘检测和亮度解耦来指导几何学习，在真实和合成数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜辅助导航中的单目深度和姿态估计对于改善筛查效果很重要，但面临纹理缺失表面、复杂光照模式、变形以及缺乏可靠真实标注数据等挑战。

Method: 提出PRISM框架，结合边缘检测和亮度解耦来提供结构指导。使用学习型边缘检测器获取边缘图，通过内在分解模块分离着色和反射，利用着色线索进行深度估计。

Result: 在多个真实和合成数据集上实现最先进性能。消融研究表明：1）在真实数据上的自监督训练优于在仿真数据上的监督训练；2）视频帧率是影响模型性能的关键因素。

Conclusion: PRISM通过结合解剖和光照先验，有效解决了结肠镜深度和姿态估计的挑战，为高质量训练数据生成提供了实用指导。

Abstract: Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.

</details>


### [4] [LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge](https://arxiv.org/abs/2602.17793)
*Peide Zhu,Linbin Lu,Zhiqin Chen,Xiong Chen*

Main category: cs.CV

TL;DR: 提出LGD-Net框架，通过跨模态特征幻觉而非像素级图像生成，从H&E切片直接预测HER2表达水平，避免计算开销和重建伪影。


<details>
  <summary>Details</summary>
Motivation: 传统IHC染色评估HER2表达水平资源密集、昂贵且耗时，许多地区无法获得。从H&E切片预测HER2水平成为替代方案，但现有像素级虚拟染色方法计算昂贵且易产生重建伪影，可能导致诊断错误。

Method: 提出Latent-Guided Dual-Stream Network (LGD-Net)，采用跨模态特征幻觉而非显式像素级图像生成。模型学习将形态学H&E特征映射到分子潜在空间，训练时由教师IHC编码器引导。通过轻量级辅助正则化任务，利用核分布和膜染色强度等任务特定领域知识正则化模型训练。

Result: 在公开BCI数据集上的广泛实验表明，LGD-Net达到最先进性能，显著优于基线方法，同时能够使用单模态H&E输入进行高效推理。

Conclusion: LGD-Net通过跨模态特征幻觉有效解决了传统虚拟染色方法的局限性，为从H&E切片准确预测HER2表达水平提供了一种高效可靠的替代方案。

Abstract: It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.

</details>


### [5] [Enabling Training-Free Text-Based Remote Sensing Segmentation](https://arxiv.org/abs/2602.17799)
*Jose Sosa,Danila Rukhovich,Anis Kacem,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出无需额外训练、完全基于现有基础模型的遥感图像文本引导分割方法，结合对比式与生成式视觉语言模型与SAM，在19个遥感基准测试中取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型和视觉基础模型为零样本遥感图像分割提供了新机会，但大多数方法仍依赖额外可训练组件，限制了泛化能力和实际应用。本研究探索完全无需额外训练、仅依赖现有基础模型实现文本引导遥感分割的可能性。

Method: 提出简单有效的训练免费方法：1) 对比式方法使用CLIP作为SAM网格提议的掩码选择器，实现完全零样本开放词汇语义分割；2) 生成式方法使用GPT-5（零样本）和LoRA微调的Qwen-VL模型生成点击提示给SAM，实现推理和指代分割。

Result: 在19个遥感基准测试（包括开放词汇、指代和基于推理的任务）上进行了广泛实验，对比式方法在完全零样本设置下实现了最先进的开放词汇语义分割，生成式方法中LoRA微调的Qwen-VL模型表现最佳。

Conclusion: 研究表明无需额外训练即可实现有效的文本引导遥感分割，结合对比式和生成式视觉语言模型与SAM的简单方法在多个任务上展现出强大能力，为遥感分割提供了实用且泛化性强的解决方案。

Abstract: Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.

</details>


### [6] [VidEoMT: Your ViT is Secretly Also a Video Segmentation Model](https://arxiv.org/abs/2602.17807)
*Narges Norouzi,Idil Esen Zulfikar,Niccol`o Cavagnero,Tommie Kerssies,Bastian Leibe,Gijs Dubbelman,Daan de Geus*

Main category: cs.CV

TL;DR: VidEoMT是一个仅使用编码器的视频分割模型，通过轻量级查询传播机制实现时间建模，无需专用跟踪模块，在保持竞争力的同时达到5-10倍速度提升


<details>
  <summary>Details</summary>
Motivation: 现有在线视频分割模型通常结合逐帧分割器和复杂的专用跟踪模块，这些模块引入了显著的架构复杂性和计算开销。受Vision Transformer在大规模预训练下无需专用模块即可进行准确图像分割的启发，作者希望开发一个简单的仅编码器视频分割模型

Method: 提出Video Encoder-only Mask Transformer (VidEoMT)，采用轻量级查询传播机制，通过重用前一帧的查询来跨帧传递信息。同时使用查询融合策略，将传播查询与一组时间无关的学习查询相结合，平衡时间一致性和对新内容的适应性

Result: VidEoMT在保持竞争力的准确率的同时，实现了5-10倍的速度提升，使用ViT-L骨干网络时运行速度可达160 FPS

Conclusion: VidEoMT证明了仅编码器架构在视频分割任务中的有效性，通过简单的查询传播机制实现了跟踪功能，无需复杂专用模块，在速度和准确性之间取得了良好平衡

Abstract: Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/

</details>


### [7] [VQPP: Video Query Performance Prediction Benchmark](https://arxiv.org/abs/2602.17814)
*Adrian Catalin Lutu,Eduard Poesina,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 提出了首个视频查询性能预测（VQPP）基准，包含两个文本到视频检索数据集和两个CBVR系统，探索了多种预测器并展示了应用价值。


<details>
  <summary>Details</summary>
Motivation: 查询性能预测（QPP）在文本和图像检索中已有广泛研究，但在基于内容的视频检索（CBVR）领域仍未被充分探索，需要建立专门的基准来推动该领域发展。

Method: 创建了VQPP基准，包含56K文本查询和51K视频，提供官方训练/验证/测试划分；探索了多种检索前和检索后性能预测器；使用最佳检索前预测器作为奖励模型，通过直接偏好优化训练LLM进行查询重写。

Result: 检索前预测器表现出有竞争力的性能，可在检索步骤前应用；成功将最佳检索前预测器应用于训练LLM进行查询重写，验证了VQPP的实际应用价值。

Conclusion: 建立了首个视频查询性能预测基准VQPP，为视频领域的QPP研究提供了标准化平台；展示了检索前预测器的实用性和在查询重写等任务中的应用潜力。

Abstract: Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.

</details>


### [8] [On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective](https://arxiv.org/abs/2602.17854)
*Domonkos Varga*

Main category: cs.CV

TL;DR: 该论文对Liu和Szirányi的手势识别方法进行方法学分析，指出其评估协议存在严重的数据泄露问题，导致报告的近完美准确率无效。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是揭示Liu和Szirányi手势识别方法评估协议中的缺陷，特别是数据划分不当导致的数据泄露问题，强调在基于视觉的手势识别研究中，尤其是无人机-人交互等需要识别未见个体手势的应用中，主体独立数据划分的重要性。

Method: 通过分析已发表的混淆矩阵、学习曲线和数据集构建方式，展示其帧级随机训练-测试划分如何不可避免地混合相同主体的样本，导致严重的数据泄露，从而证明该评估协议无法测量对未见个体的泛化能力。

Result: 研究发现报告的近完美准确率指标源于数据泄露，评估协议未能测量对未见个体的泛化能力，混淆矩阵和学习曲线分析证实了数据划分不当的问题。

Conclusion: 结论强调在基于视觉的手势识别研究中，特别是需要识别未见个体手势的应用中，必须采用主体独立的数据划分方法，以确保评估结果的有效性和实际应用价值。

Abstract: This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirányi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.

</details>


### [9] [Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.17869)
*Yuxiao Chen,Jue Wang,Zhikang Zhang,Jingru Yi,Xu Zhang,Yang Zou,Zhaowei Cai,Jianbo Yuan,Xinyu Li,Hao Yang,Davide Modolo*

Main category: cs.CV

TL;DR: 提出一种用于长视频理解的新框架，包含自适应视频采样器和时空视频压缩器，结合多模态大语言模型，能高效处理长视频并保持关键信息。


<details>
  <summary>Details</summary>
Motivation: 随着视频骨干架构和大型语言模型的发展，分析长达数十分钟的长视频变得可行且普遍。但视频序列固有的冗余性给现有模型带来两大挑战：1) 在内存限制内高效处理更多帧；2) 从大量输入数据中提取判别性信息。

Method: 提出端到端的长视频理解框架，包含基于信息密度的自适应视频采样器(AVS)和基于自动编码器的时空视频压缩器(SVC)，并与多模态大语言模型(MLLM)集成。

Result: 该框架在多个基准测试中表现优异，在长视频理解任务和标准视频理解基准上都表现出色，证明了其处理长视频复杂性的有效性和通用性。

Conclusion: 提出的框架能自适应有效地从不同时长视频序列中捕获关键信息，实现高压缩率同时保留重要判别信息，为长视频理解提供了高效解决方案。

Abstract: With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.

</details>


### [10] [Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models](https://arxiv.org/abs/2602.17871)
*Dhruba Ghosh,Yuhui Zhang,Ludwig Schmidt*

Main category: cs.CV

TL;DR: 研究发现当前视觉语言模型在细粒度图像分类任务上表现不佳，通过实验发现更好的视觉编码器能显著提升细粒度分类性能，而预训练阶段对模型权重解冻也至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多种视觉问答基准测试中取得显著进展，但在测试细粒度视觉知识的传统图像分类基准上表现落后。本文旨在探究这种差距的原因，并找出影响细粒度视觉理解的关键因素。

Method: 对大量近期视觉语言模型在细粒度分类基准上进行测试，通过一系列消融实验分析影响性能的因素。主要考察了不同LLM、视觉编码器以及预训练策略（特别是语言模型权重是否解冻）对性能的影响。

Result: 实验发现：1）使用更好的LLM能同等提升所有基准测试分数；2）更好的视觉编码器能不成比例地大幅提升细粒度分类性能；3）预训练阶段对细粒度性能至关重要，特别是当语言模型权重在预训练期间解冻时效果更佳。

Conclusion: 这些发现为增强视觉语言模型的细粒度视觉理解和视觉中心能力提供了重要指导，表明需要特别关注视觉编码器的质量和预训练策略，以缩小细粒度视觉知识与通用视觉基准之间的差距。

Abstract: Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.

</details>


### [11] [A Single Image and Multimodality Is All You Need for Novel View Synthesis](https://arxiv.org/abs/2602.17909)
*Amirhosein Javadi,Chi-Shiang Gau,Konstantinos D. Polyzos,Tara Javidi*

Main category: cs.CV

TL;DR: 该论文提出了一种多模态深度重建框架，利用极稀疏的雷达或LiDAR测距数据生成密集深度图，用于改进基于扩散的单图像新视角合成。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的方法在单图像新视角合成中表现良好，但其质量受限于单目深度估计的可靠性。在低纹理、恶劣天气和遮挡严重的真实场景中，深度估计往往不可靠，导致合成视图质量受限。

Method: 提出多模态深度重建框架，利用极稀疏的测距数据（如汽车雷达或LiDAR），在角度域中使用局部高斯过程建模深度，实现计算高效推理并显式量化不确定性。重建的深度和不确定性可直接替换现有扩散渲染管道中的单目深度估计器。

Result: 在真实世界多模态驾驶场景实验中，用稀疏测距重建深度替换纯视觉深度，显著提高了单图像新视角视频生成的几何一致性和视觉质量。

Conclusion: 研究强调了可靠几何先验对基于扩散的视角合成的重要性，并证明了即使在极端稀疏情况下，多模态传感也能带来实际效益。

Abstract: Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.

</details>


### [12] [ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging](https://arxiv.org/abs/2602.17929)
*Athanasios Angelakis*

Main category: cs.CV

TL;DR: ZACH-ViT是一种紧凑的视觉Transformer，移除了位置嵌入和[CLS]标记，通过全局平均池化实现排列不变性，在医学图像分类任务中表现出色，特别适合资源受限的临床环境。


<details>
  <summary>Details</summary>
Motivation: 传统视觉Transformer依赖位置嵌入和类别标记，这些固定的空间先验在医学图像中可能阻碍泛化能力，因为医学图像的空间布局通常信息较弱或不一致。需要设计更适合医学图像特性的架构。

Method: 提出ZACH-ViT（Zero-token Adaptive Compact Hierarchical Vision Transformer），移除位置嵌入和[CLS]标记，通过全局平均池化处理补丁表示实现排列不变性。采用自适应残差投影保持训练稳定性，同时严格控制参数数量。

Result: 在7个MedMNIST数据集上的评估显示：ZACH-ViT（0.25M参数）在BloodMNIST上表现最佳，在PathMNIST上与TransMIL竞争，但在具有强解剖先验的数据集（OCTMNIST, OrganAMNIST）上优势减弱。尽管尺寸极小且无预训练，仍能实现竞争性性能，推理时间低于1秒。

Conclusion: 将架构归纳偏置与数据结构对齐比追求通用基准优势更重要。ZACH-ViT的紧凑设计和快速推理能力使其适合资源受限的临床环境部署，为医学图像分析提供了新的架构选择。

Abstract: Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term "Zero-token" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.
  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.

</details>


### [13] [ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models](https://arxiv.org/abs/2602.17951)
*Guoheng Sun,Tingting Du,Kaixi Feng,Chenxiang Luo,Xingguo Ding,Zheyu Shen,Ziyao Wang,Yexiao He,Ang Li*

Main category: cs.CV

TL;DR: ROCKET提出了一种残差导向的多层表示对齐框架，通过共享投影器和层不变映射，将VLA模型的多个层与强大的3D视觉基础模型对齐，显著提升了3D空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型通常在2D数据上预训练，缺乏3D空间理解能力。现有表示对齐方法通常只在单层应用监督，无法充分利用深度分布的信息，而简单的多层对齐可能导致梯度干扰。

Method: 提出ROCKET框架，将多层对齐形式化为将一个残差流对齐到另一个残差流。使用共享投影器通过层不变映射将VLA骨干的多个层与3D视觉基础模型的多个层对齐，减少梯度冲突。采用Matryoshka风格的稀疏激活方案平衡多个对齐损失，并结合免训练层选择策略。

Result: 在LIBERO上仅需约4%的计算预算就达到98.5%的SOTA成功率。在LIBERO-Plus和RoboTwin等多个数据集上表现出优越性能，且适用于多种VLA模型。

Conclusion: ROCKET通过残差导向的多层表示对齐有效提升了VLA模型的3D空间理解能力，计算效率高且性能优越，为机器人操作任务提供了有效的解决方案。

Abstract: Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.

</details>


### [14] [Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching](https://arxiv.org/abs/2602.18000)
*Xuting Lan,Mingliang Zhou,Xuekai Wei,Jielu Yan,Yueting Huang,Huayan Pu,Jun Luo,Weijia Jia*

Main category: cs.CV

TL;DR: 提出基于记忆驱动的质量感知框架MQAF，通过建立存储失真模式的记忆库，在有无参考图像时动态切换双模式质量评估策略，减少对高质量参考图像的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有全参考图像质量评估方法依赖参考图像质量，限制了在理想参考源不可用的实际应用。受人类视觉系统积累视觉记忆能力的启发，提出基于长期记忆存储的图像质量评估方法。

Method: 建立存储失真模式的记忆库，采用双模式质量评估策略：有参考图像时，自适应加权参考信息并比较失真图像与记忆库中的失真模式；无参考图像时，依赖记忆库中的失真模式推断图像质量。

Result: 实验结果表明，该方法在多个数据集上优于现有最先进方法，同时适应无参考和全参考任务。

Conclusion: 提出的记忆驱动质量感知框架通过模拟人类视觉记忆机制，有效减少对高质量参考图像的依赖，在有无参考图像情况下都能实现高质量评估，具有实际应用价值。

Abstract: Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.

</details>


### [15] [MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method](https://arxiv.org/abs/2602.18006)
*Ahsan Baidar Bakht,Mohamad Alansari,Muhayy Ud Din,Muzammal Naseer,Sajid Javed,Irfan Hussain,Jiri Matas,Arif Mahmood*

Main category: cs.CV

TL;DR: 提出了首个伪多模态水下目标跟踪基准MUOT_3M（300万帧）和基于SAM的多模态到单模态跟踪器MUTrack，通过知识蒸馏实现高性能实时跟踪。


<details>
  <summary>Details</summary>
Motivation: 水下目标跟踪对海洋机器人、生态监测和海洋探索至关重要，但现有基准数据集小且仅RGB模态，限制了在颜色失真、浑浊和低能见度条件下的鲁棒性。

Method: 1) 构建MUOT_3M基准：包含3030个视频的300万帧，标注32个跟踪属性、677个细粒度类别，同步RGB、增强RGB、深度和语言模态；2) 提出MUTrack跟踪器：基于SAM架构，包含视觉几何对齐、视觉语言融合和四级知识蒸馏，将多模态知识迁移到单模态学生模型。

Result: 在五个UOT基准测试中，MUTrack比最强SOTA基线提升AUC达8.40%，精度提升7.80%，同时以24FPS实时运行。MUOT_3M基准经过海洋生物学家验证。

Conclusion: MUOT_3M和MUTrack为可扩展、多模态训练但实际可部署的水下跟踪建立了新基础，解决了现有数据集稀缺和模态单一的问题。

Abstract: Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.

</details>


### [16] [Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating](https://arxiv.org/abs/2602.18016)
*Jiamin Luo,Xuqian Gu,Jingjing Wang,Jiahong Lu*

Main category: cs.CV

TL;DR: 本文提出L-AVC任务，通过多模态LLM编辑图像的主观情感，并设计EPEM方法实现高效精确的情感操控


<details>
  <summary>Details</summary>
Motivation: 现有视觉定制研究主要关注客观对齐（如语言、布局等），忽略了主观情感内容，且缺乏面向情感视觉定制的基础模型

Method: 提出EPEM方法：1) EIC模块使LLM高效对齐编辑前后的情感语义转换；2) PER模块精确保留情感无关内容

Result: 在构建的L-AVC数据集上，EPEM方法显著优于多个SOTA基线，证明了情感信息对L-AVC的重要性及EPEM的有效性

Conclusion: 情感信息对视觉定制至关重要，EPEM方法能高效精确地操控图像中的主观情感，为情感视觉定制提供了有效解决方案

Abstract: Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.

</details>


### [17] [DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE](https://arxiv.org/abs/2602.18019)
*Yujie Jin,Wenxin Zhang,Jingjing Wang,Guodong Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度安全导向视频理解任务（DeepSVU），旨在不仅识别和定位威胁，还要归因和评估威胁原因，并提出了统一物理世界正则化MoE方法来解决该任务。


<details>
  <summary>Details</summary>
Motivation: 现有安全导向视频理解研究主要关注检测和定位威胁（如枪击、抢劫），但缺乏有效生成和评估威胁原因的能力。为填补这一空白，本文提出了新的深度安全导向视频理解任务。

Method: 提出了统一物理世界正则化MoE方法（UPRM），包含两个关键组件：统一物理世界增强MoE块（UPE）用于建模从粗到细的物理世界信息，物理世界权衡正则化器（PTR）用于自适应权衡这些因素。

Result: 在DeepSVU指令数据集（UCF-C指令和CUVA指令）上的大量实验表明，UPRM优于多种先进的视频大语言模型和非VLM方法，验证了从粗到细物理世界信息的重要性。

Conclusion: 本文提出的DeepSVU任务和UPRM方法有效解决了安全导向视频理解中的深度分析需求，证明了建模物理世界信息的重要性，并为该领域提供了新的研究方向。

Abstract: In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.

</details>


### [18] [UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models](https://arxiv.org/abs/2602.18020)
*Jiabing Yang,Yixiang Chen,Yuan Xu,Peiyan Li,Xiangnan Wu,Zichen Wen,Bowen Fang,Tao Yu,Zhengbo Zhang,Yingda Li,Kai Wang,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 提出UAOR模块，通过不确定性感知的观测信息重注入，无需训练即可提升VLA模型的性能，无需额外观测线索或模块


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型提升性能通常需要额外观测线索（如深度图、点云）或辅助模块，这些方法需要昂贵的数据收集和额外训练。受语言模型中前馈网络作为"键值记忆"的启发，希望开发一种无需训练、即插即用的模块来增强VLA模型

Method: 提出不确定性感知观测重注入（UAOR）模块。当当前语言模型层表现出高不确定性（通过动作熵衡量）时，通过注意力检索将关键观测信息重注入到下一层的前馈网络中，帮助VLA在推理过程中更好地关注观测信息

Result: 综合实验表明，该方法在各种VLA模型上都能一致提升性能，涵盖仿真和真实世界任务，且开销极小。特别地，UAOR消除了对额外观测线索或模块的需求

Conclusion: UAOR是一种有效、无需训练、即插即用的模块，可作为现有VLA流程的通用实用插件，通过不确定性感知的观测信息重注入机制提升模型性能

Abstract: Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as "key-value memory", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.

</details>


### [19] [Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers](https://arxiv.org/abs/2602.18022)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: 本文提出DCAG框架，通过同时操纵DiT注意力机制中的Key和Value通道来实现无需训练的编辑强度控制，相比仅操作Key的方法在编辑保真度方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于DiT架构的扩散图像编辑模型需要无需训练的编辑强度控制。现有注意力操纵方法仅关注Key空间来调节注意力路由，而完全忽略了控制特征聚合的Value空间。

Method: 提出Dual-Channel Attention Guidance (DCAG)框架，同时操纵Key通道（控制关注位置）和Value通道（控制聚合内容）。基于DiT多模态注意力层中Key和Value投影都表现出明显的偏置-增量结构的观察，通过二维参数空间(δ_k, δ_v)实现更精确的编辑-保真度权衡。

Result: 在PIE-Bench基准测试（700张图像，10个编辑类别）上，DCAG在所有保真度指标上都优于仅使用Key指导的方法，在对象删除（LPIPS减少4.9%）和对象添加（LPIPS减少3.2%）等局部编辑任务中改进最显著。

Conclusion: DCAG通过同时操纵Key和Value通道，提供了比单通道方法更精确的编辑控制，实现了更好的编辑质量与保真度平衡。

Abstract: Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).

</details>


### [20] [Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition](https://arxiv.org/abs/2602.18043)
*Hongyu Qu,Xiangbo Shu,Rui Yan,Hailiang Gao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: DiST提出了一种基于分解-融合框架的少样本动作识别方法，利用大语言模型提供的解耦空间和时间知识来学习多粒度原型，在五个标准数据集上达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 传统少样本动作识别方法仅使用粗粒度的类别名称作为辅助上下文，这种上下文信息过于有限，无法为捕捉动作中的新颖空间和时间概念提供足够的背景知识。

Method: 提出分解-融合框架DiST：1)分解阶段：将原始动作名称解耦为多样化的时空属性描述；2)融合阶段：提出空间/时间知识补偿器(SKC/TKC)，分别发现判别性的对象级和帧级原型，SKC在空间知识指导下自适应聚合重要补丁标记，TKC利用时间属性辅助帧间时间关系建模。

Result: 在五个标准少样本动作识别数据集上取得了最先进的性能，证明了方法的有效性。

Conclusion: 通过利用大语言模型提供的解耦空间和时间知识，DiST能够学习表达性强的多粒度原型，从而在少样本动作识别任务中实现更好的性能，为捕捉细粒度空间细节和多样化时间模式提供了透明度。

Abstract: Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.

</details>


### [21] [CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras](https://arxiv.org/abs/2602.18047)
*Rong Fu,Wenxin Zhang,Yibo Meng,Jia Yee Tan,Jiaxuan Lu,Rui Lu,Jiekai Wu,Zhaolu Kang,Simon Fong*

Main category: cs.CV

TL;DR: CityGuard：一种用于去中心化监控的拓扑感知Transformer框架，通过分散自适应度量学习、空间条件注意力和差分隐私嵌入映射，在保护隐私的同时实现城市尺度的人员重识别。


<details>
  <summary>Details</summary>
Motivation: 城市尺度的人员重识别面临视角变化、遮挡和域偏移等挑战，同时需要遵守数据保护法规，防止原始图像共享。现有方法难以在保护隐私的同时实现高效的身份检索。

Method: 1. 分散自适应度量学习：根据特征分布调整实例级边界，增强类内紧凑性；2. 空间条件注意力：将粗粒度几何信息（如GPS或部署平面图）注入图自注意力，实现投影一致的跨视角对齐；3. 差分隐私嵌入映射：结合紧凑近似索引，支持安全且成本高效的部署。

Result: 在Market-1501和其他公开基准测试中，该方法在检索精度和查询吞吐量方面均优于强基线方法，数据库规模的检索研究也证实了该框架在隐私关键的城市身份匹配中的实用性。

Conclusion: CityGuard框架通过整合度量学习、几何感知和隐私保护机制，实现了对视角变化、遮挡和域偏移具有鲁棒性的描述符，并在严格的差分隐私约束下提供了隐私与效用的可调平衡，适用于隐私关键的城市监控应用。

Abstract: City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.

</details>


### [22] [Temporal Consistency-Aware Text-to-Motion Generation](https://arxiv.org/abs/2602.18057)
*Hongsong Wang,Wenjing Yan,Qiuxia Lai,Xin Geng*

Main category: cs.CV

TL;DR: TCA-T2M是一个时间一致性感知的文本到动作生成框架，通过跨序列时间对齐和运动约束提升生成质量


<details>
  <summary>Details</summary>
Motivation: 现有两阶段文本到动作生成方法忽略了跨序列的时间一致性，导致语义错位和物理上不合理的动作，需要解决这一问题以生成更鲁棒和连贯的动作序列

Method: 提出TCA-T2M框架，包含：1) 时间一致性感知的空间VQ-VAE用于跨序列时间对齐；2) 掩码运动变换器用于文本条件动作生成；3) 运动学约束块减少离散化伪影确保物理合理性

Result: 在HumanML3D和KIT-ML基准测试中达到最先进性能，证明了时间一致性对鲁棒和连贯文本到动作生成的重要性

Conclusion: TCA-T2M通过引入时间一致性感知机制有效解决了现有方法的局限性，显著提升了文本到动作生成的语义对齐和物理合理性

Abstract: Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.

</details>


### [23] [3DMedAgent: Unified Perception-to-Understanding for 3D Medical Analysis](https://arxiv.org/abs/2602.18064)
*Ziyue Wang,Linghan Cai,Chang Han Low,Haofeng Liu,Junde Wu,Jingyu Wang,Rui Wang,Lei Song,Jiang Bian,Jingjing Fu,Yueming Jin*

Main category: cs.CV

TL;DR: 3DMedAgent：一种统一代理，使2D多模态大语言模型能够无需3D特定微调即可执行通用3D CT分析，通过协调异构工具和结构化记忆实现从感知到理解的渐进推理。


<details>
  <summary>Details</summary>
Motivation: 现有3D医学影像分析方法存在局限性：要么采用孤立的任务特定建模，要么采用任务无关的端到端范式，无法系统积累感知证据用于下游推理。同时，当前多模态大语言模型主要针对2D设计，难以有效感知和分析3D体积医学数据。

Method: 提出3DMedAgent统一代理框架，通过灵活的MLLM代理协调异构视觉和文本工具，将复杂3D分析逐步分解为可处理的子任务：从全局到局部视图、从3D体积到信息丰富的2D切片、从视觉证据到结构化文本表示。核心设计包括长期结构化记忆，用于聚合中间工具输出并支持查询自适应、证据驱动的多步推理。

Result: 在超过40个任务上的实验表明，3DMedAgent在3D胸部影像分析中一致优于通用、医学专用和3D特定的多模态大语言模型，展示了向通用3D临床助手发展的可扩展路径。

Conclusion: 3DMedAgent成功弥合了2D MLLM与3D医学影像分析之间的差距，通过代理协调和结构化记忆实现了从低层感知到高层临床理解的连续分析，为开发通用3D临床助手提供了有前景的框架。

Abstract: 3D CT analysis spans a continuum from low-level perception to high-level clinical understanding. Existing 3D-oriented analysis methods adopt either isolated task-specific modeling or task-agnostic end-to-end paradigms to produce one-hop outputs, impeding the systematic accumulation of perceptual evidence for downstream reasoning. In parallel, recent multimodal large language models (MLLMs) exhibit improved visual perception and can integrate visual and textual information effectively, yet their predominantly 2D-oriented designs fundamentally limit their ability to perceive and analyze volumetric medical data. To bridge this gap, we propose 3DMedAgent, a unified agent that enables 2D MLLMs to perform general 3D CT analysis without 3D-specific fine-tuning. 3DMedAgent coordinates heterogeneous visual and textual tools through a flexible MLLM agent, progressively decomposing complex 3D analysis into tractable subtasks that transition from global to regional views, from 3D volumes to informative 2D slices, and from visual evidence to structured textual representations. Central to this design, 3DMedAgent maintains a long-term structured memory that aggregates intermediate tool outputs and supports query-adaptive, evidence-driven multi-step reasoning. We further introduce the DeepChestVQA benchmark for evaluating unified perception-to-understanding capabilities in 3D thoracic imaging. Experiments across over 40 tasks demonstrate that 3DMedAgent consistently outperforms general, medical, and 3D-specific MLLMs, highlighting a scalable path toward general-purpose 3D clinical assistants.Code and data are available at \href{https://github.com/jinlab-imvr/3DMedAgent}{https://github.com/jinlab-imvr/3DMedAgent}.

</details>


### [24] [Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation](https://arxiv.org/abs/2602.18066)
*Daniel Busch,Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Richard Meyes,Tobias Meisen*

Main category: cs.CV

TL;DR: 提出两阶段训练策略，通过自监督预训练和少量监督微调，减少BEV语义地图对昂贵标注数据的依赖，同时提升性能


<details>
  <summary>Details</summary>
Motivation: 当前多摄像头BEV语义地图方法依赖昂贵且标注不一致的地面真值数据，需要减少对完全监督的依赖

Method: 两阶段训练：1) 自监督预训练阶段，将BEVFormer预测结果可微分地重投影到图像平面，使用Mask2Former生成的多视角语义伪标签进行训练，加入时序一致性损失；2) 监督微调阶段仅需50%数据集

Result: 在nuScenes数据集上，微调性能优于完全监督基线模型（提升达+2.5pp mIoU），同时减少50%标注数据使用，总训练时间减少三分之二

Conclusion: 可微分重投影加相机视角伪标签能产生可迁移的BEV特征，为减少标注的自动驾驶感知提供了可扩展的路径

Abstract: Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.

</details>


### [25] [Comparative Assessment of Multimodal Earth Observation Data for Soil Moisture Estimation](https://arxiv.org/abs/2602.18083)
*Ioannis Kontogiorgakis,Athanasios Askitopoulos,Iason Tsardanidis,Dimitrios Bormpoudakis,Ilias Tsoumas,Fotios Balampanis,Charalampos Kontoes*

Main category: cs.CV

TL;DR: 提出一个结合Sentinel-1 SAR、Sentinel-2光学影像和ERA-5再分析数据的10米分辨率土壤湿度估算框架，用于欧洲植被覆盖区，通过机器学习方法实现农场级应用。


<details>
  <summary>Details</summary>
Motivation: 现有卫星土壤湿度产品分辨率太粗（>1公里），无法满足农场级应用需求，需要开发高分辨率（10米）的土壤湿度估算方法。

Method: 结合Sentinel-1 SAR、Sentinel-2光学影像和ERA-5再分析数据，通过机器学习方法构建土壤湿度估算框架。使用113个国际土壤湿度网络站点进行验证，比较不同模态组合和时间参数化策略，并评估IBM-NASA Prithvi基础模型嵌入与传统手工特征的效果。

Result: 混合时间匹配（Sentinel-2当天采集与Sentinel-1降轨）达到R²=0.514，10天ERA5回溯窗口将性能提升至R²=0.518。Prithvi基础模型嵌入相比传统手工特征改进有限（R²=0.515 vs. 0.514）。

Conclusion: 领域特定的光谱指数结合基于树的集成方法为泛欧洲田间尺度土壤湿度监测提供了实用且计算高效的解决方案，传统特征工程在稀疏数据回归任务中仍然具有竞争力。

Abstract: Accurate soil moisture (SM) estimation is critical for precision agriculture, water resources management and climate monitoring. Yet, existing satellite SM products are too coarse (>1km) for farm-level applications. We present a high-resolution (10m) SM estimation framework for vegetated areas across Europe, combining Sentinel-1 SAR, Sentinel-2 optical imagery and ERA-5 reanalysis data through machine learning. Using 113 International Soil Moisture Network (ISMN) stations spanning diverse vegetated areas, we compare modality combinations with temporal parameterizations, using spatial cross-validation, to ensure geographic generalization. We also evaluate whether foundation model embeddings from IBM-NASA's Prithvi model improve upon traditional hand-crafted spectral features. Results demonstrate that hybrid temporal matching - Sentinel-2 current-day acquisitions with Sentinel-1 descending orbit - achieves R^2=0.514, with 10-day ERA5 lookback window improving performance to R^2=0.518. Foundation model (Prithvi) embeddings provide negligible improvement over hand-crafted features (R^2=0.515 vs. 0.514), indicating traditional feature engineering remains highly competitive for sparse-data regression tasks. Our findings suggest that domain-specific spectral indices combined with tree-based ensemble methods offer a practical and computationally efficient solution for operational pan-European field-scale soil moisture monitoring.

</details>


### [26] [DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text](https://arxiv.org/abs/2602.18089)
*Kunwar Arpit Singh,Ankush Prakash,Haroon R Lone*

Main category: cs.CV

TL;DR: DohaScript是一个大规模多作者手写印地语数据集，包含531位作者抄写的相同六首传统对句，用于系统分析书写风格差异，支持手写识别、作者识别、风格分析和生成建模等任务。


<details>
  <summary>Details</summary>
Motivation: 现有手写天城体文本数据集规模有限，主要关注孤立字符或短词，缺乏受控词汇内容和作者多样性，无法捕捉天城体手写连续、融合和结构复杂的特性，限制了数据驱动手写分析的发展。

Method: 收集531位独特贡献者的手写印地语文本，设计为平行风格语料库，所有作者抄写相同的六首传统印地语对句，包含非识别性人口统计元数据、基于清晰度和分辨率的严格质量筛选，以及页面级布局难度标注。

Result: 基准实验显示数据集在质量分离和未见作者泛化方面表现良好，证明了其可靠性和实用价值，能够支持手写识别、作者识别、风格分析和生成建模等多种任务。

Conclusion: DohaScript旨在作为标准化、可复现的基准数据集，推动低资源脚本环境下连续手写天城体文本的研究进展，填补了该领域大规模高质量数据集的空白。

Abstract: Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.

</details>


### [27] [Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.18093)
*Hanshuai Cui,Zhiqing Tang,Qianli Ma,Zhi Yao,Weijia Jia*

Main category: cs.CV

TL;DR: 提出PrediT框架，通过线性多步法预测扩散模型特征，实现训练免费加速，在DiT图像视频生成中达到5.54倍延迟降低


<details>
  <summary>Details</summary>
Motivation: 扩散变换器(DiT)在图像视频生成中表现出色，但其迭代去噪过程计算成本高。现有训练免费加速方法依赖特征缓存和重用，但重用特征可能导致潜在漂移和视觉质量下降。作者观察到模型输出在扩散轨迹上平滑演化，可以进行预测而非简单重用。

Method: 提出PrediT框架：1) 将特征预测建模为线性多步问题，使用经典线性多步法从历史信息预测未来模型输出；2) 在高动态区域激活校正器防止误差累积；3) 动态步长调制机制通过监测特征变化率自适应调整预测范围。

Result: 在各种基于DiT的图像和视频生成模型上实现了高达5.54倍的延迟降低，同时质量下降可忽略不计。实验验证了该方法在保持生成保真度的同时实现显著加速。

Conclusion: PrediT是一个有效的训练免费加速框架，通过特征预测而非简单重用，在DiT模型中实现了显著的计算加速，同时保持了生成质量，为扩散模型的实际部署提供了实用解决方案。

Abstract: Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.

</details>


### [28] [OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2602.18094)
*Ling Lin,Yang Bai,Heng Su,Congcong Zhu,Yaoxing Wang,Yang Zhou,Huazhu Fu,Jingrun Chen*

Main category: cs.CV

TL;DR: OODBench：首个全面评估视觉语言模型处理分布外数据能力的基准，包含4万实例级OOD数据对，揭示当前VLMs在常见类别上仍存在显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 现实应用中数据往往不满足IID假设，处理分布外对象失败可能带来安全风险（如自动驾驶、医疗辅助），但现有研究缺乏全面评估VLMs处理OOD数据能力的有效基准。

Method: 提出OODBench，一种主要自动化、最小人工验证的方法，用于构建新基准并评估VLMs处理OOD数据的能力。包含4万实例级OOD实例-类别对，并提出可靠自动化评估指标，采用从基础到高级的渐进式提示问题来更全面评估OOD数据对不同难度问题的影响。

Result: 当前VLMs在OODBench上仍表现出显著性能下降，即使底层图像类别是常见的。提出的评估方法能有效衡量OOD数据对模型性能的影响。

Conclusion: OODBench填补了VLMs处理分布外数据评估的空白，总结了重要发现和见解，为未来OOD数据获取和评估研究提供了基础。

Abstract: Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.

</details>


### [29] [Evaluating Graphical Perception Capabilities of Vision Transformers](https://arxiv.org/abs/2602.18178)
*Poonam Poonam,Pere-Pau Vázquez,Timo Ropinski*

Main category: cs.CV

TL;DR: ViTs在可视化图形感知任务中的表现不如CNN，与人类感知对齐有限


<details>
  <summary>Details</summary>
Motivation: 虽然ViTs在各种图像任务中表现出色，但其在可视化图形感知任务中的能力尚未被探索，而这类任务对解释可视化至关重要

Method: 基于Cleveland和McGill的基础研究，设计受控的图形感知任务，将ViTs与CNNs和人类参与者的表现进行对比

Result: ViTs在通用视觉任务中表现良好，但在可视化领域的图形感知任务中与人类感知的对齐有限

Conclusion: ViTs在可视化图形感知方面存在关键感知差距，这对在可视化系统和图形感知建模中应用ViTs有重要启示

Abstract: Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.

</details>


### [30] [BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards](https://arxiv.org/abs/2602.18193)
*Yiran Yang,Zhaowei Liu,Yuan Yuan,Yukun Song,Xiong Ma,Yinghao Song,Xiangji Zeng,Lu Sun,Yulu Wang,Hai Zhou,Shuai Cui,Zhaohan Gong,Jiefei Zhang*

Main category: cs.CV

TL;DR: BLM-Guard：一个用于短视频广告内容审核的框架，结合思维链推理、规则策略和强化学习，能检测多模态欺骗性内容


<details>
  <summary>Details</summary>
Motivation: 短视频平台上的多模态广告包含欺骗性视觉、语音和字幕内容，需要比社区安全过滤器更细粒度、基于策略的审核方法

Method: 1. 使用规则驱动的ICoT数据合成管道生成结构化场景描述、推理链和标签；2. 强化学习通过平衡因果一致性和策略遵循的复合奖励来优化模型；3. 多任务架构建模模态内操纵和跨模态不匹配

Result: 在真实短视频广告上的实验表明，BLM-Guard在准确性、一致性和泛化能力方面超越了强基线方法

Conclusion: BLM-Guard框架为商业广告内容审核提供了一种有效的解决方案，通过结合思维链推理、策略原则和强化学习，实现了对多模态欺骗性内容的精细检测

Abstract: Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.

</details>


### [31] [A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion](https://arxiv.org/abs/2602.18199)
*Gahyeon Shim,Soogeun Park,Hyemin Ahn*

Main category: cs.CV

TL;DR: DMC是一个后处理模块，通过自监督数据驱动方法改进文本生成动作的物理合理性，同时保持语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成方法在语义对齐方面取得进展，但难以同时保证语义和物理真实性（如脚部漂浮问题）。

Method: 提出Distortion-aware Motion Calibrator (DMC)，这是一个自监督数据驱动的后处理模块，通过输入故意扭曲的动作和原始文本描述，学习生成物理合理的动作。

Result: DMC在多个文本到动作模型上显著提升物理合理性：在T2M上FID降低42.74%，在T2M-GPT上降低13.20%，R-Precision最高；在MoMask上穿透减少33.0%，漂浮伪影更接近真实参考。

Conclusion: DMC可作为有前景的后处理框架，为各种文本到动作模型同时提升语义一致性和物理合理性。

Abstract: Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.

</details>


### [32] [On the Adversarial Robustness of Discrete Image Tokenizers](https://arxiv.org/abs/2602.18252)
*Rishika Bhagwatkar,Irina Rish,Nicolas Flammarion,Francesco Croce*

Main category: cs.CV

TL;DR: 首次研究离散图像分词器的对抗攻击脆弱性，提出高效攻击方法并通过无监督对抗训练提升鲁棒性


<details>
  <summary>Details</summary>
Motivation: 离散图像分词器在多模态系统中应用广泛，但其对抗攻击脆弱性尚未被探索，而CLIP编码器的脆弱性已有研究，需要填补这一空白

Method: 1. 提出针对离散分词器的特征扰动攻击，改变提取的token；2. 受鲁棒CLIP编码器启发，采用无监督对抗训练微调分词器，保持其他组件冻结

Result: 攻击方法计算高效、应用无关，在分类、多模态检索和字幕生成任务中有效；防御方法显著提升对无监督和端到端监督攻击的鲁棒性，并能泛化到未见任务和数据

Conclusion: 揭示了分词器鲁棒性在下游任务中的关键作用，为开发安全的多模态基础模型迈出重要一步，无监督方法比监督对抗训练更通用

Abstract: Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.

</details>


### [33] [DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control](https://arxiv.org/abs/2602.18282)
*Shiyan Du,Conghan Yue,Xinyu Cheng,Dongyu Zhang*

Main category: cs.CV

TL;DR: DEIG是一个用于细粒度可控多实例生成的框架，通过实例细节提取器和细节融合模块解决现有方法在复杂文本描述下的语义理解问题，在空间一致性、语义准确性和组合泛化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多实例生成方法在空间布局和属性绑定方面已有进步，但在处理复杂文本描述时仍面临细粒度语义理解的挑战，特别是难以精确匹配局部化文本描述。

Method: 提出DEIG框架：1) 实例细节提取器(IDE)将文本编码器嵌入转换为紧凑的实例感知表示；2) 细节融合模块(DFM)应用基于实例的掩码注意力防止实例间属性泄漏；3) 构建高质量数据集和DEIG-Bench基准；4) 可作为即插即用模块集成到标准扩散管道中。

Result: DEIG在多个基准测试中一致优于现有方法，在空间一致性、语义准确性和组合泛化方面表现优异。作为即插即用模块，易于集成到标准扩散管道中。

Conclusion: DEIG通过创新的实例细节提取和融合机制，实现了细粒度可控的多实例生成，能够精确匹配丰富的局部化文本描述，为多实例场景生成提供了有效解决方案。

Abstract: Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.

</details>


### [34] [Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation](https://arxiv.org/abs/2602.18309)
*Ziyue Liu,Davide Talon,Federico Girella,Zanxi Ruan,Mattia Mondo,Loris Bazzani,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: LOTS是一个结合全局草图引导和多个局部草图-文本对的时尚图像生成框架，通过多级条件编码和扩散对引导机制，在保持全局结构的同时利用局部语义信息。


<details>
  <summary>Details</summary>
Motivation: 在时尚设计中，草图能表达结构、轮廓和空间关系，而文本描述能补充材质、颜色和风格细节。如何有效结合这两种模态，在遵循草图视觉结构的同时利用文本的局部属性指导，是当前研究的挑战。

Method: 提出LOTS框架：1）多级条件编码阶段：在共享潜在空间中独立编码局部特征，同时保持全局结构协调；2）扩散对引导阶段：通过注意力机制在扩散模型的多步去噪过程中整合局部和全局条件。还创建了Sketchy数据集，包含专业草图和非专家草图两种类型。

Result: 实验表明，该方法在保持全局结构一致性的同时，能够利用更丰富的局部语义指导，相比现有最先进方法取得了改进。创建的数据集Sketchy提供了高质量的专业草图和非专家草图。

Conclusion: LOTS框架通过结合全局草图引导和多个局部草图-文本对，有效增强了时尚图像生成的质量，在保持结构一致性的同时利用局部语义信息。公开的数据集、平台和代码为后续研究提供了基础。

Abstract: Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an "in the wild" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.

</details>


### [35] [Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting](https://arxiv.org/abs/2602.18314)
*Tianyi Song,Danail Stoyanov,Evangelos Mazomenos,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: Diff2DGS：两阶段框架，通过扩散模型修复被器械遮挡的组织，结合可学习变形模型的2D高斯泼溅，实现手术场景的实时3D重建，在图像质量和深度精度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有手术场景重建方法在遮挡区域质量有限，且缺乏深度精度评估。EndoNeRF和StereoMIS等基准数据集缺少3D真值，难以全面评估重建质量。

Method: 1. 第一阶段：基于扩散的视频修复模块，利用时序先验修复被手术器械遮挡的组织，保证时空一致性。2. 第二阶段：结合可学习变形模型（LDM）的2D高斯泼溅（2DGS），捕捉动态组织变形和解剖几何结构。3. 在SCARED数据集上进行深度精度定量分析。

Result: 在EndoNeRF上达到38.02 dB PSNR，在StereoMIS上达到34.40 dB PSNR，均优于现有方法。实验表明仅优化图像质量不一定能获得最佳3D重建精度，因此进一步优化深度质量以获得更准确的几何结构。

Conclusion: Diff2DGS通过两阶段方法实现了遮挡手术场景的可靠3D重建，在图像保真度和几何精度方面均表现出色，为机器人手术的实时场景重建提供了有效解决方案。

Abstract: Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.

</details>


### [36] [Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis](https://arxiv.org/abs/2602.18322)
*Ziteng Cui,Shuhong Liu,Xiaoyu Dong,Xuangeng Chu,Lin Gu,Ming-Hsuan Yang,Tatsuya Harada*

Main category: cs.CV

TL;DR: 提出Luminance-GS++，一个基于3D高斯泼溅的框架，用于在多样化光照条件下实现鲁棒的新视角合成，通过全局自适应亮度调整和局部像素级残差细化来解决多视角捕获中的光度不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现实环境中高质量图像采集面临复杂光照变化和相机成像管线限制的挑战，多视角捕获中光照、传感器响应和ISP配置的差异导致光度和色彩不一致，违反了现代3D新视角合成方法（如NeRF和3DGS）所依赖的光度一致性假设，导致重建和渲染质量下降。

Method: 基于3DGS框架，结合全局视图自适应亮度调整和局部像素级残差细化进行精确色彩校正，设计无监督目标函数，联合强制执行亮度校正以及多视角几何和光度一致性。

Result: 在低光照、过曝和复杂亮度色彩变化等挑战性场景中展示了最先进的性能，保持了3DGS的显式表示，提高了重建保真度同时保持实时渲染效率。

Conclusion: Luminance-GS++能够有效处理多视角捕获中的光照不一致问题，在保持3DGS实时渲染优势的同时，显著提升了在复杂光照条件下的新视角合成质量。

Abstract: High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.

</details>


### [37] [G-LoG Bi-filtration for Medical Image Classification](https://arxiv.org/abs/2602.18329)
*Qingsong Wang,Jiaxing He,Bingzhe Hou,Tieru Wu,Yang Cao,Cailing Yao*

Main category: cs.CV

TL;DR: 提出基于高斯-拉普拉斯算子的双滤过方法G-LoG，用于医学图像拓扑特征提取，在MedMNIST数据集上表现优于单参数滤过，且MLP模型在拓扑特征上能达到与复杂深度学习模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 在拓扑数据分析中，构建实用的滤过结构以检测拓扑和几何特征至关重要。本文旨在利用拉普拉斯高斯算子增强医学图像边界的能力，定义更适合多参数持久性模块的特征。

Method: 提出G-LoG（高斯-拉普拉斯高斯）双滤过方法：1）将体积图像建模为有界函数；2）利用拉普拉斯高斯算子增强图像边界；3）构建双滤过结构生成拓扑特征；4）证明持久性模块的插值距离相对于有界函数的最大范数是稳定的。

Result: 在MedMNIST数据集上的实验表明：1）G-LoG双滤过显著优于单参数滤过；2）基于拓扑特征的简单MLP模型性能与在原始数据集上训练的复杂深度学习模型（Google AutoML Vision、ResNet、AutoKeras、auto-sklearn）相当。

Conclusion: G-LoG双滤过方法为医学图像分析提供了有效的拓扑特征提取工具，证明了拓扑特征在医学图像分析中的潜力，能够以更简单的模型达到与复杂深度学习模型相当的性能。

Abstract: Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.

</details>


### [38] [Self-Aware Object Detection via Degradation Manifolds](https://arxiv.org/abs/2602.18394)
*Stefan Becker,Simon Weiss,Wolfgang Hübner,Michael Arens*

Main category: cs.CV

TL;DR: 提出基于退化流形的退化感知自感知框架，通过对比学习在特征空间中显式结构化图像退化信息，实现无需退化标签的检测器自感知能力


<details>
  <summary>Details</summary>
Motivation: 目标检测器在正常成像条件下表现良好，但在模糊、噪声、压缩、恶劣天气或分辨率变化等退化条件下可能无声失效。在安全关键应用中，仅产生预测而不评估输入是否处于检测器名义工作状态是不够的

Method: 基于退化流形的退化感知自感知框架：在标准检测骨干网络上添加轻量级嵌入头，通过多层对比学习训练，使具有相同退化组成的图像在特征空间中靠近，不同退化配置的图像分离，形成几何组织的表示；通过估计干净训练嵌入的原始原型来锚定学习到的几何结构

Result: 在合成损坏基准测试、跨数据集零样本迁移和自然天气引起的分布偏移实验中，表现出强大的原始-退化可分离性，在多种检测器架构中行为一致，在语义偏移下具有鲁棒泛化能力

Conclusion: 退化感知表示几何为检测器自感知提供了实用且与检测器无关的基础，通过几何偏离原始参考点实现独立于检测置信度的图像级退化信号

Abstract: Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector's nominal operating regime. We refer to this capability as self-aware object detection.
  We introduce a degradation-aware self-awareness framework based on degradation manifolds, which explicitly structure a detector's feature space according to image degradation rather than semantic content. Our method augments a standard detection backbone with a lightweight embedding head trained via multi-layer contrastive learning. Images sharing the same degradation composition are pulled together, while differing degradation configurations are pushed apart, yielding a geometrically organized representation that captures degradation type and severity without requiring degradation labels or explicit density modeling.
  To anchor the learned geometry, we estimate a pristine prototype from clean training embeddings, defining a nominal operating point in representation space. Self-awareness emerges as geometric deviation from this reference, providing an intrinsic, image-level signal of degradation-induced shift that is independent of detection confidence.
  Extensive experiments on synthetic corruption benchmarks, cross-dataset zero-shot transfer, and natural weather-induced distribution shifts demonstrate strong pristine-degraded separability, consistent behavior across multiple detector architectures, and robust generalization under semantic shift. These results suggest that degradation-aware representation geometry provides a practical and detector-agnostic foundation.

</details>


### [39] [Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges](https://arxiv.org/abs/2602.18406)
*Minh Dinh,Stéphane Deny*

Main category: cs.CV

TL;DR: 论文提出通过潜在空间学习等变算子的架构，用于处理训练中罕见的对称变换，在旋转和平移的MNIST数据集上实现分布外分类，克服传统网络和等变网络的局限。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在计算机视觉中取得成功，但在处理训练中罕见的对称变换（如不常见姿态、尺度、位置等）时仍存在困难。等变神经网络需要先验的变换知识，因此需要一种能从对称变换示例中学习等变算子的替代架构。

Method: 提出在潜在空间中从对称变换示例学习等变算子的架构，使用旋转和平移的噪声MNIST数据集进行验证，实现分布外分类。

Result: 该架构成功实现了分布外分类，克服了传统神经网络和等变神经网络的局限性，在旋转和平移的MNIST数据集上表现良好。

Conclusion: 虽然概念上很有吸引力，但将这种架构扩展到更复杂的数据集仍面临挑战，需要进一步研究解决可扩展性问题。

Abstract: Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.

</details>


### [40] [Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control](https://arxiv.org/abs/2602.18422)
*Linxi Xie,Lisong C. Sun,Ashley Neall,Tong Wu,Shengqu Cai,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 提出了一种基于头手姿态控制的人本视频世界模型，用于扩展现实中的交互式虚拟环境生成


<details>
  <summary>Details</summary>
Motivation: 当前视频世界模型仅接受文本或键盘等粗粒度控制信号，无法响应真实世界的人体运动跟踪，限制了在具身交互中的应用

Method: 1) 评估现有扩散变换器条件策略；2) 提出有效的3D头手姿态控制机制；3) 训练双向视频扩散模型教师；4) 蒸馏为因果交互系统生成第一人称虚拟环境

Result: 通过人类受试者评估显示，相比基线方法，该系统提高了任务性能，并显著提升了用户对执行动作的控制感知程度

Conclusion: 该人本视频世界模型能够响应精细的头手姿态控制，支持灵巧的手-物交互，为扩展现实中的具身交互提供了有效解决方案

Abstract: Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.

</details>


### [41] [CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation](https://arxiv.org/abs/2602.18424)
*Xia Su,Ruiqi Chen,Benlin Liu,Jingwei Ma,Zonglin Di,Ranjay Krishna,Jon Froehlich*

Main category: cs.CV

TL;DR: CapNav是一个评估视觉语言模型在考虑智能体物理能力约束下进行室内导航的新基准，包含5种代表性智能体、45个真实室内场景、473个导航任务，测试发现当前VLM在能力约束下导航性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现实世界导航受智能体移动能力约束（如扫地机器人不能爬楼梯，四足机器人可以），但现有视觉语言导航研究未充分考虑这些物理限制，需要评估VLM在能力约束下的导航表现。

Method: 定义5种代表性人类和机器人智能体，描述其物理尺寸、移动能力和环境交互能力；构建包含45个真实室内场景、473个导航任务和2365个问答对的基准；评估13个现代VLM模型。

Result: 当前VLM导航性能随移动约束收紧而急剧下降；即使是SOTA模型也难以处理需要空间维度推理的障碍类型；模型在能力感知导航方面存在显著挑战。

Conclusion: 需要开发能力感知导航系统，未来VLM应加强具身空间推理能力；CapNav基准为评估和改进VLM在现实约束下的导航能力提供了重要工具。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav

</details>


### [42] [SARAH: Spatially Aware Real-time Agentic Humans](https://arxiv.org/abs/2602.18432)
*Evonne Ng,Siwei Zhang,Zhang Chen,Michael Zollhoefer,Alexander Richard*

Main category: cs.CV

TL;DR: 首个实时、完全因果的空间感知对话动作生成方法，可在VR头显上部署，结合用户位置和音频生成全身动作，实现300+FPS的实时性能


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏空间感知能力，无法让虚拟代理根据用户位置做出自然转向、响应移动和保持自然注视等行为，限制了VR、远程呈现和数字人应用的发展

Method: 结合因果Transformer VAE与流匹配模型，使用交错潜在token实现流式推理，通过注视评分机制和分类器自由引导解耦学习与控制，支持用户调整注视强度

Result: 在Embody 3D数据集上达到SOTA运动质量，运行速度超过300FPS（比非因果基线快3倍），能捕捉自然对话的细微空间动态，并在实时VR系统中验证

Conclusion: 实现了首个可实时部署的空间感知对话代理系统，为VR、远程呈现和数字人应用提供了高质量、低延迟的交互体验

Abstract: As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.

</details>


### [43] [Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory](https://arxiv.org/abs/2602.18434)
*Vatsal Agarwal,Saksham Suri,Matthew Gwilliam,Pulkit Kumar,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: MemStream通过增加token预算、自适应选择策略和无需训练的检索混合专家系统，显著提升了流式视频问答的性能。


<details>
  <summary>Details</summary>
Motivation: 现有流式视频理解方法使用有限的每帧token数量，导致细粒度视觉细节丢失，且在处理密集视频流时存在查询-帧相似度随时间增加的问题，偏向检索后期帧。

Method: 1) 扩大token预算以实现更细粒度的时空理解；2) 引入自适应选择策略减少token冗余同时保留局部时空信息；3) 提出无需训练的检索混合专家系统，利用外部模型更好地识别相关帧。

Result: 在CG-Bench上提升8.0%，LVBench上提升8.5%，VideoMME (Long)上提升2.4%（相比ReKV with Qwen2.5-VL-7B）。

Conclusion: MemStream通过扩大token预算、自适应选择策略和检索混合专家系统，显著提升了流式视频问答的准确性和鲁棒性。

Abstract: Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.

</details>
